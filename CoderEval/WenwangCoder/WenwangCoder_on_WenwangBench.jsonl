{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "hydrate_time", "docstring": "Hydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time", "ques_id": "62e60f43d76274f8a4026e28", "code": ["def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds < 0:\n        raise ValueError('Nanos must be positive number >= 0')\n\n    if tz is None:\n        tz = timezone.utc\n\n    offset = timedelta(microseconds=nanoseconds // 1000)\n    return tz.localize(datetime.utcfromtimestamp(nanoseconds // 1000) + offset)\n", "def hydrate_time(nanoseconds, tz=None):\n    if not isinstance(nanoseconds, (int, long)):\n        raise ValueError('nanoseconds must be an integer')\n    if tz is None:\n        tz = pytz.utc\n    if isinstance(tz, tzinfo):\n        return tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds))\n    return datetime.datetime.utcfromtimestamp(nanoseconds)\n", "def hydrate_time(nanoseconds, tz=None):\n    if not isinstance(nanoseconds, int):\n        raise ValueError(\"nanoseconds must be integer.\")\n    if nanoseconds < 0:\n        raise ValueError(\"nanoseconds must be greater than zero.\")\n\n    if tz is None:\n        tz = get_current_timezone()\n    # Convert to UTC if necessary\n    if tz.zone == 'UTC':\n        nanos = int(nanoseconds * 1e9)\n        time = datetime.datetime.utcfromtimestamp(nanos)\n    else:\n        time = datetime.datetime.fromtimestamp(nanoseconds)\n\n    return time\n", "def hydrate_time(nanoseconds, tz=None):\n    if tz:\n        if isinstance(tz, str):\n            tz = pytz.timezone(tz)\n\n        return tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds))\n\n    return datetime.datetime.utcfromtimestamp(nanoseconds)\n", "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds < 0:\n        raise ValueError('Nanos cannot be negative.')\n\n    if tz is None:\n        tz = pytz.utc\n\n    if tz:\n        return tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds))\n    else:\n        return datetime.datetime.utcfromtimestamp(nanoseconds)\n", "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds > 0:\n        return Time(nanoseconds * 1e-9, tz=tz)\n    else:\n        return Time(0, tz=tz)\n", "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = get_localzone()\n    return tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds))\n", "def hydrate_time(nanoseconds, tz=None):\n    if tz is not None:\n        return Time(nanoseconds, tz=tz)\n    else:\n        return Time(nanoseconds)\n", "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds >= 0:\n        return Time(nanoseconds=nanoseconds, tz=tz)\n    else:\n        return Time(tz=tz)\n", "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds is None:\n        return None\n    if tz is None:\n        tz = pytz.utc\n    if isinstance(nanoseconds, int):\n        nanoseconds = timedelta(seconds=nanoseconds)\n    delta = nanoseconds - datetime.utcfromtimestamp(nanoseconds).timestamp()\n    return Time(\n        hour=delta // 3600,\n        minute=delta % 3600 // 60,\n        second=delta % 60,\n        tzinfo=tz\n    )\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds < 0:\n        raise ValueError('Nanos must be positive number >= 0')\n\n    if tz is None:\n        tz = timezone.utc\n\n    offset = timedelta(microseconds=nanoseconds // 1000)\n    return tz.localize(datetime.utcfromtimestamp(nanoseconds // 1000) + offset)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if not isinstance(nanoseconds, (int, long)):\n        raise ValueError('nanoseconds must be an integer')\n    if tz is None:\n        tz = pytz.utc\n    if isinstance(tz, tzinfo):\n        return tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds))\n    return datetime.datetime.utcfromtimestamp(nanoseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if not isinstance(nanoseconds, int):\n        raise ValueError(\"nanoseconds must be integer.\")\n    if nanoseconds < 0:\n        raise ValueError(\"nanoseconds must be greater than zero.\")\n\n    if tz is None:\n        tz = get_current_timezone()\n    # Convert to UTC if necessary\n    if tz.zone == 'UTC':\n        nanos = int(nanoseconds * 1e9)\n        time = datetime.datetime.utcfromtimestamp(nanos)\n    else:\n        time = datetime.datetime.fromtimestamp(nanoseconds)\n\n    return time\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if tz:\n        if isinstance(tz, str):\n            tz = pytz.timezone(tz)\n\n        return tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds))\n\n    return datetime.datetime.utcfromtimestamp(nanoseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds < 0:\n        raise ValueError('Nanos cannot be negative.')\n\n    if tz is None:\n        tz = pytz.utc\n\n    if tz:\n        return tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds))\n    else:\n        return datetime.datetime.utcfromtimestamp(nanoseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds > 0:\n        return Time(nanoseconds * 1e-9, tz=tz)\n    else:\n        return Time(0, tz=tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = get_localzone()\n    return tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if tz is not None:\n        return Time(nanoseconds, tz=tz)\n    else:\n        return Time(nanoseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds >= 0:\n        return Time(nanoseconds=nanoseconds, tz=tz)\n    else:\n        return Time(tz=tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds is None:\n        return None\n    if tz is None:\n        tz = pytz.utc\n    if isinstance(nanoseconds, int):\n        nanoseconds = timedelta(seconds=nanoseconds)\n    delta = nanoseconds - datetime.utcfromtimestamp(nanoseconds).timestamp()\n    return Time(\n        hour=delta // 3600,\n        minute=delta % 3600 // 60,\n        second=delta % 60,\n        tzinfo=tz\n    )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_timedelta", "docstring": "Dehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:", "ques_id": "62e60f3bd76274f8a4026e10", "code": ["def dehydrate_timedelta(value):\n    return datetime.timedelta(seconds=value.total_seconds())\n", "def dehydrate_timedelta(value):\n    if value.days < 0:\n        raise ValueError(\"Can't represent negative timedeltas.\")\n\n    return timedelta(seconds=value.seconds, microseconds=value.microseconds)\n", "def dehydrate_timedelta(value):\n    if value.days < 0:\n        raise ValueError(\"Can't be negative timedelta: %s\" % value)\n\n    return timedelta(seconds=value.seconds + value.days * 24 * 3600)\n", "def dehydrate_timedelta(value):\n    if not isinstance(value, timedelta):\n        raise BadDehydratedValueError('Can only dehydrate timedelta values')\n\n    return value.total_seconds() // 60  # convert to hours and divide by 60\n", "def dehydrate_timedelta(value):\n    if value.days < 0:\n        raise ValueError(\"negative delta timedeltas are not supported\")\n\n    return datetime.timedelta(days=value.days)\n", "def dehydrate_timedelta(value):\n    return datetime.timedelta(seconds=value.days * 86400 + value.seconds)\n", "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n    return datetime.timedelta(seconds=value.seconds + value.days * 24 * 3600)\n", "def dehydrate_timedelta(value):\n    return datetime.timedelta(\n        seconds=value.total_seconds(),\n    )\n", "def dehydrate_timedelta(value):\n    if isinstance(value, datetime.timedelta):\n        return value.days * 86400 + value.seconds + value.microseconds // 1000\n", "def dehydrate_timedelta(value):\n    if value is None:\n        return None\n\n    return timedelta(days=value.days, seconds=value.seconds, microseconds=value.microseconds)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_timedelta(value):\n    return datetime.timedelta(seconds=value.total_seconds())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days < 0:\n        raise ValueError(\"Can't represent negative timedeltas.\")\n\n    return timedelta(seconds=value.seconds, microseconds=value.microseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days < 0:\n        raise ValueError(\"Can't be negative timedelta: %s\" % value)\n\n    return timedelta(seconds=value.seconds + value.days * 24 * 3600)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if not isinstance(value, timedelta):\n        raise BadDehydratedValueError('Can only dehydrate timedelta values')\n\n    return value.total_seconds() // 60  # convert to hours and divide by 60\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days < 0:\n        raise ValueError(\"negative delta timedeltas are not supported\")\n\n    return datetime.timedelta(days=value.days)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    return datetime.timedelta(seconds=value.days * 86400 + value.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n    return datetime.timedelta(seconds=value.seconds + value.days * 24 * 3600)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    return datetime.timedelta(\n        seconds=value.total_seconds(),\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if isinstance(value, datetime.timedelta):\n        return value.days * 86400 + value.seconds + value.microseconds // 1000\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value is None:\n        return None\n\n    return timedelta(days=value.days, seconds=value.seconds, microseconds=value.microseconds)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_time", "docstring": "Dehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:", "ques_id": "62e60f37d76274f8a4026dfd", "code": ["def dehydrate_time(value):\n    return value.date()\n", "def dehydrate_time(value):\n    return value.datetime\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "def dehydrate_time(value):\n    if value is None:\n        return None\n\n    return datetime.fromtimestamp(value.timestamp())\n", "def dehydrate_time(value):\n    if not isinstance(value, Time):\n        raise BadDehydrateValue(\"Must be a Time instance\")\n\n    return value.strftime(\"%H:%M\")\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "def dehydrate_time(value):\n    return value.date()\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_time(value):\n    return value.date()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.datetime\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    if value is None:\n        return None\n\n    return datetime.fromtimestamp(value.timestamp())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    if not isinstance(value, Time):\n        raise BadDehydrateValue(\"Must be a Time instance\")\n\n    return value.strftime(\"%H:%M\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.date()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/spatial.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_point", "docstring": "Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:", "ques_id": "62e60f33d76274f8a4026de9", "code": ["def dehydrate_point(value):\n    return Point(value.x, value.y)\n", "def dehydrate_point(value):\n    return {\n        'latitude': value.latitude,\n        'longitude': value.longitude\n    }\n", "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadPointData(\"Not a valid point: %r\" % value)\n    return value\n", "def dehydrate_point(value):\n    return {\n        'lat': value.latitude,\n        'lon': value.longitude\n    }\n", "def dehydrate_point(value):\n    return {\n        'latitude': value.latitude,\n        'longitude': value.longitude\n    }\n", "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadDehydrate(\"Can only dehydrate Point data.\")\n\n    return value.coords\n", "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadPointData(\"Wrong point data type for %s\" % value.__class__.__name__)\n    return value\n", "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadData(\"Not a valid point: %r\" % value)\n    return value\n", "def dehydrate_point(value):\n    return Point(value.latitude, value.longitude)\n", "def dehydrate_point(value):\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_point(value):\n    return Point(value.x, value.y)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'latitude': value.latitude,\n        'longitude': value.longitude\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadPointData(\"Not a valid point: %r\" % value)\n    return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'lat': value.latitude,\n        'lon': value.longitude\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'latitude': value.latitude,\n        'longitude': value.longitude\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadDehydrate(\"Can only dehydrate Point data.\")\n\n    return value.coords\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadPointData(\"Wrong point data type for %s\" % value.__class__.__name__)\n    return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadData(\"Not a valid point: %r\" % value)\n    return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return Point(value.latitude, value.longitude)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "keys", "docstring": "Return the keys of the record.\n\n:return: list of key names", "ques_id": "62e60ed4d76274f8a4026da0", "code": ["def keys(self):\n    return self.__keys\n", "def keys(self):\n    return self.__keys\n", "def keys(self):\n    return self.__keys\n", "def keys(self):\n    return self.__keys\n", "def keys(self):\n    return self.__keys\n", "def keys(self):\n    return self.__keys\n", "def keys(self):\n    return self.__keys\n", "def keys(self):\n    return self.__keys\n", "def keys(self):\n    return self.__keys\n", "def keys(self):\n    return self.__keys\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self.__keys\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_sync/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "ques_id": "62e60ecfd76274f8a4026d6a", "code": ["def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        protocol_version = neo4j.PROTOCOL_VERSION\n    return {\n        'neo4j': {\n           'version': protocol_version,\n            'handlers': {\n                'neo4j': {\n                    'cls': Neo4jHandler,\n                   'version': neo4j.__version__\n                }\n            }\n        }\n    }\n", "def protocol_handlers(cls, protocol_version=None):\n    return cls.PROTOCOL_HANDLERS_CLS.get(protocol_version, [])\n", "def protocol_handlers(cls, protocol_version=None):\n    version_list = cls.version_list(protocol_version)\n    return {\n        version: HANDLERS.get(version, handler)\n        for version, handler in version_list.items()\n    }\n", "def protocol_handlers(cls, protocol_version=None):\n    cls.PROTOCOL_VERSION = protocol_version or cls.PROTOCOL_VERSION\n    return {\n        v: k for k, v in cls.HANDLER_CLS.items()\n        if cls.PROTOCOL_VERSION >= v >= cls.DER_ENC_HANDLER\n    }\n", "def protocol_handlers(cls, protocol_version=None):\n    versions = cls.version_list(cls.PROTOCOL_VERSION)\n    if protocol_version is None:\n        return [cls(v) for v in versions]\n    else:\n        return [cls(v, protocol_version) for v in versions]\n", "def protocol_handlers(cls, protocol_version=None):\n    return cls.HANDLER_CLS.protocol_handlers(\n        cls, protocol_version=protocol_version)\n", "def protocol_handlers(cls, protocol_version=None):\n    version = cls.PROTOCOL_VERSION if protocol_version is None else protocol_version\n    return {\n        'neo4j': {\n           'version': version,\n            'unique_key': 'unique_key',\n            'neo4j_uri': 'neo4j://{}/{}'.format(\n                cls.HOST, cls.NEO4J_PORT),\n            'neo4j_user': cls.USER_AGENT,\n            'neo4j_password': cls.PASSWORD\n        }\n    }\n", "def protocol_handlers(cls, protocol_version=None):\n    return cls.HANDLER_CLS.protocol_handlers(\n        cls, protocol_version=protocol_version)\n", "def protocol_handlers(cls, protocol_version=None):\n    \"protocol_handlers(cls, protocol_version=None)\"\n    version = cls.PROTOCOL_VERSION if protocol_version is None else protocol_version\n    if version == 1:\n        return cls.HANDLER_CLS\n    elif version == 2:\n        return cls.HANDLER_STATIC\n    else:\n        raise NotImplementedError(\"Unknown protocol version: %d\" % version)\n", "def protocol_handlers(cls, protocol_version=None):\n    \"protocol_handlers(self, protocol_version=None)\"\n    result = {}\n    for p in cls.protocol_versions:\n        result[p] = getattr(self, '_protocol_handler_' + p)\n    return result\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        protocol_version = neo4j.PROTOCOL_VERSION\n    return {\n        'neo4j': {\n           'version': protocol_version,\n            'handlers': {\n                'neo4j': {\n                    'cls': Neo4jHandler,\n                   'version': neo4j.__version__\n                }\n            }\n        }\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    return cls.PROTOCOL_HANDLERS_CLS.get(protocol_version, [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    version_list = cls.version_list(protocol_version)\n    return {\n        version: HANDLERS.get(version, handler)\n        for version, handler in version_list.items()\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    cls.PROTOCOL_VERSION = protocol_version or cls.PROTOCOL_VERSION\n    return {\n        v: k for k, v in cls.HANDLER_CLS.items()\n        if cls.PROTOCOL_VERSION >= v >= cls.DER_ENC_HANDLER\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    versions = cls.version_list(cls.PROTOCOL_VERSION)\n    if protocol_version is None:\n        return [cls(v) for v in versions]\n    else:\n        return [cls(v, protocol_version) for v in versions]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    return cls.HANDLER_CLS.protocol_handlers(\n        cls, protocol_version=protocol_version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    version = cls.PROTOCOL_VERSION if protocol_version is None else protocol_version\n    return {\n        'neo4j': {\n           'version': version,\n            'unique_key': 'unique_key',\n            'neo4j_uri': 'neo4j://{}/{}'.format(\n                cls.HOST, cls.NEO4J_PORT),\n            'neo4j_user': cls.USER_AGENT,\n            'neo4j_password': cls.PASSWORD\n        }\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    return cls.HANDLER_CLS.protocol_handlers(\n        cls, protocol_version=protocol_version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    \"protocol_handlers(cls, protocol_version=None)\"\n    version = cls.PROTOCOL_VERSION if protocol_version is None else protocol_version\n    if version == 1:\n        return cls.HANDLER_CLS\n    elif version == 2:\n        return cls.HANDLER_STATIC\n    else:\n        raise NotImplementedError(\"Unknown protocol version: %d\" % version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    \"protocol_handlers(self, protocol_version=None)\"\n    result = {}\n    for p in cls.protocol_versions:\n        result[p] = getattr(self, '_protocol_handler_' + p)\n    return result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/work/query.py", "project": "neo4j/neo4j-python-driver", "name": "unit_of_work", "docstring": "This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`", "ques_id": "62e60e49d76274f8a4026d25", "code": ["def unit_of_work(metadata=None, timeout=None):\n    def _wrapper(f):\n        def _wrapped(*args, **kwargs):\n            return f(*args, **kwargs)\n        _wrapped.timeout = timeout\n        _wrapped.metadata = metadata\n        return _wrapped\n    return _wrapper\n", "def unit_of_work(metadata=None, timeout=None):\n\n    def dec(f):\n        f.__unit_of_work__ = metadata or {}\n        f.__unit_of_work_timeout__ = timeout or 0\n\n        @wraps(f)\n        def wrapper(*args, **kw):\n            return f(*args, **kw)\n\n        return wrapper\n\n    return dec\n", "def unit_of_work(metadata=None, timeout=None):\n    def decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            tx = dbms.Transaction(timeout=timeout)\n            result = tx.run(f, *args, **kwargs)\n            tx.close()\n            return result\n        return wrapper\n    if metadata is not None:\n        metadata.update({'timeout': timeout})\n        metadata.update({'query_type': 'unit_of_work'})\n        setattr(f, '__dbms_meta__', metadata)\n        return f\n    return decorator\n", "def unit_of_work(metadata=None, timeout=None):\n    def decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            with Transaction() as tx:\n                return f(tx, metadata, timeout, *args, **kwargs)\n        return wrapper\n    return decorator\n", "def unit_of_work(metadata=None, timeout=None):\n    def inner_decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with _TransactionContextManager(func, metadata, timeout):\n                return func(*args, **kwargs)\n        return wrapper\n    return inner_decorator\n", "def unit_of_work(metadata=None, timeout=None):\n    def _wrapper(fn):\n        fn.__unit_of_work__ = True\n        fn.__metadata__ = metadata or {}\n        fn.__timeout__ = timeout or None\n        return fn\n    return _wrapper\n", "def unit_of_work(metadata=None, timeout=None):\n    def wrapper(func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            result = func(*args, **kwargs)\n            return result\n\n        wrapped.timeout = timeout\n        wrapped.metadata = metadata\n\n        return wrapped\n\n    return wrapper\n", "def unit_of_work(metadata=None, timeout=None):\n    def _wrapper(f):\n        @wraps(f)\n        def wrapper(*args, **kw):\n            metadata = metadata or {}\n            timeout = timeout or metadata.get('timeout', None)\n\n            if timeout is not None:\n                timer = dbms.getTimer('query')\n                start = timer.start(timeout)\n                result = f(*args, **kw)\n                end = timer.stop()\n                dbms.setTXMetaData(timer, 'timestamp', str(end - start))\n            else:\n                result = f(*args, **kw)\n\n            return result\n\n        return wrapper\n\n    return _wrapper\n", "def unit_of_work(metadata=None, timeout=None):\n    def _decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with Transaction(func.__name__) as tx:\n                return func(*args, **kwargs)\n        wrapper.__dbclass__ = Transaction\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return _decorator\n", "def unit_of_work(metadata=None, timeout=None):\n    def dec(f):\n        f.__unit_of_work__ = metadata or {}\n        f.__unit_of_work__timeout__ = timeout or None\n\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            return f(*args, **kwargs)\n\n        return wrapper\n\n    return dec\n"], "level": "self_contained", "generate_results": [{"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def _wrapper(f):\n        def _wrapped(*args, **kwargs):\n            return f(*args, **kwargs)\n        _wrapped.timeout = timeout\n        _wrapped.metadata = metadata\n        return _wrapped\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\n    def dec(f):\n        f.__unit_of_work__ = metadata or {}\n        f.__unit_of_work_timeout__ = timeout or 0\n\n        @wraps(f)\n        def wrapper(*args, **kw):\n            return f(*args, **kw)\n\n        return wrapper\n\n    return dec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            tx = dbms.Transaction(timeout=timeout)\n            result = tx.run(f, *args, **kwargs)\n            tx.close()\n            return result\n        return wrapper\n    if metadata is not None:\n        metadata.update({'timeout': timeout})\n        metadata.update({'query_type': 'unit_of_work'})\n        setattr(f, '__dbms_meta__', metadata)\n        return f\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            with Transaction() as tx:\n                return f(tx, metadata, timeout, *args, **kwargs)\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def inner_decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with _TransactionContextManager(func, metadata, timeout):\n                return func(*args, **kwargs)\n        return wrapper\n    return inner_decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def _wrapper(fn):\n        fn.__unit_of_work__ = True\n        fn.__metadata__ = metadata or {}\n        fn.__timeout__ = timeout or None\n        return fn\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def wrapper(func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            result = func(*args, **kwargs)\n            return result\n\n        wrapped.timeout = timeout\n        wrapped.metadata = metadata\n\n        return wrapped\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def _wrapper(f):\n        @wraps(f)\n        def wrapper(*args, **kw):\n            metadata = metadata or {}\n            timeout = timeout or metadata.get('timeout', None)\n\n            if timeout is not None:\n                timer = dbms.getTimer('query')\n                start = timer.start(timeout)\n                result = f(*args, **kw)\n                end = timer.stop()\n                dbms.setTXMetaData(timer, 'timestamp', str(end - start))\n            else:\n                result = f(*args, **kw)\n\n            return result\n\n        return wrapper\n\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def _decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with Transaction(func.__name__) as tx:\n                return func(*args, **kwargs)\n        wrapper.__dbclass__ = Transaction\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return _decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def dec(f):\n        f.__unit_of_work__ = metadata or {}\n        f.__unit_of_work__timeout__ = timeout or None\n\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            return f(*args, **kwargs)\n\n        return wrapper\n\n    return dec\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "index", "docstring": "Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int", "ques_id": "62e60e05d76274f8a4026cfd", "code": ["def index(self, key):\n    try:\n        return self.__keys.index(key)\n    except ValueError:\n        raise KeyError(key)\n", "def index(self, key):\n    return self.__keys.index(key)\n", "def index(self, key):\n    try:\n        return self.__keys.index(key)\n    except ValueError:\n        raise KeyError(key) from None\n", "def index(self, key):\n    return self.__keys.index(key)\n", "def index(self, key):\n    try:\n        return self.__keys.index(key)\n    except ValueError:\n        raise KeyError(key)\n", "def index(self, key):\n    return self.__keys.index(key)\n", "def index(self, key):\n    try:\n        return self.__keys.index(key)\n    except ValueError:\n        raise KeyError(key)\n", "def index(self, key):\n    return self.__keys.index(key)\n", "def index(self, key):\n    return self.__keys.index(key)\n", "def index(self, key):\n    return self.__keys.index(key)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def index(self, key):\n    try:\n        return self.__keys.index(key)\n    except ValueError:\n        raise KeyError(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self.__keys.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    try:\n        return self.__keys.index(key)\n    except ValueError:\n        raise KeyError(key) from None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self.__keys.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    try:\n        return self.__keys.index(key)\n    except ValueError:\n        raise KeyError(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self.__keys.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    try:\n        return self.__keys.index(key)\n    except ValueError:\n        raise KeyError(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self.__keys.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self.__keys.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self.__keys.index(key)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "values", "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list", "ques_id": "62e60da4d76274f8a4026cf1", "code": ["def values(self, *keys):\n    if not keys:\n        return [item.value for item in self.__iter__()]\n    return [item.value for item in self.__iter__(*keys) if item.value]\n", "def values(self, *keys):\n    if not keys:\n        return list(self.data().values())\n    return [self.data()[k] for k in keys]\n", "def values(self, *keys):\n    return [item.value for item in self.__getitem__(keys)] if keys else self.__getitem__(keys)\n", "def values(self, *keys):\n    if not keys:\n        return [rec.value for rec in self]\n    return [rec.value for rec in self.__iter__(*keys)]\n", "def values(self, *keys):\n\n    return [record.value(key, default=None) for key, record in self.items(*keys)]\n", "def values(self, *keys):\n    if not keys:\n        keys = self.__keys\n\n    return [self.__getitem__(k) for k in keys]\n", "def values(self, *keys):\n    if keys:\n        return [item[keys[0]] for item in self.items]\n    return [item[self.index] for item in self.items]\n", "def values(self, *keys):\n    return [self.__getitem__(key) for key in keys] if keys else self.__values\n", "def values(self, *keys):\n    return [self.__getitem__(key) for key in keys] if keys else self.__values\n", "def values(self, *keys):\n    if not keys:\n        keys = self.__keys\n    return [self.__getitem__(key) for key in keys]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def values(self, *keys):\n    if not keys:\n        return [item.value for item in self.__iter__()]\n    return [item.value for item in self.__iter__(*keys) if item.value]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n    if not keys:\n        return list(self.data().values())\n    return [self.data()[k] for k in keys]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n    return [item.value for item in self.__getitem__(keys)] if keys else self.__getitem__(keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n    if not keys:\n        return [rec.value for rec in self]\n    return [rec.value for rec in self.__iter__(*keys)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n\n    return [record.value(key, default=None) for key, record in self.items(*keys)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n    if not keys:\n        keys = self.__keys\n\n    return [self.__getitem__(k) for k in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n    if keys:\n        return [item[keys[0]] for item in self.items]\n    return [item[self.index] for item in self.items]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n    return [self.__getitem__(key) for key in keys] if keys else self.__values\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n    return [self.__getitem__(key) for key in keys] if keys else self.__values\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n    if not keys:\n        keys = self.__keys\n    return [self.__getitem__(key) for key in keys]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "data", "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified", "ques_id": "62e60b10d76274f8a4026ccd", "code": ["def data(self, *keys):\n    keys = _to_keys(keys)\n    for key in keys:\n        if key not in self.__keys:\n            raise IndexError(key)\n    return {k: self.__getitem__(k) for k in keys}\n", "def data(self, *keys):\n    if len(keys) == 0:\n        keys = self.__keys\n\n    if len(keys) == 1:\n        key, = keys\n        if key in self.__keys:\n            return self[key]\n        else:\n            raise IndexError\n    else:\n        return dict([(key, self[key]) for key in keys])\n", "def data(self, *keys):\n    return dict(zip(self.__keys, self._broken_record_error(*keys)))\n", "def data(self, *keys):\n    return dict(zip(self.__keys, self.__super_getitem_single(keys)))\n", "def data(self, *keys):\n    if keys:\n        keys = _tuple(keys)\n        if len(keys) > self.__len__():\n            raise IndexError('Too many keys!')\n        for key in keys:\n            self.__getitem__(key)\n    return dict(zip(self.__keys, self._super_getitem_single(keys)))\n", "def data(self, *keys):\n    d = {}\n    for key in keys:\n        if key not in self.__keys:\n            raise IndexError('Record key \"{}\" is out of bounds'.format(key))\n        d[key] = getattr(self, key, None)\n    return d\n", "def data(self, *keys):\n    _, values = self._broken_record_error(*keys)\n    return dict(zip(self.__keys, values))\n", "def data(self, *keys):\n    if len(keys) == 0:\n        keys = self.__keys\n    else:\n        keys = tuple(keys)\n        if len(keys) < len(self.__keys):\n            for k in self.__keys[len(keys):]:\n                if k not in keys:\n                    raise IndexError('Record does not contain key: {}'.format(k))\n\n    if len(keys) == 1:\n        return self.__broken_record_error(keys[0])\n\n    result = {}\n    for k in keys:\n        if k in self:\n            result[k] = self[k]\n    return result\n", "def data(self, *keys):\n    if not keys:\n        items = self.items()\n    else:\n        items = dict.__getitem__(self, keys[0])\n        for key in keys[1:]:\n            items = items[key]\n    return items\n", "def data(self, *keys):\n    return {k: self.__getitem__(k) for k in keys if k in self.__keys}\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def data(self, *keys):\n    keys = _to_keys(keys)\n    for key in keys:\n        if key not in self.__keys:\n            raise IndexError(key)\n    return {k: self.__getitem__(k) for k in keys}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    if len(keys) == 0:\n        keys = self.__keys\n\n    if len(keys) == 1:\n        key, = keys\n        if key in self.__keys:\n            return self[key]\n        else:\n            raise IndexError\n    else:\n        return dict([(key, self[key]) for key in keys])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    return dict(zip(self.__keys, self._broken_record_error(*keys)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    return dict(zip(self.__keys, self.__super_getitem_single(keys)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    if keys:\n        keys = _tuple(keys)\n        if len(keys) > self.__len__():\n            raise IndexError('Too many keys!')\n        for key in keys:\n            self.__getitem__(key)\n    return dict(zip(self.__keys, self._super_getitem_single(keys)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    d = {}\n    for key in keys:\n        if key not in self.__keys:\n            raise IndexError('Record key \"{}\" is out of bounds'.format(key))\n        d[key] = getattr(self, key, None)\n    return d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    _, values = self._broken_record_error(*keys)\n    return dict(zip(self.__keys, values))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    if len(keys) == 0:\n        keys = self.__keys\n    else:\n        keys = tuple(keys)\n        if len(keys) < len(self.__keys):\n            for k in self.__keys[len(keys):]:\n                if k not in keys:\n                    raise IndexError('Record does not contain key: {}'.format(k))\n\n    if len(keys) == 1:\n        return self.__broken_record_error(keys[0])\n\n    result = {}\n    for k in keys:\n        if k in self:\n            result[k] = self[k]\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    if not keys:\n        items = self.items()\n    else:\n        items = dict.__getitem__(self, keys[0])\n        for key in keys[1:]:\n            items = items[key]\n    return items\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    return {k: self.__getitem__(k) for k in keys if k in self.__keys}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/packstream/v1/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "pop_u16", "docstring": "Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.", "ques_id": "62e6087bd76274f8a4026bfa", "code": ["def pop_u16(self):\n    return struct.unpack('>H', self.data[-2:])[0]\n", "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "def pop_u16(self):\n    return struct.unpack('>H', self.data[-2:])[0]\n", "def pop_u16(self):\n    return self.data.pop() << 8 | self.data.popleft()\n", "def pop_u16(self):\n    return self.data.pop() << 8 | self.data.pop()\n", "def pop_u16(self):\n    return struct.unpack('>H', self.data.pop())[0]\n", "def pop_u16(self):\n    return struct.unpack('>H', self.data[-2:])[0]\n", "def pop_u16(self):\n    return struct.unpack('>H', self.data.pop(0))[0]\n", "def pop_u16(self):\n    return self.data.pop() << 8 | self.data.pop()\n", "def pop_u16(self):\n    return struct.unpack('>H', self.data.pop())[0]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop_u16(self):\n    return struct.unpack('>H', self.data[-2:])[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return struct.unpack('>H', self.data[-2:])[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.data.pop() << 8 | self.data.popleft()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.data.pop() << 8 | self.data.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return struct.unpack('>H', self.data.pop())[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return struct.unpack('>H', self.data[-2:])[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return struct.unpack('>H', self.data.pop(0))[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.data.pop() << 8 | self.data.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return struct.unpack('>H', self.data.pop())[0]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "discard", "docstring": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object", "ques_id": "62e6087ad76274f8a4026bf2", "code": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    if dehydration_hooks is None:\n        dehydration_hooks = {}\n    if hydration_hooks is None:\n        hydration_hooks = {}\n    return self.run('DISCARD', n, qid, dehydration_hooks, **handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append('DISCARD', n, qid, dehydration_hooks, hydration_hooks,\n                        **handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    self.run('DISCARD', n=n, qid=qid,\n             dehydration_hooks=dehydration_hooks,\n             hydration_hooks=hydration_hooks, **handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append(Disconnect(n, qid, dehydration_hooks, hydration_hooks),\n                        handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append(message.DISCARD, n, qid,\n                        dehydration_hooks=dehydration_hooks,\n                        hydration_hooks=hydration_hooks, **handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append(message.DISCARD, n, qid, dehydration_hooks,\n                                                                                                                                                                                                                                                                                                                                                                                                                                                  ", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append('DISCARD', n=n, qid=qid,\n                         dehydration_hooks=dehydration_hooks,\n                         hydration_hooks=hydration_hooks, **handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self.run('DISCARD', n, qid, dehydration_hooks,\n                                                                                                                                                                                                                                                                                                                                                                                                                                                      ", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append(message.DISCARD, n, qid, dehydration_hooks,\n                                                                                                                                                                                                                                                                                                                                                                                                                                                  ", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self.run(\n        'DISCARD',\n        n=n,\n        qid=qid,\n        dehydration_hooks=dehydration_hooks,\n        hydration_hooks=hydration_hooks,\n        **handlers\n    )\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    if dehydration_hooks is None:\n        dehydration_hooks = {}\n    if hydration_hooks is None:\n        hydration_hooks = {}\n    return self.run('DISCARD', n, qid, dehydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append('DISCARD', n, qid, dehydration_hooks, hydration_hooks,\n                        **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    self.run('DISCARD', n=n, qid=qid,\n             dehydration_hooks=dehydration_hooks,\n             hydration_hooks=hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append(Disconnect(n, qid, dehydration_hooks, hydration_hooks),\n                        handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append(message.DISCARD, n, qid,\n                        dehydration_hooks=dehydration_hooks,\n                        hydration_hooks=hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append(message.DISCARD, n, qid, dehydration_hooks,\n                                                                                                                                                                                                                                                                                                                                                                                                                                                  ", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append('DISCARD', n=n, qid=qid,\n                         dehydration_hooks=dehydration_hooks,\n                         hydration_hooks=hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self.run('DISCARD', n, qid, dehydration_hooks,\n                                                                                                                                                                                                                                                                                                                                                                                                                                                      ", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self._append(message.DISCARD, n, qid, dehydration_hooks,\n                                                                                                                                                                                                                                                                                                                                                                                                                                                  ", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to \"\"\"\n    return self.run(\n        'DISCARD',\n        n=n,\n        qid=qid,\n        dehydration_hooks=dehydration_hooks,\n        hydration_hooks=hydration_hooks,\n        **handlers\n    )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "begin", "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object", "ques_id": "62e60879d76274f8a4026bec", "code": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    \"\"\"\n    if mode is not None:\n        self.mode = mode\n    if bookmarks is not None:\n        self.bookmarks = bookmarks\n    if metadata is not None:\n        self.metadata = metadata\n    if timeout is not None:\n        self.timeout = timeout\n    if db is not None:\n        self.db = db\n    if imp_user is not None:\n        self.imp_user = imp_user\n  ", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    :param mode: http or https\n    :param bookmarks: list of reference to sub-reference\n    :param metadata: dictionary of metadata\n    :param timeout: timeout in seconds\n    :param db: database to connect to\n    :param imp_user: imp user to connect to\n    :param dehydration_hooks: dehydration hooks\n    :param hydration_hooks: hyper hyper hyper hyper\n    :return:\n    \"\"\"\n    if mode is not None:\n        self._append(mode, imp_user, dehydration_hooks, hydration_hooks)\n\n    self._on_server_state_change(None, None)\n    self._on_server_state_change('DEFAULT', None)\n    self._on_server_state_change('RECONNECTING', None)\n    self._on_server_state_change('CONNECTING', None)\n    self._on_server_state_change('DISCONNECTING', None)\n\n    return self.run(query=None, parameters=None, mode=mode, bookmarks=bookmarks,\n                    metadata=metadata, timeout=timeout, **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Begin the connection to the database.\n    \"\"\"\n    if self.pool is not None:\n        self.pool.put(self, timeout=timeout)\n    self.connect()\n\n    if mode is not None:\n        self.mode = mode\n    if bookmarks is not None:\n        self.bookmarks = bookmarks\n    if metadata is not None:\n        self.metadata = metadata\n    if db is not None:\n        self.db = db\n    if imp_user is not None:\n        self.imp_user = imp_user\n    if dehydration_hooks is not None:\n        self.dehydration_hooks = dehydration_hooks\n    if hydration_hooks is not None:\n        self.hydration_hooks = hydration_hooks\n\n    self._on_server_state_change('BEGIN', 'STARTED', timeout=timeout)\n    return self\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    :param mode: http:// or https://\n    :param bookmarks: {string: string}\n    :param metadata: {string: string}\n    :param timeout: {int: int}\n    :param db: {string: object}\n    :param imp_user: {string: string}\n    :param dehydration_hooks: {string: string}\n    :param hydration_hooks: {string: string}\n    \"\"\"\n    # Handle timeout and db (if passed)\n    timeout = timeout or self.timeout\n    db = db or self.db\n    if timeout:\n        timeout = int(timeout)\n        self.socket.settimeout(timeout)\n\n    # Handle imp_user\n    if imp_user:\n        self.user_agent = imp_user\n\n    # Handle dehydration hooks\n    if dehydration_hooks:\n        self.dehydration_hooks = dehydration_hooks\n\n    # Handle metadata\n    if metadata:\n        self.metadata = metadata\n\n    # Handle timeout\n    if timeout:\n        self.timeout = time.time() +", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    :param mode: mode of operation (e.g., ``'auth'``, ``'prod'``, ``'rollback'``)\n    :param bookmarks: list of bookmarks for operation\n    :param metadata: dictionary of metadata for operation\n    :param timeout: timeout for operation\n    :param db: database for operation\n    :param imp_user: imp user to use for authentication\n    :param dehydration_hooks: dehydration hooks to use for authentication\n    :param hydration_hooks: hydration hooks to use for authentication\n    \"\"\"\n    self._append = self._append.encode()\n    self._discard = self._discard.encode()\n    self._query = self._query.encode()\n    self._parameters = self._parameters.encode()\n    self._mode = mode\n    self._bookmarks = bookmarks\n    self._metadata = metadata\n    self._timeout = timeout\n    self._db = db\n    self._imp_user = imp_user\n    self._dehydration_hooks = dehydration_hooks\n    self._hydration_hooks = hydration_hooks\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Begin a connection to the database.\n    \"\"\"\n    if self.pool:\n        self.socket.close()\n    self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n    self.socket.settimeout(timeout)\n    self.socket.connect(self.socket_path)\n    self.pool = PooledConnection(self.socket, self.pool_size, mode=mode,\n                               bookmarks=bookmarks, metadata=metadata,\n                               imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n                               hydration_hooks=hydration_hooks)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Begin routing.\n    \"\"\"\n    return self.run(\n        mode=mode,\n        bookmarks=bookmarks,\n        metadata=metadata,\n        timeout=timeout,\n        db=db,\n        imp_user=imp_user,\n        dehydration_hooks=dehydration_hooks,\n        hydration_hooks=hydration_hooks,\n        **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    \"\"\"\n    self._on_server_state_change(None, None)\n\n    super(RoutingEngine, self).begin(mode=mode, bookmarks=bookmarks,\n                                        metadata=metadata,\n                                        timeout=timeout,\n                                        db=db,\n                                        imp_user=imp_user,\n                                        dehydration_hooks=dehydration_hooks,\n    ", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    :returns: a new :class:`Connection` instance\n    \"\"\"\n    if mode is not None:\n        self._append(mode)\n    if bookmarks is not None:\n        self._append(bookmarks)\n    if metadata is not None:\n        self._append(metadata)\n    if timeout is not None:\n        self._append(timeout)\n    if db is not None:\n        self._append(db)\n    if imp_user is not None:\n        self._append(imp_user)\n    if dehydration_hooks is not None:\n        self._append(dehydration_hooks)\n    if hydration_hooks is not None:\n        self._append(hydration_hooks)\n\n    return Connection(self, **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Begin a connection with the server.\n    \"\"\"\n    if db is not None:\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.settimeout(db)\n    if imp_user is not None:\n        self.imp_user = imp_user\n    if dehydration_hooks is not None:\n        self.dehydration_hooks = dehydration_hooks\n    if hydration_hooks is not None:\n        self.hydration_hooks = []\n    if mode is not None:\n        self.mode = mode\n    if bookmarks is not None:\n        self.bookmarks = bookmarks\n    if metadata is not None:\n        self.metadata = metadata\n    self.pool = ThreadPool(10)\n    self.pool.start()\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    \"\"\"\n    if mode is not None:\n        self.mode = mode\n    if bookmarks is not None:\n        self.bookmarks = bookmarks\n    if metadata is not None:\n        self.metadata = metadata\n    if timeout is not None:\n        self.timeout = timeout\n    if db is not None:\n        self.db = db\n    if imp_user is not None:\n        self.imp_user = imp_user\n  ", "is_pass": true, "return_code": 0}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    :param mode: http or https\n    :param bookmarks: list of reference to sub-reference\n    :param metadata: dictionary of metadata\n    :param timeout: timeout in seconds\n    :param db: database to connect to\n    :param imp_user: imp user to connect to\n    :param dehydration_hooks: dehydration hooks\n    :param hydration_hooks: hyper hyper hyper hyper\n    :return:\n    \"\"\"\n    if mode is not None:\n        self._append(mode, imp_user, dehydration_hooks, hydration_hooks)\n\n    self._on_server_state_change(None, None)\n    self._on_server_state_change('DEFAULT', None)\n    self._on_server_state_change('RECONNECTING', None)\n    self._on_server_state_change('CONNECTING', None)\n    self._on_server_state_change('DISCONNECTING', None)\n\n    return self.run(query=None, parameters=None, mode=mode, bookmarks=bookmarks,\n                    metadata=metadata, timeout=timeout, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Begin the connection to the database.\n    \"\"\"\n    if self.pool is not None:\n        self.pool.put(self, timeout=timeout)\n    self.connect()\n\n    if mode is not None:\n        self.mode = mode\n    if bookmarks is not None:\n        self.bookmarks = bookmarks\n    if metadata is not None:\n        self.metadata = metadata\n    if db is not None:\n        self.db = db\n    if imp_user is not None:\n        self.imp_user = imp_user\n    if dehydration_hooks is not None:\n        self.dehydration_hooks = dehydration_hooks\n    if hydration_hooks is not None:\n        self.hydration_hooks = hydration_hooks\n\n    self._on_server_state_change('BEGIN', 'STARTED', timeout=timeout)\n    return self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    :param mode: http:// or https://\n    :param bookmarks: {string: string}\n    :param metadata: {string: string}\n    :param timeout: {int: int}\n    :param db: {string: object}\n    :param imp_user: {string: string}\n    :param dehydration_hooks: {string: string}\n    :param hydration_hooks: {string: string}\n    \"\"\"\n    # Handle timeout and db (if passed)\n    timeout = timeout or self.timeout\n    db = db or self.db\n    if timeout:\n        timeout = int(timeout)\n        self.socket.settimeout(timeout)\n\n    # Handle imp_user\n    if imp_user:\n        self.user_agent = imp_user\n\n    # Handle dehydration hooks\n    if dehydration_hooks:\n        self.dehydration_hooks = dehydration_hooks\n\n    # Handle metadata\n    if metadata:\n        self.metadata = metadata\n\n    # Handle timeout\n    if timeout:\n        self.timeout = time.time() +", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    :param mode: mode of operation (e.g., ``'auth'``, ``'prod'``, ``'rollback'``)\n    :param bookmarks: list of bookmarks for operation\n    :param metadata: dictionary of metadata for operation\n    :param timeout: timeout for operation\n    :param db: database for operation\n    :param imp_user: imp user to use for authentication\n    :param dehydration_hooks: dehydration hooks to use for authentication\n    :param hydration_hooks: hydration hooks to use for authentication\n    \"\"\"\n    self._append = self._append.encode()\n    self._discard = self._discard.encode()\n    self._query = self._query.encode()\n    self._parameters = self._parameters.encode()\n    self._mode = mode\n    self._bookmarks = bookmarks\n    self._metadata = metadata\n    self._timeout = timeout\n    self._db = db\n    self._imp_user = imp_user\n    self._dehydration_hooks = dehydration_hooks\n    self._hydration_hooks = hydration_hooks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Begin a connection to the database.\n    \"\"\"\n    if self.pool:\n        self.socket.close()\n    self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n    self.socket.settimeout(timeout)\n    self.socket.connect(self.socket_path)\n    self.pool = PooledConnection(self.socket, self.pool_size, mode=mode,\n                               bookmarks=bookmarks, metadata=metadata,\n                               imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n                               hydration_hooks=hydration_hooks)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Begin routing.\n    \"\"\"\n    return self.run(\n        mode=mode,\n        bookmarks=bookmarks,\n        metadata=metadata,\n        timeout=timeout,\n        db=db,\n        imp_user=imp_user,\n        dehydration_hooks=dehydration_hooks,\n        hydration_hooks=hydration_hooks,\n        **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n           db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None)\n    \"\"\"\n    self._on_server_state_change(None, None)\n\n    super(RoutingEngine, self).begin(mode=mode, bookmarks=bookmarks,\n                                        metadata=metadata,\n                                        timeout=timeout,\n                                        db=db,\n                                        imp_user=imp_user,\n                                        dehydration_hooks=dehydration_hooks,\n    ", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    :returns: a new :class:`Connection` instance\n    \"\"\"\n    if mode is not None:\n        self._append(mode)\n    if bookmarks is not None:\n        self._append(bookmarks)\n    if metadata is not None:\n        self._append(metadata)\n    if timeout is not None:\n        self._append(timeout)\n    if db is not None:\n        self._append(db)\n    if imp_user is not None:\n        self._append(imp_user)\n    if dehydration_hooks is not None:\n        self._append(dehydration_hooks)\n    if hydration_hooks is not None:\n        self._append(hydration_hooks)\n\n    return Connection(self, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Begin a connection with the server.\n    \"\"\"\n    if db is not None:\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.settimeout(db)\n    if imp_user is not None:\n        self.imp_user = imp_user\n    if dehydration_hooks is not None:\n        self.dehydration_hooks = dehydration_hooks\n    if hydration_hooks is not None:\n        self.hydration_hooks = []\n    if mode is not None:\n        self.mode = mode\n    if bookmarks is not None:\n        self.bookmarks = bookmarks\n    if metadata is not None:\n        self.metadata = metadata\n    self.pool = ThreadPool(10)\n    self.pool.start()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/_arithmetic.py", "project": "neo4j/neo4j-python-driver", "name": "round_half_to_even", "docstring": ">>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:", "ques_id": "62e60723d76274f8a4026b75", "code": ["def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(ten_n / 10)) * 10\n        if hundred_n == int(hundred_n) and hundred_n % 10 == 5:\n            return int(round(hundred_n / 10)) * 10\n        else:\n            return int(hundred_n)\n    else:\n        return int(round(n))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        n = int(n)\n        if 10 <= n % 10 <= 20:\n            return int(n + 0.5)\n        elif 20 <= n % 10 <= 40:\n            return int(n + 1)\n        elif 40 <= n % 10 <= 50:\n            return int(n + 2)\n        elif 50 <= n % 10 <= 60:\n            return int(n + 3)\n        elif 60 <= n % 10 <= 70:\n            return int(n + 4)\n        elif 70 <= n % 10 <= 80:\n            return int(n + 5)\n        elif 80 <= n % 10 <= 90:\n            return int(n + 6)\n        elif 90 <= n % 10 <= 100:\n            return int(n + 7)\n        elif 100 <= n ", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(ten_n / 10)) * 10\n        return hundred_n + 1\n    else:\n        return int(round(n))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        h = int(math.floor(math.log(ten_n, 2)))\n        return 10 ** (h + 1) * n\n    else:\n        return int(round(n))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(10 * n / 10))\n        return hundred_n + 1\n    else:\n        return int(round(n))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(ten_n / 10)) * 10\n        if hundred_n == int(hundred_n) and hundred_n % 10 == 5:\n            return int(round_half_to_even(hundred_n)) * 10\n        else:\n            return int(round_half_to_even(hundred_n))\n    else:\n        return int(round_half_to_even(n))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(n))\n    else:\n        return int(round(float(n)))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(n))\n    else:\n        return int(round(float(ten_n) / 10))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        h = int(n / 10)\n        return h * h\n    else:\n        return int(n)\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(ten_n / 10))\n        return hundred_n * 10\n    else:\n        return int(round(n / 10))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(ten_n / 10)) * 10\n        if hundred_n == int(hundred_n) and hundred_n % 10 == 5:\n            return int(round(hundred_n / 10)) * 10\n        else:\n            return int(hundred_n)\n    else:\n        return int(round(n))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        n = int(n)\n        if 10 <= n % 10 <= 20:\n            return int(n + 0.5)\n        elif 20 <= n % 10 <= 40:\n            return int(n + 1)\n        elif 40 <= n % 10 <= 50:\n            return int(n + 2)\n        elif 50 <= n % 10 <= 60:\n            return int(n + 3)\n        elif 60 <= n % 10 <= 70:\n            return int(n + 4)\n        elif 70 <= n % 10 <= 80:\n            return int(n + 5)\n        elif 80 <= n % 10 <= 90:\n            return int(n + 6)\n        elif 90 <= n % 10 <= 100:\n            return int(n + 7)\n        elif 100 <= n ", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(ten_n / 10)) * 10\n        return hundred_n + 1\n    else:\n        return int(round(n))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        h = int(math.floor(math.log(ten_n, 2)))\n        return 10 ** (h + 1) * n\n    else:\n        return int(round(n))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(10 * n / 10))\n        return hundred_n + 1\n    else:\n        return int(round(n))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(ten_n / 10)) * 10\n        if hundred_n == int(hundred_n) and hundred_n % 10 == 5:\n            return int(round_half_to_even(hundred_n)) * 10\n        else:\n            return int(round_half_to_even(hundred_n))\n    else:\n        return int(round_half_to_even(n))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(n))\n    else:\n        return int(round(float(n)))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(n))\n    else:\n        return int(round(float(ten_n) / 10))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        h = int(n / 10)\n        return h * h\n    else:\n        return int(n)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        hundred_n = int(round(ten_n / 10))\n        return hundred_n * 10\n    else:\n        return int(round(n / 10))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_spatial/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "point_type", "docstring": "Dynamically create a Point subclass.", "ques_id": "62e60707d76274f8a4026b69", "code": ["def point_type(name, fields, srid_map):\n    # type: (str, dict, dict) -> Point\n    \"\"\"\n    CartesianPoint is a subclass of WGS84Point. This is the base point class.\n    \"\"\"\n    class Point(WGS84Point):\n        __schema__ = {\n            'type': 'Point',\n            'coordinates': fields\n        }\n        __srid__ = srid_map.get(name)\n        __doc__ = '{} coordinates'.format(name)\n\n    return Point\n", "def point_type(name, fields, srid_map):\n    class Point(srid_table[name], fields, WGS84Point):\n        pass\n    Point.__name__ = name\n    Point.__qualname__ = '{0}Point'.format(name)\n    return Point\n", "def point_type(name, fields, srid_map):\n    klass = srid_table[srid_map[name]]\n    return klass.__new__(klass, *fields)\n", "def point_type(name, fields, srid_map):\n    class Point(object):\n        def __init__(self, x, y):\n            super(Point, self).__init__()\n            self.x = x\n            self.y = y\n            for k, v in fields.items():\n                setattr(self, k, v)\n            self.srid = srid_map[self.x][self.y]\n\n    return Point\n", "def point_type(name, fields, srid_map):\n    # See http://docs.python.org/lib/sphinx/ext/geometry.html#module-geometry\n    name = name.lower()\n    if name in srid_table:\n        return srid_table[name]\n    elif name in srid_table_lock:\n        return srid_table_lock[name]\n\n    # Not a valid srid.\n    raise ValueError('Unknown SRID \"%s\"' % name)\n", "def point_type(name, fields, srid_map):\n    srid_key = srid_map[name]\n    return type(name, (CartesianPoint,), {\n        '__module__': 'Python.{0}'.format(srid_key),\n        '__fields__': fields,\n        '__srid__': srid_key,\n    })\n", "def point_type(name, fields, srid_map):\n    class Point(srid_table[name], fields):\n        __slots__ = ()\n\n    Point.__name__ = name\n    return Point\n", "def point_type(name, fields, srid_map):\n    if name in srid_table:\n        return SridCartesianPoint(name, fields, srid_map)\n    else:\n        return Point(name, fields, srid_map)\n", "def point_type(name, fields, srid_map):\n    if '__module__' not in sys.modules:\n        sys.modules['__import__'] = __import__\n        load_module = __import__('shapely.geometry.point', fromlist=[name])\n        sys.modules['__import__'] = sys.modules['__import__']\n        module = sys.modules['__import__'].__dict__[name]\n    else:\n        module = sys.modules['__import__'].__dict__[name]\n\n    name = name.replace('_','')\n\n    if '__' not in name:\n        return Point\n\n    name, _, name_fields = name.partition('__')\n\n    if name_fields:\n        fields = name_fields.split(' ')\n\n    return type(\n        name,\n        (\n            Point,\n            ),\n        {\n            '__module__': sys.modules[__name__],\n            'fields': fields,\n    ", "def point_type(name, fields, srid_map):\n    # pylint: disable=too-many-arguments\n    \"\"\"\n    CartesianPoint subclass.\n    \"\"\"\n    class Point(DynamicalPoint):\n        __slots__ = fields\n        __init__ = lambda self, *args, **kwargs: None\n\n        def __init__(self, *args, **kwargs):\n            super(Point, self).__init__(*args, **kwargs)\n            self.__dict__['_srid'] = srid_map.get(self.__dict__['_name'], 'WGS84')\n\n        @classmethod\n        def _get_srid(cls):\n            return cls._srid\n\n        @classmethod\n        def _get_name(cls):\n            return cls._name\n\n        def __repr__(self):\n            return '%s(%r, %r, %r)' % (\n                cls.__name__,\n                self.__dict__['_name'],\n       "], "level": "file_runnable", "generate_results": [{"generate_code": "def point_type(name, fields, srid_map):\n    # type: (str, dict, dict) -> Point\n    \"\"\"\n    CartesianPoint is a subclass of WGS84Point. This is the base point class.\n    \"\"\"\n    class Point(WGS84Point):\n        __schema__ = {\n            'type': 'Point',\n            'coordinates': fields\n        }\n        __srid__ = srid_map.get(name)\n        __doc__ = '{} coordinates'.format(name)\n\n    return Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    class Point(srid_table[name], fields, WGS84Point):\n        pass\n    Point.__name__ = name\n    Point.__qualname__ = '{0}Point'.format(name)\n    return Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    klass = srid_table[srid_map[name]]\n    return klass.__new__(klass, *fields)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    class Point(object):\n        def __init__(self, x, y):\n            super(Point, self).__init__()\n            self.x = x\n            self.y = y\n            for k, v in fields.items():\n                setattr(self, k, v)\n            self.srid = srid_map[self.x][self.y]\n\n    return Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    # See http://docs.python.org/lib/sphinx/ext/geometry.html#module-geometry\n    name = name.lower()\n    if name in srid_table:\n        return srid_table[name]\n    elif name in srid_table_lock:\n        return srid_table_lock[name]\n\n    # Not a valid srid.\n    raise ValueError('Unknown SRID \"%s\"' % name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    srid_key = srid_map[name]\n    return type(name, (CartesianPoint,), {\n        '__module__': 'Python.{0}'.format(srid_key),\n        '__fields__': fields,\n        '__srid__': srid_key,\n    })\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    class Point(srid_table[name], fields):\n        __slots__ = ()\n\n    Point.__name__ = name\n    return Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    if name in srid_table:\n        return SridCartesianPoint(name, fields, srid_map)\n    else:\n        return Point(name, fields, srid_map)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    if '__module__' not in sys.modules:\n        sys.modules['__import__'] = __import__\n        load_module = __import__('shapely.geometry.point', fromlist=[name])\n        sys.modules['__import__'] = sys.modules['__import__']\n        module = sys.modules['__import__'].__dict__[name]\n    else:\n        module = sys.modules['__import__'].__dict__[name]\n\n    name = name.replace('_','')\n\n    if '__' not in name:\n        return Point\n\n    name, _, name_fields = name.partition('__')\n\n    if name_fields:\n        fields = name_fields.split(' ')\n\n    return type(\n        name,\n        (\n            Point,\n            ),\n        {\n            '__module__': sys.modules[__name__],\n            'fields': fields,\n    ", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    # pylint: disable=too-many-arguments\n    \"\"\"\n    CartesianPoint subclass.\n    \"\"\"\n    class Point(DynamicalPoint):\n        __slots__ = fields\n        __init__ = lambda self, *args, **kwargs: None\n\n        def __init__(self, *args, **kwargs):\n            super(Point, self).__init__(*args, **kwargs)\n            self.__dict__['_srid'] = srid_map.get(self.__dict__['_name'], 'WGS84')\n\n        @classmethod\n        def _get_srid(cls):\n            return cls._srid\n\n        @classmethod\n        def _get_name(cls):\n            return cls._name\n\n        def __repr__(self):\n            return '%s(%r, %r, %r)' % (\n                cls.__name__,\n                self.__dict__['_name'],\n       ", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_meta.py", "project": "neo4j/neo4j-python-driver", "name": "deprecated", "docstring": "Decorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass", "ques_id": "62e5dc9ed76274f8a4026b5b", "code": ["def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=3)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=3)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=3)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=3)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @functools.wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message, stack_level=2)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/r.py", "project": "pre-commit/pre-commit", "name": "_inline_r_setup", "docstring": "Some behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.", "ques_id": "62e4fc3c85ea98643089041e", "code": ["def _inline_r_setup(code: str) -> str:\n    if \"R\" in os.environ:\n        raise Exception(\"Cannot set environment variable R\")\n    os.environ[\"R\"] = code\n    return code\n", "def _inline_r_setup(code: str) -> str:\n    if not code.startswith(\"r\"):\n        raise ValueError(\n            \"r options can only be configured via env variables but are \"\n            \"started after R has been started.\"\n        )\n    return code\n", "def _inline_r_setup(code: str):\n    global inline_r_configuration\n    inline_r_configuration = code\n", "def _inline_r_setup(code: str):\n    # This is a workaround for a bug in r-cache. When running with\n    # R=True, R will load the environment variables that were set via the\n    # configuration file (this is a workaround for a bug in r-cache that\n    # may be triggered by a setter change in the configuration file).\n    # https://github.com/r-cache/r-cache/pull/123\n    if code.startswith(\"R=True\"):\n        from r_cache.config import environment\n\n        environment.clear()\n        environment.update(json.loads(environment_config.environment))\n\n        environment.set_variable(\"PYTHON_RUNDIR\", sys.executable)\n", "def _inline_r_setup(code: str):\n    if code == \"\":\n        code = \"export R=MockR\"\n    elif \"R\" in code:\n        raise ValueError(\"cannot be configured via env variables, but R has already been set\")\n\n    return code\n", "def _inline_r_setup(code: str):\n    env = os.environ.copy()\n\n    # If R was configured via env variables, then we need to remove R from the environment.\n    if \"R_URL\" in env:\n        del env[\"R_URL\"]\n    if \"R_TOKEN\" in env:\n        del env[\"R_TOKEN\"]\n\n    # If R was configured via command line, then we need to remove R from the environment.\n    if \"R_USERNAME\" in env:\n        del env[\"R_USERNAME\"]\n    if \"R_PASSWORD\" in env:\n        del env[\"R_PASSWORD\"]\n\n    # If R was configured via command line, and the environment has been set via env variables,\n    # then we can set the R options so we can use the command line to configure R.\n    if \"R_URL\" in env and \"R_TOKEN\" in env:\n        os.environ[\"R_URL\"] = env[\"R_TOKEN\"]\n        os.environ[\"R_USERNAME\"] = env[\"R_USERNAME\"]\n        os.environ[\"R_PASSWORD\"]", "def _inline_r_setup(code: str) -> str:\n    r_command = \"source {}/bin/activate; source {}/bin/activate\".format(\n        os.path.dirname(os.path.realpath(__file__)),\n        os.path.join(os.path.dirname(__file__), \"r.sh\"),\n    )\n    if not os.system(r_command) == 0:\n        raise Exception(\n            \"Failed to configure R: exit status was not 0, command: {}\".format(\n                r_command\n            )\n        )\n    return code\n", "def _inline_r_setup(code: str):\n    if not code.startswith(\"#!/usr/bin/env \"):\n        return\n\n    # These are only needed when R has been configured via env variables.\n    # Set this to avoid re-running R for each command.\n    os.environ[\"R_CONFIG\"] = \"/etc/yunohost/yunohost.rc\"\n\n    # Set the inline r command to run when R has started.\n    inline_r.command = [\"inline_r\"]\n\n    # Include the R code in the environment variables.\n    inline_r.code = code\n\n    # Load the inline_r command and set the inline_r_env variable to the\n    # contents of the inline_r command.\n    inline_r.load()\n    inline_r_env.set(inline_r.code)\n", "def _inline_r_setup(code: str) -> str:\n    if \"R_INLINE\" in os.environ:\n        raise ConfigurationError(\n            \"Cannot configure a context via env variables. \"\n            \"Use R instead or R_INLINE after configuring R.\"\n        )\n\n    env = {}\n    env.update(os.environ)\n    env.update({\"R_INLINE\": \"1\"})\n    if code not in env:\n        raise ConfigurationError(\"Undefined context code {}\".format(code))\n    return env[code]\n", "def _inline_r_setup(code: str) -> str:\n    code = (\n        f\"\"\"\n        export R=\"{env.R_PREFIX}{env.R_VERSION}\"\n        export R_VERSION={env.R_VERSION}\n        \"\"\"\n    )\n    return code\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _inline_r_setup(code: str) -> str:\n    if \"R\" in os.environ:\n        raise Exception(\"Cannot set environment variable R\")\n    os.environ[\"R\"] = code\n    return code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    if not code.startswith(\"r\"):\n        raise ValueError(\n            \"r options can only be configured via env variables but are \"\n            \"started after R has been started.\"\n        )\n    return code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str):\n    global inline_r_configuration\n    inline_r_configuration = code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str):\n    # This is a workaround for a bug in r-cache. When running with\n    # R=True, R will load the environment variables that were set via the\n    # configuration file (this is a workaround for a bug in r-cache that\n    # may be triggered by a setter change in the configuration file).\n    # https://github.com/r-cache/r-cache/pull/123\n    if code.startswith(\"R=True\"):\n        from r_cache.config import environment\n\n        environment.clear()\n        environment.update(json.loads(environment_config.environment))\n\n        environment.set_variable(\"PYTHON_RUNDIR\", sys.executable)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str):\n    if code == \"\":\n        code = \"export R=MockR\"\n    elif \"R\" in code:\n        raise ValueError(\"cannot be configured via env variables, but R has already been set\")\n\n    return code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str):\n    env = os.environ.copy()\n\n    # If R was configured via env variables, then we need to remove R from the environment.\n    if \"R_URL\" in env:\n        del env[\"R_URL\"]\n    if \"R_TOKEN\" in env:\n        del env[\"R_TOKEN\"]\n\n    # If R was configured via command line, then we need to remove R from the environment.\n    if \"R_USERNAME\" in env:\n        del env[\"R_USERNAME\"]\n    if \"R_PASSWORD\" in env:\n        del env[\"R_PASSWORD\"]\n\n    # If R was configured via command line, and the environment has been set via env variables,\n    # then we can set the R options so we can use the command line to configure R.\n    if \"R_URL\" in env and \"R_TOKEN\" in env:\n        os.environ[\"R_URL\"] = env[\"R_TOKEN\"]\n        os.environ[\"R_USERNAME\"] = env[\"R_USERNAME\"]\n        os.environ[\"R_PASSWORD\"]", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    r_command = \"source {}/bin/activate; source {}/bin/activate\".format(\n        os.path.dirname(os.path.realpath(__file__)),\n        os.path.join(os.path.dirname(__file__), \"r.sh\"),\n    )\n    if not os.system(r_command) == 0:\n        raise Exception(\n            \"Failed to configure R: exit status was not 0, command: {}\".format(\n                r_command\n            )\n        )\n    return code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str):\n    if not code.startswith(\"#!/usr/bin/env \"):\n        return\n\n    # These are only needed when R has been configured via env variables.\n    # Set this to avoid re-running R for each command.\n    os.environ[\"R_CONFIG\"] = \"/etc/yunohost/yunohost.rc\"\n\n    # Set the inline r command to run when R has started.\n    inline_r.command = [\"inline_r\"]\n\n    # Include the R code in the environment variables.\n    inline_r.code = code\n\n    # Load the inline_r command and set the inline_r_env variable to the\n    # contents of the inline_r command.\n    inline_r.load()\n    inline_r_env.set(inline_r.code)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    if \"R_INLINE\" in os.environ:\n        raise ConfigurationError(\n            \"Cannot configure a context via env variables. \"\n            \"Use R instead or R_INLINE after configuring R.\"\n        )\n\n    env = {}\n    env.update(os.environ)\n    env.update({\"R_INLINE\": \"1\"})\n    if code not in env:\n        raise ConfigurationError(\"Undefined context code {}\".format(code))\n    return env[code]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    code = (\n        f\"\"\"\n        export R=\"{env.R_PREFIX}{env.R_VERSION}\"\n        export R_VERSION={env.R_VERSION}\n        \"\"\"\n    )\n    return code\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/xargs.py", "project": "pre-commit/pre-commit", "name": "xargs", "docstring": "A simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently", "ques_id": "62e4fbda85ea986430890405", "code": ["def xargs(\n        cmd: Iterable[str],\n        args: Iterable[str],\n        env: Optional[Dict[str, str]] = None,\n        cwd: Optional[str] = None,\n        verbose: bool = False,\n        target_concurrency: int = None,\n) -> Tuple[int, str, str]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if verbose:\n        print('Running command:',''.join(cmd))\n\n    p = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env,\n        cwd=cwd,\n        encoding='utf-8',\n    )\n\n    if verbose:\n        print('Command stdout:', p.communicate()[0].decode('utf-8')", "def xargs(\n        cmd: Command,\n        args: Iterable[str],\n        env: Optional[Dict[str, str]] = None,\n        cwd: Optional[str] = None,\n        target_concurrency: int = None,\n        verbose: bool = False,\n) -> Tuple[Command, List[str]]:\n    \"\"\"\n    A simplified implementation of xargs.\n    color: Make a pty if on a platform that supports it target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, env=env, cwd=cwd)\n    out, err = p.communicate(input='\\n'.join(args).encode('utf-8'))\n    if p.returncode!= 0:\n        if verbose:\n            print('Command failed: {}'.format(p.returncode))\n        return None, ['Command failed: {}'.format(p.returncode)]\n    return cmd, out.decode('utf-8').splitlines()\n", "def xargs(\n        cmd: Command,\n        target_concurrency: int = None,\n        color: bool = False,\n        target_concurrency_sleep: float = 0.0,\n        *args: str,\n        **kwargs: str\n) -> Command:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not isinstance(cmd, Command):\n        raise TypeError(\"cmd must be a Command instance\")\n    if target_concurrency is None:\n        raise ValueError(\"target_concurrency must be provided\")\n    if target_concurrency_sleep < 0:\n        raise ValueError(\"target_concurrency_sleep must be a non-negative number\")\n\n    concurrency = target_concurrency\n    if concurrency is None:\n        concurrency = int(os.environ.get(\"NB_PARTITIONS\", 1))\n    concurrency = min(concu", "def xargs(\n        cmd: Iterable[str],\n        shell: bool = False,\n        target_concurrency: int = None,\n        env: Dict[str, str] = None,\n) -> Tuple[List[str], Dict[str, str]]:\n    \"\"\"\n    A simplified implementation of xargs. color: Make a pty if on a platform\n    that supports it target_concurrency: Target number of partitions to run\n    concurrently\n    \"\"\"\n    if shell:\n        p = subprocess.run(cmd, stdout=subprocess.PIPE, encoding=\"utf-8\", check=True)\n        return p.stdout.decode().splitlines(), dict(\n            (k.strip().replace(\"\\\\\", \"/\").replace(\"\\\"\", \"\"), v)\n            for k, v in p.stdout.decode().splitlines()\n            if k and k.startswith(\"target_\")\n        )\n    else:\n        p = subprocess.run(\n            [\"xargs\"] + cmd, stdout=subprocess.PIPE, encoding=\"utf-8\", check=True, env=e", "def xargs(\n        cmd: Sequence[str],\n        target_concurrency: int = None,\n        color: bool = False,\n        target_env: str = None,\n        **kwargs) -> List[str]:\n    \"\"\"\n    A simplified implementation of xargs. color: Make a pty if on a platform\n    that supports it target_concurrency: Target number of partitions to run\n    concurrently\n    \"\"\"\n    if platform.system() == 'Windows':\n        return ['\"{}\"'.format(' '.join(cmd))]\n\n    if target_concurrency is None:\n        target_concurrency = os.environ.get('NOSE_ALL_AVAILABLE_CONCURRENCIES', 1)\n\n    if target_env is None:\n        target_env = os.environ.get('NOSE_ALL_AVAILABLE_ENV', '')\n\n    pty = None\n    if os.environ.get('NOSE_NO_PIPE_PROCESS') == 'true':\n        pty = False\n\n    if color:\n        target_concurrency = int(target_concurrency)\n        target_env = in", "def xargs(\n        cmd: List[str],\n        target_concurrency: int = None,\n        color: bool = False,\n        target_concurrency_sleep: float = 0.0,\n        ) -> str:\n    \"\"\"\n    A simplified implementation of xargs.\n    color:\n\n    Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n\n    \"\"\"\n    if sys.platform.startswith('win'):\n        return ''\n\n    if target_concurrency is None:\n        target_concurrency = int(os.environ.get('NOSE_NO_CONCURRENCY', '1'))\n\n    if color:\n        cmd = [' '.join(c) for c in cmd]\n\n    try:\n        output = subprocess.check_output(\n            [' '.join(c) for c in cmd]).decode('utf-8')\n    except subprocess.CalledProcessError:\n        output = ''\n\n    if target_concurrency_sleep:\n        time.sleep", "def xargs(\n        cmd: Iterable[str],\n        args: Iterable[str],\n        target_concurrency: int = None,\n) -> Iterator[str]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently (default: concurrency of 1)\n    \"\"\"\n    if not isinstance(cmd, (list, tuple)):\n        cmd = [cmd]\n\n    if target_concurrency:\n        if target_concurrency < len(cmd):\n            for c in range(target_concurrency):\n                subprocess.run(\n                    cmd[c],\n                    check=True,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n            raise Exception(\n                \"Too many arguments fo", "def xargs(\n        cmd: Command,\n        target_concurrency: int = None,\n        color: bool = True,\n        target_env: Optional[str] = None,\n) -> Tuple[Command, List[str]]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not target_concurrency:\n        target_concurrency = os.cpu_count()\n\n    if not target_env:\n        target_env = os.environ.get(\"GRPC_TARGET_NUM_THREADS\", \"1\")\n\n    ptys = []\n    ptys.append(subprocess.Popen([\"xargs\"]))\n    for p in range(target_concurrency):\n        if color:\n            pty = subprocess.Popen([\"xargs\"])\n            pty.wait()\n            target_env = pty.communicate()[0].decode()\n        else:\n            pty =", "def xargs(\n        cmd: Iterable[str],\n        *args: Iterable[str],\n        target_concurrency: int = None,\n        **kwargs: Any\n) -> Iterable[str]:\n    \"\"\"\n    A simplified implementation of xargs.\n    color: Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if is_windows():\n        return _xargs_windows(cmd, args, target_concurrency, **kwargs)\n    else:\n        return _xargs_posix(cmd, args, target_concurrency, **kwargs)\n", "def xargs(\n        cmd: str,\n        args: Optional[Sequence[str]] = None,\n        capture: bool = False,\n        target_concurrency: int = None,\n        **kwargs) -> Tuple[subprocess.CompletedProcess, List[str]]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n\n    \"\"\"\n    if not args:\n        args = []\n\n    kwargs.setdefault('stdout', subprocess.PIPE)\n\n    if capture:\n        kwargs.setdefault('stderr', subprocess.STDOUT)\n\n    if target_concurrency is None:\n        target_concurrency = os.environ.get('JUJU_NUM_THREADS')\n\n    if target_concurrency:\n        kwargs.setdefault('stdout', subprocess.PIPE, int(target_concurrency))\n        kwargs.setdefault('stderr', subprocess.STDOUT)\n\n    proc = subprocess.run(\n    "], "level": "project_runnable", "generate_results": [{"generate_code": "def xargs(\n        cmd: Iterable[str],\n        args: Iterable[str],\n        env: Optional[Dict[str, str]] = None,\n        cwd: Optional[str] = None,\n        verbose: bool = False,\n        target_concurrency: int = None,\n) -> Tuple[int, str, str]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if verbose:\n        print('Running command:',''.join(cmd))\n\n    p = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env,\n        cwd=cwd,\n        encoding='utf-8',\n    )\n\n    if verbose:\n        print('Command stdout:', p.communicate()[0].decode('utf-8')", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: Command,\n        args: Iterable[str],\n        env: Optional[Dict[str, str]] = None,\n        cwd: Optional[str] = None,\n        target_concurrency: int = None,\n        verbose: bool = False,\n) -> Tuple[Command, List[str]]:\n    \"\"\"\n    A simplified implementation of xargs.\n    color: Make a pty if on a platform that supports it target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, env=env, cwd=cwd)\n    out, err = p.communicate(input='\\n'.join(args).encode('utf-8'))\n    if p.returncode!= 0:\n        if verbose:\n            print('Command failed: {}'.format(p.returncode))\n        return None, ['Command failed: {}'.format(p.returncode)]\n    return cmd, out.decode('utf-8').splitlines()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: Command,\n        target_concurrency: int = None,\n        color: bool = False,\n        target_concurrency_sleep: float = 0.0,\n        *args: str,\n        **kwargs: str\n) -> Command:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not isinstance(cmd, Command):\n        raise TypeError(\"cmd must be a Command instance\")\n    if target_concurrency is None:\n        raise ValueError(\"target_concurrency must be provided\")\n    if target_concurrency_sleep < 0:\n        raise ValueError(\"target_concurrency_sleep must be a non-negative number\")\n\n    concurrency = target_concurrency\n    if concurrency is None:\n        concurrency = int(os.environ.get(\"NB_PARTITIONS\", 1))\n    concurrency = min(concu", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: Iterable[str],\n        shell: bool = False,\n        target_concurrency: int = None,\n        env: Dict[str, str] = None,\n) -> Tuple[List[str], Dict[str, str]]:\n    \"\"\"\n    A simplified implementation of xargs. color: Make a pty if on a platform\n    that supports it target_concurrency: Target number of partitions to run\n    concurrently\n    \"\"\"\n    if shell:\n        p = subprocess.run(cmd, stdout=subprocess.PIPE, encoding=\"utf-8\", check=True)\n        return p.stdout.decode().splitlines(), dict(\n            (k.strip().replace(\"\\\\\", \"/\").replace(\"\\\"\", \"\"), v)\n            for k, v in p.stdout.decode().splitlines()\n            if k and k.startswith(\"target_\")\n        )\n    else:\n        p = subprocess.run(\n            [\"xargs\"] + cmd, stdout=subprocess.PIPE, encoding=\"utf-8\", check=True, env=e", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: Sequence[str],\n        target_concurrency: int = None,\n        color: bool = False,\n        target_env: str = None,\n        **kwargs) -> List[str]:\n    \"\"\"\n    A simplified implementation of xargs. color: Make a pty if on a platform\n    that supports it target_concurrency: Target number of partitions to run\n    concurrently\n    \"\"\"\n    if platform.system() == 'Windows':\n        return ['\"{}\"'.format(' '.join(cmd))]\n\n    if target_concurrency is None:\n        target_concurrency = os.environ.get('NOSE_ALL_AVAILABLE_CONCURRENCIES', 1)\n\n    if target_env is None:\n        target_env = os.environ.get('NOSE_ALL_AVAILABLE_ENV', '')\n\n    pty = None\n    if os.environ.get('NOSE_NO_PIPE_PROCESS') == 'true':\n        pty = False\n\n    if color:\n        target_concurrency = int(target_concurrency)\n        target_env = in", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: List[str],\n        target_concurrency: int = None,\n        color: bool = False,\n        target_concurrency_sleep: float = 0.0,\n        ) -> str:\n    \"\"\"\n    A simplified implementation of xargs.\n    color:\n\n    Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n\n    \"\"\"\n    if sys.platform.startswith('win'):\n        return ''\n\n    if target_concurrency is None:\n        target_concurrency = int(os.environ.get('NOSE_NO_CONCURRENCY', '1'))\n\n    if color:\n        cmd = [' '.join(c) for c in cmd]\n\n    try:\n        output = subprocess.check_output(\n            [' '.join(c) for c in cmd]).decode('utf-8')\n    except subprocess.CalledProcessError:\n        output = ''\n\n    if target_concurrency_sleep:\n        time.sleep", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: Iterable[str],\n        args: Iterable[str],\n        target_concurrency: int = None,\n) -> Iterator[str]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently (default: concurrency of 1)\n    \"\"\"\n    if not isinstance(cmd, (list, tuple)):\n        cmd = [cmd]\n\n    if target_concurrency:\n        if target_concurrency < len(cmd):\n            for c in range(target_concurrency):\n                subprocess.run(\n                    cmd[c],\n                    check=True,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n            raise Exception(\n                \"Too many arguments fo", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: Command,\n        target_concurrency: int = None,\n        color: bool = True,\n        target_env: Optional[str] = None,\n) -> Tuple[Command, List[str]]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not target_concurrency:\n        target_concurrency = os.cpu_count()\n\n    if not target_env:\n        target_env = os.environ.get(\"GRPC_TARGET_NUM_THREADS\", \"1\")\n\n    ptys = []\n    ptys.append(subprocess.Popen([\"xargs\"]))\n    for p in range(target_concurrency):\n        if color:\n            pty = subprocess.Popen([\"xargs\"])\n            pty.wait()\n            target_env = pty.communicate()[0].decode()\n        else:\n            pty =", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: Iterable[str],\n        *args: Iterable[str],\n        target_concurrency: int = None,\n        **kwargs: Any\n) -> Iterable[str]:\n    \"\"\"\n    A simplified implementation of xargs.\n    color: Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if is_windows():\n        return _xargs_windows(cmd, args, target_concurrency, **kwargs)\n    else:\n        return _xargs_posix(cmd, args, target_concurrency, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: str,\n        args: Optional[Sequence[str]] = None,\n        capture: bool = False,\n        target_concurrency: int = None,\n        **kwargs) -> Tuple[subprocess.CompletedProcess, List[str]]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n\n    \"\"\"\n    if not args:\n        args = []\n\n    kwargs.setdefault('stdout', subprocess.PIPE)\n\n    if capture:\n        kwargs.setdefault('stderr', subprocess.STDOUT)\n\n    if target_concurrency is None:\n        target_concurrency = os.environ.get('JUJU_NUM_THREADS')\n\n    if target_concurrency:\n        kwargs.setdefault('stdout', subprocess.PIPE, int(target_concurrency))\n        kwargs.setdefault('stderr', subprocess.STDOUT)\n\n    proc = subprocess.run(\n    ", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/helpers.py", "project": "pre-commit/pre-commit", "name": "_shuffled", "docstring": "Deterministically shuffle", "ques_id": "62e4fbda85ea986430890403", "code": ["def _shuffled(seq: Sequence[str]) -> Sequence[str]:\n    \"Deterministically shuffle\"\n    return sorted(seq)\n", "def _shuffled(seq: Sequence[str]) -> List[str]:\n    \"Deterministically shuffle\"\n    return list(random.shuffle(seq))\n", "def _shuffled(seq: Sequence[str]) -> List[str]:\n    return random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> Sequence[str]:\n    \"Deterministically shuffle\"\n    return list(random.sample(seq, len(seq)))\n", "def _shuffled(seq: Sequence[str]) -> Sequence[str]:\n    \"Deterministically shuffle\"\n    return random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> Sequence[str]:\n    \"Deterministically shuffle\"\n    return random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> List[str]:\n    \"Deterministically shuffle\"\n    return list(random.sample(seq, len(seq)))\n", "def _shuffled(seq: Sequence[str]) -> List[str]:\n    return random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> List[str]:\n    \"Deterministically shuffle\"\n    return random.sample(seq, len(seq))\n", "def _shuffled(seq: Iterable[str]) -> List[str]:\n    return list(random.sample(seq, len(seq)))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _shuffled(seq: Sequence[str]) -> Sequence[str]:\n    \"Deterministically shuffle\"\n    return sorted(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> List[str]:\n    \"Deterministically shuffle\"\n    return list(random.shuffle(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> List[str]:\n    return random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> Sequence[str]:\n    \"Deterministically shuffle\"\n    return list(random.sample(seq, len(seq)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> Sequence[str]:\n    \"Deterministically shuffle\"\n    return random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> Sequence[str]:\n    \"Deterministically shuffle\"\n    return random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> List[str]:\n    \"Deterministically shuffle\"\n    return list(random.sample(seq, len(seq)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> List[str]:\n    return random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> List[str]:\n    \"Deterministically shuffle\"\n    return random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Iterable[str]) -> List[str]:\n    return list(random.sample(seq, len(seq)))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/util.py", "project": "pre-commit/pre-commit", "name": "parse_version", "docstring": "poor man's version comparison", "ques_id": "62e4fb6585ea98643089032b", "code": ["def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s is None or s == '':\n        return None\n\n    version = StrictVersion(s)\n    if version.exact and version.up_to_date:\n        return version\n    else:\n        return None\n", "def parse_version(s: str) -> Optional[Version]:\n    if s:\n        return parse(s)\n    return None\n", "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s is None or s == '':\n        return None\n\n    version = StrictVersion(s)\n    if version.exact or version.micro or version.releaselevel!= 'final':\n        return version\n    else:\n        return None\n", "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s is None or s == '':\n        return None\n\n    version_parts = s.split('=')\n    if len(version_parts) == 1:\n        return StrictVersion(version_parts[0])\n    else:\n        return StrictVersion(version_parts[0]) < StrictVersion(version_parts[1])\n", "def parse_version(s: str) -> Optional[Version]:\n    if s:\n        return parse(s)\n    return None\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s is None or s == '':\n        return None\n\n    version = StrictVersion(s)\n    if version.exact and version.up_to_date:\n        return version\n    else:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> Optional[Version]:\n    if s:\n        return parse(s)\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s:\n        return StrictVersion(s)\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s is None or s == '':\n        return None\n\n    version = StrictVersion(s)\n    if version.exact or version.micro or version.releaselevel!= 'final':\n        return version\n    else:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> Optional[StrictVersion]:\n    if s is None or s == '':\n        return None\n\n    version_parts = s.split('=')\n    if len(version_parts) == 1:\n        return StrictVersion(version_parts[0])\n    else:\n        return StrictVersion(version_parts[0]) < StrictVersion(version_parts[1])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> Optional[Version]:\n    if s:\n        return parse(s)\n    return None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/parse_shebang.py", "project": "pre-commit/pre-commit", "name": "normalize_cmd", "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine", "ques_id": "62e4fb4d85ea9864308902e7", "code": ["def normalize_cmd(cmd: str) -> str:\n    if \";\" in cmd:\n        cmd = cmd.split(\";\")[0]\n    cmd = parse_filename(cmd)\n    return cmd\n", "def normalize_cmd(cmd: T.List[str]) -> T.List[str]:\n    cmd = [p.strip() for p in cmd]\n    if \"__file__\" in cmd or \"__path__\" in cmd:\n        cmd = fix_path_shebang(cmd)\n    return cmd\n", "def normalize_cmd(cmd: List[str]) -> List[str]:\n    if \"@\" in cmd[0] and \"%\" in cmd[0]:\n        filename = cmd[0].split(\"@\")[1]\n        exe = find_executable(filename)\n        cmd[0] = exe if exe else filename\n\n    return cmd\n", "def normalize_cmd(cmd: List[str]) -> List[str]:\n    cmd = [i for i in cmd if not i.startswith(\"-no-\")]\n    if sys.platform.startswith(\"win\"):\n        cmd = [i for i in cmd if not i.startswith(\"-no-\")]\n    return cmd\n", "def normalize_cmd(cmd: str) -> str:\n    cmd = parse_filename(cmd)\n    if sys.platform == 'win32' and '/usr/bin/' in cmd[0]:\n        cmd = [cmd[0], cmd[1] + '.exe']\n    return cmd\n", "def normalize_cmd(cmd: str) -> str:\n    cmd = parse_filename(cmd)\n    if sys.platform.startswith(\"win\"):\n        cmd = fix_for_windows_shebang(cmd)\n    return cmd\n", "def normalize_cmd(cmd: List[str]) -> List[str]:\n    cmd = [normexe(x) for x in cmd]\n    cmd[0] = shlex.split(cmd[0])\n    return cmd\n", "def normalize_cmd(cmd: str) -> str:\n    cmd = parse_filename(cmd)\n    if sys.platform.startswith(\"win\"):\n        cmd = fix_for_windows_shebangs(cmd)\n    return cmd\n", "def normalize_cmd(cmd: str) -> str:\n    cmd = parse_filename(cmd)\n    if sys.platform.startswith(\"win32\"):\n        cmd = fix_for_windows_shebang(cmd)\n    return cmd\n", "def normalize_cmd(cmd: List[str]) -> List[str]:\n\n    cmd = [shlex.quote(i) for i in cmd]\n    cmd[0] = normexe(cmd[0])\n\n    return cmd\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def normalize_cmd(cmd: str) -> str:\n    if \";\" in cmd:\n        cmd = cmd.split(\";\")[0]\n    cmd = parse_filename(cmd)\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: T.List[str]) -> T.List[str]:\n    cmd = [p.strip() for p in cmd]\n    if \"__file__\" in cmd or \"__path__\" in cmd:\n        cmd = fix_path_shebang(cmd)\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: List[str]) -> List[str]:\n    if \"@\" in cmd[0] and \"%\" in cmd[0]:\n        filename = cmd[0].split(\"@\")[1]\n        exe = find_executable(filename)\n        cmd[0] = exe if exe else filename\n\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: List[str]) -> List[str]:\n    cmd = [i for i in cmd if not i.startswith(\"-no-\")]\n    if sys.platform.startswith(\"win\"):\n        cmd = [i for i in cmd if not i.startswith(\"-no-\")]\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: str) -> str:\n    cmd = parse_filename(cmd)\n    if sys.platform == 'win32' and '/usr/bin/' in cmd[0]:\n        cmd = [cmd[0], cmd[1] + '.exe']\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: str) -> str:\n    cmd = parse_filename(cmd)\n    if sys.platform.startswith(\"win\"):\n        cmd = fix_for_windows_shebang(cmd)\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: List[str]) -> List[str]:\n    cmd = [normexe(x) for x in cmd]\n    cmd[0] = shlex.split(cmd[0])\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: str) -> str:\n    cmd = parse_filename(cmd)\n    if sys.platform.startswith(\"win\"):\n        cmd = fix_for_windows_shebangs(cmd)\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: str) -> str:\n    cmd = parse_filename(cmd)\n    if sys.platform.startswith(\"win32\"):\n        cmd = fix_for_windows_shebang(cmd)\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: List[str]) -> List[str]:\n\n    cmd = [shlex.quote(i) for i in cmd]\n    cmd[0] = normexe(cmd[0])\n\n    return cmd\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cached", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nresults in a cache.", "ques_id": "62b8d27a48ba5a41d1c3f4c6", "code": ["def cached(cache, key=hashkey, lock=None):\n    def decorator(fn):\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fn(*args, **kwargs)\n                return ret\n\n        def clear_cache():\n            cache.clear()\n\n        clear_cache()\n        return wrapper\n\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(fn):\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            with lock:\n                ret = cache.get(key)\n            if ret is None:\n             ", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n            return ret\n        wrapper.cache = cache\n        wrapper.key = key\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    decorator.cache = {}\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n            except MemoizeError:\n                if lock is not None:\n                    lock.acquire()\n                try:\n                    ret = cache[key]\n                finally:\n                    if lock is not None:\n                        lock.release()\n            return ret\n        return wrapper\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        def clear_cache():\n            cache.clear()\n        clear_cache()\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    decorator.clear_cache = clear_cache\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    def wrapper(*args, **kwargs):\n        with lock:\n            return func(*args, **kwargs)\n    return wrapper\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n\n        def wrapper_lock(*args, **kwargs):\n            lock = lock or threading.Lock()\n            lock.acquire()\n            try:\n                return cache[key]\n            finally:\n                lock.release()\n\n        wrapper.__cache_lock__ = wrapper_lock\n        return wrapper\n\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n            return ret\n        wrapper.cache = cache\n        wrapper.key = key\n        wrapper.lock = lock\n        return wrapper\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        def clear_cache():\n            cache.clear()\n        clear_cache()\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    decorator.clear_cache = lock.clear\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            result = cache.get(key)\n            if result is None:\n                result = cache[key] = func(*args, **kwargs)\n            return result\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    return decorator\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(fn):\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fn(*args, **kwargs)\n                return ret\n\n        def clear_cache():\n            cache.clear()\n\n        clear_cache()\n        return wrapper\n\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(fn):\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            with lock:\n                ret = cache.get(key)\n            if ret is None:\n             ", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n            return ret\n        wrapper.cache = cache\n        wrapper.key = key\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    decorator.cache = {}\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n            except MemoizeError:\n                if lock is not None:\n                    lock.acquire()\n                try:\n                    ret = cache[key]\n                finally:\n                    if lock is not None:\n                        lock.release()\n            return ret\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        def clear_cache():\n            cache.clear()\n        clear_cache()\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    decorator.clear_cache = clear_cache\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    def wrapper(*args, **kwargs):\n        with lock:\n            return func(*args, **kwargs)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n\n        def wrapper_lock(*args, **kwargs):\n            lock = lock or threading.Lock()\n            lock.acquire()\n            try:\n                return cache[key]\n            finally:\n                lock.release()\n\n        wrapper.__cache_lock__ = wrapper_lock\n        return wrapper\n\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n            return ret\n        wrapper.cache = cache\n        wrapper.key = key\n        wrapper.lock = lock\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        def clear_cache():\n            cache.clear()\n        clear_cache()\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    decorator.clear_cache = lock.clear\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key(*args, **kwargs)\n            result = cache.get(key)\n            if result is None:\n                result = cache[key] = func(*args, **kwargs)\n            return result\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    decorator.lock = lock\n    return decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "ttl_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.", "ques_id": "62b8d24048ba5a41d1c3f49f", "code": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if maxsize == 1:\n        # No cache when there are no items.\n        return functools.partial(ttl_cache, maxsize, ttl, timer, typed)\n\n    def wrapper(f):\n        cached = LruCache(maxsize, ttl, timer)\n        if typed:\n            cached.__name__ = f.__name__\n        return functools.wraps(f)(cached)\n    return wrapper\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    # XXX: cache is a dictionary mapping keys to values.  mapping[key] will\n    #      be the value for the key if it exists.  mapping[key] is the\n    #      value for the key if it doesn't exist.\n    def wrapper(fn):\n        cached_func = _Cached(fn, maxsize, ttl, timer, typed)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return wrapper\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _Cache('TTL', maxsize, ttl, timer, typed)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if maxsize < 0:\n        raise ValueError('maxsize must be non-negative')\n    if maxsize == 0:\n        return lambda f: f\n\n    def wrapper(f):\n        return Cache(f, maxsize, ttl, timer, typed)\n    return wrapper\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    # You can use a function that returns a value without having to wrap the\n    # result of a callable (e.g. :meth:`get`) to avoid having to wrap the\n    # result of a callable wrapping the result of an LRU cache value.\n    # (An alternative would be to wrap the result of a callable that returns\n    # a value directly and call the wrapped callable with that value as the\n    # argument.)\n    if callable(maxsize):\n        return _ttl_cache_wrapper(maxsize, ttl, timer, typed)\n    else:\n        return _ttl_cache_wrapper(maxsize, ttl, timer)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    # You can make this decorator more likely to be used as a decorator.\n    # Decorator takes precedence due to monkey patching.\n    if callable(maxsize):\n        return _cached(maxsize, ttl, timer, typed)\n    else:\n        return _cached(maxsize, ttl, timer)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    # Wrap the decorator\n    if not callable(maxsize):\n        raise TypeError('maxsize must be callable')\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than zero')\n    if maxsize == 1:\n        # No cache control since there is no meaningful limit\n        return lambda f: f\n\n    def wrapper(f):\n        # Memoize the function\n        ret = Memoize(f, ttl, timer, typed)\n        ret._cache = {}\n        return ret\n\n    return wrapper\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _Cache(maxsize, ttl, timer, False, typed)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than zero')\n    if maxsize == 1:\n        # No cache when there is only one item.\n        return functools.partial(_ttl_cache, ttl=ttl, timer=timer)\n\n    def wrapper(fn):\n        cached_func = _Cached(fn, maxsize, ttl, typed)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return wrapper\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n    if maxsize == 0:\n        # Do nothing.\n        return lambda f: f\n\n    def wrapper(f):\n        return _Cache(f, maxsize, ttl, timer, typed)\n    return wrapper\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if maxsize == 1:\n        # No cache when there are no items.\n        return functools.partial(ttl_cache, maxsize, ttl, timer, typed)\n\n    def wrapper(f):\n        cached = LruCache(maxsize, ttl, timer)\n        if typed:\n            cached.__name__ = f.__name__\n        return functools.wraps(f)(cached)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    # XXX: cache is a dictionary mapping keys to values.  mapping[key] will\n    #      be the value for the key if it exists.  mapping[key] is the\n    #      value for the key if it doesn't exist.\n    def wrapper(fn):\n        cached_func = _Cached(fn, maxsize, ttl, timer, typed)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _Cache('TTL', maxsize, ttl, timer, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if maxsize < 0:\n        raise ValueError('maxsize must be non-negative')\n    if maxsize == 0:\n        return lambda f: f\n\n    def wrapper(f):\n        return Cache(f, maxsize, ttl, timer, typed)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    # You can use a function that returns a value without having to wrap the\n    # result of a callable (e.g. :meth:`get`) to avoid having to wrap the\n    # result of a callable wrapping the result of an LRU cache value.\n    # (An alternative would be to wrap the result of a callable that returns\n    # a value directly and call the wrapped callable with that value as the\n    # argument.)\n    if callable(maxsize):\n        return _ttl_cache_wrapper(maxsize, ttl, timer, typed)\n    else:\n        return _ttl_cache_wrapper(maxsize, ttl, timer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    # You can make this decorator more likely to be used as a decorator.\n    # Decorator takes precedence due to monkey patching.\n    if callable(maxsize):\n        return _cached(maxsize, ttl, timer, typed)\n    else:\n        return _cached(maxsize, ttl, timer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    # Wrap the decorator\n    if not callable(maxsize):\n        raise TypeError('maxsize must be callable')\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than zero')\n    if maxsize == 1:\n        # No cache control since there is no meaningful limit\n        return lambda f: f\n\n    def wrapper(f):\n        # Memoize the function\n        ret = Memoize(f, ttl, timer, typed)\n        ret._cache = {}\n        return ret\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _Cache(maxsize, ttl, timer, False, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than zero')\n    if maxsize == 1:\n        # No cache when there is only one item.\n        return functools.partial(_ttl_cache, ttl=ttl, timer=timer)\n\n    def wrapper(fn):\n        cached_func = _Cached(fn, maxsize, ttl, typed)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n    if maxsize == 0:\n        # Do nothing.\n        return lambda f: f\n\n    def wrapper(f):\n        return _Cache(f, maxsize, ttl, timer, typed)\n    return wrapper\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "mru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.", "ques_id": "62b8d23b48ba5a41d1c3f49a", "code": ["def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize, typed)\n    else:\n        return Cache(maxsize, typed)\n", "def mru_cache(maxsize=128, typed=False):\n    # Must be a function or method with __call__ (e.g. decorator)\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(func):\n        cached_func = _MRUWrapper(func, maxsize)\n        if typed:\n            cached_func.typed = True\n        return cached_func\n    return wrapper\n", "def mru_cache(maxsize=128, typed=False):\n    # You can use a function that returns a list without assignment. To\n    # remove a key, use del instead.\n    def wrapper(f):\n        cached_func = MruCache(f, maxsize, typed)\n        return lambda key: cached_func(key)\n    return wrapper\n", "def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize, typed)\n    else:\n        return Cache(maxsize, typed)\n", "def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize, typed)\n    else:\n        return MruCache(maxsize, typed)\n", "def mru_cache(maxsize=128, typed=False):\n    # MCache is a dict that stores the results of calling a function on every\n    # iteration of the loop, where the keys are the function names and the\n    # values are the results of calling the function for every iteration.\n    # If a function is wrapped with a mru_cache wrapper, then it will be\n    # called more than once on each iteration of the loop, and each time\n    # the value of the mru_cache gets called the result of the calling\n    # function is re-computed.\n    cache = {}\n\n    def wrapper(function):\n        key = (function.__name__, function.__module__)\n        if key in cache:\n            return cache[key]\n        else:\n            cache[key] = function\n            return function\n\n    if maxsize is None:\n        return wrapper\n    else:\n        return mru_cache(maxsize, typed)(wrapper)\n", "def mru_cache(maxsize=128, typed=False):\n    # You can use a function that returns a Deferred object in the most recent\n    # call to `get`:  do not use this decorator unless you know what you're\n    # doing.  Also, you can use a MutableMapping to store your results in a\n    # cache, so you can access the deferred object every time the cache is\n    # updated.\n\n    def decorate(fn):\n        cached_result = MruCacheResult(fn, maxsize)\n        if typed:\n            cached_result.typed = True\n        return cached_result\n    return decorate\n", "def mru_cache(maxsize=128, typed=False):\n    # You can make a decorator with `cached` so that it can be applied to a\n    # function without needing to create a new function.\n    def decorator(func):\n        cached = MruCache(maxsize, typed)\n        cached.func = func\n        return cached\n    return decorator\n", "def mru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        cache = MruCache(fn, maxsize)\n        return cache(typed)\n    return wrapper\n", "def mru_cache(maxsize=128, typed=False):\n    # You can use a function that returns a MRU value either in the\n    # header or the body of your function. To get the MRU value\n    # you must use get_mru() or you can use mru_callable(). To get\n    # the callable we need to create a wrapper that returns the value\n    # of the mru value and stores it in the mru attribute. The value\n    # returned is the value of the last result from the MRU that was\n    # read in the file. If the file is read in as a function, the returned\n    # value will be the value of the last result from the file.\n    def wrapper(maxsize, typed):\n        cache = LRUCache(maxsize)\n\n        def wrapped(key):\n            value = cache.get(key)\n            if not value:\n                if typed:\n                    value = typed(key)\n                else:\n                    value = key\n  "], "level": "project_runnable", "generate_results": [{"generate_code": "def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize, typed)\n    else:\n        return Cache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # Must be a function or method with __call__ (e.g. decorator)\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(func):\n        cached_func = _MRUWrapper(func, maxsize)\n        if typed:\n            cached_func.typed = True\n        return cached_func\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # You can use a function that returns a list without assignment. To\n    # remove a key, use del instead.\n    def wrapper(f):\n        cached_func = MruCache(f, maxsize, typed)\n        return lambda key: cached_func(key)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize, typed)\n    else:\n        return Cache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize, typed)\n    else:\n        return MruCache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # MCache is a dict that stores the results of calling a function on every\n    # iteration of the loop, where the keys are the function names and the\n    # values are the results of calling the function for every iteration.\n    # If a function is wrapped with a mru_cache wrapper, then it will be\n    # called more than once on each iteration of the loop, and each time\n    # the value of the mru_cache gets called the result of the calling\n    # function is re-computed.\n    cache = {}\n\n    def wrapper(function):\n        key = (function.__name__, function.__module__)\n        if key in cache:\n            return cache[key]\n        else:\n            cache[key] = function\n            return function\n\n    if maxsize is None:\n        return wrapper\n    else:\n        return mru_cache(maxsize, typed)(wrapper)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # You can use a function that returns a Deferred object in the most recent\n    # call to `get`:  do not use this decorator unless you know what you're\n    # doing.  Also, you can use a MutableMapping to store your results in a\n    # cache, so you can access the deferred object every time the cache is\n    # updated.\n\n    def decorate(fn):\n        cached_result = MruCacheResult(fn, maxsize)\n        if typed:\n            cached_result.typed = True\n        return cached_result\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # You can make a decorator with `cached` so that it can be applied to a\n    # function without needing to create a new function.\n    def decorator(func):\n        cached = MruCache(maxsize, typed)\n        cached.func = func\n        return cached\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        cache = MruCache(fn, maxsize)\n        return cache(typed)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # You can use a function that returns a MRU value either in the\n    # header or the body of your function. To get the MRU value\n    # you must use get_mru() or you can use mru_callable(). To get\n    # the callable we need to create a wrapper that returns the value\n    # of the mru value and stores it in the mru attribute. The value\n    # returned is the value of the last result from the MRU that was\n    # read in the file. If the file is read in as a function, the returned\n    # value will be the value of the last result from the file.\n    def wrapper(maxsize, typed):\n        cache = LRUCache(maxsize)\n\n        def wrapped(key):\n            value = cache.get(key)\n            if not value:\n                if typed:\n                    value = typed(key)\n                else:\n                    value = key\n  ", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.", "ques_id": "62b8d23948ba5a41d1c3f498", "code": ["def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LRUTypedCache(maxsize)\n    else:\n        return _LRUWrapper(maxsize)\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _TypedCache(maxsize)\n    else:\n        return _LRUCache(maxsize)\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _TypedCache(maxsize)\n    else:\n        return _Cache(maxsize)\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(lru_cache, maxsize, typed)\n    else:\n        return _LRUCache(maxsize, typed)\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LruCacheTyped(maxsize)\n    else:\n        return _LruCache(maxsize)\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be >= 1')\n\n    def wrapper(fn):\n        cached = LRUCache(maxsize, typed)\n        cached.push(fn)\n        return cached\n    return wrapper\n", "def lru_cache(maxsize=128, typed=False):\n    # This function must be wrapped with ``cache`` to enable operations\n    # on functions that return cached results.\n    if maxsize is None:\n        return _LruCache(typed)\n    else:\n        return _LruCache(maxsize, typed)\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        return LRUCache(maxsize, typed, fn)\n    return wrapper\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        cache = LruCache(maxsize, typed)\n        return cache(fn)\n    return wrapper\n", "def lru_cache(maxsize=128, typed=False):\n    # You can make this decorator shorter if you need to reduce the time\n    # complexity of your decorator.\n    if maxsize < 1:\n        return lambda f: f\n\n    def wrapper(f):\n        return LRUCache(maxsize, typed, f)\n    return wrapper\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LRUTypedCache(maxsize)\n    else:\n        return _LRUWrapper(maxsize)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _TypedCache(maxsize)\n    else:\n        return _LRUCache(maxsize)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _TypedCache(maxsize)\n    else:\n        return _Cache(maxsize)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(lru_cache, maxsize, typed)\n    else:\n        return _LRUCache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LruCacheTyped(maxsize)\n    else:\n        return _LruCache(maxsize)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be >= 1')\n\n    def wrapper(fn):\n        cached = LRUCache(maxsize, typed)\n        cached.push(fn)\n        return cached\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    # This function must be wrapped with ``cache`` to enable operations\n    # on functions that return cached results.\n    if maxsize is None:\n        return _LruCache(typed)\n    else:\n        return _LruCache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        return LRUCache(maxsize, typed, fn)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        cache = LruCache(maxsize, typed)\n        return cache(fn)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    # You can make this decorator shorter if you need to reduce the time\n    # complexity of your decorator.\n    if maxsize < 1:\n        return lambda f: f\n\n    def wrapper(f):\n        return LRUCache(maxsize, typed, f)\n    return wrapper\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lfu_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.", "ques_id": "62b8d23748ba5a41d1c3f496", "code": ["def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LFUCache(maxsize)\n    else:\n        def wrapper(f):\n            return _LFUCache(f, maxsize)\n        return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be >= 1')\n    if typed:\n        return _LFUTypeCache(maxsize)\n    else:\n        return _LFUWrapper(maxsize)\n", "def lfu_cache(maxsize=128, typed=False):\n    # You can use a Memoize object to do this automatically when you created\n    # your function.  See the Python documentation for more information about\n    # Memoize objects.\n\n    def wrapper(fn):\n        return LFUCache(maxsize, typed, fn)\n    return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(f):\n        return LFUCache(f, maxsize, typed)\n    return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(f):\n        return LFUCache(f, maxsize, typed)\n    return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LFUTypedCache(maxsize)\n    else:\n        return _LFUWrapper(maxsize)\n", "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        return LFUCache(maxsize, fn, typed)\n    return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    # You can use a function that returns a value than None and you don't need\n    # to pass it as a parameter.  It returns a value even if it is not None.\n    def wrapper(fn):\n        cached = _LRUCache(maxsize, typed)\n        def wrapped(*args):\n            key = (fn, args)\n            try:\n                return cached.data[key]\n            except KeyError:\n                ret = cached.data[key] = fn(*args)\n                return ret\n        return wrapped\n    return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LFUCache(maxsize)\n    else:\n        def wrapper(fn):\n            return _LFUCache(fn, maxsize)\n        return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        cache = LruCache(maxsize)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fn(*args, **kwargs)\n                return ret\n        wrapped.cache_clear = cache.clear\n        wrapped.maxsize = maxsize\n        return wrapped\n    if typed:\n        return wrapper\n    else:\n        return wrapper(fn)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LFUCache(maxsize)\n    else:\n        def wrapper(f):\n            return _LFUCache(f, maxsize)\n        return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be >= 1')\n    if typed:\n        return _LFUTypeCache(maxsize)\n    else:\n        return _LFUWrapper(maxsize)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # You can use a Memoize object to do this automatically when you created\n    # your function.  See the Python documentation for more information about\n    # Memoize objects.\n\n    def wrapper(fn):\n        return LFUCache(maxsize, typed, fn)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(f):\n        return LFUCache(f, maxsize, typed)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(f):\n        return LFUCache(f, maxsize, typed)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LFUTypedCache(maxsize)\n    else:\n        return _LFUWrapper(maxsize)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        return LFUCache(maxsize, fn, typed)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # You can use a function that returns a value than None and you don't need\n    # to pass it as a parameter.  It returns a value even if it is not None.\n    def wrapper(fn):\n        cached = _LRUCache(maxsize, typed)\n        def wrapped(*args):\n            key = (fn, args)\n            try:\n                return cached.data[key]\n            except KeyError:\n                ret = cached.data[key] = fn(*args)\n                return ret\n        return wrapped\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n    if typed:\n        return _LFUCache(maxsize)\n    else:\n        def wrapper(fn):\n            return _LFUCache(fn, maxsize)\n        return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than 0')\n\n    def wrapper(fn):\n        cache = LruCache(maxsize)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fn(*args, **kwargs)\n                return ret\n        wrapped.cache_clear = cache.clear\n        wrapped.maxsize = maxsize\n        return wrapped\n    if typed:\n        return wrapper\n    else:\n        return wrapper(fn)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/fifo.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair first inserted.", "ques_id": "62b8d22f48ba5a41d1c3f488", "code": ["def popitem(self):\n    try:\n        key = self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n    return (key, self.pop(key))\n", "def popitem(self):\n    return self.__order.pop()\n", "def popitem(self):\n    return self.pop()\n", "def popitem(self):\n    try:\n        return self.pop()\n    except IndexError:\n        raise KeyError('pop from empty cache')\n", "def popitem(self):\n    try:\n        return self.__order.popitem()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n", "def popitem(self):\n    return self.pop()\n", "def popitem(self):\n    try:\n        key = self.__order.pop()\n        return (key, self.pop(key))\n    except IndexError:\n        raise KeyError('pop from empty Cache')\n", "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n", "def popitem(self):\n    return self.__order.popitem()\n", "def popitem(self):\n    return self.pop()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    try:\n        key = self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n    return (key, self.pop(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__order.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.pop()\n    except IndexError:\n        raise KeyError('pop from empty cache')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__order.popitem()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = self.__order.pop()\n        return (key, self.pop(key))\n    except IndexError:\n        raise KeyError('pop from empty Cache')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__order.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.pop()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "setdefault", "docstring": "D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D", "ques_id": "62b8d22a48ba5a41d1c3f47e", "code": ["def setdefault(self, key, default=None):\n    if key not in self:\n        self[key] = default\n    return super(OrderedDict, self).setdefault(key, default)\n", "def setdefault(self, key, default=None):\n    if key not in self:\n        self[key] = default\n    return self[key]\n", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default\n", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except LookupError:\n        self[key] = default\n        return default\n", "def setdefault(self, key, default=None):\n    return super().setdefault(self._keytransform_inv(key), default)\n", "def setdefault(self, key, default=None):\n    return super().setdefault(key.lower(), default)\n", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default\n", "def setdefault(self, key, default=None):\n    if key not in self:\n        self.__setitem__(key, default)\n    return super(OrderedDict, self).setdefault(key, default)\n", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default\n", "def setdefault(self, key, default=None):\n    if key not in self:\n        self.__setitem__(key, default)\n    return super().setdefault(key, default)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def setdefault(self, key, default=None):\n    if key not in self:\n        self[key] = default\n    return super(OrderedDict, self).setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    if key not in self:\n        self[key] = default\n    return self[key]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except LookupError:\n        self[key] = default\n        return default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n    return super().setdefault(self._keytransform_inv(key), default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    return super().setdefault(key.lower(), default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n    if key not in self:\n        self.__setitem__(key, default)\n    return super(OrderedDict, self).setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n    if key not in self:\n        self.__setitem__(key, default)\n    return super().setdefault(key, default)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "get", "docstring": "D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.", "ques_id": "62b8d22948ba5a41d1c3f47c", "code": ["def get(self, key, default=None):\n    return self[key] if key in self else default\n", "def get(self, key, default=None):\n    return super().get(key.lower(), default)\n", "def get(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        return default\n", "def get(self, key, default=None):\n    return super().get(key.lower(), default)\n", "def get(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        return default\n", "def get(self, key, default=None):\n    if default is None:\n        default = self.default\n    return super().get(key.upper(), default)\n", "def get(self, key, default=None):\n    return super().get(key.lower(), default)\n", "def get(self, key, default=None):\n    return super().get(key.lower(), default)\n", "def get(self, key, default=None):\n    return self._data.get(key, default)\n", "def get(self, key, default=None):\n    return self[key] if key in self else default\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get(self, key, default=None):\n    return self[key] if key in self else default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n    return super().get(key.lower(), default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        return default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n    return super().get(key.lower(), default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        return default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n    if default is None:\n        default = self.default\n    return super().get(key.upper(), default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return super().get(key.lower(), default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return super().get(key.lower(), default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return self._data.get(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return self[key] if key in self else default\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cachedmethod", "docstring": "Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.", "ques_id": "62b8d22548ba5a41d1c3f472", "code": ["def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = threading.Lock()\n\n        def newfunc(*args, **kwds):\n            # save result in cache for wrapping\n            key = keyfunc(*args, **kwds)\n            try:\n                result = cache[key]\n            except KeyError:\n                result = cache[key] = func(*args, **kwds)\n            except TypeError:\n                # Python 3\n                result = cache[key] = func(*args, **kwds)\n            return result\n        return newfunc\n    if isinstance(cache, dict):\n        cache = cache.get(key, lock)\n        if cache is None:\n            cache = {}\n            cache[key] = cache = decorator(cache)\n      ", "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = cache.lock\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            key = key(self, *args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(self, *args, **kwargs)\n                return ret\n        return wrapper\n    return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def _wrapper(func):\n        return _CachedMethod(func, cache, key, lock)\n    return _wrapper\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = Lock()\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            key = key(self, *args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(self, *args, **kwargs)\n                return ret\n\n        return wrapper\n\n    if isinstance(cache, BaseCache):\n        return decorator(cache.get)\n    else:\n        return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = threading.Lock()\n\n        def wrapper(self, *args, **kw):\n            with lock:\n                return func(self, *args, **kw)\n        return wrapper\n    if isinstance(cache, type):\n        # We were given a class, so directly wrap the callable\n        return decorator(cache)\n    else:\n        # We were given a function, so wrap the callable with a memoized\n        # instance method.\n        return decorator(cache)(key)\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def _wrapper(func):\n        if lock is None:\n            lock = threading.Lock()\n\n        def __wrapper(self, *args, **kwargs):\n            with lock:\n                return func(self, *args, **kwargs)\n        return __wrapper\n    return _wrapper(cache)\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def _wrapper(func):\n        def __wrapper(self, *args, **kwds):\n            if not hasattr(self, '_result_cache'):\n                self._result_cache = {}\n            if lock is None:\n                lock = cache.lock\n            with lock:\n                return func(self, *args, **kwds)\n        __wrapper.__name__ = func.__name__\n        __wrapper.__doc__ = func.__doc__\n        return __wrapper\n    if not callable(key):\n        key = hashkey(key)\n        key = key(self)\n    def _wrapper(self, *args, **kwds):\n        with cache:\n            return self._result_cache.get(key, _wrapper(self, *args, **kwds))\n    _wrapper.__name__ = func.__name__\n    _wrapper.__doc__ = func.__doc__\n    return _wrapper\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def _wrapper(func):\n        def __wrapper(self, *args, **kw):\n            if not hasattr(self, '_cache'):\n                self._cache = cache\n            key = key(self, *args, **kw)\n            result = self._cache.get(key)\n            if result is None:\n                result = func(self, *args, **kw)\n                self._cache[key] = result\n            return result\n        return __wrapper\n    return _wrapper\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = threading.Lock()\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kw):\n            key = key(self, *args, **kw)\n            try:\n                result = cache[key]\n            except KeyError:\n                result = cache[key] = self._callable(*args, **kw)\n                return result\n            else:\n                if not lock.acquire(False):\n                    return result\n                try:\n                    return cache[key]\n                finally:\n                    lock.release()\n\n        return wrapper\n    return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            key = lock.acquire()\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(self, *args, **kwargs)\n                lock.release()\n                return ret\n        return wrapper\n    return decorator\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = threading.Lock()\n\n        def newfunc(*args, **kwds):\n            # save result in cache for wrapping\n            key = keyfunc(*args, **kwds)\n            try:\n                result = cache[key]\n            except KeyError:\n                result = cache[key] = func(*args, **kwds)\n            except TypeError:\n                # Python 3\n                result = cache[key] = func(*args, **kwds)\n            return result\n        return newfunc\n    if isinstance(cache, dict):\n        cache = cache.get(key, lock)\n        if cache is None:\n            cache = {}\n            cache[key] = cache = decorator(cache)\n      ", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = cache.lock\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            key = key(self, *args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(self, *args, **kwargs)\n                return ret\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def _wrapper(func):\n        return _CachedMethod(func, cache, key, lock)\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = Lock()\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            key = key(self, *args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(self, *args, **kwargs)\n                return ret\n\n        return wrapper\n\n    if isinstance(cache, BaseCache):\n        return decorator(cache.get)\n    else:\n        return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = threading.Lock()\n\n        def wrapper(self, *args, **kw):\n            with lock:\n                return func(self, *args, **kw)\n        return wrapper\n    if isinstance(cache, type):\n        # We were given a class, so directly wrap the callable\n        return decorator(cache)\n    else:\n        # We were given a function, so wrap the callable with a memoized\n        # instance method.\n        return decorator(cache)(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def _wrapper(func):\n        if lock is None:\n            lock = threading.Lock()\n\n        def __wrapper(self, *args, **kwargs):\n            with lock:\n                return func(self, *args, **kwargs)\n        return __wrapper\n    return _wrapper(cache)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def _wrapper(func):\n        def __wrapper(self, *args, **kwds):\n            if not hasattr(self, '_result_cache'):\n                self._result_cache = {}\n            if lock is None:\n                lock = cache.lock\n            with lock:\n                return func(self, *args, **kwds)\n        __wrapper.__name__ = func.__name__\n        __wrapper.__doc__ = func.__doc__\n        return __wrapper\n    if not callable(key):\n        key = hashkey(key)\n        key = key(self)\n    def _wrapper(self, *args, **kwds):\n        with cache:\n            return self._result_cache.get(key, _wrapper(self, *args, **kwds))\n    _wrapper.__name__ = func.__name__\n    _wrapper.__doc__ = func.__doc__\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def _wrapper(func):\n        def __wrapper(self, *args, **kw):\n            if not hasattr(self, '_cache'):\n                self._cache = cache\n            key = key(self, *args, **kw)\n            result = self._cache.get(key)\n            if result is None:\n                result = func(self, *args, **kw)\n                self._cache[key] = result\n            return result\n        return __wrapper\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        if lock is None:\n            lock = threading.Lock()\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kw):\n            key = key(self, *args, **kw)\n            try:\n                result = cache[key]\n            except KeyError:\n                result = cache[key] = self._callable(*args, **kw)\n                return result\n            else:\n                if not lock.acquire(False):\n                    return result\n                try:\n                    return cache[key]\n                finally:\n                    lock.release()\n\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            key = lock.acquire()\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(self, *args, **kwargs)\n                lock.release()\n                return ret\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "extostr", "docstring": "Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str", "ques_id": "62b8c517e0d34b282c18122e", "code": ["def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, SocketError):\n        if e.errno == errno.ECONNABORTED:\n            logger.warning(\"Could not connect to remote master \"\n                           \"(%s:%d). Is the master running?\",\n                           e.host, e.port)\n            return \"\", \"\"\n        else:\n            raise e\n    if isinstance(e, Exception):\n        raise e\n    return super(RemoteSocket, cls).extostr(e, max_level, max_path_level)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    \"to str method as is.\"\n    if e is None:\n        e = ''\n    # replace with repr if necessary\n    return str((e, cls.__name__))\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, Exception):\n        if isinstance(e, (OSError, IOError)):\n            if e.errno == errno.ENOENT:\n                return cls._(u'[Error] File not found.')\n            elif e.errno == errno.EACCES:\n                return cls._(u'[Error] Permission denied.')\n            elif e.errno == errno.ENOENT:\n                return cls._(u'[Error] File is not found.')\n        elif isinstance(e, TypeError):\n            return cls._(u'[Error] %s' % e)\n    return cls._(u'[Error] %s' % e)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, Exception):\n        tb = ''.join(format_exception(type(e), e, e.__traceback__))\n        return tb + '\\n' + str(e) + '\\n' + tb.strip()\n    else:\n        return str(e) + '\\n' + str(max_path_level)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, (OSError, IOError)):\n        if e.errno == errno.ENOENT:\n            if max_path_level == 0:\n                # a file does not exist => raise an error\n                raise OSError('%s: not found' % e.filename)\n            else:\n                # a file already exists => raise an error\n                raise OSError('%s: file exists already' % e.filename)\n        else:\n            if max_path_level == 0:\n                # a directory does not exist => raise an error\n                raise OSError('%s: not found' % e.dirname)\n            else:\n                # a directory already exists => raise an error\n                raise OSError('%s: file already exists' % e.dirname)\n    else:\n    ", "def extostr(cls, e, max_level=30, max_path_level=5):\n    # XXX: can we do better than str(e) or e.strerror\n    return str(e).capitalize()\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Extends the log message with information that the sol will send.\n    \"\"\"\n    s = str(e)\n    if len(s) > max_path_level:\n        s = s[:max_path_level] + \"...\"\n    return \"%s: %s\" % (cls.__name__, s)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    ext = \"\"\n    if e.exc_info and hasattr(e, 'exc_text'):\n        exc_type, exc_obj, exc_tb = e.exc_info\n        traceback.print_exception(exc_type, exc_obj, exc_tb)\n        ext = \"%s: %s\" % (exc_type.__name__, exc_obj.__class__.__name__)\n    if not ext:\n        ext = str(e)\n    return \"%s %s\" % (cls.__name__, ext)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, Exception):\n        # some kind of exception - ignore it\n        return '<exception>'\n    else:\n        return str(e).replace('\\n', '<newline>')\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    s = \"\"\n    if isinstance(e, Exception):\n        s += str(e)\n        s += \"\\n\"\n        if hasattr(e, \"traceback\"):\n            for tb in e.traceback:\n                s += str(tb) + \"\\n\"\n        if hasattr(e, \"__traceback__\"):\n            s += \"\".join(traceback.format_list(e.__traceback__))\n    else:\n        s = str(e)\n    return s\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, SocketError):\n        if e.errno == errno.ECONNABORTED:\n            logger.warning(\"Could not connect to remote master \"\n                           \"(%s:%d). Is the master running?\",\n                           e.host, e.port)\n            return \"\", \"\"\n        else:\n            raise e\n    if isinstance(e, Exception):\n        raise e\n    return super(RemoteSocket, cls).extostr(e, max_level, max_path_level)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    \"to str method as is.\"\n    if e is None:\n        e = ''\n    # replace with repr if necessary\n    return str((e, cls.__name__))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, Exception):\n        if isinstance(e, (OSError, IOError)):\n            if e.errno == errno.ENOENT:\n                return cls._(u'[Error] File not found.')\n            elif e.errno == errno.EACCES:\n                return cls._(u'[Error] Permission denied.')\n            elif e.errno == errno.ENOENT:\n                return cls._(u'[Error] File is not found.')\n        elif isinstance(e, TypeError):\n            return cls._(u'[Error] %s' % e)\n    return cls._(u'[Error] %s' % e)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, Exception):\n        tb = ''.join(format_exception(type(e), e, e.__traceback__))\n        return tb + '\\n' + str(e) + '\\n' + tb.strip()\n    else:\n        return str(e) + '\\n' + str(max_path_level)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, (OSError, IOError)):\n        if e.errno == errno.ENOENT:\n            if max_path_level == 0:\n                # a file does not exist => raise an error\n                raise OSError('%s: not found' % e.filename)\n            else:\n                # a file already exists => raise an error\n                raise OSError('%s: file exists already' % e.filename)\n        else:\n            if max_path_level == 0:\n                # a directory does not exist => raise an error\n                raise OSError('%s: not found' % e.dirname)\n            else:\n                # a directory already exists => raise an error\n                raise OSError('%s: file already exists' % e.dirname)\n    else:\n    ", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    # XXX: can we do better than str(e) or e.strerror\n    return str(e).capitalize()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Extends the log message with information that the sol will send.\n    \"\"\"\n    s = str(e)\n    if len(s) > max_path_level:\n        s = s[:max_path_level] + \"...\"\n    return \"%s: %s\" % (cls.__name__, s)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    ext = \"\"\n    if e.exc_info and hasattr(e, 'exc_text'):\n        exc_type, exc_obj, exc_tb = e.exc_info\n        traceback.print_exception(exc_type, exc_obj, exc_tb)\n        ext = \"%s: %s\" % (exc_type.__name__, exc_obj.__class__.__name__)\n    if not ext:\n        ext = str(e)\n    return \"%s %s\" % (cls.__name__, ext)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, Exception):\n        # some kind of exception - ignore it\n        return '<exception>'\n    else:\n        return str(e).replace('\\n', '<newline>')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    s = \"\"\n    if isinstance(e, Exception):\n        s += str(e)\n        s += \"\\n\"\n        if hasattr(e, \"traceback\"):\n            for tb in e.traceback:\n                s += str(tb) + \"\\n\"\n        if hasattr(e, \"__traceback__\"):\n            s += \"\".join(traceback.format_list(e.__traceback__))\n    else:\n        s = str(e)\n    return s\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "append_text_to_file", "docstring": "Write to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int", "ques_id": "62b8bbbfe0d34b282c181210", "code": ["def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not os.path.isfile(file_name) or overwrite:\n        logger.info('Writing text to file: %s', file_name)\n        with codecs.open(file_name, 'wb+', encoding) as f:\n            f.write(text_buffer)\n    return len(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug('Appending text to file {}'.format(file_name))\n    if not isinstance(text_buffer, bytes):\n        raise TypeError('append_text_to_file() requires a bytes argument')\n    if not isinstance(encoding, str):\n        raise TypeError('append_text_to_file() requires a str encoding parameter')\n    if overwrite or not os.path.isfile(file_name):\n        with open(file_name, 'wb') as file_obj:\n            file_obj.write(text_buffer)\n        return len(text_buffer)\n    else:\n        return 0\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug(\"Appending text to file: '%s'\", file_name)\n    if not overwrite and os.path.isfile(file_name):\n        logger.warning(\"File '%s' already exists, skipping\", file_name)\n        return 0\n\n    if os.path.isfile(file_name):\n        if os.path.getsize(file_name) == 0:\n            logger.info(\"Skipping '%s' because it is empty.\", file_name)\n            return 0\n\n    with codecs.open(file_name, 'ab', encoding) as file_handle:\n        file_handle.write(text_buffer)\n\n    return len(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug(\"Appending text to file: '{0}'\".format(file_name))\n    if not overwrite and os.path.isfile(file_name):\n        logger.debug(\"File '{0}' already exists, no need to overwrite\".format(file_name))\n        return 0\n    with open(file_name, 'wb') as f:\n        f.write(text_buffer)\n    return len(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug('Appending text to file: %s', file_name)\n    if not overwrite and os.path.isfile(file_name):\n        logger.warning('File %s already exists. Use overwrite=True to overwrite it.', file_name)\n        return 0\n\n    file_size = self.get_file_size(file_name)\n\n    with open(file_name, 'wb') as f:\n        f.write(text_buffer)\n\n    return file_size\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug('append_text_to_file(%s, %s, %s, overwrite=%s)',\n                 file_name, text_buffer, encoding, overwrite)\n\n    if not overwrite and isfile(file_name):\n        logger.debug('File %s already exists', file_name)\n        return 0\n\n    if isfile(file_name):\n        logger.debug('File %s is already exists', file_name)\n        return -1\n\n    # Create the file if it doesn't exist\n    with open(file_name, 'wb') as f:\n        f.write(text_buffer)\n\n    return len(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug(\"append_text_to_file(%s, %s, %s, %s)\", file_name, text_buffer, encoding, overwrite)\n    if not overwrite and os.path.exists(file_name):\n        raise FileExistsError(\"File '{}' already exists.\".format(file_name))\n\n    return write_bytes(file_name, text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.info(\"Appending text to file: {}\".format(file_name))\n    if not overwrite and os.path.isfile(file_name):\n        logger.warning(\"File {} already exists. Skipping.\".format(file_name))\n        return 0\n    with codecs.open(file_name, 'wb', encoding=encoding) as f:\n        f.write(text_buffer)\n    return len(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not os.path.exists(os.path.dirname(file_name)):\n        logger.debug(\"Create directory {}\".format(os.path.dirname(file_name)))\n        os.makedirs(os.path.dirname(file_name))\n    if os.path.exists(file_name) and not overwrite:\n        logger.debug(\"File {} already exists\".format(file_name))\n        return 0\n    logger.debug(\"Writing {} to file {}\".format(text_buffer.encode(encoding), file_name))\n    with open(file_name, 'wb') as file_handle:\n        file_handle.write(text_buffer)\n    return len(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not isinstance(text_buffer, bytearray):\n        raise TypeError(\"text_buffer must be of type bytearray\")\n    if not isinstance(encoding, str):\n        raise TypeError(\"encoding must be of type str\")\n    if encoding not in ('utf-8', 'iso-8859-1'):\n        raise ValueError(\"encoding must be one of 'iso-8859-1' or 'utf-8'\")\n\n    logger.debug(\"Writing text to file: {}\".format(file_name))\n    if not overwrite:\n        if os.path.exists(file_name):\n            logger.warning(\"File {} already exists, skipping\".format(file_name))\n            return 0\n\n    with open(file_name, 'wb') as f:\n        f.write(text_buffer)\n\n    return len(text_buffer)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not os.path.isfile(file_name) or overwrite:\n        logger.info('Writing text to file: %s', file_name)\n        with codecs.open(file_name, 'wb+', encoding) as f:\n            f.write(text_buffer)\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug('Appending text to file {}'.format(file_name))\n    if not isinstance(text_buffer, bytes):\n        raise TypeError('append_text_to_file() requires a bytes argument')\n    if not isinstance(encoding, str):\n        raise TypeError('append_text_to_file() requires a str encoding parameter')\n    if overwrite or not os.path.isfile(file_name):\n        with open(file_name, 'wb') as file_obj:\n            file_obj.write(text_buffer)\n        return len(text_buffer)\n    else:\n        return 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug(\"Appending text to file: '%s'\", file_name)\n    if not overwrite and os.path.isfile(file_name):\n        logger.warning(\"File '%s' already exists, skipping\", file_name)\n        return 0\n\n    if os.path.isfile(file_name):\n        if os.path.getsize(file_name) == 0:\n            logger.info(\"Skipping '%s' because it is empty.\", file_name)\n            return 0\n\n    with codecs.open(file_name, 'ab', encoding) as file_handle:\n        file_handle.write(text_buffer)\n\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug(\"Appending text to file: '{0}'\".format(file_name))\n    if not overwrite and os.path.isfile(file_name):\n        logger.debug(\"File '{0}' already exists, no need to overwrite\".format(file_name))\n        return 0\n    with open(file_name, 'wb') as f:\n        f.write(text_buffer)\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug('Appending text to file: %s', file_name)\n    if not overwrite and os.path.isfile(file_name):\n        logger.warning('File %s already exists. Use overwrite=True to overwrite it.', file_name)\n        return 0\n\n    file_size = self.get_file_size(file_name)\n\n    with open(file_name, 'wb') as f:\n        f.write(text_buffer)\n\n    return file_size\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug('append_text_to_file(%s, %s, %s, overwrite=%s)',\n                 file_name, text_buffer, encoding, overwrite)\n\n    if not overwrite and isfile(file_name):\n        logger.debug('File %s already exists', file_name)\n        return 0\n\n    if isfile(file_name):\n        logger.debug('File %s is already exists', file_name)\n        return -1\n\n    # Create the file if it doesn't exist\n    with open(file_name, 'wb') as f:\n        f.write(text_buffer)\n\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.debug(\"append_text_to_file(%s, %s, %s, %s)\", file_name, text_buffer, encoding, overwrite)\n    if not overwrite and os.path.exists(file_name):\n        raise FileExistsError(\"File '{}' already exists.\".format(file_name))\n\n    return write_bytes(file_name, text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    logger.info(\"Appending text to file: {}\".format(file_name))\n    if not overwrite and os.path.isfile(file_name):\n        logger.warning(\"File {} already exists. Skipping.\".format(file_name))\n        return 0\n    with codecs.open(file_name, 'wb', encoding=encoding) as f:\n        f.write(text_buffer)\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not os.path.exists(os.path.dirname(file_name)):\n        logger.debug(\"Create directory {}\".format(os.path.dirname(file_name)))\n        os.makedirs(os.path.dirname(file_name))\n    if os.path.exists(file_name) and not overwrite:\n        logger.debug(\"File {} already exists\".format(file_name))\n        return 0\n    logger.debug(\"Writing {} to file {}\".format(text_buffer.encode(encoding), file_name))\n    with open(file_name, 'wb') as file_handle:\n        file_handle.write(text_buffer)\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not isinstance(text_buffer, bytearray):\n        raise TypeError(\"text_buffer must be of type bytearray\")\n    if not isinstance(encoding, str):\n        raise TypeError(\"encoding must be of type str\")\n    if encoding not in ('utf-8', 'iso-8859-1'):\n        raise ValueError(\"encoding must be one of 'iso-8859-1' or 'utf-8'\")\n\n    logger.debug(\"Writing text to file: {}\".format(file_name))\n    if not overwrite:\n        if os.path.exists(file_name):\n            logger.warning(\"File {} already exists, skipping\".format(file_name))\n            return 0\n\n    with open(file_name, 'wb') as f:\n        f.write(text_buffer)\n\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "file_to_textbuffer", "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str", "ques_id": "62b8bbbfe0d34b282c18120f", "code": ["def file_to_textbuffer(file_name, encoding):\n    logger.debug(\"file_to_textbuffer: %s, %s\", file_name, encoding)\n    try:\n        with codecs.open(file_name, 'rb', encoding) as f:\n            text_buffer = f.read()\n    except EnvironmentError as e:\n        text_buffer = None\n        logger.error(\"file_to_textbuffer: %s, %s\", file_name, e)\n    return text_buffer\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, \"rb\", encoding) as f:\n            buf = f.read()\n    except (IOError, OSError) as e:\n        logger.error(\"Could not read file %s: %s\", file_name, e)\n        return None\n    return buf\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        text_buffer = codecs.open(file_name, \"rb\", encoding).read()\n        return text_buffer\n    except IOError:\n        logger.debug(\"File '%s' could not be opened\", file_name)\n        return None\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, 'r', encoding) as f:\n            text = f.read()\n    except UnicodeDecodeError:\n        logger.error('File %s is not UTF-8 encoded or cannot be read', file_name)\n        return None\n    return text\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, mode='rb', encoding=encoding) as f:\n            text_buffer = f.read()\n    except IOError:\n        logger.error('File \"%s\" could not be read. Try again.', file_name)\n        return None\n    return text_buffer\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, mode='rb', encoding=encoding) as f:\n            text_buf = f.read()\n    except Exception as err:\n        logger.error('Error reading file {}: {}'.format(file_name, err))\n        return None\n    return text_buf\n", "def file_to_textbuffer(file_name, encoding):\n    logger.debug(\"file_to_textbuffer(%s, %s)\" % (file_name, encoding))\n\n    try:\n        file_handle = codecs.open(file_name, \"rb\", encoding)\n    except IOError:\n        logger.error(\"Unable to open file %s\" % file_name)\n        return None\n\n    text_buffer = file_handle.read().decode(encoding)\n    file_handle.close()\n\n    return text_buffer\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, \"rb\", encoding) as f:\n            buf = f.read()\n    except (IOError, OSError, UnicodeDecodeError):\n        buf = None\n    return buf\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, 'r', encoding) as f:\n            text_buffer = f.read()\n            logger.debug('loaded file {0} as textbuffer'.format(file_name))\n            return text_buffer\n    except IOError as e:\n        logger.error('unable to read file {0} ({1})'.format(\n            file_name, e))\n        return None\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        text_buffer = codecs.open(file_name, 'r', encoding).read()\n    except UnicodeDecodeError:\n        logger.error(\"File {} does not appear to be a valid UTF-8 file\".format(file_name))\n        text_buffer = None\n    return text_buffer\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def file_to_textbuffer(file_name, encoding):\n    logger.debug(\"file_to_textbuffer: %s, %s\", file_name, encoding)\n    try:\n        with codecs.open(file_name, 'rb', encoding) as f:\n            text_buffer = f.read()\n    except EnvironmentError as e:\n        text_buffer = None\n        logger.error(\"file_to_textbuffer: %s, %s\", file_name, e)\n    return text_buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, \"rb\", encoding) as f:\n            buf = f.read()\n    except (IOError, OSError) as e:\n        logger.error(\"Could not read file %s: %s\", file_name, e)\n        return None\n    return buf\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        text_buffer = codecs.open(file_name, \"rb\", encoding).read()\n        return text_buffer\n    except IOError:\n        logger.debug(\"File '%s' could not be opened\", file_name)\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, 'r', encoding) as f:\n            text = f.read()\n    except UnicodeDecodeError:\n        logger.error('File %s is not UTF-8 encoded or cannot be read', file_name)\n        return None\n    return text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, mode='rb', encoding=encoding) as f:\n            text_buffer = f.read()\n    except IOError:\n        logger.error('File \"%s\" could not be read. Try again.', file_name)\n        return None\n    return text_buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, mode='rb', encoding=encoding) as f:\n            text_buf = f.read()\n    except Exception as err:\n        logger.error('Error reading file {}: {}'.format(file_name, err))\n        return None\n    return text_buf\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    logger.debug(\"file_to_textbuffer(%s, %s)\" % (file_name, encoding))\n\n    try:\n        file_handle = codecs.open(file_name, \"rb\", encoding)\n    except IOError:\n        logger.error(\"Unable to open file %s\" % file_name)\n        return None\n\n    text_buffer = file_handle.read().decode(encoding)\n    file_handle.close()\n\n    return text_buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, \"rb\", encoding) as f:\n            buf = f.read()\n    except (IOError, OSError, UnicodeDecodeError):\n        buf = None\n    return buf\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        with codecs.open(file_name, 'r', encoding) as f:\n            text_buffer = f.read()\n            logger.debug('loaded file {0} as textbuffer'.format(file_name))\n            return text_buffer\n    except IOError as e:\n        logger.error('unable to read file {0} ({1})'.format(\n            file_name, e))\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        text_buffer = codecs.open(file_name, 'r', encoding).read()\n    except UnicodeDecodeError:\n        logger.error(\"File {} does not appear to be a valid UTF-8 file\".format(file_name))\n        text_buffer = None\n    return text_buffer\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "is_file_exist", "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool", "ques_id": "62b8bbbce0d34b282c18120d", "code": ["def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    if os.path.isfile(file_name):\n        logger.debug(\"File {} exist \".format(file_name))\n        return True\n    else:\n        logger.debug(\"File {} does not exist\".format(file_name))\n        return False\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    logger.debug(\"file_name: {}\".format(file_name))\n\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    if not os.path.isfile(file_name):\n        logger.error('File {} does not exist.'.format(file_name))\n        return False\n    else:\n        return True\n", "def is_file_exist(file_name):\n    if os.path.isfile(file_name):\n        logger.info(\"File '%s' exist!\" % file_name)\n        return True\n    else:\n        logger.info(\"File '%s' does not exist!\" % file_name)\n        return False\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    logger.debug('is_file_exist called with {}'.format(file_name))\n\n    if not os.path.isfile(file_name):\n        logger.debug('{} is not a file'.format(file_name))\n        return False\n    else:\n        return True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    if os.path.isfile(file_name):\n        logger.debug(\"File {} exist \".format(file_name))\n        return True\n    else:\n        logger.debug(\"File {} does not exist\".format(file_name))\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    logger.debug(\"file_name: {}\".format(file_name))\n\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    if not os.path.isfile(file_name):\n        logger.error('File {} does not exist.'.format(file_name))\n        return False\n    else:\n        return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    if os.path.isfile(file_name):\n        logger.info(\"File '%s' exist!\" % file_name)\n        return True\n    else:\n        logger.info(\"File '%s' does not exist!\" % file_name)\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    logger.debug('is_file_exist called with {}'.format(file_name))\n\n    if not os.path.isfile(file_name):\n        logger.debug('{} is not a file'.format(file_name))\n        return False\n    else:\n        return True\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "_reset_logging", "docstring": "Reset", "ques_id": "62b8b99de0d34b282c1811f8", "code": ["def _reset_logging(cls):\n    if cls._logging is not None:\n        logging.getLogger().removeHandler(cls._logging)\n        cls._logging = None\n", "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n    logging.basicConfig()\n", "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n", "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n    logging.StreamHandler.close = _close_logging\n", "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n    logging.StreamHandler.emit = lambda self, *args, **kwargs: None\n", "def _reset_logging(cls):\n    logging.config.fileConfig('logging.conf')\n", "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n    logging.StreamHandler.emit = []\n    logging.getLogger().setLevel(logging.NOTSET)\n", "def _reset_logging(cls):\n    logging.config.fileConfig('logging.conf')\n", "def _reset_logging(cls):\n    if cls._log_level is not None:\n        logging.getLogger().setLevel(cls._log_level)\n", "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _reset_logging(cls):\n    if cls._logging is not None:\n        logging.getLogger().removeHandler(cls._logging)\n        cls._logging = None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n    logging.basicConfig()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n    logging.StreamHandler.close = _close_logging\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n    logging.StreamHandler.emit = lambda self, *args, **kwargs: None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.config.fileConfig('logging.conf')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n    logging.StreamHandler.emit = []\n    logging.getLogger().setLevel(logging.NOTSET)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.config.fileConfig('logging.conf')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    if cls._log_level is not None:\n        logging.getLogger().setLevel(cls._log_level)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/tests/test_declarations.py", "project": "pexip/os-zope", "name": "_getTargetClass", "docstring": "Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.", "ques_id": "62b8b59feb7e40a82d2d1291", "code": ["def _getTargetClass(self):\n    return NoOpImplementation\n", "def _getTargetClass(self):\n    return CompressedBytesIO\n", "def _getTargetClass(self):\n    return logging.StreamHandler\n", "def _getTargetClass(self):\n    return Dataset\n", "def _getTargetClass(self):\n    return html.Iframe\n", "def _getTargetClass(self):\n    return html.Target\n", "def _getTargetClass(self):\n    return FilePath\n", "def _getTargetClass(self):\n    return html.Link\n", "def _getTargetClass(self):\n    return FakeTargetClass\n", "def _getTargetClass(self):\n    return FileStorage\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _getTargetClass(self):\n    return NoOpImplementation\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return CompressedBytesIO\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return logging.StreamHandler\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return Dataset\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return html.Iframe\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return html.Target\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return FilePath\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return html.Link\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return FakeTargetClass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return FileStorage\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/ro.py", "project": "pexip/os-zope", "name": "_legacy_mergeOrderings", "docstring": "Merge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']", "ques_id": "62b8b590eb7e40a82d2d1275", "code": ["def _legacy_mergeOrderings(orderings):\n    # TODO: this is a terrible hack\n    # we need to get rid of the first element in the list and replace it\n    # with the second one to prevent duplicate merging\n    result = []\n    for ordering in orderings:\n        if len(ordering) == 1:\n            result.append(ordering[0])\n        else:\n            prefix = ordering[0][0]\n            for i in range(1, len(ordering)):\n                if ordering[i][0] == prefix:\n                    ordering[i] = ordering[i][1:]\n                else:\n                    break\n            result.append(ordering[0])\n    return result\n", "def _legacy_mergeOrderings(orderings):\n    if not orderings:\n        return []\n\n    if len(orderings) == 1:\n        return orderings[0]\n\n    prefixes = [o.get('prefix') for o in orderings]\n    suffixes = [o.get('suffix') for o in orderings]\n\n    for prefix in prefixes:\n        for suffix in suffixes:\n            if prefix == suffix:\n                continue\n\n            if prefix not in prefixes or suffix not in prefixes:\n                raise ValueError('Prefix \"%s\" is not in ordering for \"%s\"' % (prefix, orderings[0].get('name')))\n\n            for i in range(len(orderings)):\n                if prefix in orderings[i] and suffix in orderings[i]:\n                    orderings[i]['suffix'] = min(orderings[i]['suffix'], suffixes[i])\n                    ordering", "def _legacy_mergeOrderings(orderings):\n    result = []\n    for ordering in orderings:\n        result.extend(ordering)\n    return result\n", "def _legacy_mergeOrderings(orderings):\n    result = []\n    for ordering in orderings:\n        for otherOrdering in result:\n            if otherOrdering.startswith(ordering):\n                raise ValueError(\n                    \"Duplicate ordering '%s' in '%s'\" % (otherOrdering, ordering))\n        result += orderings\n    return result\n", "def _legacy_mergeOrderings(orderings):\n    common_suffixes = []\n    for ordering in reversed(orderings):\n        for other_ordering in reversed(orderings):\n            if ordering[0] == other_ordering[0] and ordering[1] >= other_ordering[1]:\n                common_suffixes.insert(0, ordering[1])\n            elif ordering[0] == other_ordering[1] and ordering[1] <= other_ordering[0]:\n                common_suffixes.insert(1, ordering[1])\n    return common_suffixes\n", "def _legacy_mergeOrderings(orderings):\n    mergedOrderings = []\n    current = orderings[0]\n    for ordering in orderings[1:]:\n        while len(current) >= len(ordering):\n            mergedOrderings.append(current[0])\n            current = current[1:]\n        mergedOrderings.append(ordering)\n    return mergedOrderings\n", "def _legacy_mergeOrderings(orderings):\n    dups = []\n    for o in orderings:\n        for d in o:\n            if d not in dups:\n                dups.append(d)\n    return dups\n", "def _legacy_mergeOrderings(orderings):\n    orderings = [orderings] if not isinstance(orderings, list) else orderings\n    return sorted([o for o in orderings if _is_in_orderings(o)] +\n                  [o for o in orderings if not _is_in_orderings(o)])\n", "def _legacy_mergeOrderings(orderings):\n    merged = []\n\n    for ordering in orderings:\n        if not ordering:\n            continue\n\n        for o in ordering:\n            if o not in merged:\n                merged.append(o)\n\n    return merged\n", "def _legacy_mergeOrderings(orderings):\n    # first get rid of any duplicates\n    dups = []\n    for o in orderings:\n        for e in o:\n            if e not in dups:\n                dups.append(e)\n    return dups\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _legacy_mergeOrderings(orderings):\n    # TODO: this is a terrible hack\n    # we need to get rid of the first element in the list and replace it\n    # with the second one to prevent duplicate merging\n    result = []\n    for ordering in orderings:\n        if len(ordering) == 1:\n            result.append(ordering[0])\n        else:\n            prefix = ordering[0][0]\n            for i in range(1, len(ordering)):\n                if ordering[i][0] == prefix:\n                    ordering[i] = ordering[i][1:]\n                else:\n                    break\n            result.append(ordering[0])\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    if not orderings:\n        return []\n\n    if len(orderings) == 1:\n        return orderings[0]\n\n    prefixes = [o.get('prefix') for o in orderings]\n    suffixes = [o.get('suffix') for o in orderings]\n\n    for prefix in prefixes:\n        for suffix in suffixes:\n            if prefix == suffix:\n                continue\n\n            if prefix not in prefixes or suffix not in prefixes:\n                raise ValueError('Prefix \"%s\" is not in ordering for \"%s\"' % (prefix, orderings[0].get('name')))\n\n            for i in range(len(orderings)):\n                if prefix in orderings[i] and suffix in orderings[i]:\n                    orderings[i]['suffix'] = min(orderings[i]['suffix'], suffixes[i])\n                    ordering", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    result = []\n    for ordering in orderings:\n        result.extend(ordering)\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    result = []\n    for ordering in orderings:\n        for otherOrdering in result:\n            if otherOrdering.startswith(ordering):\n                raise ValueError(\n                    \"Duplicate ordering '%s' in '%s'\" % (otherOrdering, ordering))\n        result += orderings\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    common_suffixes = []\n    for ordering in reversed(orderings):\n        for other_ordering in reversed(orderings):\n            if ordering[0] == other_ordering[0] and ordering[1] >= other_ordering[1]:\n                common_suffixes.insert(0, ordering[1])\n            elif ordering[0] == other_ordering[1] and ordering[1] <= other_ordering[0]:\n                common_suffixes.insert(1, ordering[1])\n    return common_suffixes\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    mergedOrderings = []\n    current = orderings[0]\n    for ordering in orderings[1:]:\n        while len(current) >= len(ordering):\n            mergedOrderings.append(current[0])\n            current = current[1:]\n        mergedOrderings.append(ordering)\n    return mergedOrderings\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    dups = []\n    for o in orderings:\n        for d in o:\n            if d not in dups:\n                dups.append(d)\n    return dups\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    orderings = [orderings] if not isinstance(orderings, list) else orderings\n    return sorted([o for o in orderings if _is_in_orderings(o)] +\n                  [o for o in orderings if not _is_in_orderings(o)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    merged = []\n\n    for ordering in orderings:\n        if not ordering:\n            continue\n\n        for o in ordering:\n            if o not in merged:\n                merged.append(o)\n\n    return merged\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    # first get rid of any duplicates\n    dups = []\n    for o in orderings:\n        for e in o:\n            if e not in dups:\n                dups.append(e)\n    return dups\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "directlyProvidedBy", "docstring": "Return the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.", "ques_id": "62b8b58deb7e40a82d2d1269", "code": ["def directlyProvidedBy(object):\n    providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is not None:\n        return providedBy\n    directlyProvides = getattr(object, 'directlyProvides', None)\n    if directlyProvides is not None:\n        return directlyProvides\n    alsoProvides = getattr(object, 'alsoProvides', None)\n    if alsoProvides is not None:\n        return alsoProvides\n    next = getattr(object, '__next__', None)\n    if next is not None:\n        return providedBy(next())\n    raise TypeError('%s has no direct attribute or implementation' % (\n        object.__class__.__name__,))\n", "def directlyProvidedBy(object):\n    provided = getattr(object, '__provides__', None)\n    if provided is None:\n        raise TypeError('%s has no direct providing method' % (object,))\n    return provided\n", "def directlyProvidedBy(object):\n    spec = providedBy(object)\n    if spec is _empty:\n        return _next_super_class(object)\n    else:\n        return spec\n", "def directlyProvidedBy(object):\n    providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is not None:\n        return providedBy\n    spec = _implements_name(object)\n    if spec is not None:\n        providedBy = Provides(spec)\n        setattr(object, '__providedBy__', providedBy)\n        return providedBy\n    raise TypeError('%s has no direct attribute or method' % (object,))\n", "def directlyProvidedBy(object):\n    spec = providedBy(object)\n    if spec is _empty:\n        return _ADVICE_ERROR\n    else:\n        return spec.__of__(object)\n", "def directlyProvidedBy(object):\n    providedBy = _implements_name(object)\n    if providedBy is _empty:\n        raise AttributeError(_ADVICE_ERROR)\n    return providedBy\n", "def directlyProvidedBy(object):\n    spec = providedBy(object)\n    if spec is None:\n        return _ADVICE_ERROR\n    return InterfaceSpecification(spec, None)\n", "def directlyProvidedBy(object):\n    providedBy = providedByInstance(object)\n    if providedBy is _empty:\n        raise TypeError(_ADVICE_ERROR % (object.__name__,))\n    return implementations(providedBy)\n", "def directlyProvidedBy(object):\n    provided = _implements(object, interfaces.IDeclaration)\n    if provided is _empty:\n        raise TypeError(_ADVICE_ERROR)\n    return provided\n", "def directlyProvidedBy(object):\n    spec = providedBy(object)\n    if spec is _empty:\n        return _ADVICE_ERROR\n    else:\n        return spec.__of__(object)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is not None:\n        return providedBy\n    directlyProvides = getattr(object, 'directlyProvides', None)\n    if directlyProvides is not None:\n        return directlyProvides\n    alsoProvides = getattr(object, 'alsoProvides', None)\n    if alsoProvides is not None:\n        return alsoProvides\n    next = getattr(object, '__next__', None)\n    if next is not None:\n        return providedBy(next())\n    raise TypeError('%s has no direct attribute or implementation' % (\n        object.__class__.__name__,))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    provided = getattr(object, '__provides__', None)\n    if provided is None:\n        raise TypeError('%s has no direct providing method' % (object,))\n    return provided\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    spec = providedBy(object)\n    if spec is _empty:\n        return _next_super_class(object)\n    else:\n        return spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is not None:\n        return providedBy\n    spec = _implements_name(object)\n    if spec is not None:\n        providedBy = Provides(spec)\n        setattr(object, '__providedBy__', providedBy)\n        return providedBy\n    raise TypeError('%s has no direct attribute or method' % (object,))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    spec = providedBy(object)\n    if spec is _empty:\n        return _ADVICE_ERROR\n    else:\n        return spec.__of__(object)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = _implements_name(object)\n    if providedBy is _empty:\n        raise AttributeError(_ADVICE_ERROR)\n    return providedBy\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    spec = providedBy(object)\n    if spec is None:\n        return _ADVICE_ERROR\n    return InterfaceSpecification(spec, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = providedByInstance(object)\n    if providedBy is _empty:\n        raise TypeError(_ADVICE_ERROR % (object.__name__,))\n    return implementations(providedBy)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    provided = _implements(object, interfaces.IDeclaration)\n    if provided is _empty:\n        raise TypeError(_ADVICE_ERROR)\n    return provided\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    spec = providedBy(object)\n    if spec is _empty:\n        return _ADVICE_ERROR\n    else:\n        return spec.__of__(object)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "minimalBases", "docstring": "Reduce a list of base classes to its ordered minimum equivalent", "ques_id": "62b8b559eb7e40a82d2d11f8", "code": ["def minimalBases(classes):\n    minBases = classes[:]\n    while minBases:\n        classes = minBases\n        minBases = classes[:]\n    return classes\n", "def minimalBases(classes):\n    if len(classes) == 1:\n        return classes\n\n    classes.sort(key=lambda x: x.order)\n    result = []\n    for c in classes:\n        for m in classes:\n            if c is m or c.order < m.order:\n                break\n        else:\n            result.append(c)\n    return result\n", "def minimalBases(classes):\n    classes.sort()\n    l = []\n    for c in classes:\n        if l and c < l[0]:\n            l[0] = c\n        elif not l and c > l[0]:\n            l.append(c)\n    return l\n", "def minimalBases(classes):\n    classes = [classes[0]] + [base for base in classes[1:] if base is not classes[0]]\n    return sorted(classes, key=lambda x: x.order)\n", "def minimalBases(classes):\n    mapping = {}\n    for c in classes:\n        if c not in mapping:\n            mapping[c] = 1\n        else:\n            mapping[c] += 1\n    mapping = sorted(mapping.items(), key=lambda x: x[1])\n    return [c for c, in mapping]\n", "def minimalBases(classes):\n    return sorted(classes, key=lambda c: c.order)\n", "def minimalBases(classes):\n    minBases = []\n    for cls in classes:\n        if cls.order < min(cls.bases):\n            minBases = [cls]\n        elif cls.order == min(cls.bases):\n            minBases.appendcls(cls)\n    return minBases\n", "def minimalBases(classes):\n    result = []\n    for base in classes:\n        bases = [base]\n        while bases:\n            remaining = bases[0:len(bases) // 2]\n            bases = bases[len(bases) // 2:]\n            if remaining:\n                bases.append(remaining[0])\n            result.append(base)\n    return result\n", "def minimalBases(classes):\n    min_class = None\n    min_order = 9999\n\n    for base in classes:\n        if base is not None:\n            order = base.order\n            if order < min_order:\n                min_class = base\n                min_order = order\n\n    return min_class\n", "def minimalBases(classes):\n    bases = []\n    for classe in classes:\n        bases += classe.__bases__\n\n    if len(bases) == 0:\n        return classes\n\n    if len(bases) == 1:\n        return bases[0]\n\n    minBase = min(bases)\n    for base in bases:\n        if base == minBase:\n            continue\n        elif base!= minBase:\n            return base\n\n    return minBase\n"], "level": "self_contained", "generate_results": [{"generate_code": "def minimalBases(classes):\n    if len(classes) == 1:\n        return classes\n\n    classes.sort(key=lambda x: x.order)\n    result = []\n    for c in classes:\n        for m in classes:\n            if c is m or c.order < m.order:\n                break\n        else:\n            result.append(c)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    classes.sort()\n    l = []\n    for c in classes:\n        if l and c < l[0]:\n            l[0] = c\n        elif not l and c > l[0]:\n            l.append(c)\n    return l\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    classes = [classes[0]] + [base for base in classes[1:] if base is not classes[0]]\n    return sorted(classes, key=lambda x: x.order)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    mapping = {}\n    for c in classes:\n        if c not in mapping:\n            mapping[c] = 1\n        else:\n            mapping[c] += 1\n    mapping = sorted(mapping.items(), key=lambda x: x[1])\n    return [c for c, in mapping]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    return sorted(classes, key=lambda c: c.order)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    minBases = []\n    for cls in classes:\n        if cls.order < min(cls.bases):\n            minBases = [cls]\n        elif cls.order == min(cls.bases):\n            minBases.appendcls(cls)\n    return minBases\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    min_class = None\n    min_order = 9999\n\n    for base in classes:\n        if base is not None:\n            order = base.order\n            if order < min_order:\n                min_class = base\n                min_order = order\n\n    return min_class\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    bases = []\n    for classe in classes:\n        bases += classe.__bases__\n\n    if len(bases) == 0:\n        return classes\n\n    if len(bases) == 1:\n        return bases[0]\n\n    minBase = min(bases)\n    for base in bases:\n        if base == minBase:\n            continue\n        elif base!= minBase:\n            return base\n\n    return minBase\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "namesAndDescriptions", "docstring": "Return attribute names and descriptions defined by interface.", "ques_id": "62b8b4b9eb7e40a82d2d1134", "code": ["def namesAndDescriptions(self, all=False):\n    if all:\n        return tuple(i.__name__ for i in self.__ibmodule__.getInterfaces())\n    return tuple(i.__name__ for i in self.__ibmodule__.getInterfaces()\n                 if i.providedBy(self))\n", "def namesAndDescriptions(self, all=False):\n    result = []\n    for base in reversed(self.__bases__):\n        result.extend(base.namesAndDescriptions(all=all))\n    if self.__ibmodule__:\n        result.append('%s.%s' % (self.__module__, self.__ibmodule__.__name__))\n    result.extend(self.__attrs.keys())\n    if all:\n        return result\n    return reversed(result)\n", "def namesAndDescriptions(self, all=False):\n    if self.__ibmodule__ is None:\n        raise TypeError(\n            \"Interface %s has no descriptions and cannot be used directly\"\n            % self.__class__.__name__)\n    d = {}\n    for name in self.__iro__:\n        d[name] = getattr(self, name, None)\n    d.update(self.__ibmodule__.direct(self.__name__, all=all))\n    return d\n", "def namesAndDescriptions(self, all=False):\n    attrs = self.__attrs\n    if not all:\n        attrs = attrs.keys()\n    return [(name, attrs[name].description) for name in attrs]\n", "def namesAndDescriptions(self, all=False):\n    if all:\n        return tuple(iter(self.__iro__))\n    return self.__iro__\n", "def namesAndDescriptions(self, all=False):\n    result = []\n    for base in self.__bases__:\n        result.extend(base.namesAndDescriptions(all))\n    if all:\n        return result\n    return tuple(result)\n", "def namesAndDescriptions(self, all=False):\n    d = {}\n    for v in self.__ibmodule__.getInterfaces().values():\n        d[v.__identifier__] = v.description\n    if all:\n        return d\n    return sorted(d.items())\n", "def namesAndDescriptions(self, all=False):\n    result = []\n    for base in self.__bases__:\n        result.extend(base.namesAndDescriptions(all=all))\n    return tuple(result)\n", "def namesAndDescriptions(self, all=False):\n    if all:\n        return tuple((name, getattr(self, name).__doc__)\n                     for name in self.__iro__\n                     if hasattr(self, name))\n    return tuple((name, getattr(self, name).__doc__)\n                 for name in self.__iro__\n                 if hasattr(self, name))\n", "def namesAndDescriptions(self, all=False):\n    names = tuple(\n        name\n        for name, info in self.__iro__\n        if not getattr(info, 'exclude_from_registry', False)\n    )\n    if all:\n        return tuple(names)\n    return dict((name, getattr(self, name)) for name in names)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def namesAndDescriptions(self, all=False):\n    if all:\n        return tuple(i.__name__ for i in self.__ibmodule__.getInterfaces())\n    return tuple(i.__name__ for i in self.__ibmodule__.getInterfaces()\n                 if i.providedBy(self))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    result = []\n    for base in reversed(self.__bases__):\n        result.extend(base.namesAndDescriptions(all=all))\n    if self.__ibmodule__:\n        result.append('%s.%s' % (self.__module__, self.__ibmodule__.__name__))\n    result.extend(self.__attrs.keys())\n    if all:\n        return result\n    return reversed(result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    if self.__ibmodule__ is None:\n        raise TypeError(\n            \"Interface %s has no descriptions and cannot be used directly\"\n            % self.__class__.__name__)\n    d = {}\n    for name in self.__iro__:\n        d[name] = getattr(self, name, None)\n    d.update(self.__ibmodule__.direct(self.__name__, all=all))\n    return d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    attrs = self.__attrs\n    if not all:\n        attrs = attrs.keys()\n    return [(name, attrs[name].description) for name in attrs]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    if all:\n        return tuple(iter(self.__iro__))\n    return self.__iro__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    result = []\n    for base in self.__bases__:\n        result.extend(base.namesAndDescriptions(all))\n    if all:\n        return result\n    return tuple(result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    d = {}\n    for v in self.__ibmodule__.getInterfaces().values():\n        d[v.__identifier__] = v.description\n    if all:\n        return d\n    return sorted(d.items())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    result = []\n    for base in self.__bases__:\n        result.extend(base.namesAndDescriptions(all=all))\n    return tuple(result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    if all:\n        return tuple((name, getattr(self, name).__doc__)\n                     for name in self.__iro__\n                     if hasattr(self, name))\n    return tuple((name, getattr(self, name).__doc__)\n                 for name in self.__iro__\n                 if hasattr(self, name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = tuple(\n        name\n        for name, info in self.__iro__\n        if not getattr(info, 'exclude_from_registry', False)\n    )\n    if all:\n        return tuple(names)\n    return dict((name, getattr(self, name)) for name in names)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "names", "docstring": "Return the attribute names defined by the interface.", "ques_id": "62b8b416eb7e40a82d2d1129", "code": ["def names(self, all=False):\n    if self.__ibmodule__ is None:\n        raise TypeError(\"can't resolve %s to any Interface\" % self)\n    return self.__ibmodule__.names(all)\n", "def names(self, all=False):\n    return tuple(getattr(self, name, None) for name in self.__bases__\n                 if (not all or issubclass(self, base))\n                 and name!= 'IInterface')\n", "def names(self, all=False):\n    return tuple(\n        name for name, tag in self.getTaggedValueTags()\n        if tag not in self.__ibmodule__.NOT_IMPLEMENTED_TAGS)\n", "def names(self, all=False):\n    if all:\n        return tuple(b.__name__ for b in self.__bases__)\n    return tuple(b.__name__ for b in self.__bases__ if hasattr(b, '__name__'))\n", "def names(self, all=False):\n    return tuple(pair[0] for pair in self.__iro__\n                 if pair[1] is not _Default or not all)\n", "def names(self, all=False):\n    if all:\n        return tuple(v for v in self.__attrs.values() if isinstance(v, str))\n    return tuple(v for v in self.__attrs.values())\n", "def names(self, all=False):\n    if all:\n        return tuple(v for v in self.__attrs.values() if v.isOrExtended())\n    return tuple(v.__name__ for v in self.__attrs.values())\n", "def names(self, all=False):\n    if all:\n        return tuple((v.__name__ for v in self.__bases__))\n    return tuple((v.__name__ for v in self._v_repr))\n", "def names(self, all=False):\n    if self.__ibmodule__ is None:\n        raise TypeError(\n            \"Importing a %s interface that has no module context -- \"\n            \"can't get names\" % self.__class__.__name__)\n    return self.__ibmodule__.names(self, all)\n", "def names(self, all=False):\n    if all:\n        return tuple(iface.name for iface in self.__iro__)\n    return tuple(iface.__name__ for iface in self.__iro__)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def names(self, all=False):\n    if self.__ibmodule__ is None:\n        raise TypeError(\"can't resolve %s to any Interface\" % self)\n    return self.__ibmodule__.names(all)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    return tuple(getattr(self, name, None) for name in self.__bases__\n                 if (not all or issubclass(self, base))\n                 and name!= 'IInterface')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    return tuple(\n        name for name, tag in self.getTaggedValueTags()\n        if tag not in self.__ibmodule__.NOT_IMPLEMENTED_TAGS)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    if all:\n        return tuple(b.__name__ for b in self.__bases__)\n    return tuple(b.__name__ for b in self.__bases__ if hasattr(b, '__name__'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    return tuple(pair[0] for pair in self.__iro__\n                 if pair[1] is not _Default or not all)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    if all:\n        return tuple(v for v in self.__attrs.values() if isinstance(v, str))\n    return tuple(v for v in self.__attrs.values())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    if all:\n        return tuple(v for v in self.__attrs.values() if v.isOrExtended())\n    return tuple(v.__name__ for v in self.__attrs.values())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    if all:\n        return tuple((v.__name__ for v in self.__bases__))\n    return tuple((v.__name__ for v in self._v_repr))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    if self.__ibmodule__ is None:\n        raise TypeError(\n            \"Importing a %s interface that has no module context -- \"\n            \"can't get names\" % self.__class__.__name__)\n    return self.__ibmodule__.names(self, all)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    if all:\n        return tuple(iface.name for iface in self.__iro__)\n    return tuple(iface.__name__ for iface in self.__iro__)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "_normalizeargs", "docstring": "Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.", "ques_id": "62b8b3d6eb7e40a82d2d111c", "code": ["def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    def _normalize(arg):\n        if isinstance(arg, InterfaceClass):\n            arg = arg._implementation\n        if isinstance(arg, ComponentClass):\n            arg = arg._implementation\n        return arg\n\n    for iface in sequence:\n        output.append(_normalize(iface))\n\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    declared = []\n    for iface in sequence:\n        if _implements_advice(iface):\n            iface = _implements(iface)\n        if _implements_name(iface):\n            iface = _implements_name(iface)\n        if isinstance(iface, InterfaceClass):\n            iface = implementedBy(iface)\n        if not isinstance(iface, Specification):\n            raise TypeError(_ADVICE_ERROR % (iface, iface.__class__))\n        declared.append(iface)\n        if iface.providedBy(ob):\n            output.append(iface)\n        elif _classImplements_ordered(iface):\n            output.extend([Specification(*c) for c in _classImplements_ordered(iface)])\n        else:\n            output.append(iface)\n    return declared, output\n", "def _normalizeargs(sequence, output=None):\n    output = output or []\n    if isinstance(sequence, InterfaceClass):\n        return [sequence]\n    if isinstance(sequence, ClassType):\n        return output + [sequence]\n    for v in sequence:\n        output = output + _normalizeargs(v, output)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    output = output or []\n    if isinstance(sequence, InterfaceClass):\n        return output + sequence._impliedBy\n    if isinstance(sequence, ClassSpec):\n        return output + (sequence._impliedBy,)\n    for x in sequence:\n        _normalizeargs(x, output)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    for spec in sequence:\n        if isinstance(spec, InterfaceClass):\n            _normalizeargs(spec._declarations, output)\n        elif isinstance(spec, InterfaceClassAndInterfaceMetaClass):\n            output.extend(spec._declarations)\n            output.extend(_normalizeargs(spec._implementations, output))\n        else:\n            output.append(spec)\n\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    def _normalize(arg):\n        if isinstance(arg, InterfaceClass) or \\\n           isinstance(arg, InterfaceClassLoader):\n            output.append(arg)\n        else:\n            output.append(providedBy(arg))\n\n    sequence(_normalize)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    def _normalize(arg):\n        if isinstance(arg, InterfaceClass):\n            output.append(arg)\n            return arg\n        elif isinstance(arg, tuple):\n            output.append(InterfaceClass(*arg))\n            return arg\n        else:\n            spec = _implements_advice(arg)\n            if spec is not None:\n                output.append(spec)\n            return arg\n\n    _normalize([x for x in sequence if isinstance(x, InterfaceClass)])\n    _normalize([x for x in sequence if isinstance(x, tuple)])\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    for iface in sequence:\n        if providedBy(iface):\n            iface = iface.__implementation__\n        output.append(iface)\n        _normalizeargs(iface.__args__, output)\n        _normalizeargs(iface.__kwargs__, output)\n\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    for spec in sequence:\n        if isinstance(spec, InterfaceClass):\n            _normalizeargs(spec.__args__, output)\n        elif isinstance(spec, InterfaceClassAndInterface):\n            for iface in spec.__args__:\n                _normalizeargs(iface.__args__, output)\n        elif isinstance(spec, Specification):\n            output.append(spec)\n        else:\n            raise TypeError(_next_super_class(spec))\n\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    def traverse(spec):\n        if isinstance(spec, InterfaceClass):\n            output.append(spec)\n        elif isinstance(spec, ClassSpec):\n            output.append(InterfaceClass(spec.dotted_name, spec.args,\n                                         spec.kwargs))\n        elif isinstance(spec, ClassSpecList):\n            for i in spec.elements:\n                traverse(i)\n        elif isinstance(spec, InterfaceSpec):\n            output.append(spec)\n        elif isinstance(spec, Implements):\n            for iface in spec.interfaces:\n                traverse(iface)\n        elif spec is _empty:\n            pass\n        else:\n            raise TypeError(\n                'Cannot normalize type %s to list of types' % spe"], "level": "file_runnable", "generate_results": [{"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    def _normalize(arg):\n        if isinstance(arg, InterfaceClass):\n            arg = arg._implementation\n        if isinstance(arg, ComponentClass):\n            arg = arg._implementation\n        return arg\n\n    for iface in sequence:\n        output.append(_normalize(iface))\n\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    declared = []\n    for iface in sequence:\n        if _implements_advice(iface):\n            iface = _implements(iface)\n        if _implements_name(iface):\n            iface = _implements_name(iface)\n        if isinstance(iface, InterfaceClass):\n            iface = implementedBy(iface)\n        if not isinstance(iface, Specification):\n            raise TypeError(_ADVICE_ERROR % (iface, iface.__class__))\n        declared.append(iface)\n        if iface.providedBy(ob):\n            output.append(iface)\n        elif _classImplements_ordered(iface):\n            output.extend([Specification(*c) for c in _classImplements_ordered(iface)])\n        else:\n            output.append(iface)\n    return declared, output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    output = output or []\n    if isinstance(sequence, InterfaceClass):\n        return [sequence]\n    if isinstance(sequence, ClassType):\n        return output + [sequence]\n    for v in sequence:\n        output = output + _normalizeargs(v, output)\n    return output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    output = output or []\n    if isinstance(sequence, InterfaceClass):\n        return output + sequence._impliedBy\n    if isinstance(sequence, ClassSpec):\n        return output + (sequence._impliedBy,)\n    for x in sequence:\n        _normalizeargs(x, output)\n    return output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    for spec in sequence:\n        if isinstance(spec, InterfaceClass):\n            _normalizeargs(spec._declarations, output)\n        elif isinstance(spec, InterfaceClassAndInterfaceMetaClass):\n            output.extend(spec._declarations)\n            output.extend(_normalizeargs(spec._implementations, output))\n        else:\n            output.append(spec)\n\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    def _normalize(arg):\n        if isinstance(arg, InterfaceClass) or \\\n           isinstance(arg, InterfaceClassLoader):\n            output.append(arg)\n        else:\n            output.append(providedBy(arg))\n\n    sequence(_normalize)\n    return output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    def _normalize(arg):\n        if isinstance(arg, InterfaceClass):\n            output.append(arg)\n            return arg\n        elif isinstance(arg, tuple):\n            output.append(InterfaceClass(*arg))\n            return arg\n        else:\n            spec = _implements_advice(arg)\n            if spec is not None:\n                output.append(spec)\n            return arg\n\n    _normalize([x for x in sequence if isinstance(x, InterfaceClass)])\n    _normalize([x for x in sequence if isinstance(x, tuple)])\n    return output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    for iface in sequence:\n        if providedBy(iface):\n            iface = iface.__implementation__\n        output.append(iface)\n        _normalizeargs(iface.__args__, output)\n        _normalizeargs(iface.__kwargs__, output)\n\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    for spec in sequence:\n        if isinstance(spec, InterfaceClass):\n            _normalizeargs(spec.__args__, output)\n        elif isinstance(spec, InterfaceClassAndInterface):\n            for iface in spec.__args__:\n                _normalizeargs(iface.__args__, output)\n        elif isinstance(spec, Specification):\n            output.append(spec)\n        else:\n            raise TypeError(_next_super_class(spec))\n\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n\n    def traverse(spec):\n        if isinstance(spec, InterfaceClass):\n            output.append(spec)\n        elif isinstance(spec, ClassSpec):\n            output.append(InterfaceClass(spec.dotted_name, spec.args,\n                                         spec.kwargs))\n        elif isinstance(spec, ClassSpecList):\n            for i in spec.elements:\n                traverse(i)\n        elif isinstance(spec, InterfaceSpec):\n            output.append(spec)\n        elif isinstance(spec, Implements):\n            for iface in spec.interfaces:\n                traverse(iface)\n        elif spec is _empty:\n            pass\n        else:\n            raise TypeError(\n                'Cannot normalize type %s to list of types' % spe", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_available", "docstring": "Return the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.", "ques_id": "62b8b3d5eb7e40a82d2d1110", "code": ["def _c_optimizations_available():\n    if _c_optimizations_required():\n        _c_optimizations_available()\n        return True\n    else:\n        return False\n", "def _c_optimizations_available():\n    try:\n        from zope.interface import implementer\n    except ImportError:\n        return False\n    else:\n        return implementer._c_optimizations_available()\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n        return c_optimizations\n    except ImportError:\n        return False\n", "def _c_optimizations_available():\n    try:\n        from zope.interface import implementedBy\n        return implementedBy\n    except ImportError:\n        return _c_optimizations_required()\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations\n    except ImportError:\n        # pip is not claiming for c_optimizations when the package is not in PYPY\n        # (we don't want to install in this case, since we still want to use\n        # c_optimizations for the tests that depend on it)\n        if not sys.pypy_version_info >= (3, ):\n            raise ImportError\n        return False\n    else:\n        return c_optimizations\n", "def _c_optimizations_available():\n    try:\n        from zope.interface import alsoProvides\n        return alsoProvides(zope.interface.providedBy(PYPY))\n    except ImportError:\n        pass\n    try:\n        from zope.interface import providedBy\n        return providedBy(PYPY)\n    except ImportError:\n        pass\n    if _c_optimizations_required():\n        raise ImportError(\n            \"C optimizations required but not available, \"\n            \"cannot use the '%s' module\" % _c_optimizations_name())\n    return False\n", "def _c_optimizations_available():\n    global _c_optimizations_available\n    if _c_optimizations_required():\n        import c_optimizations  # noqa\n        _c_optimizations_available = True\n    return _c_optimizations_available\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations\n    except ImportError:\n        return False\n    else:\n        return c_optimizations\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n        return c_optimizations\n    except ImportError:\n        return False\n", "def _c_optimizations_available():\n    global _c_optimizations_available\n    if _c_optimizations_required():\n        import c_optimizations  # noqa\n        _c_optimizations_available = True\n    return _c_optimizations_available\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _c_optimizations_available():\n    if _c_optimizations_required():\n        _c_optimizations_available()\n        return True\n    else:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        from zope.interface import implementer\n    except ImportError:\n        return False\n    else:\n        return implementer._c_optimizations_available()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n        return c_optimizations\n    except ImportError:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        from zope.interface import implementedBy\n        return implementedBy\n    except ImportError:\n        return _c_optimizations_required()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations\n    except ImportError:\n        # pip is not claiming for c_optimizations when the package is not in PYPY\n        # (we don't want to install in this case, since we still want to use\n        # c_optimizations for the tests that depend on it)\n        if not sys.pypy_version_info >= (3, ):\n            raise ImportError\n        return False\n    else:\n        return c_optimizations\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        from zope.interface import alsoProvides\n        return alsoProvides(zope.interface.providedBy(PYPY))\n    except ImportError:\n        pass\n    try:\n        from zope.interface import providedBy\n        return providedBy(PYPY)\n    except ImportError:\n        pass\n    if _c_optimizations_required():\n        raise ImportError(\n            \"C optimizations required but not available, \"\n            \"cannot use the '%s' module\" % _c_optimizations_name())\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    global _c_optimizations_available\n    if _c_optimizations_required():\n        import c_optimizations  # noqa\n        _c_optimizations_available = True\n    return _c_optimizations_available\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations\n    except ImportError:\n        return False\n    else:\n        return c_optimizations\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n        return c_optimizations\n    except ImportError:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    global _c_optimizations_available\n    if _c_optimizations_required():\n        import c_optimizations  # noqa\n        _c_optimizations_available = True\n    return _c_optimizations_available\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_should_attempt_c_optimizations", "docstring": "Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.", "ques_id": "62b8b3d4eb7e40a82d2d110f", "code": ["def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy.lower() == 'true'\n    return _c_optimizations_required() and _c_optimizations_available()\n", "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy == 'true'\n    return _c_optimizations_required()\n", "def _should_attempt_c_optimizations():\n    if PYPY:\n        return os.environ.get('PURE_PYTHON') == '1'\n    return _use_c_impl()\n", "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy in ('1', '0', 'False', 'false', '0False')\n    return _c_optimizations_available() or _c_optimizations_ignored()\n", "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy.lower() == 'true'\n\n    return _c_optimizations_available() or _c_optimizations_ignored()\n", "def _should_attempt_c_optimizations():\n    pure_env_name = os.environ.get('PURE_PYTHON')\n    if pure_env_name is not None:\n        return pure_env_name == '1'\n    return _c_optimizations_required()\n", "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy.lower() == 'true'\n\n    return _c_optimizations_ignored() or _c_optimizations_required()\n", "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    return pure_pypy is not None and pure_pypy == 'true'\n", "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy and pure_pypy == '1':\n        return True\n    return _c_optimizations_required() or _c_optimizations_available()\n", "def _should_attempt_c_optimizations():\n    pure_env_var = \"PURE_PYTHON\"\n    return pure_env_var in os.environ\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy.lower() == 'true'\n    return _c_optimizations_required() and _c_optimizations_available()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy == 'true'\n    return _c_optimizations_required()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    if PYPY:\n        return os.environ.get('PURE_PYTHON') == '1'\n    return _use_c_impl()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy in ('1', '0', 'False', 'false', '0False')\n    return _c_optimizations_available() or _c_optimizations_ignored()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy.lower() == 'true'\n\n    return _c_optimizations_available() or _c_optimizations_ignored()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_env_name = os.environ.get('PURE_PYTHON')\n    if pure_env_name is not None:\n        return pure_env_name == '1'\n    return _c_optimizations_required()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy is not None:\n        return pure_pypy.lower() == 'true'\n\n    return _c_optimizations_ignored() or _c_optimizations_required()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    return pure_pypy is not None and pure_pypy == 'true'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy and pure_pypy == '1':\n        return True\n    return _c_optimizations_required() or _c_optimizations_available()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_env_var = \"PURE_PYTHON\"\n    return pure_env_var in os.environ\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_ignored", "docstring": "The opposite of `_c_optimizations_required`.", "ques_id": "62b8b3d4eb7e40a82d2d110e", "code": ["def _c_optimizations_ignored():\n    return ['-g', '--gui']\n", "def _c_optimizations_ignored():\n    return ['-std=c++11', '-std=c++14', '-std=gnu++14']\n", "def _c_optimizations_ignored():\n    return ['-O0', '-g', '-O2', '-gcc']\n", "def _c_optimizations_ignored():\n    return ['-O0', '-g']\n", "def _c_optimizations_ignored():\n    return ['-funoptimized', '-funrestricted']\n", "def _c_optimizations_ignored():\n    return not sys.platform.startswith('win32')\n", "def _c_optimizations_ignored():\n    return ['-O0', '-g', '-I.']\n", "def _c_optimizations_ignored():\n    return ['-std=c++11', '-std=c++14', '-std=c++17']\n", "def _c_optimizations_ignored():\n    return ['-std=c++11', '-std=c++0']\n", "def _c_optimizations_ignored():\n    return ['-Wunused-variable', '-Wundefined']\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_ignored():\n    return ['-g', '--gui']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return ['-std=c++11', '-std=c++14', '-std=gnu++14']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return ['-O0', '-g', '-O2', '-gcc']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return ['-O0', '-g']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return ['-funoptimized', '-funrestricted']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return not sys.platform.startswith('win32')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return ['-O0', '-g', '-I.']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return ['-std=c++11', '-std=c++14', '-std=c++17']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return ['-std=c++11', '-std=c++0']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return ['-Wunused-variable', '-Wundefined']\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_required", "docstring": "Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.", "ques_id": "62b8b3d4eb7e40a82d2d110d", "code": ["def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == \"1\"\n    return False\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= '0'\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    else:\n        return pure_env == '1'\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    else:\n        return pure_env == \"0\"\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= '0'\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= \"0\"\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n    return False\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n    else:\n        return False\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    else:\n        return pure_env == '1'\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n    return False\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == \"1\"\n    return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= '0'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    else:\n        return pure_env == '1'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    else:\n        return pure_env == \"0\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= '0'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= \"0\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n    return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n    else:\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    else:\n        return pure_env == '1'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n    return False\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "reset", "docstring": "Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).", "ques_id": "62b87b989a0c4fa8b80b35ee", "code": ["def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "def reset(self):\n    self._hist = {}\n    self._cur_context = None\n", "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "def reset(self):\n    self._hist = {}\n    self._cur_context = copy.copy(self._initial_bins)\n", "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "def reset(self):\n    self._hist = {}\n    self._cur_context = copy.copy(self._initial_bins)\n", "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = copy.copy(self._initial_bins)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = copy.copy(self._initial_bins)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist = {}\n    self._cur_context = {}\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "to_csv", "docstring": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.", "ques_id": "62b87b859a0c4fa8b80b35d7", "code": ["def to_csv(self, separator=\",\", header=None):\n    from.output import ToCSV\n\n    return ToCSV(self, separator=separator, header=header)\n", "def to_csv(self, separator=\",\", header=None):\n    from..utils.to_csv import ToCSV\n    return ToCSV(self, separator=separator, header=header)\n", "def to_csv(self, separator=\",\", header=None):\n    return output.ToCSV(self, separator, header)\n", "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(separator=separator, header=header).dumps(self)\n", "def to_csv(self, separator=\",\", header=None):\n    if self._context is None:\n        raise LenaError(\"graph has no context\")\n    return output.ToCSV(self._context, separator, header)\n", "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(self, separator=separator, header=header)\n", "def to_csv(self, separator=\",\", header=None):\n    from..utils import to_csv\n    return to_csv(self.points, self.dim, separator, header)\n", "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(self, separator=separator, header=header)\n", "def to_csv(self, separator=\",\", header=None):\n    if header is None:\n        header = []\n    return header + [\"{}{}\".format(x, separator) for x in self.points]\n", "def to_csv(self, separator=\",\", header=None):\n    return self._to_csv(separator, header)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def to_csv(self, separator=\",\", header=None):\n    from.output import ToCSV\n\n    return ToCSV(self, separator=separator, header=header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    from..utils.to_csv import ToCSV\n    return ToCSV(self, separator=separator, header=header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return output.ToCSV(self, separator, header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(separator=separator, header=header).dumps(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    if self._context is None:\n        raise LenaError(\"graph has no context\")\n    return output.ToCSV(self._context, separator, header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(self, separator=separator, header=header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    from..utils import to_csv\n    return to_csv(self.points, self.dim, separator, header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(self, separator=separator, header=header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    if header is None:\n        header = []\n    return header + [\"{}{}\".format(x, separator) for x in self.points]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return self._to_csv(separator, header)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_get_err_indices", "docstring": "Get error indices corresponding to a coordinate.", "ques_id": "62b87b839a0c4fa8b80b35cb", "code": ["def _get_err_indices(self, coord_name):\n    try:\n        return self._coord_names.index(coord_name)\n    except ValueError:\n        raise ValueError(\n            \"coordinate '{}' is not a valid coordinate name\".format(coord_name)\n        )\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._parsed_error_names:\n        raise KeyError(\n            \"coordinate '{}' is not in the \"\n            \"required format (one of {})\".format(coord_name,\n                                                self._parsed_error_names))\n    return self._parsed_error_names[coord_name]\n", "def _get_err_indices(self, coord_name):\n    if coord_name in self._parsed_error_names:\n        return self._parsed_error_names[coord_name]\n    else:\n        return []\n", "def _get_err_indices(self, coord_name):\n    try:\n        return self._parsed_error_names[coord_name]\n    except KeyError:\n        msg = (\n            \"Unknown coordinate '{coord_name}'. \"\n            \"Valid coordinate names are: \"\n            \"{field_names}\"\n        ).format(coord_name=coord_name, field_names=self.field_names)\n        raise KeyError(msg)\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError(\n            'coordinate \"{0}\" is not in the '\n           'required coordinate names for the '\n            'format \"{1}\"'.format(coord_name, self.coords)\n        )\n\n    return self._parsed_error_names[coord_name]\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._parsed_error_names:\n        err_name = self._scale.get(coord_name, coord_name)\n        if err_name not in self._scale:\n            raise KeyError(\n                'Unknown coordinate \"{}\". Valid names are: '\n                '{}'.format(coord_name, self._scale.keys())\n            )\n        return self._scale[err_name]\n    return self._parsed_error_names[coord_name]\n", "def _get_err_indices(self, coord_name):\n    return self._parsed_error_names[coord_name]\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError('coordinate {!r} is not in '\n                        'representation of field {!r}'\n                        .format(coord_name, self.field_names))\n    return self._parsed_error_names[coord_name]\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError(\n            'coordinate \"{0!s}\" is not in the '\n           'standard coordinate names ({1!s})'.format(coord_name,\n                                                       self._coord_names))\n    return self._parsed_error_names[coord_name]\n", "def _get_err_indices(self, coord_name):\n    return self._parsed_error_names[coord_name]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _get_err_indices(self, coord_name):\n    try:\n        return self._coord_names.index(coord_name)\n    except ValueError:\n        raise ValueError(\n            \"coordinate '{}' is not a valid coordinate name\".format(coord_name)\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._parsed_error_names:\n        raise KeyError(\n            \"coordinate '{}' is not in the \"\n            \"required format (one of {})\".format(coord_name,\n                                                self._parsed_error_names))\n    return self._parsed_error_names[coord_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name in self._parsed_error_names:\n        return self._parsed_error_names[coord_name]\n    else:\n        return []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    try:\n        return self._parsed_error_names[coord_name]\n    except KeyError:\n        msg = (\n            \"Unknown coordinate '{coord_name}'. \"\n            \"Valid coordinate names are: \"\n            \"{field_names}\"\n        ).format(coord_name=coord_name, field_names=self.field_names)\n        raise KeyError(msg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError(\n            'coordinate \"{0}\" is not in the '\n           'required coordinate names for the '\n            'format \"{1}\"'.format(coord_name, self.coords)\n        )\n\n    return self._parsed_error_names[coord_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._parsed_error_names:\n        err_name = self._scale.get(coord_name, coord_name)\n        if err_name not in self._scale:\n            raise KeyError(\n                'Unknown coordinate \"{}\". Valid names are: '\n                '{}'.format(coord_name, self._scale.keys())\n            )\n        return self._scale[err_name]\n    return self._parsed_error_names[coord_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    return self._parsed_error_names[coord_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError('coordinate {!r} is not in '\n                        'representation of field {!r}'\n                        .format(coord_name, self.field_names))\n    return self._parsed_error_names[coord_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError(\n            'coordinate \"{0!s}\" is not in the '\n           'standard coordinate names ({1!s})'.format(coord_name,\n                                                       self._coord_names))\n    return self._parsed_error_names[coord_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    return self._parsed_error_names[coord_name]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_update_context", "docstring": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.", "ques_id": "62b87b7e9a0c4fa8b80b35bc", "code": ["def _update_context(self, context):\n    for field_name, value in context.items():\n        if field_name not in self.field_names:\n            continue\n        if isinstance(value, tuple):\n            # this is a value tuple\n            value = {self._coord_names[i]: v for i, v in enumerate(value)}\n        else:\n            value = self._coord_names[self._get_err_indices(field_name)]\n        if field_name not in self._parsed_error_names:\n            context[field_name].update(value)\n        else:\n            context[field_name].append(value)\n", "def _update_context(self, context):\n    context.error = {}\n    for field_name in self.field_names:\n        context.error[field_name] = {}\n        for name in self._parsed_error_names:\n            context.error[field_name][name] = getattr(\n                self, \"_{}\".format(name))\n", "def _update_context(self, context):\n    for name in self.field_names:\n        if name not in context:\n            context[name] = {}\n        context[name].update(getattr(self, name))\n", "def _update_context(self, context):\n    context.error = {}\n    for name in self.field_names:\n        if name in context.error:\n            context.error[name] = {\"index\": len(context.error[name])}\n        else:\n            context.error[name] = {\"index\": 0}\n", "def _update_context(self, context):\n    context.error = {}\n    for name in self.field_names:\n        if name not in context.error:\n            context.error[name] = {}\n        context.error[name][\"index\"] = self._parse_error_names(name)\n", "def _update_context(self, context):\n    for field_name in self.field_names:\n        try:\n            value = getattr(self, field_name)\n        except AttributeError:\n            pass\n        else:\n            context.error[field_name] = value\n    context.error_names = self._parsed_error_names\n", "def _update_context(self, context):\n    for key, val in self._parsed_error_names.items():\n        if key not in context:\n            context[key] = val\n        else:\n            context[key].update(val)\n", "def _update_context(self, context):\n    for field_name in self.field_names:\n        context.add(field_name, getattr(self, field_name))\n", "def _update_context(self, context):\n    context.error = {}\n    for field_name in self.field_names:\n        context.error[field_name] = {}\n        for i, coord_name in enumerate(self._coord_names):\n            context.error[field_name][coord_name] = getattr(self, coord_name)[i]\n", "def _update_context(self, context):\n    context.error = {}\n\n    if self._coord_names is not None:\n        context.error[\"x_low\"] = {self._coord_names[0]: {\"index\": 0}}\n\n    if self._scale is not None:\n        context.error[\"y\"] = {self._scale[0]: {\"index\": 1}}\n\n    if self._err_names is not None:\n        context.error[\"error\"] = {}\n\n        for err_name in self._err_names:\n            err_idx = self._parse_error_names(err_name)[0]\n            context.error[\"error\"][err_name] = {\"index\": err_idx}\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _update_context(self, context):\n    for field_name, value in context.items():\n        if field_name not in self.field_names:\n            continue\n        if isinstance(value, tuple):\n            # this is a value tuple\n            value = {self._coord_names[i]: v for i, v in enumerate(value)}\n        else:\n            value = self._coord_names[self._get_err_indices(field_name)]\n        if field_name not in self._parsed_error_names:\n            context[field_name].update(value)\n        else:\n            context[field_name].append(value)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _update_context(self, context):\n    context.error = {}\n    for field_name in self.field_names:\n        context.error[field_name] = {}\n        for name in self._parsed_error_names:\n            context.error[field_name][name] = getattr(\n                self, \"_{}\".format(name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    for name in self.field_names:\n        if name not in context:\n            context[name] = {}\n        context[name].update(getattr(self, name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    context.error = {}\n    for name in self.field_names:\n        if name in context.error:\n            context.error[name] = {\"index\": len(context.error[name])}\n        else:\n            context.error[name] = {\"index\": 0}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    context.error = {}\n    for name in self.field_names:\n        if name not in context.error:\n            context.error[name] = {}\n        context.error[name][\"index\"] = self._parse_error_names(name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    for field_name in self.field_names:\n        try:\n            value = getattr(self, field_name)\n        except AttributeError:\n            pass\n        else:\n            context.error[field_name] = value\n    context.error_names = self._parsed_error_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    for key, val in self._parsed_error_names.items():\n        if key not in context:\n            context[key] = val\n        else:\n            context[key].update(val)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    for field_name in self.field_names:\n        context.add(field_name, getattr(self, field_name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    context.error = {}\n    for field_name in self.field_names:\n        context.error[field_name] = {}\n        for i, coord_name in enumerate(self._coord_names):\n            context.error[field_name][coord_name] = getattr(self, coord_name)[i]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    context.error = {}\n\n    if self._coord_names is not None:\n        context.error[\"x_low\"] = {self._coord_names[0]: {\"index\": 0}}\n\n    if self._scale is not None:\n        context.error[\"y\"] = {self._scale[0]: {\"index\": 1}}\n\n    if self._err_names is not None:\n        context.error[\"error\"] = {}\n\n        for err_name in self._err_names:\n            err_idx = self._parse_error_names(err_name)[0]\n            context.error[\"error\"][err_name] = {\"index\": err_idx}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "integral", "docstring": "Compute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.", "ques_id": "62b87b4f9a0c4fa8b80b3580", "code": ["def integral(bins, edges):\n    return _hist.integral(bins, edges)\n", "def integral(bins, edges):\n    return _hist.integral(bins, edges)\n", "def integral(bins, edges):\n    return bin_edges_to_graph(bins, edges)\n", "def integral(bins, edges):\n    bins, edges = _check_edges_increasing_1d(bins, edges)\n\n    # get the edges of the histogram and normalize\n    edges = _get_edges_normalized(edges)\n\n    # create histogram\n    hist, edges = _make_hist_context(bins, edges)\n\n    # compute the integral\n    out = np.diff(hist, axis=1)\n\n    # remove the 0 values and normalize\n    out = out / np.sum(out, axis=1)[:, None]\n\n    return out\n", "def integral(bins, edges):\n    return hist_to_graph(hist(bins, edges), make_value, get_coordinate=lambda args: args[0])\n", "def integral(bins, edges):\n    return _hist.integral(bins, edges)\n", "def integral(bins, edges):\n    return _bin_op(hist_to_graph, bins, edges, func=integrate_trapz)\n", "def integral(bins, edges):\n    return _hist.integral(bins, edges)\n", "def integral(bins, edges):\n    bin_edges, cell_edges = get_bin_edges(bins, edges)\n    hist, _, _ = hist_to_graph(hist_to_bins(bins, bin_edges),\n                               make_value=lambda x: x,\n                               get_coordinate=lambda x: x.value,\n                               field_names=(\"x\", \"y\"),\n                               scale=get_example_bin(bins))\n    return hist\n", "def integral(bins, edges):\n    return _hist.integral(bins, edges)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def integral(bins, edges):\n    return _hist.integral(bins, edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return _hist.integral(bins, edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return bin_edges_to_graph(bins, edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    bins, edges = _check_edges_increasing_1d(bins, edges)\n\n    # get the edges of the histogram and normalize\n    edges = _get_edges_normalized(edges)\n\n    # create histogram\n    hist, edges = _make_hist_context(bins, edges)\n\n    # compute the integral\n    out = np.diff(hist, axis=1)\n\n    # remove the 0 values and normalize\n    out = out / np.sum(out, axis=1)[:, None]\n\n    return out\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return hist_to_graph(hist(bins, edges), make_value, get_coordinate=lambda args: args[0])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return _hist.integral(bins, edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return _bin_op(hist_to_graph, bins, edges, func=integrate_trapz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return _hist.integral(bins, edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    bin_edges, cell_edges = get_bin_edges(bins, edges)\n    hist, _, _ = hist_to_graph(hist_to_bins(bins, bin_edges),\n                               make_value=lambda x: x,\n                               get_coordinate=lambda x: x.value,\n                               field_names=(\"x\", \"y\"),\n                               scale=get_example_bin(bins))\n    return hist\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return _hist.integral(bins, edges)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_seq", "docstring": "Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.", "ques_id": "62b87b199a0c4fa8b80b354e", "code": ["def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n", "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n", "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n", "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n", "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or is_fill_request_el(seq) or is_source(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return is_fill_compute_el(seq) or (is_fill_request_el(seq) and not is_source(seq))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_el", "docstring": "Object contains executable methods 'fill' and 'request'.", "ques_id": "62b87b099a0c4fa8b80b3538", "code": ["def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.attrib['method'] =='request'\n", "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.get('method') =='request'\n", "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.attrib.get('method') =='request'\n", "def is_fill_request_el(obj):\n    return obj.tag == 'fill' or obj.tag =='request'\n", "def is_fill_request_el(obj):\n    return obj.tag == 'FILL' and obj.attrib['METHOD'] == 'REQUEST'\n", "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.get('method') =='request'\n", "def is_fill_request_el(obj):\n    return isinstance(obj, dict) and obj.get('method') == 'fill' \\\n        and obj.get('params') is not None\n", "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.attrib['method'] =='request'\n", "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.attrib.get('method') =='request'\n", "def is_fill_request_el(obj):\n    return obj.tag == \"fill\" and obj.attrib.get(\"method\") == \"request\"\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.attrib['method'] =='request'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.get('method') =='request'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.attrib.get('method') =='request'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return obj.tag == 'fill' or obj.tag =='request'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return obj.tag == 'FILL' and obj.attrib['METHOD'] == 'REQUEST'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.get('method') =='request'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, dict) and obj.get('method') == 'fill' \\\n        and obj.get('params') is not None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.attrib['method'] =='request'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return obj.tag == 'fill' and obj.attrib.get('method') =='request'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return obj.tag == \"fill\" and obj.attrib.get(\"method\") == \"request\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_run_el", "docstring": "Object contains executable method 'run'.", "ques_id": "62b87af99a0c4fa8b80b3524", "code": ["def is_run_el(obj):\n    return hasattr(obj, \"run\")\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return isinstance(obj, CommandLineTool) and hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return isinstance(obj, AbstractExecutable) and hasattr(obj, \"run\")\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_run_el(obj):\n    return hasattr(obj, \"run\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return isinstance(obj, CommandLineTool) and hasattr(obj, 'run')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return isinstance(obj, AbstractExecutable) and hasattr(obj, \"run\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_compute_el", "docstring": "Object contains executable methods 'fill' and 'compute'.", "ques_id": "62b87af69a0c4fa8b80b351a", "code": ["def is_fill_compute_el(obj):\n    return (isinstance(obj, FillComputeEl) or\n            isinstance(obj, Fill) or\n            isinstance(obj, Compute))\n", "def is_fill_compute_el(obj):\n    return isinstance(obj, FillBase) or \\\n        isinstance(obj, ComputeBase)\n", "def is_fill_compute_el(obj):\n    return ((isinstance(obj, FillBase) or\n             isinstance(obj, FillProperty) or\n             isinstance(obj, ComputeBase)) and\n            obj.method == \"compute\")\n", "def is_fill_compute_el(obj):\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "def is_fill_compute_el(obj):\n    return isinstance(obj, FillComputeEl)\n", "def is_fill_compute_el(obj):\n    return ((hasattr(obj, 'fill') and callable(obj.fill)) or\n            (hasattr(obj, 'compute') and callable(obj.compute)))\n", "def is_fill_compute_el(obj):\n    return isinstance(obj, FillComputeElMethods)\n", "def is_fill_compute_el(obj):\n    return (isinstance(obj, FillElement) or\n            isinstance(obj, ComputeElement))\n", "def is_fill_compute_el(obj):\n    return (hasattr(obj, 'fill') and\n            hasattr(obj, 'compute'))\n", "def is_fill_compute_el(obj):\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_compute_el(obj):\n    return (isinstance(obj, FillComputeEl) or\n            isinstance(obj, Fill) or\n            isinstance(obj, Compute))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return isinstance(obj, FillBase) or \\\n        isinstance(obj, ComputeBase)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return ((isinstance(obj, FillBase) or\n             isinstance(obj, FillProperty) or\n             isinstance(obj, ComputeBase)) and\n            obj.method == \"compute\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n    return isinstance(obj, FillComputeEl)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return ((hasattr(obj, 'fill') and callable(obj.fill)) or\n            (hasattr(obj, 'compute') and callable(obj.compute)))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n    return isinstance(obj, FillComputeElMethods)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return (isinstance(obj, FillElement) or\n            isinstance(obj, ComputeElement))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return (hasattr(obj, 'fill') and\n            hasattr(obj, 'compute'))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/context/functions.py", "project": "ynikitenko/lena", "name": "difference", "docstring": "Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.", "ques_id": "62b87af19a0c4fa8b80b34f7", "code": ["def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if isinstance(d1, dict) and isinstance(d2, dict):\n        d1 = deepcopy(d1)\n        for k in d2:\n            if k in d1 and d1[k] == d2[k]:\n                d1[k] = difference(d1[k], d2[k], level+1)\n        return d1\n    elif isinstance(d1, list) and isinstance(d2, list):\n        return [update_nested(k, d1, d2) for k in d1 if k not in d2]\n    elif isinstance(d1, set) and isinstance(d2, set):\n        return {update_nested(k, d1, d2) for k in d1 if k not in d2}\n    elif isinstance(d1, tuple) and isinstance(d2, tuple):\n        return tuple(update_nested(k, d1, d2) for k in d1 if k not in d2)\n    else:\n        return d1 - d2\n", "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    d2_dict = d2()\n    d1_dict = d1()\n    for k in d1_dict:\n        if k not in d2_dict or d1_dict[k]!= d2_dict[k]:\n            return d1.copy()\n    return d2\n", "def difference(d1, d2, level=-1):\n    if level < 0:\n        level = max(len(d1), len(d2))\n\n    d2copy = copy.deepcopy(d2)\n    d2 = set(d2)\n\n    ret = {}\n    for k in d1:\n        if k not in d2:\n            ret[k] = d2copy[k]\n        elif d1[k]!= d2[k]:\n            ret[k] = d1[k]\n\n    for k in d2:\n        if k not in d1:\n            ret[k] = _sentinel\n        elif d1[k]!= d2[k]:\n            ret[k] = d1[k]\n        elif d1[k] == _sentinel and d2[k] == _sentinel:\n            # Don't include keys that are equal.\n            pass\n        else:\n            # Don't include keys that are in the difference.\n            if k in d1 and k in d2:\n             ", "def difference(d1, d2, level=-1):\n    if not isinstance(d1, dict):\n        raise TypeError('d1 must be a dictionary')\n    if not isinstance(d2, dict):\n        raise TypeError('d2 must be a dictionary')\n    if level < 0:\n        level = max(d1.keys())\n    d = copy.deepcopy(d1)\n    for key in d2:\n        if key not in d or d2[key]!= d[key]:\n            return d.setdefault(key, _sentinel)\n    return d\n", "def difference(d1, d2, level=-1):\n    if level == 0:\n        return dict(d1)\n\n    keys = d1.keys() & d2.keys()\n    d1_sub = d1.copy()\n    d2_sub = d2.copy()\n\n    for k in keys:\n        d1_sub[k] = intersection(d1_sub.get(k, _sentinel), d2_sub.get(k, _sentinel))\n\n    return d1_sub\n", "def difference(d1, d2, level=-1):\n    if level == 0:\n        d1 = dict(d1)\n        d2 = dict(d2)\n        return dict((k, v) for k, v in iteritems(d1).items() if k not in d2)\n    else:\n        d1 = dict(d1.items())\n        d2 = dict(d2.items())\n        return dict((k, v) for k, v in iteritems(d1).items() if k not in d2)\n", "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if level < 0:\n        level = max(0, max(d1.keys() - d2.keys()))\n    d2_copy = copy.deepcopy(d2)\n    for k in d1:\n        if k not in d2 or d1[k]!= d2[k]:\n            d2[k] = d1[k]\n            if level == 0:\n                yield d2_copy\n            elif level > 0:\n                yield from intersection(d2_copy, d1, level-1)\n", "def difference(d1, d2, level=-1):\n    d = copy.copy(d1)\n    _max_depth = get_recursively(d, _sentinel, level)\n    if _max_depth >= 0:\n        d = intersection(d, d2)\n    return _max_depth\n", "def difference(d1, d2, level=-1):\n    if level == 0:\n        d2 = d2 or {}\n        d1 = d1 or {}\n        return dict((k, v) for k, v in iteritems(d1) if k not in d2)\n    d2_copy = copy.deepcopy(d2)\n    if d2_copy:\n        update_nested(keys(d1), d2_copy, d1)\n        return d1\n    return {}\n", "def difference(d1, d2, level=-1):\n    d = copy.copy(d1)\n    _check_recursively(d, d2, level)\n    return dict((k, v) for k, v in iterate_update(d, d2, _sentinel))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if isinstance(d1, dict) and isinstance(d2, dict):\n        d1 = deepcopy(d1)\n        for k in d2:\n            if k in d1 and d1[k] == d2[k]:\n                d1[k] = difference(d1[k], d2[k], level+1)\n        return d1\n    elif isinstance(d1, list) and isinstance(d2, list):\n        return [update_nested(k, d1, d2) for k in d1 if k not in d2]\n    elif isinstance(d1, set) and isinstance(d2, set):\n        return {update_nested(k, d1, d2) for k in d1 if k not in d2}\n    elif isinstance(d1, tuple) and isinstance(d2, tuple):\n        return tuple(update_nested(k, d1, d2) for k in d1 if k not in d2)\n    else:\n        return d1 - d2\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    d2_dict = d2()\n    d1_dict = d1()\n    for k in d1_dict:\n        if k not in d2_dict or d1_dict[k]!= d2_dict[k]:\n            return d1.copy()\n    return d2\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if level < 0:\n        level = max(len(d1), len(d2))\n\n    d2copy = copy.deepcopy(d2)\n    d2 = set(d2)\n\n    ret = {}\n    for k in d1:\n        if k not in d2:\n            ret[k] = d2copy[k]\n        elif d1[k]!= d2[k]:\n            ret[k] = d1[k]\n\n    for k in d2:\n        if k not in d1:\n            ret[k] = _sentinel\n        elif d1[k]!= d2[k]:\n            ret[k] = d1[k]\n        elif d1[k] == _sentinel and d2[k] == _sentinel:\n            # Don't include keys that are equal.\n            pass\n        else:\n            # Don't include keys that are in the difference.\n            if k in d1 and k in d2:\n             ", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if not isinstance(d1, dict):\n        raise TypeError('d1 must be a dictionary')\n    if not isinstance(d2, dict):\n        raise TypeError('d2 must be a dictionary')\n    if level < 0:\n        level = max(d1.keys())\n    d = copy.deepcopy(d1)\n    for key in d2:\n        if key not in d or d2[key]!= d[key]:\n            return d.setdefault(key, _sentinel)\n    return d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if level == 0:\n        return dict(d1)\n\n    keys = d1.keys() & d2.keys()\n    d1_sub = d1.copy()\n    d2_sub = d2.copy()\n\n    for k in keys:\n        d1_sub[k] = intersection(d1_sub.get(k, _sentinel), d2_sub.get(k, _sentinel))\n\n    return d1_sub\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if level == 0:\n        d1 = dict(d1)\n        d2 = dict(d2)\n        return dict((k, v) for k, v in iteritems(d1).items() if k not in d2)\n    else:\n        d1 = dict(d1.items())\n        d2 = dict(d2.items())\n        return dict((k, v) for k, v in iteritems(d1).items() if k not in d2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if level < 0:\n        level = max(0, max(d1.keys() - d2.keys()))\n    d2_copy = copy.deepcopy(d2)\n    for k in d1:\n        if k not in d2 or d1[k]!= d2[k]:\n            d2[k] = d1[k]\n            if level == 0:\n                yield d2_copy\n            elif level > 0:\n                yield from intersection(d2_copy, d1, level-1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    d = copy.copy(d1)\n    _max_depth = get_recursively(d, _sentinel, level)\n    if _max_depth >= 0:\n        d = intersection(d, d2)\n    return _max_depth\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if level == 0:\n        d2 = d2 or {}\n        d1 = d1 or {}\n        return dict((k, v) for k, v in iteritems(d1) if k not in d2)\n    d2_copy = copy.deepcopy(d2)\n    if d2_copy:\n        update_nested(keys(d1), d2_copy, d1)\n        return d1\n    return {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    d = copy.copy(d1)\n    _check_recursively(d, d2, level)\n    return dict((k, v) for k, v in iterate_update(d, d2, _sentinel))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "fill", "docstring": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.", "ques_id": "62b87af09a0c4fa8b80b34f1", "code": ["def fill(self, coord, weight=1):\n    if self.edges is not None and coord in self.edges:\n        midpoint = 0.5 * (self.edges[coord] + self.edges[coord + 1])\n        bin_width = (self.edges[coord + 1] - self.edges[coord])\n        bin_width = bin_width * weight\n        self.bins[coord] = np.digitize(\n            self.edges[coord], midpoint) - 1\n        self.bins[coord + 1] = np.digitize(\n            self.edges[coord + 1], midpoint + bin_width) - 1\n    else:\n        raise KeyError(\n            'Histo bin {} does not exist in {}.'.format(coord, self))\n", "def fill(self, coord, weight=1):\n    context = self.context.copy()\n    context.update({'coord': coord, 'weight': weight})\n    self._update_context(context)\n", "def fill(self, coord, weight=1):\n    if self.bins is None:\n        raise ValueError(\"fill() requires histogram bins to fill\")\n    context = {'coord': coord, 'weight': weight}\n    self._update_context(context)\n    return self\n", "def fill(self, coord, weight=1):\n    context = self.context.copy()\n    context[coord] = weight\n    self._update_context(context)\n", "def fill(self, coord, weight=1):\n    # TODO: Implement bin filling in in the future\n    if self.edges is not None:\n        outside = (coord > self.edges[0]) & (coord < self.edges[-1])\n    else:\n        outside = True\n\n    if outside:\n        raise ValueError('Outside bin edges: %r' % (self.edges,))\n\n    context = {}\n    context['coord'] = coord\n    context['weight'] = weight\n\n    return Histo1D(self.bins, self.nbins, context)\n", "def fill(self, coord, weight=1):\n    self.hist.fill(coord, weight)\n", "def fill(self, coord, weight=1):\n    if self.edges[-1] > coord:\n        raise ValueError(\"Outside of range\")\n    self._update_context({coord: weight})\n", "def fill(self, coord, weight=1):\n    # This is a stub implementation that should be over-ridden in subclasses.\n    if self.ranges is not None:\n        ranges = self.ranges\n    else:\n        ranges = []\n    ranges.append((coord, weight))\n    self.ranges = ranges\n", "def fill(self, coord, weight=1):\n    context = self._update_context({coord: weight})\n    self.ranges[coord] = context\n    self._update_context({})\n", "def fill(self, coord, weight=1):\n    ctx = self._update_context({coord: weight})\n    self._context.update(ctx)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fill(self, coord, weight=1):\n    if self.edges is not None and coord in self.edges:\n        midpoint = 0.5 * (self.edges[coord] + self.edges[coord + 1])\n        bin_width = (self.edges[coord + 1] - self.edges[coord])\n        bin_width = bin_width * weight\n        self.bins[coord] = np.digitize(\n            self.edges[coord], midpoint) - 1\n        self.bins[coord + 1] = np.digitize(\n            self.edges[coord + 1], midpoint + bin_width) - 1\n    else:\n        raise KeyError(\n            'Histo bin {} does not exist in {}.'.format(coord, self))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    context = self.context.copy()\n    context.update({'coord': coord, 'weight': weight})\n    self._update_context(context)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    if self.bins is None:\n        raise ValueError(\"fill() requires histogram bins to fill\")\n    context = {'coord': coord, 'weight': weight}\n    self._update_context(context)\n    return self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    context = self.context.copy()\n    context[coord] = weight\n    self._update_context(context)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    # TODO: Implement bin filling in in the future\n    if self.edges is not None:\n        outside = (coord > self.edges[0]) & (coord < self.edges[-1])\n    else:\n        outside = True\n\n    if outside:\n        raise ValueError('Outside bin edges: %r' % (self.edges,))\n\n    context = {}\n    context['coord'] = coord\n    context['weight'] = weight\n\n    return Histo1D(self.bins, self.nbins, context)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    self.hist.fill(coord, weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    if self.edges[-1] > coord:\n        raise ValueError(\"Outside of range\")\n    self._update_context({coord: weight})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    # This is a stub implementation that should be over-ridden in subclasses.\n    if self.ranges is not None:\n        ranges = self.ranges\n    else:\n        ranges = []\n    ranges.append((coord, weight))\n    self.ranges = ranges\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fill(self, coord, weight=1):\n    context = self._update_context({coord: weight})\n    self.ranges[coord] = context\n    self._update_context({})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    ctx = self._update_context({coord: weight})\n    self._context.update(ctx)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_validate_labels", "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]", "ques_id": "62b86aa3b4d922cb0e688d36", "code": ["def _validate_labels(labels):\n    _validate_labels_regex(labels)\n    _validate_labels_regex({k: _get_labels_regex(v) for k, v in labels.items()})\n", "def _validate_labels(labels):\n    for key, label_list in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(f'labels keys must be strings: {key}')\n        for label in label_list:\n            for key_pattern, value_pattern in label.items():\n                if not _get_labels_regex().match(key_pattern):\n                    raise ValueError(f'labels keys must be strings matching {key_pattern}')\n                if not _get_labels_regex().match(value_pattern):\n                    raise ValueError(f'labels values must be strings matching {value_pattern}')\n", "def _validate_labels(labels):\n    errors = []\n    for key, values in labels.items():\n        for value in values:\n            error = validate_key(key)\n            if error:\n                errors.append(error)\n                continue\n            error = validate_value(value)\n            if error:\n                errors.append(error)\n    if errors:\n        raise ValidationError(errors)\n", "def _validate_labels(labels):\n    for label_name, labels_key in labels.items():\n        labels_key_pattern = _label_key_pattern.format(label_name)\n        labels_value_pattern = _label_value_pattern.format(label_name)\n        labels_value_regex = _label_value_regex.format(label_name)\n\n        for key, value in labels_key.items():\n            if not isinstance(key, str):\n                raise ValueError(f\"{label_name} label key must be a string\")\n            if not isinstance(value, (list, tuple)):\n                raise ValueError(f\"{label_name} label value must be a list\")\n            for value_key, value_value in value:\n                if not isinstance(value_key, str):\n                    raise ValueError(f\"{label_name} label value key must be a string\")\n                if not isinstance(value_value, (list, tuple)):\n          ", "def _validate_labels(labels):\n    _validate_labels_regex(_get_labels_regex(labels))\n", "def _validate_labels(labels):\n    _validate_labels_regex(labels)\n    _validate_labels_regex({k: v for k, v in labels.items() if k!= \"$$\"})\n", "def _validate_labels(labels):\n    _validate_labels_regex(labels)\n", "def _validate_labels(labels):\n    for key, value in labels.items():\n        _validate_labels_key(key)\n        _validate_labels_value(value)\n", "def _validate_labels(labels):\n    for label, regex in labels.items():\n        if not isinstance(regex, dict):\n            raise ValueError(f\"labels['{label}'] must be a dict, got {type(regex).__name__}\")\n        for key, value in labels.items():\n            if not isinstance(key, str):\n                raise ValueError(f\"labels['{label}'] must be a str, got {type(key).__name__}\")\n            if not regex[key].match(value):\n                raise ValidationError(f\"labels['{label}'] does not match {regex[key].pattern}\")\n", "def _validate_labels(labels):\n    for key, values in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"labels keys must be strings: {}\".format(key))\n\n        if not isinstance(values, (list, tuple)):\n            raise ValueError(\"labels values must be lists: {}\".format(values))\n\n        for value in values:\n            if not _validate_label_regex(value):\n                raise ValueError(\n                    \"Label key '{}' must be a regular expression: {}\".format(\n                        key, _get_label_regex()\n                    )\n                )\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _validate_labels(labels):\n    _validate_labels_regex(labels)\n    _validate_labels_regex({k: _get_labels_regex(v) for k, v in labels.items()})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, label_list in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(f'labels keys must be strings: {key}')\n        for label in label_list:\n            for key_pattern, value_pattern in label.items():\n                if not _get_labels_regex().match(key_pattern):\n                    raise ValueError(f'labels keys must be strings matching {key_pattern}')\n                if not _get_labels_regex().match(value_pattern):\n                    raise ValueError(f'labels values must be strings matching {value_pattern}')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    errors = []\n    for key, values in labels.items():\n        for value in values:\n            error = validate_key(key)\n            if error:\n                errors.append(error)\n                continue\n            error = validate_value(value)\n            if error:\n                errors.append(error)\n    if errors:\n        raise ValidationError(errors)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for label_name, labels_key in labels.items():\n        labels_key_pattern = _label_key_pattern.format(label_name)\n        labels_value_pattern = _label_value_pattern.format(label_name)\n        labels_value_regex = _label_value_regex.format(label_name)\n\n        for key, value in labels_key.items():\n            if not isinstance(key, str):\n                raise ValueError(f\"{label_name} label key must be a string\")\n            if not isinstance(value, (list, tuple)):\n                raise ValueError(f\"{label_name} label value must be a list\")\n            for value_key, value_value in value:\n                if not isinstance(value_key, str):\n                    raise ValueError(f\"{label_name} label value key must be a string\")\n                if not isinstance(value_value, (list, tuple)):\n          ", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    _validate_labels_regex(_get_labels_regex(labels))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    _validate_labels_regex(labels)\n    _validate_labels_regex({k: v for k, v in labels.items() if k!= \"$$\"})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    _validate_labels_regex(labels)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, value in labels.items():\n        _validate_labels_key(key)\n        _validate_labels_value(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for label, regex in labels.items():\n        if not isinstance(regex, dict):\n            raise ValueError(f\"labels['{label}'] must be a dict, got {type(regex).__name__}\")\n        for key, value in labels.items():\n            if not isinstance(key, str):\n                raise ValueError(f\"labels['{label}'] must be a str, got {type(key).__name__}\")\n            if not regex[key].match(value):\n                raise ValidationError(f\"labels['{label}'] does not match {regex[key].pattern}\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, values in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"labels keys must be strings: {}\".format(key))\n\n        if not isinstance(values, (list, tuple)):\n            raise ValueError(\"labels values must be lists: {}\".format(values))\n\n        for value in values:\n            if not _validate_label_regex(value):\n                raise ValueError(\n                    \"Label key '{}' must be a regular expression: {}\".format(\n                        key, _get_label_regex()\n                    )\n                )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_get_resource_name_regex", "docstring": "Build or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.", "ques_id": "62b86a9eb4d922cb0e688d25", "code": ["def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "def _get_resource_name_regex():\n    return re.compile(_get_resource_name_regex())\n", "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return re.compile(_get_resource_name_regex())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_label_key_regex() + _get_label_value_regex()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_value", "docstring": "Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.", "ques_id": "62b86a4fb4d922cb0e688cf8", "code": ["def validate_value(value):\n    _validate_labels(value)\n    _validate_resource_name_regex(value)\n", "def validate_value(value):\n    if not _label_key_pattern.match(value):\n        raise ValidationError(\"Invalid label key pattern: {}\".format(value))\n    if not _label_value_pattern.match(value):\n        value = value.replace(\"_\", \" \").title()\n        raise ValidationError(\n            \"Invalid label value pattern: {}\".format(value),\n        )\n", "def validate_value(value):\n    _validate_labels(value.split(\",\"))\n", "def validate_value(value):\n    return _validate_resource_name(value)\n", "def validate_value(value):\n    _validate_resource_name(value)\n    _validate_resource_namespace(value)\n\n    match = re.match(_get_resource_name_regex(), value)\n    if not match:\n        raise ValidationError(f'Invalid label value: {value}')\n", "def validate_value(value):\n    try:\n        return _validate_labels(re.sub(_label_key_regex, '', value))\n    except Exception:\n        raise ValidationError(f'Invalid label {value!r}.')\n", "def validate_value(value):\n    if not _label_key_pattern.match(value):\n        raise ValidationError(f\"'{value}' is not a valid label\")\n\n    if not _label_value_regex.match(value):\n        raise ValidationError(\n            f\"'{value}' does not match the regular expression '{_label_value_regex.pattern}'\"\n        )\n", "def validate_value(value):\n    _validate_resource_name(value)\n    _validate_resource_namespace(value)\n", "def validate_value(value):\n    try:\n        match = _label_value_regex.match(value)\n        if not match:\n            raise ValidationError(\"Value {value} does not match regex {regex}\".format(value=value, regex=_label_value_regex.pattern))\n    except TypeError:\n        raise ValidationError(\"Value {value} is not a string or a regex\".format(value=value))\n", "def validate_value(value):\n    try:\n        re.compile(_get_resource_name_regex())\n    except re.error as e:\n        raise ValidationError(f'Invalid resource name pattern: {e}') from e\n    return value\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_value(value):\n    _validate_labels(value)\n    _validate_resource_name_regex(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    if not _label_key_pattern.match(value):\n        raise ValidationError(\"Invalid label key pattern: {}\".format(value))\n    if not _label_value_pattern.match(value):\n        value = value.replace(\"_\", \" \").title()\n        raise ValidationError(\n            \"Invalid label value pattern: {}\".format(value),\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    _validate_labels(value.split(\",\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    return _validate_resource_name(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    _validate_resource_name(value)\n    _validate_resource_namespace(value)\n\n    match = re.match(_get_resource_name_regex(), value)\n    if not match:\n        raise ValidationError(f'Invalid label value: {value}')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    try:\n        return _validate_labels(re.sub(_label_key_regex, '', value))\n    except Exception:\n        raise ValidationError(f'Invalid label {value!r}.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    if not _label_key_pattern.match(value):\n        raise ValidationError(f\"'{value}' is not a valid label\")\n\n    if not _label_value_regex.match(value):\n        raise ValidationError(\n            f\"'{value}' does not match the regular expression '{_label_value_regex.pattern}'\"\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    _validate_resource_name(value)\n    _validate_resource_namespace(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    try:\n        match = _label_value_regex.match(value)\n        if not match:\n            raise ValidationError(\"Value {value} does not match regex {regex}\".format(value=value, regex=_label_value_regex.pattern))\n    except TypeError:\n        raise ValidationError(\"Value {value} is not a string or a regex\".format(value=value))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    try:\n        re.compile(_get_resource_name_regex())\n    except re.error as e:\n        raise ValidationError(f'Invalid resource name pattern: {e}') from e\n    return value\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_key", "docstring": "Validate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.", "ques_id": "62b86a4fb4d922cb0e688cf7", "code": ["def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name(key)\n", "def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name_regex(_get_resource_name_regex(key))\n", "def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name(key)\n", "def validate_key(key):\n    if not _label_key_regex.match(key):\n        raise ValidationError(\n            f\"'{key}' does not match the regular expression {_label_key_pattern}\"\n        )\n", "def validate_key(key):\n    if not _label_key_pattern.match(key):\n        raise ValidationError(f'Invalid label key: {key!r}')\n\n    if not _label_value_pattern.match(key):\n        raise ValidationError(f'Invalid label value: {key!r}')\n", "def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name(key)\n    _validate_resource_namespace(key)\n", "def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name(key)\n", "def validate_key(key):\n    if not _label_key_pattern.match(key):\n        raise ValidationError(\n            f\"Invalid key: {repr(key)}. Must match regex: {_label_key_regex.pattern}\"\n        )\n", "def validate_key(key):\n    _validate_labels(re.split(_label_key_regex, key))\n", "def validate_key(key):\n    _validate_labels(key.split(_label_key_pattern))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name_regex(_get_resource_name_regex(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    if not _label_key_regex.match(key):\n        raise ValidationError(\n            f\"'{key}' does not match the regular expression {_label_key_pattern}\"\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    if not _label_key_pattern.match(key):\n        raise ValidationError(f'Invalid label key: {key!r}')\n\n    if not _label_value_pattern.match(key):\n        raise ValidationError(f'Invalid label value: {key!r}')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name(key)\n    _validate_resource_namespace(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    _validate_labels(key)\n    _validate_resource_name(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    if not _label_key_pattern.match(key):\n        raise ValidationError(\n            f\"Invalid key: {repr(key)}. Must match regex: {_label_key_regex.pattern}\"\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    _validate_labels(re.split(_label_key_regex, key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    _validate_labels(key.split(_label_key_pattern))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema_dict", "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.", "ques_id": "62b86a01b4d922cb0e688ccc", "code": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        # first level observer schemas are the only ones we care about\n        return manifest_dict\n    return {\n        'observer_schema': generate_default_observer_schema_list(manifest_dict.get('observer_schema', {})),\n       'resource_status': manifest_dict.get('resource_status', {}),\n    }\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if not manifest_dict:\n        return {}\n    if first_level:\n        return {'$schema': 'http://json-schema.org/draft-04/schema#'}\n    result = {'$schema': 'http://json-schema.org/draft-04/schema#',\n              'definitions': generate_default_observer_schema_list(manifest_dict['definitions'])}\n    result.update(manifest_dict)\n    return result\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        schema_part = {\"name\": \"observer_schema\"}\n    else:\n        schema_part = {\"name\": \"observer_schema\", \"type\": \"string\"}\n\n    logger.debug(\"generate_default_observer_schema_dict part: %s\", schema_part)\n\n    if manifest_dict.get(\"observer_schema\"):\n        schema_part[\"default\"] = manifest_dict[\"observer_schema\"]\n    else:\n        schema_part[\"default\"] = \"\"\n\n    return schema_part\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if manifest_dict is None:\n        return\n    if first_level:\n        current_observer_schema = {}\n    else:\n        current_observer_schema = manifest_dict.get('observer_schema')\n    if current_observer_schema is None:\n        return\n    for key, value in six.iteritems(current_observer_schema):\n        if key not in manifest_dict:\n            manifest_dict[key] = value\n        elif isinstance(manifest_dict[key], list):\n            manifest_dict[key] = generate_default_observer_schema_list(\n                manifest_dict[key], first_level=False)\n        elif isinstance(manifest_dict[key], dict):\n            manifest_dict[key] = generate_default_observer_schema_dict(\n                manifest_dict[key], first_level=False)\n    return manifest_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        schema = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    else:\n        schema = {\"type\": \"object\"}\n    return update_last_applied_manifest_dict_from_resp(manifest_dict, schema)\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        schema_dict = {}\n    else:\n        schema_dict = copy.deepcopy(manifest_dict['observer_schema'])\n\n    for k, v in schema_dict.items():\n        if isinstance(v, list):\n            schema_dict[k] = generate_default_observer_schema_list(v)\n        elif isinstance(v, dict):\n            schema_dict[k] = generate_default_observer_schema_dict(v)\n        else:\n            schema_dict[k] = v\n\n    return schema_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if manifest_dict.get('observer_schema'):\n        if not first_level:\n            yield 'observer_schema'\n        else:\n            yield from generate_default_observer_schema_list(manifest_dict.get('observer_schema'), first_level)\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if not manifest_dict.get(\"observer_schema\"):\n        return\n    if first_level:\n        schema_dict = {\"@id\": \"observer_schema\", \"@type\": \"ObserverSchema\"}\n    else:\n        schema_dict = {\"@id\": \"observer_schema_parts\", \"@type\": \"ObserverSchemaParts\"}\n    if \"observer_schema_list\" in manifest_dict:\n        for obs in manifest_dict[\"observer_schema_list\"]:\n            schema_dict[\"observer_schema_list\"][obs] = generate_default_observer_schema_dict(\n                obs, first_level\n            )\n    manifest_dict[\"observer_schema\"] = schema_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        observer_schema = {}\n    else:\n        observer_schema = manifest_dict.get('observer_schema', {})\n    return generate_observer_schema_dict(observer_schema)\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if not manifest_dict:\n        return\n\n    if first_level:\n        observer_schema = {\n            '@type': 'com.apple.security.observer.SecurityObserverSchema',\n           'version': '1.0',\n           'manifest': manifest_dict,\n        }\n        return observer_schema\n\n    if 'observer_schema' not in manifest_dict:\n        return\n\n    observer_schema = manifest_dict['observer_schema']\n    for k, v in observer_schema.items():\n        if isinstance(v, dict):\n            observer_schema[k] = generate_default_observer_schema_dict(v, first_level=False)\n\n    return observer_schema\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        # first level observer schemas are the only ones we care about\n        return manifest_dict\n    return {\n        'observer_schema': generate_default_observer_schema_list(manifest_dict.get('observer_schema', {})),\n       'resource_status': manifest_dict.get('resource_status', {}),\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if not manifest_dict:\n        return {}\n    if first_level:\n        return {'$schema': 'http://json-schema.org/draft-04/schema#'}\n    result = {'$schema': 'http://json-schema.org/draft-04/schema#',\n              'definitions': generate_default_observer_schema_list(manifest_dict['definitions'])}\n    result.update(manifest_dict)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        schema_part = {\"name\": \"observer_schema\"}\n    else:\n        schema_part = {\"name\": \"observer_schema\", \"type\": \"string\"}\n\n    logger.debug(\"generate_default_observer_schema_dict part: %s\", schema_part)\n\n    if manifest_dict.get(\"observer_schema\"):\n        schema_part[\"default\"] = manifest_dict[\"observer_schema\"]\n    else:\n        schema_part[\"default\"] = \"\"\n\n    return schema_part\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if manifest_dict is None:\n        return\n    if first_level:\n        current_observer_schema = {}\n    else:\n        current_observer_schema = manifest_dict.get('observer_schema')\n    if current_observer_schema is None:\n        return\n    for key, value in six.iteritems(current_observer_schema):\n        if key not in manifest_dict:\n            manifest_dict[key] = value\n        elif isinstance(manifest_dict[key], list):\n            manifest_dict[key] = generate_default_observer_schema_list(\n                manifest_dict[key], first_level=False)\n        elif isinstance(manifest_dict[key], dict):\n            manifest_dict[key] = generate_default_observer_schema_dict(\n                manifest_dict[key], first_level=False)\n    return manifest_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        schema = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    else:\n        schema = {\"type\": \"object\"}\n    return update_last_applied_manifest_dict_from_resp(manifest_dict, schema)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        schema_dict = {}\n    else:\n        schema_dict = copy.deepcopy(manifest_dict['observer_schema'])\n\n    for k, v in schema_dict.items():\n        if isinstance(v, list):\n            schema_dict[k] = generate_default_observer_schema_list(v)\n        elif isinstance(v, dict):\n            schema_dict[k] = generate_default_observer_schema_dict(v)\n        else:\n            schema_dict[k] = v\n\n    return schema_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if manifest_dict.get('observer_schema'):\n        if not first_level:\n            yield 'observer_schema'\n        else:\n            yield from generate_default_observer_schema_list(manifest_dict.get('observer_schema'), first_level)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if not manifest_dict.get(\"observer_schema\"):\n        return\n    if first_level:\n        schema_dict = {\"@id\": \"observer_schema\", \"@type\": \"ObserverSchema\"}\n    else:\n        schema_dict = {\"@id\": \"observer_schema_parts\", \"@type\": \"ObserverSchemaParts\"}\n    if \"observer_schema_list\" in manifest_dict:\n        for obs in manifest_dict[\"observer_schema_list\"]:\n            schema_dict[\"observer_schema_list\"][obs] = generate_default_observer_schema_dict(\n                obs, first_level\n            )\n    manifest_dict[\"observer_schema\"] = schema_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        observer_schema = {}\n    else:\n        observer_schema = manifest_dict.get('observer_schema', {})\n    return generate_observer_schema_dict(observer_schema)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if not manifest_dict:\n        return\n\n    if first_level:\n        observer_schema = {\n            '@type': 'com.apple.security.observer.SecurityObserverSchema',\n           'version': '1.0',\n           'manifest': manifest_dict,\n        }\n        return observer_schema\n\n    if 'observer_schema' not in manifest_dict:\n        return\n\n    observer_schema = manifest_dict['observer_schema']\n    for k, v in observer_schema.items():\n        if isinstance(v, dict):\n            observer_schema[k] = generate_default_observer_schema_dict(v, first_level=False)\n\n    return observer_schema\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_list_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "ques_id": "62b869ebb4d922cb0e688cc6", "code": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result: observer schema dict\n    \"\"\"\n    result = {}\n    for manifest_id, manifest in last_applied_manifest.items():\n        manifest_dict = generate_default_observer_schema_dict(\n            manifest, observer_schema\n        )\n        result[manifest_id] = manifest_dict\n    return result\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result:\n        list[ObserverSchema]: A list of observer schemas\n    \"\"\"\n    result = []\n    for observer_schema_dict in last_applied_manifest:\n        new_obs = update_last_applied_manifest_dict_from_resp(\n            observer_schema_dict, observer_schema, response\n        )\n        if not new_obs:\n            continue\n        result.append(new_obs)\n    return result\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_dict_from_resp(last_applied_manifest, observer_schema, response)\n    return result\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    result.update(last_applied_manifest)\n    update_last_applied_manifest_dict(\n        result, observer_schema, response\n    )\n    return result\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    for manifest in response:\n        manifest_dict = manifest.to_dict()\n        manifest_dict[\"_source\"] = manifest\n        manifest_dict[\"_last_applied\"] = last_applied_manifest.to_dict()\n        manifest_dict[\"_observer_schema\"] = observer_schema\n        result[manifest[\"_key\"]] = manifest_dict\n    return result\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = []\n    for manifest_item in response:\n        manifest = update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest, observer_schema, manifest_item\n        )\n        manifest[\"@id\"] = manifest_item[\"@id\"]\n        result.append(manifest)\n    return result\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = []\n    for manifest in response.json():\n        if manifest[\"observerSchema\"] == observer_schema:\n            manifest[\"lastAppliedManifest\"] = last_applied_manifest\n            result.append(manifest)\n\n    return result\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = []\n    for item in last_applied_manifest:\n        result.append(update_last_applied_manifest_dict_from_resp(\n            item, observer_schema, response\n        ))\n    return result\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_dict_from_resp(\n        last_applied_manifest, observer_schema, response\n    )\n    logger.info(\n        \"result of update_last_applied_manifest_list_from_resp = %s\", result\n    )\n    return result\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result: aiohttp response\n    \"\"\"\n    result = update_last_applied_manifest_dict(last_applied_manifest, response)\n    observer_schema.validate(result)\n    return result\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result: observer schema dict\n    \"\"\"\n    result = {}\n    for manifest_id, manifest in last_applied_manifest.items():\n        manifest_dict = generate_default_observer_schema_dict(\n            manifest, observer_schema\n        )\n        result[manifest_id] = manifest_dict\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result:\n        list[ObserverSchema]: A list of observer schemas\n    \"\"\"\n    result = []\n    for observer_schema_dict in last_applied_manifest:\n        new_obs = update_last_applied_manifest_dict_from_resp(\n            observer_schema_dict, observer_schema, response\n        )\n        if not new_obs:\n            continue\n        result.append(new_obs)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_dict_from_resp(last_applied_manifest, observer_schema, response)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    result.update(last_applied_manifest)\n    update_last_applied_manifest_dict(\n        result, observer_schema, response\n    )\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    for manifest in response:\n        manifest_dict = manifest.to_dict()\n        manifest_dict[\"_source\"] = manifest\n        manifest_dict[\"_last_applied\"] = last_applied_manifest.to_dict()\n        manifest_dict[\"_observer_schema\"] = observer_schema\n        result[manifest[\"_key\"]] = manifest_dict\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = []\n    for manifest_item in response:\n        manifest = update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest, observer_schema, manifest_item\n        )\n        manifest[\"@id\"] = manifest_item[\"@id\"]\n        result.append(manifest)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = []\n    for manifest in response.json():\n        if manifest[\"observerSchema\"] == observer_schema:\n            manifest[\"lastAppliedManifest\"] = last_applied_manifest\n            result.append(manifest)\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = []\n    for item in last_applied_manifest:\n        result.append(update_last_applied_manifest_dict_from_resp(\n            item, observer_schema, response\n        ))\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_dict_from_resp(\n        last_applied_manifest, observer_schema, response\n    )\n    logger.info(\n        \"result of update_last_applied_manifest_list_from_resp = %s\", result\n    )\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result: aiohttp response\n    \"\"\"\n    result = update_last_applied_manifest_dict(last_applied_manifest, response)\n    observer_schema.validate(result)\n    return result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_dict_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "ques_id": "62b869eab4d922cb0e688cc5", "code": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_list_from_resp(\n        last_applied_manifest, observer_schema, response\n    )\n    logger.debug(\"update_last_applied_manifest_dict_from_resp %s\", result)\n    return result\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result: {}\n    \"\"\"\n    manifest_dict = update_last_applied_manifest_dict_from_spec(\n        response, observer_schema\n    )\n    if not manifest_dict:\n        return last_applied_manifest\n    return manifest_dict\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\" generate_certificate resurrects with :func:``update_last_applied_manifest_dict_from_resp``, this\n    \"\"\"\n    manifest_dict, manifest_list = update_last_applied_manifest_dict_from_spec(\n        resource_status_new=last_applied_manifest[\"resourceStatus\"],\n        resource_status_old=response[\"resourceStatus\"],\n        resource_observed=response[\"resourceObserved\"],\n    )\n\n    return manifest_dict, manifest_list\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    for manifest in response:\n        if manifest['type']!='manifest':\n            continue\n        manifest_dict = manifest['content']\n        manifest_dict.update(update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest,\n            observer_schema,\n            manifest_dict\n        ))\n        result[manifest['id']] = manifest_dict\n    return result\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    for manifest in response.get(\"manifests\", []):\n        manifest_dict = manifest.get(\"manifest\")\n        manifest_dict[\"_id\"] = manifest[\"_id\"]\n        if manifest_dict is not None:\n            manifest_dict[\"observerSchema\"] = observer_schema\n            manifest_dict[\"lastApplied\"] = utc_difference(last_applied_manifest, manifest_dict)\n            result[manifest_dict[\"_id\"]] = manifest_dict\n    return result\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_list(\n        last_applied_manifest, response\n    )  # type: Dict[str, Any]\n    logger.debug(result)\n    logger.debug(observer_schema)\n    logger.debug(observer_schema.dump(last_applied_manifest))\n    return result\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result:\n\n        result = {\n           'resources': {},\n            'last_applied_manifest': last_applied_manifest,\n            'observer_schema': observer_schema\n        }\n    \"\"\"\n    result = copy.deepcopy(last_applied_manifest)\n    result['observer_schema'] = observer_schema\n    result['resources'] = update_last_applied_manifest_dict_from_spec(\n        result['observer_schema'], response\n    )\n    return result\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_list_from_resp(\n        last_applied_manifest, observer_schema, response\n    )\n    return result\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    for manifest_dict in response:\n        manifest = manifest_dict.copy()\n        manifest[\"_id\"] = manifest_dict[\"_id\"]\n        manifest[\"_rev\"] = manifest_dict[\"_rev\"]\n        manifest[\"observer_schema\"] = observer_schema\n        manifest[\"last_applied_manifest\"] = last_applied_manifest\n        result[manifest_dict[\"_id\"]] = manifest\n    return result\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    result[\"manifest\"] = {}\n    if response is not None:\n        result[\"manifest\"][\"observer_schema\"] = observer_schema\n        result[\"manifest\"][\"resource_status_new\"] = response.status\n        result[\"manifest\"][\"resource_status_old\"] = response.status_code\n        result[\"manifest\"][\"resources\"] = []\n        for resource in response.resources:\n            result[\"manifest\"][\"resources\"].append(\n                generate_default_observer_schema_dict(resource, last_applied_manifest)\n            )\n    return result\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_list_from_resp(\n        last_applied_manifest, observer_schema, response\n    )\n    logger.debug(\"update_last_applied_manifest_dict_from_resp %s\", result)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result: {}\n    \"\"\"\n    manifest_dict = update_last_applied_manifest_dict_from_spec(\n        response, observer_schema\n    )\n    if not manifest_dict:\n        return last_applied_manifest\n    return manifest_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\" generate_certificate resurrects with :func:``update_last_applied_manifest_dict_from_resp``, this\n    \"\"\"\n    manifest_dict, manifest_list = update_last_applied_manifest_dict_from_spec(\n        resource_status_new=last_applied_manifest[\"resourceStatus\"],\n        resource_status_old=response[\"resourceStatus\"],\n        resource_observed=response[\"resourceObserved\"],\n    )\n\n    return manifest_dict, manifest_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    for manifest in response:\n        if manifest['type']!='manifest':\n            continue\n        manifest_dict = manifest['content']\n        manifest_dict.update(update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest,\n            observer_schema,\n            manifest_dict\n        ))\n        result[manifest['id']] = manifest_dict\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    for manifest in response.get(\"manifests\", []):\n        manifest_dict = manifest.get(\"manifest\")\n        manifest_dict[\"_id\"] = manifest[\"_id\"]\n        if manifest_dict is not None:\n            manifest_dict[\"observerSchema\"] = observer_schema\n            manifest_dict[\"lastApplied\"] = utc_difference(last_applied_manifest, manifest_dict)\n            result[manifest_dict[\"_id\"]] = manifest_dict\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_list(\n        last_applied_manifest, response\n    )  # type: Dict[str, Any]\n    logger.debug(result)\n    logger.debug(observer_schema)\n    logger.debug(observer_schema.dump(last_applied_manifest))\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result:\n\n        result = {\n           'resources': {},\n            'last_applied_manifest': last_applied_manifest,\n            'observer_schema': observer_schema\n        }\n    \"\"\"\n    result = copy.deepcopy(last_applied_manifest)\n    result['observer_schema'] = observer_schema\n    result['resources'] = update_last_applied_manifest_dict_from_spec(\n        result['observer_schema'], response\n    )\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = update_last_applied_manifest_list_from_resp(\n        last_applied_manifest, observer_schema, response\n    )\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    for manifest_dict in response:\n        manifest = manifest_dict.copy()\n        manifest[\"_id\"] = manifest_dict[\"_id\"]\n        manifest[\"_rev\"] = manifest_dict[\"_rev\"]\n        manifest[\"observer_schema\"] = observer_schema\n        manifest[\"last_applied_manifest\"] = last_applied_manifest\n        result[manifest_dict[\"_id\"]] = manifest\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    result = {}\n    result[\"manifest\"] = {}\n    if response is not None:\n        result[\"manifest\"][\"observer_schema\"] = observer_schema\n        result[\"manifest\"][\"resource_status_new\"] = response.status\n        result[\"manifest\"][\"resource_status_old\"] = response.status_code\n        result[\"manifest\"][\"resources\"] = []\n        for resource in response.resources:\n            result[\"manifest\"][\"resources\"].append(\n                generate_default_observer_schema_dict(resource, last_applied_manifest)\n            )\n    return result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema", "docstring": "Generate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema", "ques_id": "62b869eab4d922cb0e688cbf", "code": ["def generate_default_observer_schema(app):\n    manifest = app.spec.get('manifest')\n    if manifest is not None:\n        for resource in manifest.get('resources', []):\n            if resource.get('status')!= 'ACTIVE':\n                resource_status_old = resource.get('status')\n                resource_status_new = update_last_applied_manifest_dict_from_resp(\n                    resource,\n                    resource_status_old,\n                    resource_observed=False\n                )\n                resource['status'] = resource_status_new\n", "def generate_default_observer_schema(app):\n    manifest_list = []\n\n    for manifest in app.manifest['configuration']['resources']:\n        if manifest.get('type')!= 'custom-observer-schema':\n            manifest_list.append(generate_default_observer_schema_dict(manifest))\n\n    app.manifest['configuration']['resources'] = manifest_list\n", "def generate_default_observer_schema(app):\n    if 'observer-schema' not in app.manifest:\n        spec = app.manifest['kubernetes-spec']\n        if 'observer-schema' in spec:\n            app.manifest['observer-schema'] = generate_default_observer_schema_dict(spec['observer-schema'])\n", "def generate_default_observer_schema(app):\n    for manifest in app.manifest.get('manifests', []):\n        if manifest.get('type') == 'custom':\n            yield generate_default_observer_schema_dict(manifest)\n", "def generate_default_observer_schema(app):\n    manifest_dict = app.config.get('KUR_SCHEMA_MANIFEST', {})\n    if manifest_dict:\n        manifest_list = [\n            {\n                'name': manifest['name'],\n                'type': manifest['type'],\n                'attributes': manifest['attributes']\n            }\n            for manifest in manifest_dict.values()\n        ]\n        return generate_observer_schema(manifest_list)\n", "def generate_default_observer_schema(app):\n    manifest_dict = load_yaml_file(app.config['Kubernetes']['manifest'])\n    spec_schema = generate_default_observer_schema_dict(manifest_dict)\n    schema = yaml.dump(spec_schema, default_flow_style=False)\n\n    with open(app.config['Kubernetes']['observer_schema_path'], 'w') as f:\n        f.write(schema)\n", "def generate_default_observer_schema(app):\n    for manifest in app.manifest.get('spec', {}).get('observerSchema', []):\n        if manifest.get('type') == 'custom':\n            yield generate_observer_schema_from_spec(manifest)\n", "def generate_default_observer_schema(app):\n    if 'observer_schema' in app.manifest:\n        logger.debug('Observer schema spec is not specified in the manifest')\n        return\n\n    manifest_list = app.manifest['kubernetes']['resources']\n    logger.debug('Generating default observer schema for %d resources',\n                 len(manifest_list))\n\n    for manifest in manifest_list:\n        manifest_dict = generate_default_observer_schema_dict(\n            manifest, app.manifest['kubernetes'])\n        app.manifest['kubernetes']['resources'][manifest] = manifest_dict\n", "def generate_default_observer_schema(app):\n    manifest_list = [\n        manifest.get(\"schema\")\n        for manifest in app.manifest\n        if manifest.get(\"schema\")\n        and not manifest.get(\"observer-schema\")\n    ]\n    for manifest in manifest_list:\n        if manifest.get(\"observer-schema\"):\n            continue\n        logger.info(\n            \"Generating default observer schema for Kubernetes resource: %s\",\n            manifest[\"name\"],\n        )\n        manifest[\"observer-schema\"] = generate_default_observer_schema_dict(manifest)\n", "def generate_default_observer_schema(app):\n    for manifest_dict in app.manifest.get('manifest', []):\n        if 'observer_schema' not in manifest_dict:\n            manifest_dict['observer_schema'] = generate_default_observer_schema_dict(\n                manifest_dict)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema(app):\n    manifest = app.spec.get('manifest')\n    if manifest is not None:\n        for resource in manifest.get('resources', []):\n            if resource.get('status')!= 'ACTIVE':\n                resource_status_old = resource.get('status')\n                resource_status_new = update_last_applied_manifest_dict_from_resp(\n                    resource,\n                    resource_status_old,\n                    resource_observed=False\n                )\n                resource['status'] = resource_status_new\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    manifest_list = []\n\n    for manifest in app.manifest['configuration']['resources']:\n        if manifest.get('type')!= 'custom-observer-schema':\n            manifest_list.append(generate_default_observer_schema_dict(manifest))\n\n    app.manifest['configuration']['resources'] = manifest_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    if 'observer-schema' not in app.manifest:\n        spec = app.manifest['kubernetes-spec']\n        if 'observer-schema' in spec:\n            app.manifest['observer-schema'] = generate_default_observer_schema_dict(spec['observer-schema'])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    for manifest in app.manifest.get('manifests', []):\n        if manifest.get('type') == 'custom':\n            yield generate_default_observer_schema_dict(manifest)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    manifest_dict = app.config.get('KUR_SCHEMA_MANIFEST', {})\n    if manifest_dict:\n        manifest_list = [\n            {\n                'name': manifest['name'],\n                'type': manifest['type'],\n                'attributes': manifest['attributes']\n            }\n            for manifest in manifest_dict.values()\n        ]\n        return generate_observer_schema(manifest_list)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    manifest_dict = load_yaml_file(app.config['Kubernetes']['manifest'])\n    spec_schema = generate_default_observer_schema_dict(manifest_dict)\n    schema = yaml.dump(spec_schema, default_flow_style=False)\n\n    with open(app.config['Kubernetes']['observer_schema_path'], 'w') as f:\n        f.write(schema)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    for manifest in app.manifest.get('spec', {}).get('observerSchema', []):\n        if manifest.get('type') == 'custom':\n            yield generate_observer_schema_from_spec(manifest)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    if 'observer_schema' in app.manifest:\n        logger.debug('Observer schema spec is not specified in the manifest')\n        return\n\n    manifest_list = app.manifest['kubernetes']['resources']\n    logger.debug('Generating default observer schema for %d resources',\n                 len(manifest_list))\n\n    for manifest in manifest_list:\n        manifest_dict = generate_default_observer_schema_dict(\n            manifest, app.manifest['kubernetes'])\n        app.manifest['kubernetes']['resources'][manifest] = manifest_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    manifest_list = [\n        manifest.get(\"schema\")\n        for manifest in app.manifest\n        if manifest.get(\"schema\")\n        and not manifest.get(\"observer-schema\")\n    ]\n    for manifest in manifest_list:\n        if manifest.get(\"observer-schema\"):\n            continue\n        logger.info(\n            \"Generating default observer schema for Kubernetes resource: %s\",\n            manifest[\"name\"],\n        )\n        manifest[\"observer-schema\"] = generate_default_observer_schema_dict(manifest)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    for manifest_dict in app.manifest.get('manifest', []):\n        if 'observer_schema' not in manifest_dict:\n            manifest_dict['observer_schema'] = generate_default_observer_schema_dict(\n                manifest_dict)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "format", "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).", "ques_id": "62b43427903eeb48555d3ea5", "code": ["def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[str, Union[int, bytes]]\n) -> Tuple[str, Mapping[str, Union[int, bytes]]]:\n\nif self._in_obj is not None:\nreturn self._in_obj.format(sql, params)\n\nif self._out_obj is not None:\nreturn self._out_obj.format(sql, params)\n\nreturn self._create_converter().format(sql, params)\n", "def format(\n\t\tself,\n\t\tsql: Union[bytes, str],\nparams: Mapping[Union[str, int], Any] = None\n) -> Tuple[bytes, Mapping[Union[str, int], Any]]:\n\nif isinstance(self._in_style, str):\nreturn self._create_converter().format(\nsql=sql,\nparams=params\n)\n\nif isinstance(self._in_style, int):\nreturn self._create_in_regex().format(\nsql=sql,\nparams=params\n)\n\nraise TypeError(\n\"SQL Query style (%s) is not supported.\" %\ntype(self._in_style).__name__\n)\n", "def format(\n\t\tself,\n\t\tsql: Union[bytes, str],\nparams: Mapping[Union[str, int], Any] = None\n) -> Tuple[bytes, Mapping[Union[str, int], Any]]:\nif self._out_obj is None:\nraise TypeError('{} object has no format() method'.format(self.__class__.__name__))\n\nif isinstance(self._out_obj, SQLParams):\nout_params = self._out_obj.params\nelse:\nout_params = self._out_obj\n\nif params is None:\nparams = {}\n\nif isinstance(params, Mapping):\nparams = {self._in_obj.get_param_name(k): v for k, v in params.items()}\n\nparam_tuples = tuple(self._in_obj.convert(param, params) for param in params)\n\nreturn sql, param_tuples\n", "def format(\n\t\tself,\n\t\tsql: Union['bytes','str'],\nparams: Union['Mapping[str, int]', 'Sequence[Union[str, int]]']\n) -> Tuple['str', 'Sequence[Union[str, int]]']:\nif isinstance(self._in_obj, SQLParams):\nreturn self._converter.format(sql, params)\nelse:\nreturn self._converter.formatmany(sql, params)\n", "def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[Union[str, int], Union[str, int]] = None\n) -> Tuple[str,...]:\nif isinstance(sql, bytes):\nparams = self._in_obj.get_params(sql)\nelif isinstance(sql, str):\nparams = self._in_obj.get_params(sql.encode('utf-8'))\n\nif self._out_obj is not None:\nreturn self._out_obj.format(sql, params)\nelse:\nreturn self._converter.format(sql, params)\n", "def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[str, int] = None\n) -> Tuple[str, Mapping[str, int]]:\nif self.in_style is None:\nif isinstance(sql, bytes):\nreturn self._format_bytes(sql, params)\nelse:\nreturn self._format_sql(sql, params)\nelse:\nif isinstance(sql, bytes):\nreturn self._format_sql(sql, params)\nelif isinstance(sql, str):\nreturn self._format_str(sql, params)\nelse:\nraise TypeError(\"sql must be str or bytes, not %r\" % type(sql))\n", "def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[Union[str, int], Union[str, int]]\n) -> Tuple[str, Mapping[Union[str, int], Union[str, int]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.\n\n*params* must be a\n:class:`~collections.abc.Mapping` or\n:class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`\n\"\"\"+self._in_regex.pattern()+\"\"\"\n\"\"\"+self._out_regex.pattern()+\"\"\"\n\"\"\"+self._expand_tuples.pattern()+\"\"\"\n\"\"\"+self._out_style.pattern()+\"\"\"\n\"\"\")\n\"\"\"\nreturn self._out_obj.format(sql, params)\n", "def format(\n\t\tself,\n\t\tsql: Union['bytes','str'],\nparams: Union['Mapping[str, int]', 'Sequence[Union[str, int]]'] = None\n) -> Tuple['str', 'Sequence[Union[str, int]]']:\nreturn self._converter.format(sql, params)\n", "def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[str, int] = None\n) -> Tuple[str, Mapping[str, int]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`) contains\nthe set of in-style parameters. It maps each parameter (:class:`str` or\n:class:`int`) to value. If :attr:`.SQLParams.in_style` is a named\nparameter style. then *params* must be a\n:class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style` is\nan ordinal parameter style, then *params* must be a\n:class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:``)\n- The formatted parameters (:class:`~collections.abc.Mapping`)\n\"\"\"\nif params is None:\nparams = self._in_obj.params\n\nif isinstance(sql, bytes):\nparams = {self._in", "def format(\n\t\tself,\n\t\tsql: Union[bytes, str],\nparams: Union[Mapping[Union[str, int], Any], Sequence[Any]]\n) -> Tuple[bytes, Sequence[Any]]:\nif self._out_obj is not None:\nreturn self._out_obj.format(sql, params)\n\nif self._in_obj is not None:\nreturn self._in_obj.format(sql, params)\n\nif self._in_style is None:\nreturn self._create_converter().format(sql, params)\n\nif self._out_style is None:\nreturn self._create_in_regex().format(sql, params)\n\nreturn self._create_converter().format(sql, params, self._out_style)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[str, Union[int, bytes]]\n) -> Tuple[str, Mapping[str, Union[int, bytes]]]:\n\nif self._in_obj is not None:\nreturn self._in_obj.format(sql, params)\n\nif self._out_obj is not None:\nreturn self._out_obj.format(sql, params)\n\nreturn self._create_converter().format(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: Union[bytes, str],\nparams: Mapping[Union[str, int], Any] = None\n) -> Tuple[bytes, Mapping[Union[str, int], Any]]:\n\nif isinstance(self._in_style, str):\nreturn self._create_converter().format(\nsql=sql,\nparams=params\n)\n\nif isinstance(self._in_style, int):\nreturn self._create_in_regex().format(\nsql=sql,\nparams=params\n)\n\nraise TypeError(\n\"SQL Query style (%s) is not supported.\" %\ntype(self._in_style).__name__\n)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: Union[bytes, str],\nparams: Mapping[Union[str, int], Any] = None\n) -> Tuple[bytes, Mapping[Union[str, int], Any]]:\nif self._out_obj is None:\nraise TypeError('{} object has no format() method'.format(self.__class__.__name__))\n\nif isinstance(self._out_obj, SQLParams):\nout_params = self._out_obj.params\nelse:\nout_params = self._out_obj\n\nif params is None:\nparams = {}\n\nif isinstance(params, Mapping):\nparams = {self._in_obj.get_param_name(k): v for k, v in params.items()}\n\nparam_tuples = tuple(self._in_obj.convert(param, params) for param in params)\n\nreturn sql, param_tuples\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: Union['bytes','str'],\nparams: Union['Mapping[str, int]', 'Sequence[Union[str, int]]']\n) -> Tuple['str', 'Sequence[Union[str, int]]']:\nif isinstance(self._in_obj, SQLParams):\nreturn self._converter.format(sql, params)\nelse:\nreturn self._converter.formatmany(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[Union[str, int], Union[str, int]] = None\n) -> Tuple[str,...]:\nif isinstance(sql, bytes):\nparams = self._in_obj.get_params(sql)\nelif isinstance(sql, str):\nparams = self._in_obj.get_params(sql.encode('utf-8'))\n\nif self._out_obj is not None:\nreturn self._out_obj.format(sql, params)\nelse:\nreturn self._converter.format(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[str, int] = None\n) -> Tuple[str, Mapping[str, int]]:\nif self.in_style is None:\nif isinstance(sql, bytes):\nreturn self._format_bytes(sql, params)\nelse:\nreturn self._format_sql(sql, params)\nelse:\nif isinstance(sql, bytes):\nreturn self._format_sql(sql, params)\nelif isinstance(sql, str):\nreturn self._format_str(sql, params)\nelse:\nraise TypeError(\"sql must be str or bytes, not %r\" % type(sql))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[Union[str, int], Union[str, int]]\n) -> Tuple[str, Mapping[Union[str, int], Union[str, int]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.\n\n*params* must be a\n:class:`~collections.abc.Mapping` or\n:class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`\n\"\"\"+self._in_regex.pattern()+\"\"\"\n\"\"\"+self._out_regex.pattern()+\"\"\"\n\"\"\"+self._expand_tuples.pattern()+\"\"\"\n\"\"\"+self._out_style.pattern()+\"\"\"\n\"\"\")\n\"\"\"\nreturn self._out_obj.format(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: Union['bytes','str'],\nparams: Union['Mapping[str, int]', 'Sequence[Union[str, int]]'] = None\n) -> Tuple['str', 'Sequence[Union[str, int]]']:\nreturn self._converter.format(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: Union[str, bytes],\nparams: Mapping[str, int] = None\n) -> Tuple[str, Mapping[str, int]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`) contains\nthe set of in-style parameters. It maps each parameter (:class:`str` or\n:class:`int`) to value. If :attr:`.SQLParams.in_style` is a named\nparameter style. then *params* must be a\n:class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style` is\nan ordinal parameter style, then *params* must be a\n:class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:``)\n- The formatted parameters (:class:`~collections.abc.Mapping`)\n\"\"\"\nif params is None:\nparams = self._in_obj.params\n\nif isinstance(sql, bytes):\nparams = {self._in", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: Union[bytes, str],\nparams: Union[Mapping[Union[str, int], Any], Sequence[Any]]\n) -> Tuple[bytes, Sequence[Any]]:\nif self._out_obj is not None:\nreturn self._out_obj.format(sql, params)\n\nif self._in_obj is not None:\nreturn self._in_obj.format(sql, params)\n\nif self._in_style is None:\nreturn self._create_converter().format(sql, params)\n\nif self._out_style is None:\nreturn self._create_in_regex().format(sql, params)\n\nreturn self._create_converter().format(sql, params, self._out_style)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "formatmany", "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).", "ques_id": "62b43428903eeb48555d3eaa", "code": ["def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Union[Iterable[Union[str, int]], Mapping[Union[str, int], int]]\n) -> Sequence[Tuple[str, Union[str, int]]]:\nif self._out_obj is None:\nself._out_obj = []\n\nif isinstance(many_params, Mapping):\nparams = many_params\nelse:\nparams = {\nk: int(v) for k, v in many_params\n}\n\nparams_iter = self._in_obj.items()\nparams_iter.extend(params.items())\nparams = tuple(params_iter)\n\nif self._out_style is None:\nreturn self._out_obj + [(sql, params)]\n\nout_style = self._out_style\n\nif out_style == self.OUT_STYLE_ORDERED:\nreturn self._out_obj + [(sql, params)]\n\nif out_style == self.OUT_STYLE_LITERAL:\nreturn self._out_obj + [(self._in_obj.get(k, k), v) for k, v in params]\n\nreturn self._out_obj + [(self._in_obj.get(k, k), v) for k, v in params]\n", "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Sequence[Union[str, bytes]]:\nif self._out_obj is None:\nraise RuntimeError(\"Out-style parameters must be specified before \"\n\"calling format().\")\n\nif isinstance(sql, bytes):\nsql = sql.decode(\"utf-8\")\n\nparams = {}\nfor param in many_params:\nparams[param] = None\n\nreturn self._out_obj.formatmany(sql, params)\n", "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Sequence[Union[str, bytes]]:\nif self._out_obj is None:\nraise exc.ArgumentOutOfRangeError(self)\n\nif self._in_obj is None:\nraise exc.ArgumentInOperationError(self)\n\nparams = {}\nfor p in many_params:\nif isinstance(p, str):\nparams[p] = p\nelse:\nparams[p] = p.value\n\nif self._in_style is None:\nraise exc.ArgumentInOperationError(self)\n\nparams_in_style = self._in_obj.get_params()\nparams_out_style = self._out_obj.get_params()\nparams = {**params, **params_in_style}\n\nif self._out_style is None:\nraise exc.ArgumentOutOfRangeError(self)\n\nout_params = self._out_obj.get_params()\nout_params.update(params)\n\nreturn self._converter.formatmany(sql, params)\n", "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Sequence[Union[str, bytes]]:\nreturn self._converter.formatmany(sql, many_params)\n", "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Mapping[Union[str, bytes], Union[str, bytes]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of in-style parameters.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str` or :class:`int`)\nto value.  If :attr:`.SQLParams.in_style` is a named parameter style.\nthen *params* must be a\n:class:`~collections.abc.Mapping`.  If :attr:`.SQLParams.in_style` is\nan ordinal parameter style. then *params* must be a\n:class:`~collections.abc.Sequence`.\n\n\"\"\"\n\nself._converter.convert_many(sql, many_params)\nreturn self._out_obj\n", "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Sequence[Union[str, bytes]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters.\n\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`) contains\nthe set of in-style parameters. It maps each parameter (:class:`str` or\n:class:`int`) to value. If :attr:`.SQLParams.in_style` is a named\nparameter style. then *params* must be a\n:class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style` is\nan ordinal parameter style. then *params* must be a\n:class:`~collections.abc.Sequence`\n\"\"\"\nif isinstance(sql, bytes):\nsql = [sql]\n\nparams = {}\nif self._in_style:\nparams = {self._in_style: params}\nparams = self._in_obj.formatman", "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]] = ()\n) -> Sequence[Tuple[str, Any]]:\n\nout_style = self._out_style\n\nif out_style is None:\nout_style = self._in_style\n\nparams = {}\nfor param in many_params:\nif isinstance(param, str):\nparams[param] = self._in_obj[param]\nelse:\nparams[param.name] = param\n\nreturn self._out_obj[out_style](sql, params)\n", "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Iterable[Mapping[Union[str, bytes], Union[str, bytes]]]:\nif self._out_obj:\nraise TypeError(\"out parameter is not supported on objects other than in-style parameters\")\n\nparams = self._in_obj\nif isinstance(sql, bytes):\nsql = sql.decode(self._in_obj.encoding)\nparams = self._in_obj.parse_string(sql)\nparams = self._expand_tuples(params)\n\nout_params = {}\nfor k, v in params.items():\nout_params[k] = self._out_obj.get(k, v)\n\nif self._out_style:\nout_params = self._out_style.format(out_params)\n\nreturn out_params\n", "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Union[Iterable[str], Mapping[str, int]]\n) -> Sequence[Tuple[str, Mapping[str, Any]]]:\n\nif self._in_style is None:\nraise NotImplementedError()\n\nout_style = self._out_style\n\nif isinstance(out_style, str):\nout_style = self._out_obj.get_params(out_style)\n\nif isinstance(many_params, Mapping):\nout_params = many_params\nelif isinstance(many_params, Sequence):\nout_params = {}\nfor param in many_params:\nout_params[param] = self._out_obj.get_params(param)\n\nreturn self._converter.formatmany(sql, out_params)\n", "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Union[Iterable, Mapping, Sequence]\n) -> Sequence[Mapping[Union[str, int], Any]]:\nif self._in_obj:\nreturn [self._converter.convert_params(params) for params in many_params]\nelse:\nreturn [self._converter.convert_params(params) for params in many_params]\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Union[Iterable[Union[str, int]], Mapping[Union[str, int], int]]\n) -> Sequence[Tuple[str, Union[str, int]]]:\nif self._out_obj is None:\nself._out_obj = []\n\nif isinstance(many_params, Mapping):\nparams = many_params\nelse:\nparams = {\nk: int(v) for k, v in many_params\n}\n\nparams_iter = self._in_obj.items()\nparams_iter.extend(params.items())\nparams = tuple(params_iter)\n\nif self._out_style is None:\nreturn self._out_obj + [(sql, params)]\n\nout_style = self._out_style\n\nif out_style == self.OUT_STYLE_ORDERED:\nreturn self._out_obj + [(sql, params)]\n\nif out_style == self.OUT_STYLE_LITERAL:\nreturn self._out_obj + [(self._in_obj.get(k, k), v) for k, v in params]\n\nreturn self._out_obj + [(self._in_obj.get(k, k), v) for k, v in params]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Sequence[Union[str, bytes]]:\nif self._out_obj is None:\nraise RuntimeError(\"Out-style parameters must be specified before \"\n\"calling format().\")\n\nif isinstance(sql, bytes):\nsql = sql.decode(\"utf-8\")\n\nparams = {}\nfor param in many_params:\nparams[param] = None\n\nreturn self._out_obj.formatmany(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Sequence[Union[str, bytes]]:\nif self._out_obj is None:\nraise exc.ArgumentOutOfRangeError(self)\n\nif self._in_obj is None:\nraise exc.ArgumentInOperationError(self)\n\nparams = {}\nfor p in many_params:\nif isinstance(p, str):\nparams[p] = p\nelse:\nparams[p] = p.value\n\nif self._in_style is None:\nraise exc.ArgumentInOperationError(self)\n\nparams_in_style = self._in_obj.get_params()\nparams_out_style = self._out_obj.get_params()\nparams = {**params, **params_in_style}\n\nif self._out_style is None:\nraise exc.ArgumentOutOfRangeError(self)\n\nout_params = self._out_obj.get_params()\nout_params.update(params)\n\nreturn self._converter.formatmany(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Sequence[Union[str, bytes]]:\nreturn self._converter.formatmany(sql, many_params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Mapping[Union[str, bytes], Union[str, bytes]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of in-style parameters.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str` or :class:`int`)\nto value.  If :attr:`.SQLParams.in_style` is a named parameter style.\nthen *params* must be a\n:class:`~collections.abc.Mapping`.  If :attr:`.SQLParams.in_style` is\nan ordinal parameter style. then *params* must be a\n:class:`~collections.abc.Sequence`.\n\n\"\"\"\n\nself._converter.convert_many(sql, many_params)\nreturn self._out_obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Sequence[Union[str, bytes]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters.\n\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`) contains\nthe set of in-style parameters. It maps each parameter (:class:`str` or\n:class:`int`) to value. If :attr:`.SQLParams.in_style` is a named\nparameter style. then *params* must be a\n:class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style` is\nan ordinal parameter style. then *params* must be a\n:class:`~collections.abc.Sequence`\n\"\"\"\nif isinstance(sql, bytes):\nsql = [sql]\n\nparams = {}\nif self._in_style:\nparams = {self._in_style: params}\nparams = self._in_obj.formatman", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]] = ()\n) -> Sequence[Tuple[str, Any]]:\n\nout_style = self._out_style\n\nif out_style is None:\nout_style = self._in_style\n\nparams = {}\nfor param in many_params:\nif isinstance(param, str):\nparams[param] = self._in_obj[param]\nelse:\nparams[param.name] = param\n\nreturn self._out_obj[out_style](sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Iterable[Union[str, bytes]]\n) -> Iterable[Mapping[Union[str, bytes], Union[str, bytes]]]:\nif self._out_obj:\nraise TypeError(\"out parameter is not supported on objects other than in-style parameters\")\n\nparams = self._in_obj\nif isinstance(sql, bytes):\nsql = sql.decode(self._in_obj.encoding)\nparams = self._in_obj.parse_string(sql)\nparams = self._expand_tuples(params)\n\nout_params = {}\nfor k, v in params.items():\nout_params[k] = self._out_obj.get(k, v)\n\nif self._out_style:\nout_params = self._out_style.format(out_params)\n\nreturn out_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Union[Iterable[str], Mapping[str, int]]\n) -> Sequence[Tuple[str, Mapping[str, Any]]]:\n\nif self._in_style is None:\nraise NotImplementedError()\n\nout_style = self._out_style\n\nif isinstance(out_style, str):\nout_style = self._out_obj.get_params(out_style)\n\nif isinstance(many_params, Mapping):\nout_params = many_params\nelif isinstance(many_params, Sequence):\nout_params = {}\nfor param in many_params:\nout_params[param] = self._out_obj.get_params(param)\n\nreturn self._converter.formatmany(sql, out_params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: Union[str, bytes],\nmany_params: Union[Iterable, Mapping, Sequence]\n) -> Sequence[Mapping[Union[str, int], Any]]:\nif self._in_obj:\nreturn [self._converter.convert_params(params) for params in many_params]\nelse:\nreturn [self._converter.convert_params(params) for params in many_params]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.", "ques_id": "62b45df05108cfac7f2109ce", "code": ["def validate(self, path):\n    try:\n        self.validate_content(path)\n    except Exception as e:\n        self.log.error(\"Failed to validate file: %s\", e)\n        self.log.error(\"Please check the file path and try again\")\n        raise e\n\n    return self.status_str(\"ok\")\n", "def validate(self, path):\n    if not os.path.exists(path):\n        self.log.error(\"%s does not exist!\", path)\n        return False\n    if not os.path.isfile(path):\n        self.log.error(\"%s is not a file!\", path)\n        return False\n    if not self.spec_version:\n        self.log.error(\"no version specified!\")\n        return False\n    return True\n", "def validate(self, path):\n    super().validate(path)\n\n    if not os.path.isfile(path):\n        raise IOError(\"Path does not exist {}\".format(path))\n\n    with open(path, \"rb\") as inv_digest:\n        self.read_inventory_digest(inv_digest)\n\n    self.check_additional_digests(path, self.registered_extensions, additional_digests=[])\n\n    self.obj_fs.rm_safe(path)\n", "def validate(self, path):\n    self.log.debug('validate(%s)', path)\n    if self.inventory_digest_files:\n        self.read_inventory_digest(path)\n    if self.content_directory:\n        self.read_content_directory(path)\n    if self.root_inv_validator:\n        self.root_inv_validator.validate(path)\n", "def validate(self, path):\n    if self.spec_version is not None:\n        self.spec_version = self.read_inventory_digest(path)\n    if self.digest_algorithm is not None:\n        self.digest_algorithm = self.read_inventory_digest(path)\n    if self.digest_version is not None:\n        self.digest_version = self.digest_algorithm\n    self.check_digests(path)\n", "def validate(self, path):\n    if not self.spec_version:\n        self.log.debug(\"No specification version found. Assuming latest version for %s\", path)\n        return\n\n    if self.check_digests(path):\n        return True\n\n    return self.status_str(\"incomplete\")\n", "def validate(self, path):\n    self.log.debug('Validating %s', path)\n    path = os.path.abspath(path)\n\n    # Read the inventory file and validate it\n    inv = self.read_inventory_digest(path)\n    if not self.validate_inventory(inv, path):\n        self.log.error('Inventory file corrupted, invalid or missing file or directory')\n        return False\n\n    # Validate the additional digests\n    self.validate_additional_digests(path, additional_digests=inv.get('additional_digests', []))\n\n    # Validate the extensions directory\n    self.validate_extensions_dir()\n\n    return True\n", "def validate(self, path):\n    self.log.debug('validate %s', path)\n    self.digest_algorithm.validate(path)\n    self.digest_inventory.validate(path)\n    self.spec_version.validate(path)\n    self.inventory_digest_files.validate(path)\n    self.initialize()\n", "def validate(self, path):\n    try:\n        self.validate_version_inventories()\n        self.validate_extensions_dir()\n        self.validate_inventory()\n    except FileDigestException as e:\n        self.log.error('FileDigestException: %s', e)\n        raise\n    except Exception as e:\n        self.log.error('Validator: %s', e)\n        raise\n    return True\n", "def validate(self, path):\n    self.inventory_digest_files = []\n    self.root_inv_validator.validate(path)\n    self.inventory_digest_files += self.obj_fs.list_files(path)\n\n    return self.status_str('Valid')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, path):\n    try:\n        self.validate_content(path)\n    except Exception as e:\n        self.log.error(\"Failed to validate file: %s\", e)\n        self.log.error(\"Please check the file path and try again\")\n        raise e\n\n    return self.status_str(\"ok\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    if not os.path.exists(path):\n        self.log.error(\"%s does not exist!\", path)\n        return False\n    if not os.path.isfile(path):\n        self.log.error(\"%s is not a file!\", path)\n        return False\n    if not self.spec_version:\n        self.log.error(\"no version specified!\")\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    super().validate(path)\n\n    if not os.path.isfile(path):\n        raise IOError(\"Path does not exist {}\".format(path))\n\n    with open(path, \"rb\") as inv_digest:\n        self.read_inventory_digest(inv_digest)\n\n    self.check_additional_digests(path, self.registered_extensions, additional_digests=[])\n\n    self.obj_fs.rm_safe(path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    self.log.debug('validate(%s)', path)\n    if self.inventory_digest_files:\n        self.read_inventory_digest(path)\n    if self.content_directory:\n        self.read_content_directory(path)\n    if self.root_inv_validator:\n        self.root_inv_validator.validate(path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    if self.spec_version is not None:\n        self.spec_version = self.read_inventory_digest(path)\n    if self.digest_algorithm is not None:\n        self.digest_algorithm = self.read_inventory_digest(path)\n    if self.digest_version is not None:\n        self.digest_version = self.digest_algorithm\n    self.check_digests(path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    if not self.spec_version:\n        self.log.debug(\"No specification version found. Assuming latest version for %s\", path)\n        return\n\n    if self.check_digests(path):\n        return True\n\n    return self.status_str(\"incomplete\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    self.log.debug('Validating %s', path)\n    path = os.path.abspath(path)\n\n    # Read the inventory file and validate it\n    inv = self.read_inventory_digest(path)\n    if not self.validate_inventory(inv, path):\n        self.log.error('Inventory file corrupted, invalid or missing file or directory')\n        return False\n\n    # Validate the additional digests\n    self.validate_additional_digests(path, additional_digests=inv.get('additional_digests', []))\n\n    # Validate the extensions directory\n    self.validate_extensions_dir()\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    self.log.debug('validate %s', path)\n    self.digest_algorithm.validate(path)\n    self.digest_inventory.validate(path)\n    self.spec_version.validate(path)\n    self.inventory_digest_files.validate(path)\n    self.initialize()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    try:\n        self.validate_version_inventories()\n        self.validate_extensions_dir()\n        self.validate_inventory()\n    except FileDigestException as e:\n        self.log.error('FileDigestException: %s', e)\n        raise\n    except Exception as e:\n        self.log.error('Validator: %s', e)\n        raise\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    self.inventory_digest_files = []\n    self.root_inv_validator.validate(path)\n    self.inventory_digest_files += self.obj_fs.list_files(path)\n\n    return self.status_str('Valid')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validation_logger.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string of validator status, with optional prefix.", "ques_id": "62b45df15108cfac7f2109dc", "code": ["def status_str(self, prefix=''):\n    if self.show_errors:\n        return '%svalidation failures: %d/%d' % (\n            prefix, self.num_errors, self.num_warnings)\n    else:\n        return '%svalidation passes: %d/%d' % (\n            prefix, self.num_errors, self.num_warnings)\n", "def status_str(self, prefix=''):\n    if self.status_str:\n        return \"%s%s: %s\" % (prefix, self.status_str, self.info)\n    else:\n        return self.info\n", "def status_str(self, prefix=''):\n    if self.show_errors:\n        return ''\n    if self.show_warnings:\n        return 'Warnings'\n    return 'Errors'\n", "def status_str(self, prefix=''):\n    if self.status_str:\n        return prefix + \": \" + self.status_str\n    else:\n        return ''\n", "def status_str(self, prefix=''):\n    result = ''\n    if self.show_errors:\n        result +=' %sWarnings: %d\\n' % (prefix, self.num_warnings)\n    if self.show_errors or self.show_errors is None:\n        result +=' %sErrors: %d\\n' % (prefix, self.num_errors)\n    if self.show_warnings or self.show_warnings is None:\n        result +=' %sInfo: %d\\n' % (prefix, self.num_warnings + self.num_errors)\n    return result\n", "def status_str(self, prefix=''):\n    if self.show_errors:\n        return '%s: %s' % (prefix, self.status_str)\n    else:\n        return '%s:' % (prefix,) + self.status_str\n", "def status_str(self, prefix=''):\n    if self.show_warnings:\n        return '%sWarnings: %d' % (prefix, self.num_warnings)\n    elif self.show_errors:\n        return '%sErrors: %d' % (prefix, self.num_errors)\n    else:\n        return 'Validator status: %s' % self.status\n", "def status_str(self, prefix=''):\n    return '{0}{1}: '.format(prefix, self.status_str)\n", "def status_str(self, prefix=''):\n    s = ''\n    if self.show_errors:\n        s += '{}errors found: '.format(prefix)\n    if self.show_warnings:\n        s += '{}warnings found: '.format(prefix)\n    if self.num_errors:\n        s += '{}errors/{} '.format(prefix, self.num_errors)\n    if self.num_warnings:\n        s += '{}warnings/{} '.format(prefix, self.num_warnings)\n    return s\n", "def status_str(self, prefix=''):\n    s = ''\n    if self.show_errors:\n        s += '{0}{1}: '.format(prefix, self.status_str)\n    if self.show_warnings:\n        s += '{0}{1}: '.format(prefix, self.warning_or_warning.status_str)\n    s += '{0}{1}'.format(prefix, self.status_str)\n    return s\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n    if self.show_errors:\n        return '%svalidation failures: %d/%d' % (\n            prefix, self.num_errors, self.num_warnings)\n    else:\n        return '%svalidation passes: %d/%d' % (\n            prefix, self.num_errors, self.num_warnings)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.status_str:\n        return \"%s%s: %s\" % (prefix, self.status_str, self.info)\n    else:\n        return self.info\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.show_errors:\n        return ''\n    if self.show_warnings:\n        return 'Warnings'\n    return 'Errors'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.status_str:\n        return prefix + \": \" + self.status_str\n    else:\n        return ''\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    result = ''\n    if self.show_errors:\n        result +=' %sWarnings: %d\\n' % (prefix, self.num_warnings)\n    if self.show_errors or self.show_errors is None:\n        result +=' %sErrors: %d\\n' % (prefix, self.num_errors)\n    if self.show_warnings or self.show_warnings is None:\n        result +=' %sInfo: %d\\n' % (prefix, self.num_warnings + self.num_errors)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.show_errors:\n        return '%s: %s' % (prefix, self.status_str)\n    else:\n        return '%s:' % (prefix,) + self.status_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.show_warnings:\n        return '%sWarnings: %d' % (prefix, self.num_warnings)\n    elif self.show_errors:\n        return '%sErrors: %d' % (prefix, self.num_errors)\n    else:\n        return 'Validator status: %s' % self.status\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    return '{0}{1}: '.format(prefix, self.status_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = ''\n    if self.show_errors:\n        s += '{}errors found: '.format(prefix)\n    if self.show_warnings:\n        s += '{}warnings found: '.format(prefix)\n    if self.num_errors:\n        s += '{}errors/{} '.format(prefix, self.num_errors)\n    if self.num_warnings:\n        s += '{}warnings/{} '.format(prefix, self.num_warnings)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = ''\n    if self.show_errors:\n        s += '{0}{1}: '.format(prefix, self.status_str)\n    if self.show_warnings:\n        s += '{0}{1}: '.format(prefix, self.warning_or_warning.status_str)\n    s += '{0}{1}'.format(prefix, self.status_str)\n    return s\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string representation of validation log, with optional prefix.", "ques_id": "62b45df15108cfac7f2109dd", "code": ["def status_str(self, prefix=''):\n    if self.log:\n        self.log.write(''+ prefix +'' + self.status_str_short)\n    return self.status_str_short\n", "def status_str(self, prefix=''):\n    if self.status_str:\n        return self.status_str\n    elif self.status_str is None:\n        return ''\n    else:\n        return prefix +'status:'+ self.status_str\n", "def status_str(self, prefix=''):\n    if self.status_str:\n        return self.status_str\n\n    status_str = ''\n    for k, v in sorted(self.items()):\n        if k.startswith(prefix):\n            status_str += k +''\n            status_str += v\n            status_str += '\\n'\n    return status_str\n", "def status_str(self, prefix=''):\n    self.log.debug('status_str({0}) -> {1}'.format(self.id, prefix))\n    return str(self)\n", "def status_str(self, prefix=''):\n    s = \"\"\n    if self.id is not None:\n        s += \"%sID: %s\\n\" % (prefix, self.id)\n    if self.status_str_no_fs is not None:\n        s += \"%sStatus: %s\\n\" % (prefix, self.status_str_no_fs)\n    return s\n", "def status_str(self, prefix=''):\n    msg = \"{}{}\".format(prefix, self.status_str_value)\n    return msg\n", "def status_str(self, prefix=''):\n    s = ''\n    if self.status_str_flag:\n        s += '[{}] '.format(prefix) + str(self)\n        self.status_str_flag = False\n    return s\n", "def status_str(self, prefix=''):\n    l = []\n    l.append(prefix + \"status: \")\n    l.append(self.status_str_success)\n    l.append(self.status_str_failure)\n    if self.lax_digests:\n        l.append(\" (lax)\")\n    if len(self.lax_digests) > 1:\n        l.append(\" (more than one digest)\")\n    if len(self.spec_version) > 1:\n        l.append(\" (more than one version)\")\n    if len(self.inventory_digest) > 1:\n        l.append(\" (more than one digest for inventory)\")\n    if len(self.inventory_digest_files) > 1:\n        l.append(\" (more than one digest for inventory files)\")\n    if len(self.content_directory) > 1:\n        l.append(\" (more than one content directory)\")\n    return''.join(l)\n", "def status_str(self, prefix=''):\n    status_str = ''\n    if self.status_str:\n        status_str ='({})'.format(self.status_str)\n    return '{}{}'.format(prefix, status_str)\n", "def status_str(self, prefix=''):\n    s = \"{}ID: {}\\n\".format(prefix, self.id)\n    s += \"{}Root: {}\\n\".format(prefix, self.root_inv_validator.status_str(prefix=''))\n    s += \"{}Digests:\\n\".format(prefix)\n    for digest in self.lax_digests:\n        s += \"  {}\\n\".format(digest)\n    s += \"{}Extensions Digests:\\n\".format(prefix)\n    for digest in self.extensions_dir:\n        s += \"  {}\\n\".format(digest)\n    return s\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n    if self.log:\n        self.log.write(''+ prefix +'' + self.status_str_short)\n    return self.status_str_short\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.status_str:\n        return self.status_str\n    elif self.status_str is None:\n        return ''\n    else:\n        return prefix +'status:'+ self.status_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.status_str:\n        return self.status_str\n\n    status_str = ''\n    for k, v in sorted(self.items()):\n        if k.startswith(prefix):\n            status_str += k +''\n            status_str += v\n            status_str += '\\n'\n    return status_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    self.log.debug('status_str({0}) -> {1}'.format(self.id, prefix))\n    return str(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = \"\"\n    if self.id is not None:\n        s += \"%sID: %s\\n\" % (prefix, self.id)\n    if self.status_str_no_fs is not None:\n        s += \"%sStatus: %s\\n\" % (prefix, self.status_str_no_fs)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    msg = \"{}{}\".format(prefix, self.status_str_value)\n    return msg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = ''\n    if self.status_str_flag:\n        s += '[{}] '.format(prefix) + str(self)\n        self.status_str_flag = False\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    l = []\n    l.append(prefix + \"status: \")\n    l.append(self.status_str_success)\n    l.append(self.status_str_failure)\n    if self.lax_digests:\n        l.append(\" (lax)\")\n    if len(self.lax_digests) > 1:\n        l.append(\" (more than one digest)\")\n    if len(self.spec_version) > 1:\n        l.append(\" (more than one version)\")\n    if len(self.inventory_digest) > 1:\n        l.append(\" (more than one digest for inventory)\")\n    if len(self.inventory_digest_files) > 1:\n        l.append(\" (more than one digest for inventory files)\")\n    if len(self.content_directory) > 1:\n        l.append(\" (more than one content directory)\")\n    return''.join(l)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    status_str = ''\n    if self.status_str:\n        status_str ='({})'.format(self.status_str)\n    return '{}{}'.format(prefix, status_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = \"{}ID: {}\\n\".format(prefix, self.id)\n    s += \"{}Root: {}\\n\".format(prefix, self.root_inv_validator.status_str(prefix=''))\n    s += \"{}Digests:\\n\".format(prefix)\n    for digest in self.lax_digests:\n        s += \"  {}\\n\".format(digest)\n    s += \"{}Extensions Digests:\\n\".format(prefix)\n    for digest in self.extensions_dir:\n        s += \"  {}\\n\".format(digest)\n    return s\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "is_valid", "docstring": "Return True if identifier is valid, always True in this base implementation.", "ques_id": "62b45e135108cfac7f2109f4", "code": ["def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.", "ques_id": "62b45e145108cfac7f210a07", "code": ["def validate(self, inventory, extract_spec_version=False):\n    self.log.info(\"Validating %s\" % self.id)\n    self.check_digests_present_and_used(self.manifest_files, self.digest_regex())\n    self.check_content_paths_map_to_versions(inventory, self.spec_versions_supported)\n    self.check_content_paths_map_to_versions(inventory, self.logical_paths)\n    self.check_logical_path(self.content_path, version=self.id, logical_paths=self.logical_paths)\n    self.check_logical_path(self.logical_path, version=self.id, logical_paths=self.logical_paths)\n\n    if self.digest_algorithm == \"sha512\":\n        self.check_digests_present_and_used(self.unnormalized_digests, self.digest_regex())\n\n    if extract_spec_version:\n        self.check_digests_present_and_used(inventory, self.digest_regex())\n\n    self.check_content_paths_map_to_versions(inventory, self.spec_versions_supported)\n    self.check_content_paths_map_to_versions(inventory, self.logical_paths)\n\n    self.check_content_paths_map_to_versions(inventory, self.logical_paths)\n\n    self.check_digests_present_and_used(self.unnormalized_digests, self.digest_regex())\n    self.check_digests_present_and", "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        [\n            self.spec_version,\n            self.content_directory,\n            self.content_logical_path,\n            self.content_path,\n            self.spec_versions_supported,\n        ],\n        [\"specification\", \"content\"],\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory,\n        all_versions=True,\n    )\n\n    if extract_spec_version:\n        self.check_content_paths_map_to_versions(\n            inventory,\n            all_versions=True,\n        )\n\n    self.check_digests_present_and_used(\n        [self.head],\n        [\"head\"],\n    )\n", "def validate(self, inventory, extract_spec_version=False):\n    self.check_content_paths_map_to_versions(\n        inventory,\n        self.manifest_files,\n        self.manifest_files_unnormalized_digests,\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory,\n        self.spec_versions_supported,\n        self.spec_versions_unnormalized_digests,\n    )\n\n    self.check_logical_path(\n        self.content_path,\n        versions=self.spec_versions_supported,\n        logical_paths=self.logical_paths,\n    )\n\n    self.check_logical_path(\n        self.content_path_unnormalized,\n        versions=self.spec_versions_unnormalized_digests,\n        logical_paths=self.logical_paths,\n    )\n\n    self.check_content_path(\n        self.content_path_normalized,\n        versions=self.spec_versions_normalized,\n        logical_paths=self.logical_paths,\n    )\n\n    self.check_content_path(\n        self.content_path_unnormalized_digest,\n        versions=self.spec_versions_unnormalized_digests,\n        logical_paths=self.logical_paths,\n  ", "def validate(self, inventory, extract_spec_version=False):\n    if self.where == \"source\":\n        self.check_content_paths_map_to_versions(\n            inventory, self.spec_version, [\"source\"]\n        )\n    elif self.where == \"target\":\n        self.check_content_paths_map_to_versions(\n            inventory, self.spec_version, [\"target\"]\n        )\n\n    self.check_digests_present_and_used(\n        inventory, self.digest_regex\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory, self.spec_version, [\"source\", \"target\"]\n    )\n\n    self.check_digests_present_and_used(\n        inventory, self.digest_regex, logical_paths=self.digest_paths\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory, self.spec_version, logical_paths=[\"source\", \"target\"]\n    )\n\n    self.check_digests_present_and_used(\n        inventory, self.digest_regex, logical_paths=[\"source\", \"target\"]\n    )\n\n    self.check_lax_digests(inventory, self.lax_digests)\n\n   ", "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        inventory, self.digest_regex(), extract_spec_version\n    )\n    self.check_content_paths_map_to_versions(\n        inventory, self.manifest_files, all_versions=True\n    )\n    self.check_logical_path(\n        inventory, self.spec_version, logical_paths=self.logical_paths\n    )\n    self.check_content_path(inventory, self.content_paths, content_directories=True)\n    self.check_logical_path(inventory, self.spec_version, logical_paths=True)\n    self.check_content_path(inventory, self.unnormalized_digests, unnormalized_digests=True)\n    self.check_content_paths_map_to_versions(\n        inventory, self.spec_versions_supported, all_versions=True\n    )\n    self.check_state_block(inventory, self.state, versions=True)\n\n    return inventory\n", "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        manifest_files=self.manifest_files,\n        digests_used=self.digest_regex(),\n        extract_spec_version=extract_spec_version,\n    )\n\n    if not self.spec_version:\n        self.check_content_paths_map_to_versions(\n            manifest_files=self.manifest_files,\n            all_versions=True,\n        )\n\n    self.check_content_paths_map_to_versions(\n        manifest_files=self.manifest_files,\n        all_versions=True,\n        unnormalized_digests=self.unnormalized_digests,\n    )\n\n    self.check_content_paths_map_to_versions(\n        manifest_files=self.manifest_files,\n        all_versions=True,\n        unnormalized_digests=self.unnormalized_digests,\n    )\n\n    self.check_logical_path(self.content_path, versions=self.versions, logical_paths=self.logical_paths)\n", "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        inventory=inventory,\n        digests_used=self.digests_used,\n        extract_spec_version=extract_spec_version,\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory=inventory,\n        all_versions=bool(self.digest_regex),\n    )\n\n    self.check_logical_path(self.content_path, version=self.id, logical_paths=self.logical_paths)\n    self.check_logical_path(self.content_directory, version=self.id, logical_paths=self.logical_paths)\n\n    self.check_content_path(\n        path=self.content_path,\n        content_directory=self.content_directory,\n        unnormalized_digests=self.unnormalized_digests,\n    )\n\n    self.check_content_path(\n        path=self.content_directory,\n        content_path=self.content_path,\n        unnormalized_digests=self.unnormalized_digests,\n    )\n", "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        manifest_files=self.manifest_files,\n        digests_used=self.digests_used,\n    )\n\n    if extract_spec_version:\n        self.check_content_paths_map_to_versions(\n            manifest_files=self.manifest_files,\n            all_versions=True\n        )\n\n    self.check_logical_path(self.where, version=True, logical_paths=True)\n    self.check_content_path(self.where, content_paths=True, content_directories=True)\n\n    self.check_state_block(\n        inventory=inventory,\n        version=True,\n        logical_paths=True,\n        logical_directories=True,\n    )\n\n    self.check_content_path(self.where, content_paths=True, content_directories=True)\n\n    self.check_content_path(self.where, content_paths=True, logical_directories=True)\n\n    self.check_logical_path(self.where, version=True, logical_paths=True)\n    self.check_logical_path(self.where, content_paths=True, logical_directories=True)\n", "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        inventory, [self.id], self.digest_regex()\n    )\n    self.check_content_paths_map_to_versions(\n        inventory, all_versions=True\n    )\n\n    if extract_spec_version:\n        self.check_content_paths_map_to_versions(inventory, all_versions=False)\n\n    self.check_logical_path(self.head, versions=False, logical_paths=True)\n\n    self.check_digests_present_and_used(\n        inventory, [self.id], self.digest_regex()\n    )\n\n    self.check_digests_present_and_used(\n        inventory, [self.id], self.digest_regex(), raw_only=True\n    )\n\n    self.check_content_paths_map_to_versions(inventory, all_versions=True)\n\n    self.check_digests_present_and_used(\n        inventory, [self.id], self.digest_regex(), raw_only=True\n    )\n\n    self.check_content_paths_map_to_versions(inventory, all_versions=True)\n\n    self.check_logical_path(self.head, versions=True)\n\n    self.check_digests_present_and_used(inventory, all_versio", "def validate(self, inventory, extract_spec_version=False):\n\n    self.check_content_paths_map_to_versions(inventory)\n\n    self.check_digests_present_and_used(\n        inventory,\n        self.digest_regex(),\n        digests_used=[self.id],\n    )\n\n    self.check_digests_present_and_used(\n        inventory,\n        self.digest_regex(version=True),\n        digests_used=[self.id, self.id],\n    )\n\n    self.check_content_paths_map_to_versions(inventory)\n\n    self.check_content_paths_map_to_versions(inventory)\n\n    self.check_logical_path(inventory)\n\n    self.check_digests_present_and_used(\n        inventory,\n        self.digest_regex(version=True, logical_paths=True),\n        digests_used=[self.id],\n    )\n\n    self.check_digests_present_and_used(\n        inventory,\n        self.digest_regex(version=True, logical_paths=False),\n        digests_used=[self.id],\n    )\n\n    if extract_spec_version:\n        self.check_digests_present_and_used(\n          "], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    self.log.info(\"Validating %s\" % self.id)\n    self.check_digests_present_and_used(self.manifest_files, self.digest_regex())\n    self.check_content_paths_map_to_versions(inventory, self.spec_versions_supported)\n    self.check_content_paths_map_to_versions(inventory, self.logical_paths)\n    self.check_logical_path(self.content_path, version=self.id, logical_paths=self.logical_paths)\n    self.check_logical_path(self.logical_path, version=self.id, logical_paths=self.logical_paths)\n\n    if self.digest_algorithm == \"sha512\":\n        self.check_digests_present_and_used(self.unnormalized_digests, self.digest_regex())\n\n    if extract_spec_version:\n        self.check_digests_present_and_used(inventory, self.digest_regex())\n\n    self.check_content_paths_map_to_versions(inventory, self.spec_versions_supported)\n    self.check_content_paths_map_to_versions(inventory, self.logical_paths)\n\n    self.check_content_paths_map_to_versions(inventory, self.logical_paths)\n\n    self.check_digests_present_and_used(self.unnormalized_digests, self.digest_regex())\n    self.check_digests_present_and", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        [\n            self.spec_version,\n            self.content_directory,\n            self.content_logical_path,\n            self.content_path,\n            self.spec_versions_supported,\n        ],\n        [\"specification\", \"content\"],\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory,\n        all_versions=True,\n    )\n\n    if extract_spec_version:\n        self.check_content_paths_map_to_versions(\n            inventory,\n            all_versions=True,\n        )\n\n    self.check_digests_present_and_used(\n        [self.head],\n        [\"head\"],\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    self.check_content_paths_map_to_versions(\n        inventory,\n        self.manifest_files,\n        self.manifest_files_unnormalized_digests,\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory,\n        self.spec_versions_supported,\n        self.spec_versions_unnormalized_digests,\n    )\n\n    self.check_logical_path(\n        self.content_path,\n        versions=self.spec_versions_supported,\n        logical_paths=self.logical_paths,\n    )\n\n    self.check_logical_path(\n        self.content_path_unnormalized,\n        versions=self.spec_versions_unnormalized_digests,\n        logical_paths=self.logical_paths,\n    )\n\n    self.check_content_path(\n        self.content_path_normalized,\n        versions=self.spec_versions_normalized,\n        logical_paths=self.logical_paths,\n    )\n\n    self.check_content_path(\n        self.content_path_unnormalized_digest,\n        versions=self.spec_versions_unnormalized_digests,\n        logical_paths=self.logical_paths,\n  ", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    if self.where == \"source\":\n        self.check_content_paths_map_to_versions(\n            inventory, self.spec_version, [\"source\"]\n        )\n    elif self.where == \"target\":\n        self.check_content_paths_map_to_versions(\n            inventory, self.spec_version, [\"target\"]\n        )\n\n    self.check_digests_present_and_used(\n        inventory, self.digest_regex\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory, self.spec_version, [\"source\", \"target\"]\n    )\n\n    self.check_digests_present_and_used(\n        inventory, self.digest_regex, logical_paths=self.digest_paths\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory, self.spec_version, logical_paths=[\"source\", \"target\"]\n    )\n\n    self.check_digests_present_and_used(\n        inventory, self.digest_regex, logical_paths=[\"source\", \"target\"]\n    )\n\n    self.check_lax_digests(inventory, self.lax_digests)\n\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        inventory, self.digest_regex(), extract_spec_version\n    )\n    self.check_content_paths_map_to_versions(\n        inventory, self.manifest_files, all_versions=True\n    )\n    self.check_logical_path(\n        inventory, self.spec_version, logical_paths=self.logical_paths\n    )\n    self.check_content_path(inventory, self.content_paths, content_directories=True)\n    self.check_logical_path(inventory, self.spec_version, logical_paths=True)\n    self.check_content_path(inventory, self.unnormalized_digests, unnormalized_digests=True)\n    self.check_content_paths_map_to_versions(\n        inventory, self.spec_versions_supported, all_versions=True\n    )\n    self.check_state_block(inventory, self.state, versions=True)\n\n    return inventory\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        manifest_files=self.manifest_files,\n        digests_used=self.digest_regex(),\n        extract_spec_version=extract_spec_version,\n    )\n\n    if not self.spec_version:\n        self.check_content_paths_map_to_versions(\n            manifest_files=self.manifest_files,\n            all_versions=True,\n        )\n\n    self.check_content_paths_map_to_versions(\n        manifest_files=self.manifest_files,\n        all_versions=True,\n        unnormalized_digests=self.unnormalized_digests,\n    )\n\n    self.check_content_paths_map_to_versions(\n        manifest_files=self.manifest_files,\n        all_versions=True,\n        unnormalized_digests=self.unnormalized_digests,\n    )\n\n    self.check_logical_path(self.content_path, versions=self.versions, logical_paths=self.logical_paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        inventory=inventory,\n        digests_used=self.digests_used,\n        extract_spec_version=extract_spec_version,\n    )\n\n    self.check_content_paths_map_to_versions(\n        inventory=inventory,\n        all_versions=bool(self.digest_regex),\n    )\n\n    self.check_logical_path(self.content_path, version=self.id, logical_paths=self.logical_paths)\n    self.check_logical_path(self.content_directory, version=self.id, logical_paths=self.logical_paths)\n\n    self.check_content_path(\n        path=self.content_path,\n        content_directory=self.content_directory,\n        unnormalized_digests=self.unnormalized_digests,\n    )\n\n    self.check_content_path(\n        path=self.content_directory,\n        content_path=self.content_path,\n        unnormalized_digests=self.unnormalized_digests,\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        manifest_files=self.manifest_files,\n        digests_used=self.digests_used,\n    )\n\n    if extract_spec_version:\n        self.check_content_paths_map_to_versions(\n            manifest_files=self.manifest_files,\n            all_versions=True\n        )\n\n    self.check_logical_path(self.where, version=True, logical_paths=True)\n    self.check_content_path(self.where, content_paths=True, content_directories=True)\n\n    self.check_state_block(\n        inventory=inventory,\n        version=True,\n        logical_paths=True,\n        logical_directories=True,\n    )\n\n    self.check_content_path(self.where, content_paths=True, content_directories=True)\n\n    self.check_content_path(self.where, content_paths=True, logical_directories=True)\n\n    self.check_logical_path(self.where, version=True, logical_paths=True)\n    self.check_logical_path(self.where, content_paths=True, logical_directories=True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    self.check_digests_present_and_used(\n        inventory, [self.id], self.digest_regex()\n    )\n    self.check_content_paths_map_to_versions(\n        inventory, all_versions=True\n    )\n\n    if extract_spec_version:\n        self.check_content_paths_map_to_versions(inventory, all_versions=False)\n\n    self.check_logical_path(self.head, versions=False, logical_paths=True)\n\n    self.check_digests_present_and_used(\n        inventory, [self.id], self.digest_regex()\n    )\n\n    self.check_digests_present_and_used(\n        inventory, [self.id], self.digest_regex(), raw_only=True\n    )\n\n    self.check_content_paths_map_to_versions(inventory, all_versions=True)\n\n    self.check_digests_present_and_used(\n        inventory, [self.id], self.digest_regex(), raw_only=True\n    )\n\n    self.check_content_paths_map_to_versions(inventory, all_versions=True)\n\n    self.check_logical_path(self.head, versions=True)\n\n    self.check_digests_present_and_used(inventory, all_versio", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\n    self.check_content_paths_map_to_versions(inventory)\n\n    self.check_digests_present_and_used(\n        inventory,\n        self.digest_regex(),\n        digests_used=[self.id],\n    )\n\n    self.check_digests_present_and_used(\n        inventory,\n        self.digest_regex(version=True),\n        digests_used=[self.id, self.id],\n    )\n\n    self.check_content_paths_map_to_versions(inventory)\n\n    self.check_content_paths_map_to_versions(inventory)\n\n    self.check_logical_path(inventory)\n\n    self.check_digests_present_and_used(\n        inventory,\n        self.digest_regex(version=True, logical_paths=True),\n        digests_used=[self.id],\n    )\n\n    self.check_digests_present_and_used(\n        inventory,\n        self.digest_regex(version=True, logical_paths=False),\n        digests_used=[self.id],\n    )\n\n    if extract_spec_version:\n        self.check_digests_present_and_used(\n          ", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "check_digests_present_and_used", "docstring": "Check all digests in manifest that are needed are present and used.", "ques_id": "62b45e145108cfac7f210a09", "code": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    for manifest_file in manifest_files:\n        if self.check_digests_present(manifest_file, digests_used):\n            self.log.info(\"Digest file %s already present and used\", manifest_file.path)\n            return True\n    self.log.info(\"Digest files %s not found and not used\", \", \".join(manifest_files))\n    return False\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest in digests_used:\n        found_digest = self.check_digest(manifest_files, digest)\n        if found_digest is None:\n            self.error(\"Digest {} not found in manifest files {}\".format(digest, manifest_files))\n            self.log.warning(\"Using unnormalized digest {}\".format(found_digest))\n            return found_digest\n    return None\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\n    for digest_type in digests_used:\n        digests = digests_used[digest_type]\n        for digest in digests:\n            if digest not in unnormalized_digests[digest_type]:\n                self.log.debug(\"Digest {} not found in {}\".format(digest, digest_type))\n                self.digest_regex(digest, manifest_files)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\n    self.check_digests_present(manifest_files, digests_used)\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    digests = self.digest_regex()\n    for digest in digests:\n        if digest not in manifest_files:\n            self.log.info(\"Digest '%s' not found in manifest files.\", digest)\n            digests_used.append(digest)\n    return digests_used\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    self.check_digests_present(manifest_files, digests_used)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    # check if digests are present in all versions\n    for version in manifest_files.keys():\n        self.check_digests_present_and_used_version(version, manifest_files, digests_used)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for file in manifest_files:\n        if file not in digests_used:\n            self.log.debug(\"Digest file {} is missing from manifest\".format(file))\n            self.error(\"Digest file {} is missing from manifest\".format(file))\n            return False\n    return True\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for manifest_file in manifest_files:\n        self.check_digests_present(manifest_file, digests_used)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for manifest_file in manifest_files:\n        if manifest_file not in digests_used:\n            self.digest_regex()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for manifest_file in manifest_files:\n        if self.check_digests_present(manifest_file, digests_used):\n            self.log.info(\"Digest file %s already present and used\", manifest_file.path)\n            return True\n    self.log.info(\"Digest files %s not found and not used\", \", \".join(manifest_files))\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest in digests_used:\n        found_digest = self.check_digest(manifest_files, digest)\n        if found_digest is None:\n            self.error(\"Digest {} not found in manifest files {}\".format(digest, manifest_files))\n            self.log.warning(\"Using unnormalized digest {}\".format(found_digest))\n            return found_digest\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\n    for digest_type in digests_used:\n        digests = digests_used[digest_type]\n        for digest in digests:\n            if digest not in unnormalized_digests[digest_type]:\n                self.log.debug(\"Digest {} not found in {}\".format(digest, digest_type))\n                self.digest_regex(digest, manifest_files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\n    self.check_digests_present(manifest_files, digests_used)\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    digests = self.digest_regex()\n    for digest in digests:\n        if digest not in manifest_files:\n            self.log.info(\"Digest '%s' not found in manifest files.\", digest)\n            digests_used.append(digest)\n    return digests_used\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    self.check_digests_present(manifest_files, digests_used)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    # check if digests are present in all versions\n    for version in manifest_files.keys():\n        self.check_digests_present_and_used_version(version, manifest_files, digests_used)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for file in manifest_files:\n        if file not in digests_used:\n            self.log.debug(\"Digest file {} is missing from manifest\".format(file))\n            self.error(\"Digest file {} is missing from manifest\".format(file))\n            return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for manifest_file in manifest_files:\n        self.check_digests_present(manifest_file, digests_used)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for manifest_file in manifest_files:\n        if manifest_file not in digests_used:\n            self.digest_regex()\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_as_prior_version", "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.", "ques_id": "62b45e165108cfac7f210a16", "code": ["def validate_as_prior_version(self, prior):\n    if prior.is_special:\n        self.warning(\n            \"It's assumed that the file {} is the reference \"\n            \"to a special file {}, which is probably not \"\n            \"necessary for this package.\".format(\n                prior.file_path,\n                prior.special_file_path,\n            )\n        )\n        self.warning(\"This is a warning for an unknown special file: {}\".format(prior.special_file_path))\n", "def validate_as_prior_version(self, prior):\n    if prior in self.lax_digests:\n        self.lax_digests.remove(prior)\n\n    if prior not in self.digest_algorithm.algorithms:\n        self.log.error(\"Unknown version %s in manifest files %s\",\n                       prior, self.manifest_files)\n        raise errors.UnknownManifestException(\n            \"Unknown version %s in manifest files %s\" %\n            (prior, self.manifest_files)\n        )\n", "def validate_as_prior_version(self, prior):\n    if prior.version!= self.id:\n        self.error(\n            code='unknown',\n            message='Can only validate in a previous version of the inventory',\n        )\n", "def validate_as_prior_version(self, prior):\n\n    if prior is None:\n        return\n\n    if self.lax_digests and prior.version in self.lax_digests:\n        if self.lax_digests[prior.version]!= prior.digest:\n            self.error(code='inconsistent-lax-digest',\n                       message='{} (expected {}) and {} (actual {})'\n                      .format(prior.digest, self.lax_digests[prior.version],\n                               prior.digest, self.lax_digests[prior.version]))\n    elif self.digest_regex and not self.digest_regex.match(prior.digest):\n        self.error(code='inconsistent-lax-digest',\n                   message='{} (expected {})'\n                  .format(prior.digest, self.digest_regex.pattern))\n", "def validate_as_prior_version(self, prior):\n    for version in prior.versions:\n        self.check_content_paths_map_to_versions(\n            prior.content_paths, version)\n        self.check_logical_path(\n            prior.logical_path, version, logical_paths=prior.logical_path)\n", "def validate_as_prior_version(self, prior):\n    self.error(\"This test requires at least {} version of the inventory.\".format(prior))\n", "def validate_as_prior_version(self, prior):\n    self.log.debug(\"Validating as prior version %s\" % prior)\n    state = self.where.get_current_state()\n    if not self.validate_state_block(state, prior):\n        self.error(\"Unexpected state %s in %s\" % (state, prior))\n", "def validate_as_prior_version(self, prior):\n    self.error(\"This version of this package is prior to version %s\" % prior)\n", "def validate_as_prior_version(self, prior):\n    if prior not in self.lax_digests:\n        self.error(\n            code='MISSING_HASH',\n            message='Missing lax_digest {}. Found {}'.format(prior, self.lax_digests)\n        )\n\n    for version in self.lax_digests[prior]:\n        self.check_content_paths_map_to_versions(\n            manifest_files=self.manifest_files,\n            all_versions=True,\n            version=version\n        )\n", "def validate_as_prior_version(self, prior):\n\n    self.log.debug(\"Validating as prior version %d\" % prior)\n\n    # If the version is not known, raise an error\n    if prior not in self.spec_versions_supported:\n        raise RuntimeError(\"Unknown version: %d\" % prior)\n\n    # Make sure the manifest files are present\n    self.check_digests_present_and_used(self.manifest_files, self.digest_regex())\n\n    # Make sure the content directory is empty\n    self.check_content_paths_map_to_versions(self.manifest_files, [])\n\n    # Make sure the logical path is empty\n    self.check_logical_path(self.content_directory, self.spec_versions_supported, [])\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_as_prior_version(self, prior):\n    if prior.is_special:\n        self.warning(\n            \"It's assumed that the file {} is the reference \"\n            \"to a special file {}, which is probably not \"\n            \"necessary for this package.\".format(\n                prior.file_path,\n                prior.special_file_path,\n            )\n        )\n        self.warning(\"This is a warning for an unknown special file: {}\".format(prior.special_file_path))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    if prior in self.lax_digests:\n        self.lax_digests.remove(prior)\n\n    if prior not in self.digest_algorithm.algorithms:\n        self.log.error(\"Unknown version %s in manifest files %s\",\n                       prior, self.manifest_files)\n        raise errors.UnknownManifestException(\n            \"Unknown version %s in manifest files %s\" %\n            (prior, self.manifest_files)\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    if prior.version!= self.id:\n        self.error(\n            code='unknown',\n            message='Can only validate in a previous version of the inventory',\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\n    if prior is None:\n        return\n\n    if self.lax_digests and prior.version in self.lax_digests:\n        if self.lax_digests[prior.version]!= prior.digest:\n            self.error(code='inconsistent-lax-digest',\n                       message='{} (expected {}) and {} (actual {})'\n                      .format(prior.digest, self.lax_digests[prior.version],\n                               prior.digest, self.lax_digests[prior.version]))\n    elif self.digest_regex and not self.digest_regex.match(prior.digest):\n        self.error(code='inconsistent-lax-digest',\n                   message='{} (expected {})'\n                  .format(prior.digest, self.digest_regex.pattern))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    for version in prior.versions:\n        self.check_content_paths_map_to_versions(\n            prior.content_paths, version)\n        self.check_logical_path(\n            prior.logical_path, version, logical_paths=prior.logical_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    self.error(\"This test requires at least {} version of the inventory.\".format(prior))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_as_prior_version(self, prior):\n    self.log.debug(\"Validating as prior version %s\" % prior)\n    state = self.where.get_current_state()\n    if not self.validate_state_block(state, prior):\n        self.error(\"Unexpected state %s in %s\" % (state, prior))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    self.error(\"This version of this package is prior to version %s\" % prior)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_as_prior_version(self, prior):\n    if prior not in self.lax_digests:\n        self.error(\n            code='MISSING_HASH',\n            message='Missing lax_digest {}. Found {}'.format(prior, self.lax_digests)\n        )\n\n    for version in self.lax_digests[prior]:\n        self.check_content_paths_map_to_versions(\n            manifest_files=self.manifest_files,\n            all_versions=True,\n            version=version\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\n    self.log.debug(\"Validating as prior version %d\" % prior)\n\n    # If the version is not known, raise an error\n    if prior not in self.spec_versions_supported:\n        raise RuntimeError(\"Unknown version: %d\" % prior)\n\n    # Make sure the manifest files are present\n    self.check_digests_present_and_used(self.manifest_files, self.digest_regex())\n\n    # Make sure the content directory is empty\n    self.check_content_paths_map_to_versions(self.manifest_files, [])\n\n    # Make sure the logical path is empty\n    self.check_logical_path(self.content_directory, self.spec_versions_supported, [])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "get_logical_path_map", "docstring": "Get a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.", "ques_id": "62b45e165108cfac7f210a17", "code": ["def get_logical_path_map(inventory, version):\n    ret = {}\n    for (path, content_files) in inventory.contents.items():\n        for (content_file, content_type) in content_files:\n            if content_type == \"versioned_file\":\n                continue\n            logical_path = os.path.join(path, content_file)\n            ret.setdefault(logical_path, set()).update(content_files)\n    return ret\n", "def get_logical_path_map(inventory, version):\n    # Get the path to the inventory file.\n    inventory_file = inventory.get_inventory_file(version)\n    if not inventory_file:\n        return {}\n    # We need to look at the file contents since it's a generator.\n    content_files = set([content.filename for content in inventory.contents()])\n    # We need to look at the files in the inventory directory since it's a generator.\n    inventory_files_in_version = set(content_files)\n    # Map logical paths in state to files on disk.\n    result = {}\n    for logical_path in inventory_files:\n        file_in_inventory = set()\n        for content_file in inventory_files_in_version:\n            if logical_path.startswith(content_file):\n                file_in_inventory.add(logical_path)\n        result[logical_path] = file_in_inventory\n    return result\n", "def get_logical_path_map(inventory, version):\n    return {\n        path: set(content_files)\n        for path, content_files in inventory.items()\n        if version in content_files\n    }\n", "def get_logical_path_map(inventory, version):\n    # Make a set of all the logical paths for this version.\n    # This is necessary because the inventory may have multiple instances\n    # pointing to the same version, and this is not strict enough.\n    all_paths = set([])\n    for inst in inventory.instances:\n        all_paths.update(inst.logical_paths)\n    return {\n        path: {\n            content_file.content_type for content_file in content_files\n        }\n        for path, content_files in all_paths.items()\n    }\n", "def get_logical_path_map(inventory, version):\n    logical_path_in_state = {}\n    for path in inventory.get('files', {}).get(version, []):\n        if not logical_path_in_state.get(path):\n            logical_path_in_state[path] = set()\n        logical_path_in_state[path].add(path)\n    return logical_path_in_state\n", "def get_logical_path_map(inventory, version):\n    if version not in inventory:\n        raise InventoryVersionNotFound(inventory, version)\n    return inventory[version]\n", "def get_logical_path_map(inventory, version):\n    return {\n        path: set(content_files)\n        for path, content_files in inventory.items()\n        if version in content_files\n    }\n", "def get_logical_path_map(inventory, version):\n    if version not in inventory:\n        raise InventoryVersionNotFound(version)\n    return inventory[version]\n", "def get_logical_path_map(inventory, version):\n    state_map = {}\n\n    for file_dict in inventory['files']:\n        file_path = file_dict['path']\n        content_files = file_dict['content_files']\n\n        if version not in state_map:\n            state_map[version] = set()\n\n        state_map[version].add(file_path)\n\n        for content_file in content_files:\n            if content_file not in state_map:\n                state_map[content_file] = set()\n\n            state_map[content_file].add(file_path)\n\n    return state_map\n", "def get_logical_path_map(inventory, version):\n    if version not in inventory:\n        return {}\n\n    return inventory[version].get('logical_path_in_state', {})\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get_logical_path_map(inventory, version):\n    ret = {}\n    for (path, content_files) in inventory.contents.items():\n        for (content_file, content_type) in content_files:\n            if content_type == \"versioned_file\":\n                continue\n            logical_path = os.path.join(path, content_file)\n            ret.setdefault(logical_path, set()).update(content_files)\n    return ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    # Get the path to the inventory file.\n    inventory_file = inventory.get_inventory_file(version)\n    if not inventory_file:\n        return {}\n    # We need to look at the file contents since it's a generator.\n    content_files = set([content.filename for content in inventory.contents()])\n    # We need to look at the files in the inventory directory since it's a generator.\n    inventory_files_in_version = set(content_files)\n    # Map logical paths in state to files on disk.\n    result = {}\n    for logical_path in inventory_files:\n        file_in_inventory = set()\n        for content_file in inventory_files_in_version:\n            if logical_path.startswith(content_file):\n                file_in_inventory.add(logical_path)\n        result[logical_path] = file_in_inventory\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    return {\n        path: set(content_files)\n        for path, content_files in inventory.items()\n        if version in content_files\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    # Make a set of all the logical paths for this version.\n    # This is necessary because the inventory may have multiple instances\n    # pointing to the same version, and this is not strict enough.\n    all_paths = set([])\n    for inst in inventory.instances:\n        all_paths.update(inst.logical_paths)\n    return {\n        path: {\n            content_file.content_type for content_file in content_files\n        }\n        for path, content_files in all_paths.items()\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    logical_path_in_state = {}\n    for path in inventory.get('files', {}).get(version, []):\n        if not logical_path_in_state.get(path):\n            logical_path_in_state[path] = set()\n        logical_path_in_state[path].add(path)\n    return logical_path_in_state\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    if version not in inventory:\n        raise InventoryVersionNotFound(inventory, version)\n    return inventory[version]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    return {\n        path: set(content_files)\n        for path, content_files in inventory.items()\n        if version in content_files\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    if version not in inventory:\n        raise InventoryVersionNotFound(version)\n    return inventory[version]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    state_map = {}\n\n    for file_dict in inventory['files']:\n        file_path = file_dict['path']\n        content_files = file_dict['content_files']\n\n        if version not in state_map:\n            state_map[version] = set()\n\n        state_map[version].add(file_path)\n\n        for content_file in content_files:\n            if content_file not in state_map:\n                state_map[content_file] = set()\n\n            state_map[content_file].add(file_path)\n\n    return state_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    if version not in inventory:\n        return {}\n\n    return inventory[version].get('logical_path_in_state', {})\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_fixity", "docstring": "Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.", "ques_id": "62b45e175108cfac7f210a19", "code": ["def validate_fixity(self, fixity, manifest_files):\n    self.check_content_paths_map_to_versions(\n        manifest_files,\n        all_versions=True\n    )\n    self.check_logical_path(fixity.logical_path, version=fixity.version, logical_paths=fixity.logical_paths)\n    self.check_content_path(fixity.content_path, content_directories=fixity.content_directories)\n    self.check_logical_path(fixity.content_path_with_version, version=fixity.version, logical_paths=fixity.logical_paths)\n    self.check_content_path(fixity.normalized_content_path, content_directories=fixity.normalized_content_directories)\n    self.check_content_path_with_version(fixity.version, version=fixity.version, logical_paths=fixity.logical_paths)\n", "def validate_fixity(self, fixity, manifest_files):\n    self.check_digests_present_and_used(manifest_files, self.digest_regex())\n\n    # check the content paths\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=False)\n", "def validate_fixity(self, fixity, manifest_files):\n    self.log.debug(\"Validating fixity {0}\".format(fixity))\n\n    if fixity == \"error\":\n        self.error(fixity)\n        return\n\n    if self.digest_regex.match(fixity):\n        self.error(fixity)\n        return\n\n    # Check versions of the files\n    versions = self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n    self.check_versions(versions, all_versions=True)\n\n    # Check if the files are in the same version as the one we are looking for\n    for file_path in manifest_files:\n        if self.where.match(os.path.basename(file_path)):\n            self.check_content_path(file_path, manifest_files)\n", "def validate_fixity(self, fixity, manifest_files):\n    if self.digest_algorithm == \"sha512\":\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    elif self.digest_algorithm == \"sha256\":\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    elif self.digest_algorithm == \"sha512l\":\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    elif self.digest_algorithm == \"sha256l\":\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    else:\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    self.check_digests_present_and_used(manifest_files, digests_used={\"sha512\": self.digest_algorithm})\n", "def validate_fixity(self, fixity, manifest_files):\n\n    # check that the fixity is valid\n    for content_path in manifest_files:\n        content = self.content_directory.get_content(content_path)\n        if self.digest_regex.match(content.digest) is None:\n            self.log.error(\"Unsupported content digest for %s: %s\",\n                           content_path, content.digest)\n            self.digest_algorithm.add(content.digest)\n\n            if self.error is not None:\n                self.error(self.error)\n\n    return True\n", "def validate_fixity(self, fixity, manifest_files):\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n    self.check_logical_path(fixity.logical_path, version=fixity.version, logical_paths=fixity.logical_paths)\n    self.check_content_path(fixity.content_path, version=fixity.version, logical_paths=fixity.logical_paths)\n    self.check_logical_path(fixity.content_path_with_extension, version=fixity.version, logical_paths=fixity.logical_paths)\n", "def validate_fixity(self, fixity, manifest_files):\n\n    if fixity.lower() not in self.fixity_map:\n        self.error(\"Invalid fixity type '{}' for manifest files '{}'. Valid fixities are: {}\"\n                 .format(fixity, manifest_files, self.fixity_map.keys()))\n\n    return self.fixity_map[fixity.lower()](manifest_files)\n", "def validate_fixity(self, fixity, manifest_files):\n\n    # check for bad versions\n    if fixity.version not in self.spec_versions_supported:\n        self.error(\"Unsupported version: {}\".format(fixity.version))\n        self.error(\"Supported versions: {}\".format(self.spec_versions_supported))\n        self.log.warning(\"Did not understand version: {}\".format(fixity.version))\n        return\n\n    # check for bad paths\n    if not all([os.path.exists(path) for path in fixity.content_paths]):\n        self.error(\"Bad content paths: {}\".format(fixity.content_paths))\n        self.log.warning(\"Bad content paths: {}\".format(self.spec_content_paths))\n        return\n\n    # check for bad digests\n    if fixity.digest_algorithm not in self.digest_regex:\n        self.error(\"Unknown digest algorithm: {}\".format(fixity.digest_algorithm))\n        self.log.warning(\"Bad digest algorithm: {}\".format(fixity.digest_algorithm))\n        return\n", "def validate_fixity(self, fixity, manifest_files):\n    self.log.debug('validate_fixity(%s)', fixity)\n\n    self.check_digests_present_and_used(manifest_files, digests_used=[self.id])\n\n    if fixity.normalize:\n        normalized_fixity = self.normalize_fixity(fixity)\n        self.check_digests_present_and_used(\n            normalized_fixity, digests_used=[self.id]\n        )\n    else:\n        normalized_fixity = fixity\n\n    if fixity.content_paths is None:\n        self.check_content_paths_map_to_versions(\n            manifest_files=manifest_files, all_versions=True\n        )\n\n    self.check_content_paths_map_to_versions(\n        manifest_files=manifest_files, content_paths=[self.content_path], all_versions=True\n    )\n\n    self.check_content_paths_map_to_versions(\n        manifest_files=manifest_files, content_paths=normalized_fixity.content_paths, all_versions=True\n    )\n\n    self.check_logical_path(normalized_fixity.logical_path, versions=normalized_fixity.versions, logical_paths=normalized_fixity.logical_paths)\n", "def validate_fixity(self, fixity, manifest_files):\n\n    # Check the fixity object\n    if not isinstance(fixity, Fixity):\n        self.error(\"Fixity object is not of type 'Fixity'\")\n\n    # Check the version of the fixity\n    if fixity.version!= self.spec_version:\n        self.error(\"Fixity version ({}) does not match the expected version ({})\".format(\n            fixity.version, self.spec_version\n        ))\n\n    # Check the content paths\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n    self.check_content_path(fixity.content_path, fixity.content_directories)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    self.check_content_paths_map_to_versions(\n        manifest_files,\n        all_versions=True\n    )\n    self.check_logical_path(fixity.logical_path, version=fixity.version, logical_paths=fixity.logical_paths)\n    self.check_content_path(fixity.content_path, content_directories=fixity.content_directories)\n    self.check_logical_path(fixity.content_path_with_version, version=fixity.version, logical_paths=fixity.logical_paths)\n    self.check_content_path(fixity.normalized_content_path, content_directories=fixity.normalized_content_directories)\n    self.check_content_path_with_version(fixity.version, version=fixity.version, logical_paths=fixity.logical_paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    self.check_digests_present_and_used(manifest_files, self.digest_regex())\n\n    # check the content paths\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=False)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    self.log.debug(\"Validating fixity {0}\".format(fixity))\n\n    if fixity == \"error\":\n        self.error(fixity)\n        return\n\n    if self.digest_regex.match(fixity):\n        self.error(fixity)\n        return\n\n    # Check versions of the files\n    versions = self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n    self.check_versions(versions, all_versions=True)\n\n    # Check if the files are in the same version as the one we are looking for\n    for file_path in manifest_files:\n        if self.where.match(os.path.basename(file_path)):\n            self.check_content_path(file_path, manifest_files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    if self.digest_algorithm == \"sha512\":\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    elif self.digest_algorithm == \"sha256\":\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    elif self.digest_algorithm == \"sha512l\":\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    elif self.digest_algorithm == \"sha256l\":\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    else:\n        self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n\n    self.check_digests_present_and_used(manifest_files, digests_used={\"sha512\": self.digest_algorithm})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\n    # check that the fixity is valid\n    for content_path in manifest_files:\n        content = self.content_directory.get_content(content_path)\n        if self.digest_regex.match(content.digest) is None:\n            self.log.error(\"Unsupported content digest for %s: %s\",\n                           content_path, content.digest)\n            self.digest_algorithm.add(content.digest)\n\n            if self.error is not None:\n                self.error(self.error)\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n    self.check_logical_path(fixity.logical_path, version=fixity.version, logical_paths=fixity.logical_paths)\n    self.check_content_path(fixity.content_path, version=fixity.version, logical_paths=fixity.logical_paths)\n    self.check_logical_path(fixity.content_path_with_extension, version=fixity.version, logical_paths=fixity.logical_paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\n    if fixity.lower() not in self.fixity_map:\n        self.error(\"Invalid fixity type '{}' for manifest files '{}'. Valid fixities are: {}\"\n                 .format(fixity, manifest_files, self.fixity_map.keys()))\n\n    return self.fixity_map[fixity.lower()](manifest_files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\n    # check for bad versions\n    if fixity.version not in self.spec_versions_supported:\n        self.error(\"Unsupported version: {}\".format(fixity.version))\n        self.error(\"Supported versions: {}\".format(self.spec_versions_supported))\n        self.log.warning(\"Did not understand version: {}\".format(fixity.version))\n        return\n\n    # check for bad paths\n    if not all([os.path.exists(path) for path in fixity.content_paths]):\n        self.error(\"Bad content paths: {}\".format(fixity.content_paths))\n        self.log.warning(\"Bad content paths: {}\".format(self.spec_content_paths))\n        return\n\n    # check for bad digests\n    if fixity.digest_algorithm not in self.digest_regex:\n        self.error(\"Unknown digest algorithm: {}\".format(fixity.digest_algorithm))\n        self.log.warning(\"Bad digest algorithm: {}\".format(fixity.digest_algorithm))\n        return\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    self.log.debug('validate_fixity(%s)', fixity)\n\n    self.check_digests_present_and_used(manifest_files, digests_used=[self.id])\n\n    if fixity.normalize:\n        normalized_fixity = self.normalize_fixity(fixity)\n        self.check_digests_present_and_used(\n            normalized_fixity, digests_used=[self.id]\n        )\n    else:\n        normalized_fixity = fixity\n\n    if fixity.content_paths is None:\n        self.check_content_paths_map_to_versions(\n            manifest_files=manifest_files, all_versions=True\n        )\n\n    self.check_content_paths_map_to_versions(\n        manifest_files=manifest_files, content_paths=[self.content_path], all_versions=True\n    )\n\n    self.check_content_paths_map_to_versions(\n        manifest_files=manifest_files, content_paths=normalized_fixity.content_paths, all_versions=True\n    )\n\n    self.check_logical_path(normalized_fixity.logical_path, versions=normalized_fixity.versions, logical_paths=normalized_fixity.logical_paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\n    # Check the fixity object\n    if not isinstance(fixity, Fixity):\n        self.error(\"Fixity object is not of type 'Fixity'\")\n\n    # Check the version of the fixity\n    if fixity.version!= self.spec_version:\n        self.error(\"Fixity version ({}) does not match the expected version ({})\".format(\n            fixity.version, self.spec_version\n        ))\n\n    # Check the content paths\n    self.check_content_paths_map_to_versions(manifest_files, all_versions=True)\n    self.check_content_path(fixity.content_path, fixity.content_directories)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list", "docstring": "Return the files in `path`", "ques_id": "62b463153879012d19481498", "code": ["def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    if not os.path.isdir(path):\n        os.makedirs(path)\n    for filename in os.listdir(path):\n        filepath = os.path.join(path, filename)\n        if os.path.isfile(filepath):\n            yield filepath\n", "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    for zfile in zipfile.ZipFile(path):\n        if zfile.filename.endswith('/__init__.py'):\n            yield zfile.filename\n", "def files_list(path):\n    if not os.path.isdir(path):\n        return []\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    files = []\n    for entry in os.scandir(path):\n        if entry.is_file():\n            files.append(entry.name)\n    return files\n", "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    if not os.path.isdir(path):\n        os.makedirs(path)\n    for filename in os.listdir(path):\n        filepath = os.path.join(path, filename)\n        if os.path.isfile(filepath):\n            yield filepath\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    for zfile in zipfile.ZipFile(path):\n        if zfile.filename.endswith('/__init__.py'):\n            yield zfile.filename\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    if not os.path.isdir(path):\n        return []\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    files = []\n    for entry in os.scandir(path):\n        if entry.is_file():\n            files.append(entry.name)\n    return files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_group_files_by_xml_filename", "docstring": "Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package", "ques_id": "62b463153879012d1948149a", "code": ["def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for file_path in files:\n        pack = explore_source(source, file_path)\n        if pack is not None:\n            data[pack.basename] = pack\n    logger.debug(\"Found %i XML files in %s\", len(data), source)\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        for xml_filename in select_filenames_by_prefix(prefix=f, files=xmls):\n            package = explore_source(source=source, file_path=xml_filename)\n            data.setdefault(package.filename, []).append(package)\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        fname = os.path.basename(f)\n        for xml in xmls:\n            if xml.startswith(fname):\n                data[fname] = explore_source(source / xml)\n                break\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml_filename in select_filenames_by_prefix(prefix=xmls, files=files):\n        package = explore_source(source=source, xml_filename=xml_filename)\n        if package:\n            data.update({xml_filename: package})\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        filename = os.path.basename(f)\n        data[filename] = explore_source(source, f)\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for file_path in files:\n        xml_basename = os.path.basename(file_path).replace('.xml', '')\n        for package in explore_source(source, file_path):\n            for name in package.name:\n                if name in xmls:\n                    if name not in data:\n                        data[name] = []\n                    data[name].append(package)\n                else:\n                    data[name] = [package]\n    logger.debug('found {} XML files in {}'.format(len(data), source))\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for file_path in files:\n        package = _explore_file(source, file_path)\n        if package is None:\n            continue\n        data.setdefault(package.xml_basename, []).append(package)\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml_filename in select_filenames_by_prefix(prefix=xmls, files=files):\n        package = explore_source(source=source, file_path=xml_filename)\n        data[xml_filename] = package\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml_file in xmls:\n        xml_basename = os.path.splitext(os.path.basename(xml_file))[0]\n        for file in files:\n            if match_file_by_prefix(xml_basename, file):\n                data[file] = explore_source(source + '/' + file)\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml in xmls:\n        _explore_folder(source + '/' + xml)\n        for filename in _group_files_by_xml_filename(source + '/' + xml, files):\n            data[filename] = source + '/' + xml\n    return data\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for file_path in files:\n        pack = explore_source(source, file_path)\n        if pack is not None:\n            data[pack.basename] = pack\n    logger.debug(\"Found %i XML files in %s\", len(data), source)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        for xml_filename in select_filenames_by_prefix(prefix=f, files=xmls):\n            package = explore_source(source=source, file_path=xml_filename)\n            data.setdefault(package.filename, []).append(package)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        fname = os.path.basename(f)\n        for xml in xmls:\n            if xml.startswith(fname):\n                data[fname] = explore_source(source / xml)\n                break\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml_filename in select_filenames_by_prefix(prefix=xmls, files=files):\n        package = explore_source(source=source, xml_filename=xml_filename)\n        if package:\n            data.update({xml_filename: package})\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        filename = os.path.basename(f)\n        data[filename] = explore_source(source, f)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for file_path in files:\n        xml_basename = os.path.basename(file_path).replace('.xml', '')\n        for package in explore_source(source, file_path):\n            for name in package.name:\n                if name in xmls:\n                    if name not in data:\n                        data[name] = []\n                    data[name].append(package)\n                else:\n                    data[name] = [package]\n    logger.debug('found {} XML files in {}'.format(len(data), source))\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for file_path in files:\n        package = _explore_file(source, file_path)\n        if package is None:\n            continue\n        data.setdefault(package.xml_basename, []).append(package)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml_filename in select_filenames_by_prefix(prefix=xmls, files=files):\n        package = explore_source(source=source, file_path=xml_filename)\n        data[xml_filename] = package\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml_file in xmls:\n        xml_basename = os.path.splitext(os.path.basename(xml_file))[0]\n        for file in files:\n            if match_file_by_prefix(xml_basename, file):\n                data[file] = explore_source(source + '/' + file)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml in xmls:\n        _explore_folder(source + '/' + xml)\n        for filename in _group_files_by_xml_filename(source + '/' + xml, files):\n            data[filename] = source + '/' + xml\n    return data\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "match_file_by_prefix", "docstring": "Identify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package", "ques_id": "62b463153879012d1948149b", "code": ["def match_file_by_prefix(prefix, file_path):\n    return prefix in zipfile.prefix2file(file_path).decode(\"utf-8\")\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    return prefix in zipfile.prefixers[zipfile.ZIP_DEFLATED].search(file_path)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    if not os.path.isabs(file_path):\n        file_path = os.path.join(os.path.dirname(prefix), file_path)\n\n    return os.path.realpath(file_path) == file_path\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix + '/')\n", "def match_file_by_prefix(prefix, file_path):\n    return prefix in zipfile.prefix2file(file_path)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix + '/')\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return prefix in zipfile.prefix2file(file_path).decode(\"utf-8\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return prefix in zipfile.prefixers[zipfile.ZIP_DEFLATED].search(file_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    if not os.path.isabs(file_path):\n        file_path = os.path.join(os.path.dirname(prefix), file_path)\n\n    return os.path.realpath(file_path) == file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix + '/')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return prefix in zipfile.prefix2file(file_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix + '/')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "select_filenames_by_prefix", "docstring": "Get files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix", "ques_id": "62b463153879012d1948149c", "code": ["def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if match_file_by_prefix(prefix, f)]\n", "def select_filenames_by_prefix(prefix, files):\n    sel_files = []\n    for file in files:\n        if file.startswith(prefix):\n            sel_files.append(file)\n    return sel_files\n", "def select_filenames_by_prefix(prefix, files):\n    result = []\n    for f in files:\n        if match_file_by_prefix(prefix, f):\n            result.append(f)\n    return result\n", "def select_filenames_by_prefix(prefix, files):\n    files_by_prefix = {}\n    for f in files:\n        f_path = os.path.join(prefix, f)\n        if os.path.isfile(f_path):\n            files_by_prefix[f] = f_path\n    return list(files_by_prefix.keys())\n", "def select_filenames_by_prefix(prefix, files):\n    files = [f for f in files if match_file_by_prefix(prefix, f)]\n    if len(files) == 0:\n        logger.error('No files matching prefix {} found in {}'.format(prefix, files))\n        raise ValueError('No files matching prefix {} found in {}'.format(prefix, files))\n    return files\n", "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if match_file_by_prefix(prefix, f)]\n", "def select_filenames_by_prefix(prefix, files):\n    files = [f for f in files if match_file_by_prefix(prefix, f)]\n    return files\n", "def select_filenames_by_prefix(prefix, files):\n    matches = []\n    for xml_file in files:\n        if match_file_by_prefix(prefix, xml_file):\n            matches.append(xml_file)\n    return matches\n", "def select_filenames_by_prefix(prefix, files):\n    files = [f for f in files if match_file_by_prefix(prefix, f)]\n    return files\n", "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if match_file_by_prefix(prefix, f)]\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if match_file_by_prefix(prefix, f)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    sel_files = []\n    for file in files:\n        if file.startswith(prefix):\n            sel_files.append(file)\n    return sel_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    result = []\n    for f in files:\n        if match_file_by_prefix(prefix, f):\n            result.append(f)\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    files_by_prefix = {}\n    for f in files:\n        f_path = os.path.join(prefix, f)\n        if os.path.isfile(f_path):\n            files_by_prefix[f] = f_path\n    return list(files_by_prefix.keys())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    files = [f for f in files if match_file_by_prefix(prefix, f)]\n    if len(files) == 0:\n        logger.error('No files matching prefix {} found in {}'.format(prefix, files))\n        raise ValueError('No files matching prefix {} found in {}'.format(prefix, files))\n    return files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if match_file_by_prefix(prefix, f)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    files = [f for f in files if match_file_by_prefix(prefix, f)]\n    return files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    matches = []\n    for xml_file in files:\n        if match_file_by_prefix(prefix, xml_file):\n            matches.append(xml_file)\n    return matches\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    files = [f for f in files if match_file_by_prefix(prefix, f)]\n    return files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if match_file_by_prefix(prefix, f)]\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_folder", "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict", "ques_id": "62b463153879012d1948149d", "code": ["def _explore_folder(folder):\n    data = {}\n    for filename in _group_files_by_xml_filename(folder, []):\n        data[filename] = explore_source(os.path.join(folder, filename))\n    return data\n", "def _explore_folder(folder):\n    packages = {}\n    for filename in select_filenames_by_prefix(folder, os.listdir(folder)):\n        xml_file_path = os.path.join(folder, filename)\n        if not os.path.isfile(xml_file_path):\n            continue\n        xml = _eval_file(prefix=folder, file_path=xml_file_path)\n        packages[filename] = xml\n    return packages\n", "def _explore_folder(folder):\n    xmls = {}\n    for xml_filename in select_filenames_by_prefix(folder, ['package.xml']):\n        xml = _eval_file(folder, xml_filename)\n        xmls[xml_filename] = xml\n    return xmls\n", "def _explore_folder(folder):\n    packages = {}\n    for zip_path in _explore_zipfile(folder):\n        xml_path = os.path.splitext(zip_path)[0] + '.xml'\n        packages[os.path.basename(zip_path)] = _group_files_by_xml_filename(\n            zip_path, xmls=[xml_path])\n    logger.debug('Found packages in {}'.format(folder))\n    return packages\n", "def _explore_folder(folder):\n    data = {}\n    for zip_file in _explore_zipfile(folder):\n        file_path = zip_file.filename\n        xml_name = select_filenames_by_prefix(folder, file_path)[0]\n        data[xml_name] = _eval_file(folder, file_path)\n    return data\n", "def _explore_folder(folder):\n    data = {}\n    for filename in os.listdir(folder):\n        xml_basename = os.path.splitext(filename)[0]\n        xml_path = os.path.join(folder, filename)\n        data[xml_basename] = _group_files_by_xml_filename(xml_path, files=_eval_file(xml_path))\n    return data\n", "def _explore_folder(folder):\n    files = []\n    for root, dirs, files in os.walk(folder):\n        for file in files:\n            if file.endswith('.xml'):\n                files.append(os.path.join(root, file))\n                files = _group_files_by_xml_filename(files, files[0], file)\n                files = _eval_file(prefix='', file_path=files[0])\n                if files:\n                    return files\n", "def _explore_folder(folder):\n    packages = {}\n    for xml_filename in select_filenames_by_prefix(folder, ['xml']):\n        xml_data = _eval_file(folder, xml_filename)\n        if xml_data is not None:\n            packages[xml_filename] = xml_data\n    return packages\n", "def _explore_folder(folder):\n    packages = {}\n    for root, dirs, files in os.walk(folder):\n        for fname in files:\n            fpath = os.path.join(root, fname)\n            packages.update(_group_files_by_xml_filename(fpath, dirs))\n    return packages\n", "def _explore_folder(folder):\n    data = {}\n    for xml_file in os.listdir(folder):\n        xml_path = os.path.join(folder, xml_file)\n        if not os.path.isfile(xml_path):\n            logger.debug('{} is not a file'.format(xml_path))\n            continue\n        xml_basename = os.path.basename(xml_path)\n        data[xml_basename] = explore_source(xml_path)\n    return data\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_folder(folder):\n    data = {}\n    for filename in _group_files_by_xml_filename(folder, []):\n        data[filename] = explore_source(os.path.join(folder, filename))\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    packages = {}\n    for filename in select_filenames_by_prefix(folder, os.listdir(folder)):\n        xml_file_path = os.path.join(folder, filename)\n        if not os.path.isfile(xml_file_path):\n            continue\n        xml = _eval_file(prefix=folder, file_path=xml_file_path)\n        packages[filename] = xml\n    return packages\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    xmls = {}\n    for xml_filename in select_filenames_by_prefix(folder, ['package.xml']):\n        xml = _eval_file(folder, xml_filename)\n        xmls[xml_filename] = xml\n    return xmls\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    packages = {}\n    for zip_path in _explore_zipfile(folder):\n        xml_path = os.path.splitext(zip_path)[0] + '.xml'\n        packages[os.path.basename(zip_path)] = _group_files_by_xml_filename(\n            zip_path, xmls=[xml_path])\n    logger.debug('Found packages in {}'.format(folder))\n    return packages\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    data = {}\n    for zip_file in _explore_zipfile(folder):\n        file_path = zip_file.filename\n        xml_name = select_filenames_by_prefix(folder, file_path)[0]\n        data[xml_name] = _eval_file(folder, file_path)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    data = {}\n    for filename in os.listdir(folder):\n        xml_basename = os.path.splitext(filename)[0]\n        xml_path = os.path.join(folder, filename)\n        data[xml_basename] = _group_files_by_xml_filename(xml_path, files=_eval_file(xml_path))\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    files = []\n    for root, dirs, files in os.walk(folder):\n        for file in files:\n            if file.endswith('.xml'):\n                files.append(os.path.join(root, file))\n                files = _group_files_by_xml_filename(files, files[0], file)\n                files = _eval_file(prefix='', file_path=files[0])\n                if files:\n                    return files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    packages = {}\n    for xml_filename in select_filenames_by_prefix(folder, ['xml']):\n        xml_data = _eval_file(folder, xml_filename)\n        if xml_data is not None:\n            packages[xml_filename] = xml_data\n    return packages\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    packages = {}\n    for root, dirs, files in os.walk(folder):\n        for fname in files:\n            fpath = os.path.join(root, fname)\n            packages.update(_group_files_by_xml_filename(fpath, dirs))\n    return packages\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    data = {}\n    for xml_file in os.listdir(folder):\n        xml_path = os.path.join(folder, xml_file)\n        if not os.path.isfile(xml_path):\n            logger.debug('{} is not a file'.format(xml_path))\n            continue\n        xml_basename = os.path.basename(xml_path)\n        data[xml_basename] = explore_source(xml_path)\n    return data\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_eval_file", "docstring": "Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict", "ques_id": "62b463153879012d1948149f", "code": ["def _eval_file(prefix, file_path):\n    # Cria o endereco do arquivo\n    file_path = _eval_file(prefix, file_path)\n\n    # Verifica se existe o arquivo existe em existe como endereco\n    file_path = _group_files_by_xml_filename(file_path, ['asset','rendition'])\n\n    # Verifica se existe o endereco do arquivo existe em externo\n    file_path = _group_files_by_xml_filename(file_path, ['metadata', 'externals'])\n\n    # Escreve o arquivo xml\n    logger.info('Escreva o arquivo {}'.format(file_path))\n    xml_path = os.path.join('packages', file_path)\n\n    # Escribir o endereco do arquivo\n    logger.info('Escriba o endereco do arquivo {}'.format(xml_path))\n    select_filenames_by_prefix(prefix, [xml_path])\n\n    return xml_path\n", "def _eval_file(prefix, file_path):\n    if 'asset' in file_path:\n        file_path = _explore_folder(file_path)\n        return {prefix: file_path}\n    elif'rendition' in file_path:\n        file_path = _explore_zipfile(file_path)\n        return {prefix: file_path}\n    else:\n        return {prefix: file_path}\n", "def _eval_file(prefix, file_path):\n    if 'asset' in file_path or'rendition' in file_path:\n        return {'packages': [prefix]}\n    elif '.' in file_path:\n        return {'packages': [prefix]}\n    else:\n        return {'packages': [file_path]}\n", "def _eval_file(prefix, file_path):\n    return {\n        'package': os.path.basename(file_path).split('.')[0],\n        'type': 'file',\n        'content': _eval_file(prefix, file_path)\n    }\n", "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n    file_name, ext = os.path.splitext(filename)\n    if ext in ['.xml', '.xsd']:\n        packages = _group_files_by_xml_filename(source=source, xmls=files, files=[file_path])\n    elif ext in ['.yaml', '.yml']:\n        packages = _group_files_by_yaml_filename(source=source, yamls=files, files=[file_path])\n    else:\n        raise ValueError('Extension invalid!')\n\n    if len(packages) == 0:\n        logger.info('No files found in %s', file_path)\n        return None\n    else:\n        logger.info('Found %d packages in %s', len(packages), file_path)\n        return packages[0]\n", "def _eval_file(prefix, file_path):\n    _, ext = os.path.splitext(file_path)\n    if ext in ('.xml', '.xsd', '.xml.bz2', '.xsd.bz2'):\n        return {'packages': _explore_zipfile(file_path)}\n    if ext in ('.zip', '.zip.bz2', '.zip.xml', '.zip.xml.bz2'):\n        return {'packages': _explore_zipfile(file_path)}\n    logger.debug('File %s not supported', file_path)\n    return {'packages': {}}\n", "def _eval_file(prefix, file_path):\n    if 'asset' in file_path.lower():\n        return {'packages': [prefix + '/' + file_path]}\n    elif'rendition' in file_path.lower():\n        return {'packages': [prefix + '/' + file_path]}\n", "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n    short_filename = filename.split('.')[0]\n\n    if 'asset' in short_filename:\n        packages = _eval_file_asset(prefix, file_path)\n    else:\n        packages = _eval_file_rendition(prefix, file_path)\n\n    return packages\n", "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n    filename = filename.split('.')[0] if '.' in filename else filename\n    filename = '{}.{}'.format(prefix, filename)\n    packages = _group_files_by_xml_filename(source=file_path, xmls=['asset','rendition'])\n    return packages.get(filename, {})\n", "def _eval_file(prefix, file_path):\n    if file_path.endswith('.asset'):\n        return {'packages': [{'filename': file_path, 'file_type': 'asset'}]}\n    elif file_path.endswith('.rendition'):\n        return {'packages': [{'filename': file_path, 'file_type':'rendition'}]}\n    else:\n        return {'packages': [{'filename': file_path, 'file_type': 'file'}]}\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _eval_file(prefix, file_path):\n    # Cria o endereco do arquivo\n    file_path = _eval_file(prefix, file_path)\n\n    # Verifica se existe o arquivo existe em existe como endereco\n    file_path = _group_files_by_xml_filename(file_path, ['asset','rendition'])\n\n    # Verifica se existe o endereco do arquivo existe em externo\n    file_path = _group_files_by_xml_filename(file_path, ['metadata', 'externals'])\n\n    # Escreve o arquivo xml\n    logger.info('Escreva o arquivo {}'.format(file_path))\n    xml_path = os.path.join('packages', file_path)\n\n    # Escribir o endereco do arquivo\n    logger.info('Escriba o endereco do arquivo {}'.format(xml_path))\n    select_filenames_by_prefix(prefix, [xml_path])\n\n    return xml_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    if 'asset' in file_path:\n        file_path = _explore_folder(file_path)\n        return {prefix: file_path}\n    elif'rendition' in file_path:\n        file_path = _explore_zipfile(file_path)\n        return {prefix: file_path}\n    else:\n        return {prefix: file_path}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    if 'asset' in file_path or'rendition' in file_path:\n        return {'packages': [prefix]}\n    elif '.' in file_path:\n        return {'packages': [prefix]}\n    else:\n        return {'packages': [file_path]}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    return {\n        'package': os.path.basename(file_path).split('.')[0],\n        'type': 'file',\n        'content': _eval_file(prefix, file_path)\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n    file_name, ext = os.path.splitext(filename)\n    if ext in ['.xml', '.xsd']:\n        packages = _group_files_by_xml_filename(source=source, xmls=files, files=[file_path])\n    elif ext in ['.yaml', '.yml']:\n        packages = _group_files_by_yaml_filename(source=source, yamls=files, files=[file_path])\n    else:\n        raise ValueError('Extension invalid!')\n\n    if len(packages) == 0:\n        logger.info('No files found in %s', file_path)\n        return None\n    else:\n        logger.info('Found %d packages in %s', len(packages), file_path)\n        return packages[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    _, ext = os.path.splitext(file_path)\n    if ext in ('.xml', '.xsd', '.xml.bz2', '.xsd.bz2'):\n        return {'packages': _explore_zipfile(file_path)}\n    if ext in ('.zip', '.zip.bz2', '.zip.xml', '.zip.xml.bz2'):\n        return {'packages': _explore_zipfile(file_path)}\n    logger.debug('File %s not supported', file_path)\n    return {'packages': {}}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    if 'asset' in file_path.lower():\n        return {'packages': [prefix + '/' + file_path]}\n    elif'rendition' in file_path.lower():\n        return {'packages': [prefix + '/' + file_path]}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n    short_filename = filename.split('.')[0]\n\n    if 'asset' in short_filename:\n        packages = _eval_file_asset(prefix, file_path)\n    else:\n        packages = _eval_file_rendition(prefix, file_path)\n\n    return packages\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n    filename = filename.split('.')[0] if '.' in filename else filename\n    filename = '{}.{}'.format(prefix, filename)\n    packages = _group_files_by_xml_filename(source=file_path, xmls=['asset','rendition'])\n    return packages.get(filename, {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    if file_path.endswith('.asset'):\n        return {'packages': [{'filename': file_path, 'file_type': 'asset'}]}\n    elif file_path.endswith('.rendition'):\n        return {'packages': [{'filename': file_path, 'file_type':'rendition'}]}\n    else:\n        return {'packages': [{'filename': file_path, 'file_type': 'file'}]}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_rendition", "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}", "ques_id": "62b463153879012d194814a1", "code": ["def add_rendition(self, lang, file_path):\n    asset = self.get_asset(basename=file_path)\n    self._renditions[lang] = asset.get_rendition(lang)\n", "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    asset = self.get_asset(basename=file_path)\n    self._renditions[lang] = asset.content\n", "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    if lang not in self._renditions:\n        self._renditions[lang] = {}\n\n    if file_path not in self._renditions[lang]:\n        self._renditions[lang][file_path] = \"\"\n", "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    if lang not in self._renditions:\n        self._renditions[lang] = {}\n\n    self._renditions[lang][file_path] = None\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_rendition(self, lang, file_path):\n    asset = self.get_asset(basename=file_path)\n    self._renditions[lang] = asset.get_rendition(lang)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    asset = self.get_asset(basename=file_path)\n    self._renditions[lang] = asset.content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    if lang not in self._renditions:\n        self._renditions[lang] = {}\n\n    if file_path not in self._renditions[lang]:\n        self._renditions[lang][file_path] = \"\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    self._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    if lang not in self._renditions:\n        self._renditions[lang] = {}\n\n    self._renditions[lang][file_path] = None\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_asset", "docstring": "\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}", "ques_id": "62b463163879012d194814a2", "code": ["def add_asset(self, basename, file_path):\n    self._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n    if basename not in self._assets:\n        self._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n    asset = self._assets.setdefault(basename, {})\n    asset[\"file_path\"] = file_path\n", "def add_asset(self, basename, file_path):\n    asset = etree.Element(\"asset\", {\"lang\": self._renditions[basename].lang})\n    asset.text = basename\n    asset.attrib[\"file\"] = file_path\n    self._assets.append(asset)\n", "def add_asset(self, basename, file_path):\n    asset = {\n        \"artigo02-gf02.tiff\": file_path,\n        \"artigo02-gf02.jpg\": \"/path/artigo02-gf02.jpg\",\n        \"artigo02-gf02.png\": \"/path/artigo02-gf02.png\",\n    }\n    if \"asset\" not in self._assets:\n        self._assets[\"asset\"] = {}\n    self._assets[\"asset\"].update(asset)\n", "def add_asset(self, basename, file_path):\n    asset = {\n        \"artigo02-gf03\": \"{}/{}\".format(self.file_path, file_path),\n        \"artigo02-gf03-r\": \"{}/{}-r{}.tiff\".format(self.file_path, basename, self._renditions['r']),\n        \"artigo02-gf03-g\": \"{}/{}-g{}.tiff\".format(self.file_path, basename, self._renditions['g']),\n        \"artigo02-gf03-b\": \"{}/{}-b{}.tiff\".format(self.file_path, basename, self._renditions['b']),\n        \"artigo02-gf03-a\": \"{}/{}-a{}.tiff\".format(self.file_path, basename, self._renditions['a']),\n    }\n    self._assets.append(asset)\n", "def add_asset(self, basename, file_path):\n    asset_dict = {\n        \"artigo02-gf03\": {\n            \"file_path\": file_path,\n            \"content\": self.xml_content(),\n        },\n        \"artigo02-gf03-asset\": {\n            \"basename\": basename,\n            \"asset_path\": file_path,\n            \"content\": self.xml_content(),\n        },\n    }\n    self._assets.append(asset_dict)\n", "def add_asset(self, basename, file_path):\n    self._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n    if basename not in self._assets:\n        self._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n    asset = assets.Asset(basename, self._assets, file_path, self._name)\n    self._assets[basename] = asset\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_asset(self, basename, file_path):\n    self._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    if basename not in self._assets:\n        self._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset = self._assets.setdefault(basename, {})\n    asset[\"file_path\"] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset = etree.Element(\"asset\", {\"lang\": self._renditions[basename].lang})\n    asset.text = basename\n    asset.attrib[\"file\"] = file_path\n    self._assets.append(asset)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset = {\n        \"artigo02-gf02.tiff\": file_path,\n        \"artigo02-gf02.jpg\": \"/path/artigo02-gf02.jpg\",\n        \"artigo02-gf02.png\": \"/path/artigo02-gf02.png\",\n    }\n    if \"asset\" not in self._assets:\n        self._assets[\"asset\"] = {}\n    self._assets[\"asset\"].update(asset)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset = {\n        \"artigo02-gf03\": \"{}/{}\".format(self.file_path, file_path),\n        \"artigo02-gf03-r\": \"{}/{}-r{}.tiff\".format(self.file_path, basename, self._renditions['r']),\n        \"artigo02-gf03-g\": \"{}/{}-g{}.tiff\".format(self.file_path, basename, self._renditions['g']),\n        \"artigo02-gf03-b\": \"{}/{}-b{}.tiff\".format(self.file_path, basename, self._renditions['b']),\n        \"artigo02-gf03-a\": \"{}/{}-a{}.tiff\".format(self.file_path, basename, self._renditions['a']),\n    }\n    self._assets.append(asset)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset_dict = {\n        \"artigo02-gf03\": {\n            \"file_path\": file_path,\n            \"content\": self.xml_content(),\n        },\n        \"artigo02-gf03-asset\": {\n            \"basename\": basename,\n            \"asset_path\": file_path,\n            \"content\": self.xml_content(),\n        },\n    }\n    self._assets.append(asset_dict)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n    self._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    if basename not in self._assets:\n        self._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset = assets.Asset(basename, self._assets, file_path, self._name)\n    self._assets[basename] = asset\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_zipfile", "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict", "ques_id": "62b463163879012d194814a4", "code": ["def _explore_zipfile(zip_path):\n    results = {}\n    for root, dirs, files in os.walk(zip_path):\n        for fn in files:\n            fn_path = os.path.join(root, fn)\n            results[fn_path] = explore_source(fn_path)\n    logger.info('Found %d files in zip with %d XML files',\n                len(results), len(files))\n    return results\n", "def _explore_zipfile(zip_path):\n    xmls = {}\n    for xml_filename in select_filenames_by_prefix('', zip_path):\n        xml_path = os.path.join(zip_path, xml_filename)\n        xml = _eval_file(xml_filename, xml_path)\n        xmls[xml_filename] = xml\n    return xmls\n", "def _explore_zipfile(zip_path):\n    data = {}\n    for filename in _group_files_by_xml_filename(zip_path, ['xml']):\n        with open(filename, 'rb') as f:\n            data[filename] = eval(f.read().decode('utf-8'))\n    return data\n", "def _explore_zipfile(zip_path):\n    data = {}\n    for root, dirs, files in os.walk(zip_path):\n        for f in files:\n            filename = os.path.basename(f)\n            data[filename] = explore_source(os.path.join(root, f))\n    return data\n", "def _explore_zipfile(zip_path):\n    data = {}\n    for zip_file in _explore_folder(zip_path):\n        xml_files = _group_files_by_xml_filename(zip_file, xmls=True, files=True)\n        data.update(xml_files)\n    return data\n", "def _explore_zipfile(zip_path):\n    files = {}\n    for file_path in _explore_zipfile(zip_path):\n        files[os.path.basename(file_path)] = file_path\n\n    return files\n", "def _explore_zipfile(zip_path):\n    logger.info('Extracting XML data from zip file {}'.format(zip_path))\n    with zipfile.ZipFile(zip_path) as zip_file:\n        xml_files = _group_files_by_xml_filename(zip_file.infolist(), zip_path)\n    return xml_files\n", "def _explore_zipfile(zip_path):\n    xmls = []\n    files = []\n    with zipfile.ZipFile(zip_path) as zf:\n        for path, subdirs, files in walk(zip_path):\n            for filename in files:\n                xml_path = join(path, filename)\n                if _eval_file(prefix=filename, xml_path=xml_path):\n                    xmls.append(xml_path)\n                    files.append(join(path, filename))\n            for subdirname in subdirs:\n                _explore_zipfile(join(path, subdirname))\n    return {'xmls': xmls, 'files': files}\n", "def _explore_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        xml_files = _group_files_by_xml_filename(zf.namelist(), zf.namelist())\n        data = {}\n        for xml_file in xml_files:\n            data[xml_file] = _eval_file(prefix=xml_file, file_path=zf.open(xml_file))\n    return data\n", "def _explore_zipfile(zip_path):\n    packages = {}\n    with zipfile.ZipFile(zip_path) as zf:\n        for name in zf.namelist():\n            xml_basename = os.path.basename(name).split('.')[0]\n            packages[xml_basename] = _group_files_by_xml_filename(\n                zf.open(name), xmls=[name], files=[name]\n            )\n\n    return packages\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_zipfile(zip_path):\n    results = {}\n    for root, dirs, files in os.walk(zip_path):\n        for fn in files:\n            fn_path = os.path.join(root, fn)\n            results[fn_path] = explore_source(fn_path)\n    logger.info('Found %d files in zip with %d XML files',\n                len(results), len(files))\n    return results\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    xmls = {}\n    for xml_filename in select_filenames_by_prefix('', zip_path):\n        xml_path = os.path.join(zip_path, xml_filename)\n        xml = _eval_file(xml_filename, xml_path)\n        xmls[xml_filename] = xml\n    return xmls\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    data = {}\n    for filename in _group_files_by_xml_filename(zip_path, ['xml']):\n        with open(filename, 'rb') as f:\n            data[filename] = eval(f.read().decode('utf-8'))\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    data = {}\n    for root, dirs, files in os.walk(zip_path):\n        for f in files:\n            filename = os.path.basename(f)\n            data[filename] = explore_source(os.path.join(root, f))\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    data = {}\n    for zip_file in _explore_folder(zip_path):\n        xml_files = _group_files_by_xml_filename(zip_file, xmls=True, files=True)\n        data.update(xml_files)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    files = {}\n    for file_path in _explore_zipfile(zip_path):\n        files[os.path.basename(file_path)] = file_path\n\n    return files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    logger.info('Extracting XML data from zip file {}'.format(zip_path))\n    with zipfile.ZipFile(zip_path) as zip_file:\n        xml_files = _group_files_by_xml_filename(zip_file.infolist(), zip_path)\n    return xml_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    xmls = []\n    files = []\n    with zipfile.ZipFile(zip_path) as zf:\n        for path, subdirs, files in walk(zip_path):\n            for filename in files:\n                xml_path = join(path, filename)\n                if _eval_file(prefix=filename, xml_path=xml_path):\n                    xmls.append(xml_path)\n                    files.append(join(path, filename))\n            for subdirname in subdirs:\n                _explore_zipfile(join(path, subdirname))\n    return {'xmls': xmls, 'files': files}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        xml_files = _group_files_by_xml_filename(zf.namelist(), zf.namelist())\n        data = {}\n        for xml_file in xml_files:\n            data[xml_file] = _eval_file(prefix=xml_file, file_path=zf.open(xml_file))\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    packages = {}\n    with zipfile.ZipFile(zip_path) as zf:\n        for name in zf.namelist():\n            xml_basename = os.path.basename(name).split('.')[0]\n            packages[xml_basename] = _group_files_by_xml_filename(\n                zf.open(name), xmls=[name], files=[name]\n            )\n\n    return packages\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list_from_zipfile", "docstring": "Return the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```", "ques_id": "62b463163879012d194814a6", "code": ["def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [fname.filename for fname in zf.namelist()]\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [f.filename for f in zf.filelist]\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files = []\n        for info in zf.infolist():\n            files.append(zf.open(info, 'r'))\n        return files\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [f.filename for f in zf.filelist]\n", "def files_list_from_zipfile(zip_path):\n    with ZipFile(zip_path) as zf:\n        return [fname.filename for fname in zf.namelist()]\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [fname.filename for fname in zf.namelist()]\n", "def files_list_from_zipfile(zip_path):\n    with ZipFile(zip_path) as zf:\n        files = zf.namelist()\n    return files\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [fname.filename for fname in zf.namelist()]\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        return [f.filename for f in zf.infolist()]\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files_list = zf.namelist()\n        return files_list\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [fname.filename for fname in zf.namelist()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [f.filename for f in zf.filelist]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files = []\n        for info in zf.infolist():\n            files.append(zf.open(info, 'r'))\n        return files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [f.filename for f in zf.filelist]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with ZipFile(zip_path) as zf:\n        return [fname.filename for fname in zf.namelist()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [fname.filename for fname in zf.namelist()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with ZipFile(zip_path) as zf:\n        files = zf.namelist()\n    return files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        return [fname.filename for fname in zf.namelist()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        return [f.filename for f in zf.infolist()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files_list = zf.namelist()\n        return files_list\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "fix_namespace_prefix_w", "docstring": "Convert text that defaults to 'w:st=\"' to 'w-st=\"'", "ques_id": "62b4631b3879012d194814dd", "code": ["def fix_namespace_prefix_w(content):\n    return content.replace(' w:st=\"','w-st=\"')\n", "def fix_namespace_prefix_w(content):\n    return re.sub(r'\\s+w:(\\w+)=\"',\n                  r'\\g<1>-\\g<2>',\n                  content)\n", "def fix_namespace_prefix_w(content):\n    return content.replace(' w:st=\"','w-st=\"')\n", "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "def fix_namespace_prefix_w(content):\n    return re.sub(r'\\s*w:([^\"]+)\"', r'w-st=\"\\1\"', content)\n", "def fix_namespace_prefix_w(content):\n    return content.replace(' w:st=\"','w-st=\"')\n", "def fix_namespace_prefix_w(content):\n    if content is None:\n        return None\n    elif isinstance(content, str):\n        content = content.replace('w:', 'w-st=\"')\n    return content\n", "def fix_namespace_prefix_w(content):\n    return re.sub(r' w:([^\"]+)=\"', r' w-st=\"\\1\"', content)\n", "def fix_namespace_prefix_w(content):\n    return re.sub(r'\\:(\\w+)=\"', r': w-st=\"\\1\"', content)\n", "def fix_namespace_prefix_w(content):\n    return re.sub(r'\\s*w:(\\w+)=\"',\n                  r'\\g<1>-\\g<2>',\n                  content)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def fix_namespace_prefix_w(content):\n    return content.replace(' w:st=\"','w-st=\"')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(r'\\s+w:(\\w+)=\"',\n                  r'\\g<1>-\\g<2>',\n                  content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return content.replace(' w:st=\"','w-st=\"')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(r'\\s*w:([^\"]+)\"', r'w-st=\"\\1\"', content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return content.replace(' w:st=\"','w-st=\"')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    if content is None:\n        return None\n    elif isinstance(content, str):\n        content = content.replace('w:', 'w-st=\"')\n    return content\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(r' w:([^\"]+)=\"', r' w-st=\"\\1\"', content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(r'\\:(\\w+)=\"', r': w-st=\"\\1\"', content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(r'\\s*w:(\\w+)=\"',\n                  r'\\g<1>-\\g<2>',\n                  content)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "match_pubdate", "docstring": "Returns the first match in the pubdate_xpaths list", "ques_id": "62b463283879012d1948153d", "code": ["def match_pubdate(node, pubdate_xpaths):\n    return next(\n        (\n            item\n            for item in node.xpath(pubdate_xpaths[0])\n            if item.xpath(\"@type\")\n            if item.xpath(\"@id\")\n        ),\n        None,\n    )\n", "def match_pubdate(node, pubdate_xpaths):\n    return next((\n        node.xpath(pubdate_xpath)\n        for pubdate_xpath in pubdate_xpaths\n    ), None)\n", "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        match = pubdate_xpath(node)\n        if match:\n            return match\n", "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        try:\n            return next(\n                (\n                    el\n                    for el in node.xpath(pubdate_xpath)\n                    if el.text is not None\n                )\n            )\n        except StopIteration:\n            pass\n", "def match_pubdate(node, pubdate_xpaths):\n    return next((\n        x for x in pubdate_xpaths\n        if x.match(node)\n    ), None)\n", "def match_pubdate(node, pubdate_xpaths):\n    return next(\n        (\n            x\n            for x in pubdate_xpaths\n            if x.match(node)\n        ),\n        None,\n    )\n", "def match_pubdate(node, pubdate_xpaths):\n    return next((\n        node.xpath(pubdate_xpath)\n        for pubdate_xpath in pubdate_xpaths\n    ), None)\n", "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        match = pubdate_xpath(node)\n        if match:\n            return match\n"], "level": "self_contained", "generate_results": [{"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next(\n        (\n            item\n            for item in node.xpath(pubdate_xpaths[0])\n            if item.xpath(\"@type\")\n            if item.xpath(\"@id\")\n        ),\n        None,\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next((\n        node.xpath(pubdate_xpath)\n        for pubdate_xpath in pubdate_xpaths\n    ), None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        match = pubdate_xpath(node)\n        if match:\n            return match\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        try:\n            return next(\n                (\n                    el\n                    for el in node.xpath(pubdate_xpath)\n                    if el.text is not None\n                )\n            )\n        except StopIteration:\n            pass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next((\n        x for x in pubdate_xpaths\n        if x.match(node)\n    ), None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next(\n        (\n            x\n            for x in pubdate_xpaths\n            if x.match(node)\n        ),\n        None,\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next((\n        node.xpath(pubdate_xpath)\n        for pubdate_xpath in pubdate_xpaths\n    ), None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        match = pubdate_xpath(node)\n        if match:\n            return match\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/front_articlemeta_issue.py", "project": "scieloorg/packtools", "name": "_extract_number_and_supplment_from_issue_element", "docstring": "Extract the possible values of number and suppl from the contents of issue.", "ques_id": "62b463303879012d19481579", "code": ["def _extract_number_and_supplment_from_issue_element(issue):\n    number_values = []\n    suppl = []\n    for element in issue.findall('.//number'):\n        number_values.append(int(element.text))\n    for element in issue.findall('.//suppl'):\n        suppl.append(int(element.text))\n    return number_values, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n\n    if issue.find('sb:Number', NS) is not None:\n        number = issue.find('sb:Number', NS).text\n\n    if issue.find('sb:Supplement', NS) is not None:\n        suppl = issue.find('sb:Supplement', NS).text\n\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    if issue.find('v10:Number') is not None:\n        number = issue.find('v10:Number').text\n    if issue.find('v10:Supplement') is not None:\n        suppl = issue.find('v10:Supplement').text\n\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    for element in issue.iter('{%s}number' % xmlconst.TAG_NAMESPACE):\n        try:\n            number = int(element.text)\n        except ValueError:\n            pass\n    for element in issue.iter('{%s}suppl' % xmlconst.TAG_NAMESPACE):\n        try:\n            suppl = int(element.text)\n        except ValueError:\n            pass\n    return (number, suppl)\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('number').text\n    suppl = issue.find('suppl').text\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = ''\n    suppl = ''\n    for element in issue:\n        if element.tag == 'Number':\n            number = element.text\n        elif element.tag == 'Supplement':\n            suppl = element.text\n\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('.//{%s}number' % xmlconst.TAG_NS).text\n    suppl = issue.find('.//{%s}suppl' % xmlconst.TAG_NS).text\n    try:\n        number = int(number)\n    except ValueError:\n        pass\n    try:\n        suppl = int(suppl)\n    except ValueError:\n        pass\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number_values = []\n    suppl = []\n    for element in issue:\n        if element.tag == 'Number':\n            number_values.append(element.text)\n        elif element.tag == 'Supplement':\n            suppl.append(element.text)\n    return number_values, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = extract_number_from_issue_element(issue)\n    suppl = extract_supplement_from_issue_element(issue)\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    if issue.find('xbrli:Number') is not None:\n        number = issue.find('xbrli:Number').text\n    if issue.find('xbrli:Supplement') is not None:\n        suppl = issue.find('xbrli:Supplement').text\n    return number, suppl\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number_values = []\n    suppl = []\n    for element in issue.findall('.//number'):\n        number_values.append(int(element.text))\n    for element in issue.findall('.//suppl'):\n        suppl.append(int(element.text))\n    return number_values, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n\n    if issue.find('sb:Number', NS) is not None:\n        number = issue.find('sb:Number', NS).text\n\n    if issue.find('sb:Supplement', NS) is not None:\n        suppl = issue.find('sb:Supplement', NS).text\n\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    if issue.find('v10:Number') is not None:\n        number = issue.find('v10:Number').text\n    if issue.find('v10:Supplement') is not None:\n        suppl = issue.find('v10:Supplement').text\n\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    for element in issue.iter('{%s}number' % xmlconst.TAG_NAMESPACE):\n        try:\n            number = int(element.text)\n        except ValueError:\n            pass\n    for element in issue.iter('{%s}suppl' % xmlconst.TAG_NAMESPACE):\n        try:\n            suppl = int(element.text)\n        except ValueError:\n            pass\n    return (number, suppl)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('number').text\n    suppl = issue.find('suppl').text\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = ''\n    suppl = ''\n    for element in issue:\n        if element.tag == 'Number':\n            number = element.text\n        elif element.tag == 'Supplement':\n            suppl = element.text\n\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('.//{%s}number' % xmlconst.TAG_NS).text\n    suppl = issue.find('.//{%s}suppl' % xmlconst.TAG_NS).text\n    try:\n        number = int(number)\n    except ValueError:\n        pass\n    try:\n        suppl = int(suppl)\n    except ValueError:\n        pass\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number_values = []\n    suppl = []\n    for element in issue:\n        if element.tag == 'Number':\n            number_values.append(element.text)\n        elif element.tag == 'Supplement':\n            suppl.append(element.text)\n    return number_values, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = extract_number_from_issue_element(issue)\n    suppl = extract_supplement_from_issue_element(issue)\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    if issue.find('xbrli:Number') is not None:\n        number = issue.find('xbrli:Number').text\n    if issue.find('xbrli:Supplement') is not None:\n        suppl = issue.find('xbrli:Supplement').text\n    return number, suppl\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "pretty", "docstring": "Return a pretty formatted representation of self.", "ques_id": "62b46740d2f69a53b466171a", "code": ["def pretty(self, indent=0, debug=False):\n    output = \"\"\n    if self.iscanonical:\n        output += \"Canonical(%s)\" % self.name\n    else:\n        output += \"Dual(%s)\" % self.name\n    if self.sort_order == self.FALSE:\n        return \"%s(%s)\" % (output, indent)\n    if self.sort_order == self.TRUE:\n        return \"%s(%s, %s)\" % (output, indent, self.name)\n    return \"%s(%s)\" % (output, self.name)\n", "def pretty(self, indent=0, debug=False):\n    debugstr ='' * indent\n    if self.iscanonical:\n        s = \"%siscanonical '%s'\" % (debugstr, self.name)\n    else:\n        s = \"%s%s'%s'\" % (debugstr,'' * indent, self.name)\n    if self.dual is not None:\n        s +='(%s)' % self.dual.pretty(indent+1, debug)\n    return s\n", "def pretty(self, indent=0, debug=False):\n    if self.iscanonical:\n        return \"({0.__class__.__name__} {0.iscanonical})\".format(self)\n    else:\n        return \"({0.__class__.__name__} {0.dual})\".format(self)\n", "def pretty(self, indent=0, debug=False):\n    lines = []\n    if self.iscanonical:\n        lines.append(\"Canonical: %s\" % repr(self.dual))\n    else:\n        lines.append(\"Canonical (%s)\" % self.dual.pretty(indent=indent, debug=debug))\n    return \"\\n\".join(lines)\n", "def pretty(self, indent=0, debug=False):\n    if debug:\n        dump = lambda item, indent=0: print(''* indent + str(item))\n    else:\n        dump = lambda item, indent=0: repr(item)\n\n    debug_str = ''\n    if self.iscanonical:\n        debug_str +='canonical '\n    else:\n        debug_str +='non-canonical '\n    debug_str +='({})'.format(self.sort_order)\n\n    if self.dual is not None:\n        debug_str +='dual {}'.format(self.dual)\n\n    return '{}{} {}'.format(debug_str, str(self), dump(self))\n", "def pretty(self, indent=0, debug=False):\n    if self.iscanonical:\n        rep = '{} = {}'.format(self.name, self.dual.pretty(indent + 2, debug))\n    else:\n        rep = '{} <= {}'.format(self.name, self.dual.pretty(indent + 2, debug))\n    if self._args:\n        rep += '({})'.format(', '.join(a.pretty(indent + 2, debug) for a in self._args))\n    return '{}({})'.format(self.__class__.__name__, rep)\n", "def pretty(self, indent=0, debug=False):\n    lines = []\n    if self.iscanonical:\n        if debug:\n            lines.append(''* indent + 'Canonical:'+ str(self))\n        else:\n            lines.append(''* indent + 'Canonical: FALSE')\n    else:\n        if debug:\n            lines.append(''* indent + 'Uncanonical:'+ str(self))\n        else:\n            lines.append(''* indent + 'Uncanonical: FALSE')\n    for arg in self.args:\n        lines.append(arg.pretty(indent + 2, debug))\n    return '\\n'.join(lines)\n", "def pretty(self, indent=0, debug=False):\n    out = \"\"\n    if self.iscanonical:\n        out += \"Canonical form is '%s'. \" % self.name\n    else:\n        out += \"Canonical form is '%s'. \" % self.name[:-1]\n        out += \"Its canonical form is '%s'. \" % self.canonical.name\n    out += \"Canonical form is '%s'. \" % self.canonical.name[:-1]\n    if self.dual is not None:\n        out += \"Canonical form is '%s'. \" % self.dual.name\n        out += \"Its canonical form is '%s'. \" % self.dual.canonical.name\n    out += \"Dependencies are %s.\" % \", \".join([str(dep) for dep in self.deps])\n    return (\"\\n\" + indent * \" \").join([out])\n", "def pretty(self, indent=0, debug=False):\n    indent_str = \" \" * indent\n    if debug:\n        return \"%s%s (%s) %s\" % (indent_str, self.__class__.__name__, self.iscanonical, self)\n    else:\n        return \"%s%s (%s)\" % (indent_str, self.__class__.__name__, self.iscanonical)\n", "def pretty(self, indent=0, debug=False):\n    return \"[{}, {}]\".format(self.sort_order, \"FALSE\" if self.iscanonical else \"TRUE\")\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pretty(self, indent=0, debug=False):\n    output = \"\"\n    if self.iscanonical:\n        output += \"Canonical(%s)\" % self.name\n    else:\n        output += \"Dual(%s)\" % self.name\n    if self.sort_order == self.FALSE:\n        return \"%s(%s)\" % (output, indent)\n    if self.sort_order == self.TRUE:\n        return \"%s(%s, %s)\" % (output, indent, self.name)\n    return \"%s(%s)\" % (output, self.name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    debugstr ='' * indent\n    if self.iscanonical:\n        s = \"%siscanonical '%s'\" % (debugstr, self.name)\n    else:\n        s = \"%s%s'%s'\" % (debugstr,'' * indent, self.name)\n    if self.dual is not None:\n        s +='(%s)' % self.dual.pretty(indent+1, debug)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    if self.iscanonical:\n        return \"({0.__class__.__name__} {0.iscanonical})\".format(self)\n    else:\n        return \"({0.__class__.__name__} {0.dual})\".format(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    lines = []\n    if self.iscanonical:\n        lines.append(\"Canonical: %s\" % repr(self.dual))\n    else:\n        lines.append(\"Canonical (%s)\" % self.dual.pretty(indent=indent, debug=debug))\n    return \"\\n\".join(lines)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    if debug:\n        dump = lambda item, indent=0: print(''* indent + str(item))\n    else:\n        dump = lambda item, indent=0: repr(item)\n\n    debug_str = ''\n    if self.iscanonical:\n        debug_str +='canonical '\n    else:\n        debug_str +='non-canonical '\n    debug_str +='({})'.format(self.sort_order)\n\n    if self.dual is not None:\n        debug_str +='dual {}'.format(self.dual)\n\n    return '{}{} {}'.format(debug_str, str(self), dump(self))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    if self.iscanonical:\n        rep = '{} = {}'.format(self.name, self.dual.pretty(indent + 2, debug))\n    else:\n        rep = '{} <= {}'.format(self.name, self.dual.pretty(indent + 2, debug))\n    if self._args:\n        rep += '({})'.format(', '.join(a.pretty(indent + 2, debug) for a in self._args))\n    return '{}({})'.format(self.__class__.__name__, rep)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    lines = []\n    if self.iscanonical:\n        if debug:\n            lines.append(''* indent + 'Canonical:'+ str(self))\n        else:\n            lines.append(''* indent + 'Canonical: FALSE')\n    else:\n        if debug:\n            lines.append(''* indent + 'Uncanonical:'+ str(self))\n        else:\n            lines.append(''* indent + 'Uncanonical: FALSE')\n    for arg in self.args:\n        lines.append(arg.pretty(indent + 2, debug))\n    return '\\n'.join(lines)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    out = \"\"\n    if self.iscanonical:\n        out += \"Canonical form is '%s'. \" % self.name\n    else:\n        out += \"Canonical form is '%s'. \" % self.name[:-1]\n        out += \"Its canonical form is '%s'. \" % self.canonical.name\n    out += \"Canonical form is '%s'. \" % self.canonical.name[:-1]\n    if self.dual is not None:\n        out += \"Canonical form is '%s'. \" % self.dual.name\n        out += \"Its canonical form is '%s'. \" % self.dual.canonical.name\n    out += \"Dependencies are %s.\" % \", \".join([str(dep) for dep in self.deps])\n    return (\"\\n\" + indent * \" \").join([out])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    indent_str = \" \" * indent\n    if debug:\n        return \"%s%s (%s) %s\" % (indent_str, self.__class__.__name__, self.iscanonical, self)\n    else:\n        return \"%s%s (%s)\" % (indent_str, self.__class__.__name__, self.iscanonical)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    return \"[{}, {}]\".format(self.sort_order, \"FALSE\" if self.iscanonical else \"TRUE\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "absorb", "docstring": "Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B", "ques_id": "62b46746d2f69a53b4661722", "code": ["def absorb(self, args):\n    args = list(args)\n    for i, expr in enumerate(args):\n        if isinstance(expr, self.__class__):\n            args[i] = expr.absorb()\n        elif isinstance(expr, self.NOT):\n            args[i] = not expr.absorb()\n        elif isinstance(expr, self.AND):\n            args[i] = not args[i]\n        elif isinstance(expr, self.ABSORBIC):\n            args[i] = not args[i].absorb()\n        elif isinstance(expr, self.NOT_NOT):\n            args[i] = not args[i].absorb()\n        elif isinstance(expr, self.ABSORBIC_AND):\n            args[i] = not args[i].absorb(args[i].args[0])\n        elif isinstance(expr, self.ABSORBIC_OR):\n            args[i] = not args[i].absorb(args[i].args[1])\n        else:\n         ", "def absorb(self, args):\n    args = list(args)\n    if len(args) >= 2:\n        a, b = args[:2]\n        if a.is_dual() and b.is_dual():\n            for i in range(2, len(args)):\n                if args[i].is_dual():\n                    args[i - 1] = ~a\n                else:\n                    args[i - 1] = b\n        else:\n            for i in range(2, len(args)):\n                if args[i].is_dual():\n                    args[i - 1] = ~b\n                else:\n                    args[i - 1] = a\n    elif len(args) == 1:\n        if args[0].is_dual():\n            args[0] = ~args[0]\n        else:\n            args[0] =", "def absorb(self, args):\n    args = list(args)\n    if len(args) == 1:\n        return self & args[0]\n    elif len(args) == 2:\n        return self | args[0] ^ args[1]\n    else:\n        result = self.NOT\n        for i, arg in enumerate(args[:-1]):\n            result &= arg\n            result |= args[i + 1]\n        return result\n", "def absorb(self, args):\n    args = tuple(args)\n    if len(args) < 2:\n        return args\n\n    if args[0] == self.NOT:\n        return self.absorb(args[1:])\n\n    if args[1] == self.NOT:\n        return args[0] | self.absorb(args[0])\n\n    if args[0] == ~args[1]:\n        return self.NOT | args[0] & ~args[1]\n\n    return self.NOT & ~args[0] | args[1]\n", "def absorb(self, args):\n    args = list(args)\n    if len(args) == 1:\n        return self & args[0]\n    elif len(args) == 2:\n        return self & ~args[0] | args[1]\n    elif len(args) == 3:\n        return self & ~args[1] | args[0] | ~args[2]\n    else:\n        raise ValueError(\"Too many arguments for absorb\")\n", "def absorb(self, args):\n    args = tuple(args)\n    if args in self.annihilator:\n        return self\n    args = tuple([~arg if self!= arg else arg for arg in args])\n    return self.__class__(*args)\n", "def absorb(self, args):\n    args = list(args)\n    for i in range(len(args)):\n        if args[i] == self:\n            continue\n\n        args[i] = self.absorb(args[i].args)\n\n    if self.iscanonical:\n        if len(args) == 1:\n            return args[0]\n        elif len(args) == 2:\n            return args[0] | args[1]\n        elif len(args) == 3:\n            return args[0] & args[1] | args[2]\n        else:\n            raise ValueError(\"Unknown absorption operator\")\n    return self._pyoperator(*args)\n", "def absorb(self, args):\n    args = list(args)\n    if len(args) == 1:\n        return args[0] + self.args[0]\n    elif len(args) == 2:\n        return self - args[0] * args[1]\n    elif len(args) == 3:\n        return self * (~self & args[1])\n    elif len(args) == 4:\n        return ~(~self & args[1]) * (self & args[2]) - self * args[0]\n    else:\n        raise TypeError(\"absorb expected at most 3 arguments, got %d\" % len(args))\n", "def absorb(self, args):\n    args = list(args)\n    for i, a in enumerate(args):\n        if a.iscanonical:\n            if i > 0:\n                args[i - 1] = a.absorb(args[i - 1])\n            else:\n                args[i - 1] = self.NOT\n    return args\n", "def absorb(self, args):\n    args = list(args)\n    for i, arg in enumerate(args):\n        if isinstance(arg, self.__class__):\n            args[i] = arg.args[0] if i == 0 else arg.args[0] | args[i - 1]\n        else:\n            args[i] = ~arg & self.args[i]\n    return self._pyoperator(*args)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def absorb(self, args):\n    args = list(args)\n    for i, expr in enumerate(args):\n        if isinstance(expr, self.__class__):\n            args[i] = expr.absorb()\n        elif isinstance(expr, self.NOT):\n            args[i] = not expr.absorb()\n        elif isinstance(expr, self.AND):\n            args[i] = not args[i]\n        elif isinstance(expr, self.ABSORBIC):\n            args[i] = not args[i].absorb()\n        elif isinstance(expr, self.NOT_NOT):\n            args[i] = not args[i].absorb()\n        elif isinstance(expr, self.ABSORBIC_AND):\n            args[i] = not args[i].absorb(args[i].args[0])\n        elif isinstance(expr, self.ABSORBIC_OR):\n            args[i] = not args[i].absorb(args[i].args[1])\n        else:\n         ", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    args = list(args)\n    if len(args) >= 2:\n        a, b = args[:2]\n        if a.is_dual() and b.is_dual():\n            for i in range(2, len(args)):\n                if args[i].is_dual():\n                    args[i - 1] = ~a\n                else:\n                    args[i - 1] = b\n        else:\n            for i in range(2, len(args)):\n                if args[i].is_dual():\n                    args[i - 1] = ~b\n                else:\n                    args[i - 1] = a\n    elif len(args) == 1:\n        if args[0].is_dual():\n            args[0] = ~args[0]\n        else:\n            args[0] =", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    args = list(args)\n    if len(args) == 1:\n        return self & args[0]\n    elif len(args) == 2:\n        return self | args[0] ^ args[1]\n    else:\n        result = self.NOT\n        for i, arg in enumerate(args[:-1]):\n            result &= arg\n            result |= args[i + 1]\n        return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    args = tuple(args)\n    if len(args) < 2:\n        return args\n\n    if args[0] == self.NOT:\n        return self.absorb(args[1:])\n\n    if args[1] == self.NOT:\n        return args[0] | self.absorb(args[0])\n\n    if args[0] == ~args[1]:\n        return self.NOT | args[0] & ~args[1]\n\n    return self.NOT & ~args[0] | args[1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    args = list(args)\n    if len(args) == 1:\n        return self & args[0]\n    elif len(args) == 2:\n        return self & ~args[0] | args[1]\n    elif len(args) == 3:\n        return self & ~args[1] | args[0] | ~args[2]\n    else:\n        raise ValueError(\"Too many arguments for absorb\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    args = tuple(args)\n    if args in self.annihilator:\n        return self\n    args = tuple([~arg if self!= arg else arg for arg in args])\n    return self.__class__(*args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    args = list(args)\n    for i in range(len(args)):\n        if args[i] == self:\n            continue\n\n        args[i] = self.absorb(args[i].args)\n\n    if self.iscanonical:\n        if len(args) == 1:\n            return args[0]\n        elif len(args) == 2:\n            return args[0] | args[1]\n        elif len(args) == 3:\n            return args[0] & args[1] | args[2]\n        else:\n            raise ValueError(\"Unknown absorption operator\")\n    return self._pyoperator(*args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    args = list(args)\n    if len(args) == 1:\n        return args[0] + self.args[0]\n    elif len(args) == 2:\n        return self - args[0] * args[1]\n    elif len(args) == 3:\n        return self * (~self & args[1])\n    elif len(args) == 4:\n        return ~(~self & args[1]) * (self & args[2]) - self * args[0]\n    else:\n        raise TypeError(\"absorb expected at most 3 arguments, got %d\" % len(args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    args = list(args)\n    for i, a in enumerate(args):\n        if a.iscanonical:\n            if i > 0:\n                args[i - 1] = a.absorb(args[i - 1])\n            else:\n                args[i - 1] = self.NOT\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    args = list(args)\n    for i, arg in enumerate(args):\n        if isinstance(arg, self.__class__):\n            args[i] = arg.args[0] if i == 0 else arg.args[0] | args[i - 1]\n        else:\n            args[i] = ~arg & self.args[i]\n    return self._pyoperator(*args)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "on", "docstring": "Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.", "ques_id": "62b86707b4d922cb0e688c2a", "code": ["def on(self, hook):\n    def register(listener):\n        self.registry[hook].add(listener)\n    return register\n", "def on(self, hook):\n    def wrapper(func):\n        self.registry[hook] = func\n        return func\n\n    return wrapper\n", "def on(self, hook):\n    def register_listener(listener):\n        if listener not in self.registry[hook]:\n            self.registry[hook].append(listener)\n        return listener\n    return register_listener\n", "def on(self, hook):\n    def register_listener(listener):\n        self.registry[hook].append(listener)\n    return register_listener\n", "def on(self, hook):\n    def wrapper(handler):\n        if not hook in self.registry:\n            self.registry[hook] = [handler]\n        else:\n            self.registry[hook].append(handler)\n        return handler\n\n    return wrapper\n", "def on(self, hook):\n\n    def wrapper(func):\n        self.registry[hook] = func\n        return func\n\n    return wrapper\n", "def on(self, hook):\n\n    def register_listener(listener):\n        self.registry[hook].add(listener)\n        return listener\n\n    return register_listener\n", "def on(self, hook):\n    def register_listener(listener):\n        self.registry[hook].add(listener)\n    return register_listener\n", "def on(self, hook):\n\n    def register_listener(listener):\n        if listener not in self.registry:\n            self.registry.append(listener)\n        return listener\n\n    return register_listener\n", "def on(self, hook):\n    def register_listener(listener):\n        if hook not in self.registry:\n            self.registry[hook] = []\n        self.registry[hook].append(listener)\n    return register_listener\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def on(self, hook):\n    def register(listener):\n        self.registry[hook].add(listener)\n    return register\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def wrapper(func):\n        self.registry[hook] = func\n        return func\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def register_listener(listener):\n        if listener not in self.registry[hook]:\n            self.registry[hook].append(listener)\n        return listener\n    return register_listener\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def register_listener(listener):\n        self.registry[hook].append(listener)\n    return register_listener\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def wrapper(handler):\n        if not hook in self.registry:\n            self.registry[hook] = [handler]\n        else:\n            self.registry[hook].append(handler)\n        return handler\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\n    def wrapper(func):\n        self.registry[hook] = func\n        return func\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\n    def register_listener(listener):\n        self.registry[hook].add(listener)\n        return listener\n\n    return register_listener\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def register_listener(listener):\n        self.registry[hook].add(listener)\n    return register_listener\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\n    def register_listener(listener):\n        if listener not in self.registry:\n            self.registry.append(listener)\n        return listener\n\n    return register_listener\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def register_listener(listener):\n        if hook not in self.registry:\n            self.registry[hook] = []\n        self.registry[hook].append(listener)\n    return register_listener\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/tests/conftest.py", "project": "rak-n-rok/Krake", "name": "base_config", "docstring": "Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.", "ques_id": "62b86729b4d922cb0e688c2f", "code": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"static_auth\": {\n            \"user\": {\"type\": \"string\"},\n            \"password\": {\"type\": \"string\"},\n        },\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"static_auth\": {\n            \"user\": {\n                \"name\": \"username\",\n                \"value\": \"user-{}\".format(user)\n            },\n            \"password\": {\n                \"name\": \"password\",\n                \"value\": \"password-{}\".format(user)\n            }\n        },\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"key\": \"some-secret\",\n        \"value\": \"some-value\",\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"database_password\": \"test\",\n        \"database_user\": \"root\",\n        \"database_admin\": \"root\",\n        \"cache_dir\": None,\n        \"salt_master_key\": \"\",\n        \"salt_minion_key\": \"\",\n        \"salt_master_key_salt\": \"\",\n        \"salt_minion_key_salt\": \"\",\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"database\": {\n            \"username\": user,\n            \"password\": \"\",\n            \"name\": \"default\",\n            \"database_type\": \"standard\"\n        }\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"realm\": \"aws-public-api\",\n        \"password\": \"\",\n        \"key\": \"\",\n        \"name\": \"aws-public-api\",\n        \"auth\": {\n            \"username\": user,\n            \"apiKey\": \"\"\n        },\n        \"ttl\": 3600,\n        \"active\": True\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"\",\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"use_ssl\": False,\n        \"cacert\": None,\n        \"ssl_verify\": False,\n        \"auth\": {\n            \"username\": \"admin\",\n            \"api_key\": \"1234567890\"\n        }\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"\",\n        \"key\": \"\",\n        \"database\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n        },\n        \"write_only\": False,\n        \"allow_inheritance\": False,\n        \"create_key\": False,\n        \"create_secret\": False,\n        \"create_cert\": False,\n        \"allow_write_only\": False,\n        \"allow_write_write_access\": False,\n        \"allow_read_only\": False,\n        \"allow_decryption\": False,\n        \"encrypt_key\": False,\n        \"encrypt_secret\": False,\n        \"encrypt_cert\": False,\n    }\n"], "level": "self_contained", "generate_results": [{"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"static_auth\": {\n            \"user\": {\"type\": \"string\"},\n            \"password\": {\"type\": \"string\"},\n        },\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"static_auth\": {\n            \"user\": {\n                \"name\": \"username\",\n                \"value\": \"user-{}\".format(user)\n            },\n            \"password\": {\n                \"name\": \"password\",\n                \"value\": \"password-{}\".format(user)\n            }\n        },\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"key\": \"some-secret\",\n        \"value\": \"some-value\",\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"database_password\": \"test\",\n        \"database_user\": \"root\",\n        \"database_admin\": \"root\",\n        \"cache_dir\": None,\n        \"salt_master_key\": \"\",\n        \"salt_minion_key\": \"\",\n        \"salt_master_key_salt\": \"\",\n        \"salt_minion_key_salt\": \"\",\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"database\": {\n            \"username\": user,\n            \"password\": \"\",\n            \"name\": \"default\",\n            \"database_type\": \"standard\"\n        }\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"realm\": \"aws-public-api\",\n        \"password\": \"\",\n        \"key\": \"\",\n        \"name\": \"aws-public-api\",\n        \"auth\": {\n            \"username\": user,\n            \"apiKey\": \"\"\n        },\n        \"ttl\": 3600,\n        \"active\": True\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"\",\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"use_ssl\": False,\n        \"cacert\": None,\n        \"ssl_verify\": False,\n        \"auth\": {\n            \"username\": \"admin\",\n            \"api_key\": \"1234567890\"\n        }\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"\",\n        \"key\": \"\",\n        \"database\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n        },\n        \"write_only\": False,\n        \"allow_inheritance\": False,\n        \"create_key\": False,\n        \"create_secret\": False,\n        \"create_cert\": False,\n        \"allow_write_only\": False,\n        \"allow_write_write_access\": False,\n        \"allow_read_only\": False,\n        \"allow_decryption\": False,\n        \"encrypt_key\": False,\n        \"encrypt_secret\": False,\n        \"encrypt_cert\": False,\n    }\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "_fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "ques_id": "62b8a4a4755ee91dce50a3d2", "code": ["def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=UTC)\n\n    return dt.astimezone(UTC)\n", "def _fromutc(self, dt):\n    return dt + timedelta(hours=1)\n", "def _fromutc(self, dt):\n    return dt.astimezone(self.tz)\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=timezone.utc)\n\n    return dt.astimezone(timezone.utc)\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=UTC)\n\n    return dt.astimezone(self.tz)\n", "def _fromutc(self, dt):\n    return dt.replace(tzinfo=None)\n", "def _fromutc(self, dt):\n    if dt.tzinfo:\n        raise ValueError(\"{!r} is already a fromutc datetime\".format(dt))\n\n    return dt.replace(tzinfo=self)\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        # No timezone was given. Check to make sure we're not folding the\n        # input datetime to an ambiguous timezone.\n        if dt.tzname() == 'UTC':\n            return dt\n        raise ValueError('No ambiguous timezone in fromutc')\n    # If we have a tzinfo and the input is in UTC, then we can fold it to\n    # an ambiguous timezone.\n    if dt.tzinfo is not self:\n        return dt.astimezone(self)\n    return dt\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=UTC)\n    else:\n        return dt\n", "def _fromutc(self, dt):\n    return dt.astimezone(self.tz)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=UTC)\n\n    return dt.astimezone(UTC)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    return dt + timedelta(hours=1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    return dt.astimezone(self.tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=timezone.utc)\n\n    return dt.astimezone(timezone.utc)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=UTC)\n\n    return dt.astimezone(self.tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    return dt.replace(tzinfo=None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo:\n        raise ValueError(\"{!r} is already a fromutc datetime\".format(dt))\n\n    return dt.replace(tzinfo=self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        # No timezone was given. Check to make sure we're not folding the\n        # input datetime to an ambiguous timezone.\n        if dt.tzname() == 'UTC':\n            return dt\n        raise ValueError('No ambiguous timezone in fromutc')\n    # If we have a tzinfo and the input is in UTC, then we can fold it to\n    # an ambiguous timezone.\n    if dt.tzinfo is not self:\n        return dt.astimezone(self)\n    return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=UTC)\n    else:\n        return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    return dt.astimezone(self.tz)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/relativedelta.py", "project": "pexip/os-python-dateutil", "name": "normalized", "docstring": "Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.", "ques_id": "62b8982f755ee91dce50a241", "code": ["def normalized(self):\n\n    return self.__class__(self.days, self.seconds)\n", "def normalized(self):\n\n    return self.__class__(self.days, self.seconds, self.microseconds)\n", "def normalized(self):\n    return self.__class__(self.days, self.seconds, self.microseconds)\n", "def normalized(self):\n\n    return self.__class__(self.days, self.seconds)\n", "def normalized(self):\n    return self.__class__(self.days, self.seconds, self.microseconds)\n", "def normalized(self):\n\n    return self.__class__(self.days, self.seconds)\n", "def normalized(self):\n\n    return self.__class__(self.days, self.seconds, self.microseconds)\n", "def normalized(self):\n\n    return self.__class__(self.days, self.seconds)\n", "def normalized(self):\n\n    return self.__class__(self.days, self.hours, self.minutes, self.seconds,\n                          self.microseconds, self.year, self.month, self.day,\n                          weekday=self.weekday, leapdays=self.leapdays)\n", "def normalized(self):\n\n    return self.__class__(self.days, self.seconds, self.microseconds)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def normalized(self):\n\n    return self.__class__(self.days, self.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\n    return self.__class__(self.days, self.seconds, self.microseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n    return self.__class__(self.days, self.seconds, self.microseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\n    return self.__class__(self.days, self.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n    return self.__class__(self.days, self.seconds, self.microseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\n    return self.__class__(self.days, self.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\n    return self.__class__(self.days, self.seconds, self.microseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\n    return self.__class__(self.days, self.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\n    return self.__class__(self.days, self.hours, self.minutes, self.seconds,\n                          self.microseconds, self.year, self.month, self.day,\n                          weekday=self.weekday, leapdays=self.leapdays)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\n    return self.__class__(self.days, self.seconds, self.microseconds)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "tzname_in_python2", "docstring": "Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings", "ques_id": "62b89640755ee91dce50a114", "code": ["def tzname_in_python2(namefunc):\n    @wraps(namefunc)\n    def adjust_encoding(*args, **kwargs):\n        name = namefunc(*args, **kwargs)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return adjust_encoding\n", "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self):\n        name = namefunc(self)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return wrapper\n", "def tzname_in_python2(namefunc):\n    if six.PY3:\n        @wraps(namefunc)\n        def adjust_encoding(*args, **kwargs):\n            name = namefunc(*args, **kwargs)\n            if name is not None:\n                name = name.encode()\n\n            return name\n\n        return adjust_encoding\n    else:\n        return namefunc\n", "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def adjust_encoding(*args, **kwargs):\n        name = namefunc(*args, **kwargs)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return adjust_encoding\n", "def tzname_in_python2(namefunc):\n    @wraps(namefunc)\n    def wrapper(self):\n        name = namefunc(self)\n        if name is not None:\n            name = name.encode('utf-8')\n\n        return name\n\n    return wrapper\n", "def tzname_in_python2(namefunc):\n    def inner(self, dt):\n        return namefunc(self, dt).encode('utf-8')\n    return inner\n", "def tzname_in_python2(namefunc):\n    if six.PY2:\n        @wraps(namefunc)\n        def adjust_encoding(*args, **kwargs):\n            name = namefunc(*args, **kwargs)\n            if name is not None:\n                name = name.encode()\n\n            return name\n\n        return adjust_encoding\n    else:\n        return namefunc\n", "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(*args, **kwargs):\n        name = namefunc(*args, **kwargs)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return wrapper\n", "def tzname_in_python2(namefunc):\n    @wraps(namefunc)\n    def adjust_encoding(*args, **kwargs):\n        name = namefunc(*args, **kwargs)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return adjust_encoding\n", "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self):\n        name = namefunc(self)\n        if name is not None:\n            # Force tzname to bytestring if it's not already\n            if not isinstance(name, six.string_types):\n                name = name.encode()\n            return name\n    return wrapper\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def tzname_in_python2(namefunc):\n    @wraps(namefunc)\n    def adjust_encoding(*args, **kwargs):\n        name = namefunc(*args, **kwargs)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return adjust_encoding\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self):\n        name = namefunc(self)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    if six.PY3:\n        @wraps(namefunc)\n        def adjust_encoding(*args, **kwargs):\n            name = namefunc(*args, **kwargs)\n            if name is not None:\n                name = name.encode()\n\n            return name\n\n        return adjust_encoding\n    else:\n        return namefunc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def adjust_encoding(*args, **kwargs):\n        name = namefunc(*args, **kwargs)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return adjust_encoding\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    @wraps(namefunc)\n    def wrapper(self):\n        name = namefunc(self)\n        if name is not None:\n            name = name.encode('utf-8')\n\n        return name\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    def inner(self, dt):\n        return namefunc(self, dt).encode('utf-8')\n    return inner\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    if six.PY2:\n        @wraps(namefunc)\n        def adjust_encoding(*args, **kwargs):\n            name = namefunc(*args, **kwargs)\n            if name is not None:\n                name = name.encode()\n\n            return name\n\n        return adjust_encoding\n    else:\n        return namefunc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(*args, **kwargs):\n        name = namefunc(*args, **kwargs)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    @wraps(namefunc)\n    def adjust_encoding(*args, **kwargs):\n        name = namefunc(*args, **kwargs)\n        if name is not None:\n            name = name.encode()\n\n        return name\n\n    return adjust_encoding\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self):\n        name = namefunc(self)\n        if name is not None:\n            # Force tzname to bytestring if it's not already\n            if not isinstance(name, six.string_types):\n                name = name.encode()\n            return name\n    return wrapper\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_versions", "docstring": "Get version information or return default if unable to do so.", "ques_id": "62b87d24d292efb640a55670", "code": ["def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "def get_versions():\n    try:\n        return get_versions()\n    except Exception:\n        return defaults.versions\n", "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "def get_versions():\n    try:\n        return get_versions()\n    except Exception:\n        return versions_from_parentdir(os.getcwd(), \"releases\")\n", "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "def get_versions():\n    try:\n        return get_versions()\n    except OSError:\n        return defaults.versions\n", "def get_versions():\n    try:\n        return get_versions()\n    except Exception:\n        return default_versions\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except Exception:\n        return defaults.versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except Exception:\n        return versions_from_parentdir(os.getcwd(), \"releases\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except IOError:\n        return defaults.versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except OSError:\n        return defaults.versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    try:\n        return get_versions()\n    except Exception:\n        return default_versions\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "render", "docstring": "Render the given version pieces into the requested style.", "ques_id": "62b87d24d292efb640a5566f", "code": ["def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre-branch\":\n        rendered = render_pep440_pre_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces"], "level": "file_runnable", "generate_results": [{"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre-branch\":\n        rendered = render_pep440_pre_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "plus_or_dot", "docstring": "Return a + if we don't already have one, else return a .", "ques_id": "62b87d24d292efb640a5566d", "code": ["def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n"], "level": "self_contained", "generate_results": [{"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "run_command", "docstring": "Call the given command(s).", "ques_id": "62b87d23d292efb640a5566b", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_config", "docstring": "Create, populate and return the VersioneerConfig() object.", "ques_id": "62b87d23d292efb640a55668", "code": ["def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"tidepy/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"tableauserverclient/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-post\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"tetris_generator/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"sorenata/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"seq2seq/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"seminario/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"fba/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"taurus_csv/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"trig_synth/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/nodedash/_version.py\"\n    cfg.verbose = False\n    return cfg\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"tidepy/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"tableauserverclient/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-post\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"tetris_generator/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"sorenata/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"seq2seq/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"seminario/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"fba/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"taurus_csv/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"trig_synth/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/nodedash/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "register_vcs_handler", "docstring": "Create decorator to mark a method as the handler of a VCS.", "ques_id": "62b87d23d292efb640a55667", "code": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n"], "level": "self_contained", "generate_results": [{"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "validate_hierarchy", "docstring": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid", "ques_id": "62b45e945108cfac7f210a4a", "code": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects, good_objects = 0, 0\n\n    # check root structure\n    self.check_root_structure()\n\n    # check for valid dispositions\n    for root, _, objects in self.check_root_structure():\n        for obj in objects:\n            if obj.is_valid():\n                num_objects += 1\n\n                if validate_objects:\n                    good_objects += 1\n\n                if show_warnings:\n                    self.log.debug('Root {} has valid disposition {}'.format(root, obj.fs.get_disposition()))\n\n    if num_objects == 0 and good_objects == 0:\n        self.dispositor.dispositor.dispose('valid storage root structure')\n        return 0, 0\n\n    if num_objects > 0:\n        self.dispositor.dispositor.dispose('{} objects found to be valid'.format(num_objects))\n        return good_objects, num_objects\n    else:\n        sel", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if validate_objects:\n        self._dispositor.validate_objects(self.open_root_fs)\n    self._dispositor.validate_hierarchy(self.open_root_fs, check_digests, show_warnings)\n    if self.log:\n        self.log.log(\"Validation of storage root hierarchy complete.\")\n    return self.num_objects, self.good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    self.log.debug(\"Validating storage root hierarchy\")\n    good_objects = 0\n    num_objects = 0\n    if validate_objects:\n        self.log.debug(\"Validating storage root hierarchy for objects\")\n        num_objects = len(self.open_root_fs.listdir('/'))\n        if num_objects > 0:\n            self.log.debug(\"Found %d objects to validate storage root hierarchy\" % num_objects)\n    if check_digests:\n        self.log.debug(\"Validating storage root hierarchy for digests\")\n        good_objects = self.check_digests_dir()\n    if show_errors:\n        self.log.debug(\"Showing storage root hierarchy validation failures\")\n        for error in self.dispositor():\n            self.log.error(error)\n    return num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\n    num_objects = len(self.object_paths)\n    good_objects = 0\n    if validate_objects:\n        for path in self.object_paths:\n            self._dispositor.dispositor(path)\n            try:\n                self.open_root_fs(path)\n                good_objects += 1\n            except FSError:\n                self.log.error('%s invalid', path)\n                self.log.error('%s recheck', path)\n                self.dispositor.dispositor(path)\n\n    if check_digests:\n        for path in self.object_paths:\n            self._dispositor.dispositor(path, check_fs=True)\n            try:\n                self.open_root_fs(path)\n                good_objects += 1\n            except FSError:\n                self.log.error('%s invalid', path)\n         ", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    self.log.info(\"Validating storage root hierarchy\")\n    if validate_objects:\n        self._dispositor.dispositor.check_storage_objects()\n    if check_digests:\n        self._dispositor.dispositor.check_digest_hierarchy()\n    self._dispositor.dispositor.check_hierarchy_traversal()\n    num_objects, good_objects = self._dispositor.validate_hierarchy()\n    if show_warnings:\n        self.log.info(\"Validation succeeded. {} objects checked, {} good objects found\".format(num_objects, good_objects))\n    return num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    # Try to open root FS\n    num_objects, good_objects = self.open_root_fs(create=False)\n\n    # Try to validate layout file\n    num_objects, good_objects = self.validate_layout_file()\n\n    # Try to validate extensions directory\n    num_objects, good_objects = self.validate_extensions_dir()\n\n    if validate_objects:\n        if check_digests:\n            self.check_digests()\n        if show_warnings:\n            self.log.warn(\"Validating object paths in %s - extensions are missing\", self.object_path)\n\n    # Return results\n    return num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects = len(self.object_paths)\n\n    if validate_objects:\n        self.log.info(\"Validating storage root structure\")\n        if self.open_root_fs():\n            self._dispositor.add_message(\"Validating storage root structure\", 1)\n            for path in self.object_paths:\n                self._dispositor.add_message(path)\n\n            self._dispositor.check_root_structure(self.open_root_fs())\n\n            if self._dispositor.has_errors():\n                self._dispositor.add_message(\"Validation failed\", 2)\n                if show_warnings:\n                    self.log.warning(self._dispositor.get_errors())\n\n    if check_digests:\n        self._dispositor.add_message(\"Validating storage root structure\", 1)\n        self._dispositor.check_root_structure(self.open_root_fs())\n\n    return (num_objects, good_objects)\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if not self.object_paths:\n        self.log.error(\"No object paths found\")\n        return None, None\n\n    num_objects = len(self.object_paths)\n    if validate_objects:\n        good_objects = len(self.check_root_structure(force_reload=True))\n    else:\n        good_objects = num_objects\n\n    if check_digests:\n        self.dispositor()\n        self._dispositor.check_digests(self.lax_digests, show_errors=show_errors)\n        good_objects = self._dispositor.num_good_objects\n\n    return num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    self.log.debug(\"Validating storage root hierarchy\")\n    object_paths = self.list()\n\n    num_objects = len(object_paths)\n\n    if validate_objects:\n        good_objects = self.check_objects_in_hierarchy(object_paths)\n\n    if check_digests:\n        self.check_digests(object_paths)\n\n    return num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if validate_objects:\n        self._dispositor.validate_objects(self.object_paths)\n    if check_digests:\n        self._dispositor.validate_digests(self.object_paths)\n    if not self.num_objects:\n        self.log.warning('No objects found.')\n        if show_warnings:\n            self.log.warning('This is likely a problem when using the local '\n                            'storage system. The storage root must '\n                             'be mounted on the local host to be valid.')\n        return 0, 0\n    return self.num_objects, self.good_objects\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects, good_objects = 0, 0\n\n    # check root structure\n    self.check_root_structure()\n\n    # check for valid dispositions\n    for root, _, objects in self.check_root_structure():\n        for obj in objects:\n            if obj.is_valid():\n                num_objects += 1\n\n                if validate_objects:\n                    good_objects += 1\n\n                if show_warnings:\n                    self.log.debug('Root {} has valid disposition {}'.format(root, obj.fs.get_disposition()))\n\n    if num_objects == 0 and good_objects == 0:\n        self.dispositor.dispositor.dispose('valid storage root structure')\n        return 0, 0\n\n    if num_objects > 0:\n        self.dispositor.dispositor.dispose('{} objects found to be valid'.format(num_objects))\n        return good_objects, num_objects\n    else:\n        sel", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if validate_objects:\n        self._dispositor.validate_objects(self.open_root_fs)\n    self._dispositor.validate_hierarchy(self.open_root_fs, check_digests, show_warnings)\n    if self.log:\n        self.log.log(\"Validation of storage root hierarchy complete.\")\n    return self.num_objects, self.good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    self.log.debug(\"Validating storage root hierarchy\")\n    good_objects = 0\n    num_objects = 0\n    if validate_objects:\n        self.log.debug(\"Validating storage root hierarchy for objects\")\n        num_objects = len(self.open_root_fs.listdir('/'))\n        if num_objects > 0:\n            self.log.debug(\"Found %d objects to validate storage root hierarchy\" % num_objects)\n    if check_digests:\n        self.log.debug(\"Validating storage root hierarchy for digests\")\n        good_objects = self.check_digests_dir()\n    if show_errors:\n        self.log.debug(\"Showing storage root hierarchy validation failures\")\n        for error in self.dispositor():\n            self.log.error(error)\n    return num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\n    num_objects = len(self.object_paths)\n    good_objects = 0\n    if validate_objects:\n        for path in self.object_paths:\n            self._dispositor.dispositor(path)\n            try:\n                self.open_root_fs(path)\n                good_objects += 1\n            except FSError:\n                self.log.error('%s invalid', path)\n                self.log.error('%s recheck', path)\n                self.dispositor.dispositor(path)\n\n    if check_digests:\n        for path in self.object_paths:\n            self._dispositor.dispositor(path, check_fs=True)\n            try:\n                self.open_root_fs(path)\n                good_objects += 1\n            except FSError:\n                self.log.error('%s invalid', path)\n         ", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    self.log.info(\"Validating storage root hierarchy\")\n    if validate_objects:\n        self._dispositor.dispositor.check_storage_objects()\n    if check_digests:\n        self._dispositor.dispositor.check_digest_hierarchy()\n    self._dispositor.dispositor.check_hierarchy_traversal()\n    num_objects, good_objects = self._dispositor.validate_hierarchy()\n    if show_warnings:\n        self.log.info(\"Validation succeeded. {} objects checked, {} good objects found\".format(num_objects, good_objects))\n    return num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    # Try to open root FS\n    num_objects, good_objects = self.open_root_fs(create=False)\n\n    # Try to validate layout file\n    num_objects, good_objects = self.validate_layout_file()\n\n    # Try to validate extensions directory\n    num_objects, good_objects = self.validate_extensions_dir()\n\n    if validate_objects:\n        if check_digests:\n            self.check_digests()\n        if show_warnings:\n            self.log.warn(\"Validating object paths in %s - extensions are missing\", self.object_path)\n\n    # Return results\n    return num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects = len(self.object_paths)\n\n    if validate_objects:\n        self.log.info(\"Validating storage root structure\")\n        if self.open_root_fs():\n            self._dispositor.add_message(\"Validating storage root structure\", 1)\n            for path in self.object_paths:\n                self._dispositor.add_message(path)\n\n            self._dispositor.check_root_structure(self.open_root_fs())\n\n            if self._dispositor.has_errors():\n                self._dispositor.add_message(\"Validation failed\", 2)\n                if show_warnings:\n                    self.log.warning(self._dispositor.get_errors())\n\n    if check_digests:\n        self._dispositor.add_message(\"Validating storage root structure\", 1)\n        self._dispositor.check_root_structure(self.open_root_fs())\n\n    return (num_objects, good_objects)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if not self.object_paths:\n        self.log.error(\"No object paths found\")\n        return None, None\n\n    num_objects = len(self.object_paths)\n    if validate_objects:\n        good_objects = len(self.check_root_structure(force_reload=True))\n    else:\n        good_objects = num_objects\n\n    if check_digests:\n        self.dispositor()\n        self._dispositor.check_digests(self.lax_digests, show_errors=show_errors)\n        good_objects = self._dispositor.num_good_objects\n\n    return num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    self.log.debug(\"Validating storage root hierarchy\")\n    object_paths = self.list()\n\n    num_objects = len(object_paths)\n\n    if validate_objects:\n        good_objects = self.check_objects_in_hierarchy(object_paths)\n\n    if check_digests:\n        self.check_digests(object_paths)\n\n    return num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if validate_objects:\n        self._dispositor.validate_objects(self.object_paths)\n    if check_digests:\n        self._dispositor.validate_digests(self.object_paths)\n    if not self.num_objects:\n        self.log.warning('No objects found.')\n        if show_warnings:\n            self.log.warning('This is likely a problem when using the local '\n                            'storage system. The storage root must '\n                             'be mounted on the local host to be valid.')\n        return 0, 0\n    return self.num_objects, self.good_objects\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "initialize", "docstring": "Create and initialize a new OCFL storage root.", "ques_id": "62b45e515108cfac7f210a3c", "code": ["def initialize(self):\n    self.log.debug(\"initialize\")\n    self.spec_file = None\n    self.log.debug(\"finalize\")\n    self.open_root_fs(create=True)\n", "def initialize(self):\n    self.log = logging.getLogger('ocfl')\n\n    # initialize the storage root\n    self.root_fs = self.open_root_fs(create=False)\n\n    # keep track of the number of objects in the storage root\n    self.num_objects = 0\n\n    # keep track of the number of files that have been created\n    self.num_files = 0\n\n    # keep track of the number of extensions that have been registered\n    self.num_extensions = 0\n\n    # keep track of the number of valid extensions\n    self.num_valid_extensions = 0\n\n    # keep track of the number of extensions that have been loaded\n    self.num_loaded_extensions = 0\n\n    # keep track of the number of object paths that have been loaded\n    self.num_object_paths = 0\n\n    # keep track of the number of extensions that have been loaded\n    self.num_loaded_extensions = 0\n\n    # keep track of the number of layout files that have been loaded\n    self.num_layout_files = 0\n\n    # keep track of the number of valid layout files\n   ", "def initialize(self):\n    self.log.info('initializing')\n\n    self.root_fs = OCFLFSession(self.root, self.log)\n    self.num_objects = self.root_fs.num_objects\n\n    self.spec_file = None\n    self.dispositor = None\n    self.log.debug('initialized with %d objects', self.num_objects)\n", "def initialize(self):\n    self.log.debug(\"creating OCFL storage root\")\n    self.open_root_fs(create=True)\n", "def initialize(self):\n    self.log.info(\"creating a new OCFL storage root\")\n    self.open_root_fs = self.open_root_fs(create=True)\n", "def initialize(self):\n    self.log.debug(\"Initializing OCFL storage root at '%s'\", self.root)\n    self.open_root_fs(create=True)\n", "def initialize(self):\n    self.log.debug(\"initializing OCFL storage root\")\n    self.open_root_fs(create=True)\n", "def initialize(self):\n    self.log.info(\"Initializing OCFL storage root '%s'.\", self.root)\n    self.open_root_fs(create=True)\n", "def initialize(self):\n    self.log = logging.getLogger('ocfl.storage')\n    self.log.debug('initializing')\n    self.root_fs = None\n    self.num_objects = 0\n    self.num_traversal_errors = 0\n    self.object_paths = []\n    self.object_paths_index = {}\n    self.open_root_fs(create=True)\n    self.dispositor()\n", "def initialize(self):\n    self.log.info(\"creating new OCFL storage root\")\n    self.open_root_fs(create=True)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def initialize(self):\n    self.log.debug(\"initialize\")\n    self.spec_file = None\n    self.log.debug(\"finalize\")\n    self.open_root_fs(create=True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self.log = logging.getLogger('ocfl')\n\n    # initialize the storage root\n    self.root_fs = self.open_root_fs(create=False)\n\n    # keep track of the number of objects in the storage root\n    self.num_objects = 0\n\n    # keep track of the number of files that have been created\n    self.num_files = 0\n\n    # keep track of the number of extensions that have been registered\n    self.num_extensions = 0\n\n    # keep track of the number of valid extensions\n    self.num_valid_extensions = 0\n\n    # keep track of the number of extensions that have been loaded\n    self.num_loaded_extensions = 0\n\n    # keep track of the number of object paths that have been loaded\n    self.num_object_paths = 0\n\n    # keep track of the number of extensions that have been loaded\n    self.num_loaded_extensions = 0\n\n    # keep track of the number of layout files that have been loaded\n    self.num_layout_files = 0\n\n    # keep track of the number of valid layout files\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self.log.info('initializing')\n\n    self.root_fs = OCFLFSession(self.root, self.log)\n    self.num_objects = self.root_fs.num_objects\n\n    self.spec_file = None\n    self.dispositor = None\n    self.log.debug('initialized with %d objects', self.num_objects)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self.log.debug(\"creating OCFL storage root\")\n    self.open_root_fs(create=True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self.log.info(\"creating a new OCFL storage root\")\n    self.open_root_fs = self.open_root_fs(create=True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self.log.debug(\"Initializing OCFL storage root at '%s'\", self.root)\n    self.open_root_fs(create=True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self.log.debug(\"initializing OCFL storage root\")\n    self.open_root_fs(create=True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self.log.info(\"Initializing OCFL storage root '%s'.\", self.root)\n    self.open_root_fs(create=True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self.log = logging.getLogger('ocfl.storage')\n    self.log.debug('initializing')\n    self.root_fs = None\n    self.num_objects = 0\n    self.num_traversal_errors = 0\n    self.object_paths = []\n    self.object_paths_index = {}\n    self.open_root_fs(create=True)\n    self.dispositor()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self.log.info(\"creating new OCFL storage root\")\n    self.open_root_fs(create=True)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "next_version", "docstring": "Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.", "ques_id": "62b45e2eb89c9fd354170232", "code": ["def next_version(version):\n    if version[0] == \"0\":\n        return version\n    else:\n        return \"0.\" + version\n", "def next_version(version):\n    if version == \"\":\n        return \"latest\"\n    prefix, suffix = find_path_type(version)\n    if prefix == \"\":\n        return version\n    return \"%s.%s\" % (prefix, suffix)\n", "def next_version(version):\n    if version[:2] == \"0\":\n        return \"0\" + version\n    else:\n        return version\n", "def next_version(version):\n    if version == '':\n        return None\n    else:\n        m = re.match(r'^(\\d+)$', version)\n        if not m:\n            sys.stderr.write(\"%s: invalid next version identifier: '%s'\\n\" %\n                             (sys.argv[0], version))\n            sys.exit(1)\n        else:\n            return int(m.group(1))\n", "def next_version(version):\n    if version == '':\n        return None\n    if re.match(r'^[0-9]*$', version):\n        return version\n    m = re.search(r'^(\\d+)$', version)\n    if m is None:\n        raise ValueError('Invalid version %s' % version)\n    version = int(m.group(1)) + 1\n    return str(version)\n", "def next_version(version):\n    m = re.match(r'^v(\\d+)$', version)\n    if not m:\n        raise ValueError('Unsupported version: {}'.format(version))\n    number = int(m.group(1)) + 1\n    return 'v{}'.format(str(number))\n", "def next_version(version):\n    \"Must deal with both zero-prefixed and non-zero prefixed versions.\"\n    if version[:2] == '0':\n        return version\n    else:\n        return '0' + version\n", "def next_version(version):\n    prefix, sep, idx = version.partition(':')\n    if prefix in NORMALIZATIONS:\n        idx = int(idx) + 1\n    else:\n        idx = 1\n    return '%s:%d' % (prefix, idx)\n", "def next_version(version):\n    if version[0] == '0':\n        raise ValueError('Must deal with zero-prefixed versions.')\n    return version + '.1'\n", "def next_version(version):\n    if version == '':\n        return None\n    if version[0] == '0':\n        version = version[1:]\n    m = re.search('([0-9]+\\.[0-9]+)$', version)\n    if not m:\n        sys.stderr.write('Warning: version %s does not match pattern %s\\n' %\n                         (version, NORMALIZATIONS[version]))\n        return None\n    return int(m.group(1))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def next_version(version):\n    if version[0] == \"0\":\n        return version\n    else:\n        return \"0.\" + version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == \"\":\n        return \"latest\"\n    prefix, suffix = find_path_type(version)\n    if prefix == \"\":\n        return version\n    return \"%s.%s\" % (prefix, suffix)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version[:2] == \"0\":\n        return \"0\" + version\n    else:\n        return version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == '':\n        return None\n    else:\n        m = re.match(r'^(\\d+)$', version)\n        if not m:\n            sys.stderr.write(\"%s: invalid next version identifier: '%s'\\n\" %\n                             (sys.argv[0], version))\n            sys.exit(1)\n        else:\n            return int(m.group(1))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == '':\n        return None\n    if re.match(r'^[0-9]*$', version):\n        return version\n    m = re.search(r'^(\\d+)$', version)\n    if m is None:\n        raise ValueError('Invalid version %s' % version)\n    version = int(m.group(1)) + 1\n    return str(version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    m = re.match(r'^v(\\d+)$', version)\n    if not m:\n        raise ValueError('Unsupported version: {}'.format(version))\n    number = int(m.group(1)) + 1\n    return 'v{}'.format(str(number))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def next_version(version):\n    \"Must deal with both zero-prefixed and non-zero prefixed versions.\"\n    if version[:2] == '0':\n        return version\n    else:\n        return '0' + version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    prefix, sep, idx = version.partition(':')\n    if prefix in NORMALIZATIONS:\n        idx = int(idx) + 1\n    else:\n        idx = 1\n    return '%s:%d' % (prefix, idx)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version[0] == '0':\n        raise ValueError('Must deal with zero-prefixed versions.')\n    return version + '.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == '':\n        return None\n    if version[0] == '0':\n        version = version[1:]\n    m = re.search('([0-9]+\\.[0-9]+)$', version)\n    if not m:\n        sys.stderr.write('Warning: version %s does not match pattern %s\\n' %\n                         (version, NORMALIZATIONS[version]))\n        return None\n    return int(m.group(1))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate_version_inventories", "docstring": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).", "ques_id": "62b45e23e0d4551b0392c90a", "code": ["def validate_version_inventories(self, version_dirs):\n    if version_dirs is None:\n        version_dirs = {}\n\n    for version_dir in version_dirs:\n        self.check_version_inventories(version_dir)\n", "def validate_version_inventories(self, version_dirs):\n    for version_dir in version_dirs:\n        self.__validate_version_inventories_dir(version_dir)\n", "def validate_version_inventories(self, version_dirs):\n\n    for version_dir in version_dirs:\n        version_path = os.path.join(version_dir, self.version_fs.name)\n\n        # TODO: Check if the version is a digest file\n        # TODO: Check if the digest file is already in the inventory\n        # TODO: Remove this line after implementation add check_digests\n\n        self.log.info(\"checking inventory for %s\", version_path)\n\n        for f in self.spec_version:\n            self.status_str(f)\n            self.check_digests(f, version_path, f.digest_algorithm)\n", "def validate_version_inventories(self, version_dirs):\n    version_inventories = self.obj_fs.get_version_inventories(version_dirs)\n    self.log.info(\"Found %d inventories\", len(version_inventories))\n\n    if not version_inventories:\n        raise Failure(\"Inventory missing from %s\" % version_dirs)\n\n    for inv_path in version_inventories:\n        self.read_inventory_digest(inv_path)\n        self.check_digests(inv_path)\n", "def validate_version_inventories(self, version_dirs):\n    for version_dir in version_dirs:\n        version_path = os.path.join(self.content_directory, version_dir)\n        for dirpath, _, filenames in os.walk(version_path):\n            for filename in filenames:\n                self.check_digests(os.path.join(dirpath, filename),\n                                    self.spec_version)\n", "def validate_version_inventories(self, version_dirs):\n    if len(version_dirs) > 1:\n        self.log.error(\"More than one version dir specified in the inventory. Using first version\")\n    if len(version_dirs) == 1:\n        version_dir = version_dirs.keys()[0]\n        self.check_digests(version_dirs[version_dir], version_dirs)\n", "def validate_version_inventories(self, version_dirs):\n    for version_dir in version_dirs:\n        self.check_version_inventories(version_dir)\n", "def validate_version_inventories(self, version_dirs):\n    self.spec_version = self.content_directory.get_version()\n    self.log.info(\"Version inventory digest validated using spec version: {}\".format(self.spec_version))\n", "def validate_version_inventories(self, version_dirs):\n    self.check_digests(version_dirs)\n", "def validate_version_inventories(self, version_dirs):\n    self.log.debug(\"validate_version_inventories(%s)\", version_dirs)\n    validate_paths = []\n    for version_dir in version_dirs:\n        validate_path = self.validate_inventory(version_dir,\n                                                where='version_inventories')\n        validate_paths.append(validate_path)\n    return validate_paths\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_version_inventories(self, version_dirs):\n    if version_dirs is None:\n        version_dirs = {}\n\n    for version_dir in version_dirs:\n        self.check_version_inventories(version_dir)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    for version_dir in version_dirs:\n        self.__validate_version_inventories_dir(version_dir)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\n    for version_dir in version_dirs:\n        version_path = os.path.join(version_dir, self.version_fs.name)\n\n        # TODO: Check if the version is a digest file\n        # TODO: Check if the digest file is already in the inventory\n        # TODO: Remove this line after implementation add check_digests\n\n        self.log.info(\"checking inventory for %s\", version_path)\n\n        for f in self.spec_version:\n            self.status_str(f)\n            self.check_digests(f, version_path, f.digest_algorithm)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    version_inventories = self.obj_fs.get_version_inventories(version_dirs)\n    self.log.info(\"Found %d inventories\", len(version_inventories))\n\n    if not version_inventories:\n        raise Failure(\"Inventory missing from %s\" % version_dirs)\n\n    for inv_path in version_inventories:\n        self.read_inventory_digest(inv_path)\n        self.check_digests(inv_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    for version_dir in version_dirs:\n        version_path = os.path.join(self.content_directory, version_dir)\n        for dirpath, _, filenames in os.walk(version_path):\n            for filename in filenames:\n                self.check_digests(os.path.join(dirpath, filename),\n                                    self.spec_version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    if len(version_dirs) > 1:\n        self.log.error(\"More than one version dir specified in the inventory. Using first version\")\n    if len(version_dirs) == 1:\n        version_dir = version_dirs.keys()[0]\n        self.check_digests(version_dirs[version_dir], version_dirs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    for version_dir in version_dirs:\n        self.check_version_inventories(version_dir)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    self.spec_version = self.content_directory.get_version()\n    self.log.info(\"Version inventory digest validated using spec version: {}\".format(self.spec_version))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    self.check_digests(version_dirs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    self.log.debug(\"validate_version_inventories(%s)\", version_dirs)\n    validate_paths = []\n    for version_dir in version_dirs:\n        validate_path = self.validate_inventory(version_dir,\n                                                where='version_inventories')\n        validate_paths.append(validate_path)\n    return validate_paths\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "find_path_type", "docstring": "Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.", "ques_id": "62b45e21e0d4551b0392c8ed", "code": ["def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'root'\n    else:\n        if re.search(r'^0(?:[A-Za-z0-9\\.]+)$', path):\n            return 'object'\n        else:\n            return 'file'\n", "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'file'\n    elif os.path.islink(path):\n        return 'object'\n    else:\n        # Check for OCFL files\n        try:\n            f = open(path, 'rb')\n            f.seek(16)\n            f.seek(17)\n            f.seek(32)\n            f.seek(64)\n            f.seek(128)\n            return 'file'\n        except:\n            return 'object'\n", "def find_path_type(path):\n    # TODO: return 'root' or 'object' or 'file'?\n    #       A better way would be to look for files in the OCFL Data Directory\n    #       that contain \"0=*\"\n    if fs.isfile(path):\n        return 'file'\n    elif fs.isdir(path):\n        return 'object'\n    elif fs.islink(path):\n        return 'file'\n    else:\n        return 'root'\n", "def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'object'\n    elif re.match('^0*', path):\n        return 'root'\n    else:\n        return ''\n", "def find_path_type(path):\n\n    if os.path.isfile(path):\n        return \"file\"\n    elif os.path.isdir(path):\n        return \"root\"\n    else:\n        # should not be possible\n        sys.stderr.write(\"WARNING: unable to determine OCFL file type for %s\\n\" % path)\n        return \"file\"\n", "def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'root'\n    else:\n        path = fs.abspath(path)\n        if path.startswith('0'):\n            return 'object'\n        else:\n            # find \"0=*\" files to determine the directory type\n            return 'file'\n", "def find_path_type(path):\n    if path is None:\n        return 'root'\n    elif os.path.isdir(path):\n        return 'object'\n    elif os.path.isfile(path):\n        return 'file'\n    else:\n        # TODO: return \"N/A\"?\n        return 'N/A'\n", "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'file'\n    elif os.path.isfile(path):\n        return 'object'\n    elif fs.is_fs_type(path):\n        return 'root'\n    else:\n        # Maybe a directory in another filesystem\n        for f in os.listdir(path):\n            fp = os.path.join(path, f)\n            if os.path.isdir(fp):\n                return 'dir'\n        raise NamasteError('No directory type found at {}'.format(path))\n", "def find_path_type(path):\n    fs = fsspec.filesystem(path)\n    if fs.isfilesystem:\n        return 'file'\n    elif fs.isdir:\n        return 'root'\n    elif fs.ismount:\n        return 'object'\n    else:\n        return ''\n", "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'object'\n    elif os.path.isdir(os.path.dirname(path)):\n        return 'file'\n    else:\n        # Maybe it's a file like an OCFL File.\n        return 'file'\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'root'\n    else:\n        if re.search(r'^0(?:[A-Za-z0-9\\.]+)$', path):\n            return 'object'\n        else:\n            return 'file'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'file'\n    elif os.path.islink(path):\n        return 'object'\n    else:\n        # Check for OCFL files\n        try:\n            f = open(path, 'rb')\n            f.seek(16)\n            f.seek(17)\n            f.seek(32)\n            f.seek(64)\n            f.seek(128)\n            return 'file'\n        except:\n            return 'object'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    # TODO: return 'root' or 'object' or 'file'?\n    #       A better way would be to look for files in the OCFL Data Directory\n    #       that contain \"0=*\"\n    if fs.isfile(path):\n        return 'file'\n    elif fs.isdir(path):\n        return 'object'\n    elif fs.islink(path):\n        return 'file'\n    else:\n        return 'root'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'object'\n    elif re.match('^0*', path):\n        return 'root'\n    else:\n        return ''\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\n    if os.path.isfile(path):\n        return \"file\"\n    elif os.path.isdir(path):\n        return \"root\"\n    else:\n        # should not be possible\n        sys.stderr.write(\"WARNING: unable to determine OCFL file type for %s\\n\" % path)\n        return \"file\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'root'\n    else:\n        path = fs.abspath(path)\n        if path.startswith('0'):\n            return 'object'\n        else:\n            # find \"0=*\" files to determine the directory type\n            return 'file'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if path is None:\n        return 'root'\n    elif os.path.isdir(path):\n        return 'object'\n    elif os.path.isfile(path):\n        return 'file'\n    else:\n        # TODO: return \"N/A\"?\n        return 'N/A'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'file'\n    elif os.path.isfile(path):\n        return 'object'\n    elif fs.is_fs_type(path):\n        return 'root'\n    else:\n        # Maybe a directory in another filesystem\n        for f in os.listdir(path):\n            fp = os.path.join(path, f)\n            if os.path.isdir(fp):\n                return 'dir'\n        raise NamasteError('No directory type found at {}'.format(path))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    fs = fsspec.filesystem(path)\n    if fs.isfilesystem:\n        return 'file'\n    elif fs.isdir:\n        return 'root'\n    elif fs.ismount:\n        return 'object'\n    else:\n        return ''\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'object'\n    elif os.path.isdir(os.path.dirname(path)):\n        return 'file'\n    else:\n        # Maybe it's a file like an OCFL File.\n        return 'file'\n", "is_pass": false, "return_code": 1}]}
{"file_path": "auto_nag/bzcleaner.py", "project": "mozilla/relman-auto-nag", "name": "amend_bzparams", "docstring": "Amend the Bugzilla params", "ques_id": "62b45b396decaeff903e1001", "code": ["def amend_bzparams(self, params, bug_ids):\n    if not isinstance(params, list):\n        params = [params]\n\n    amended_bugs = []\n    for param in params:\n        amended_bugs.extend(self.amend_bzparams_once(param, bug_ids))\n\n    return amended_bugs\n", "def amend_bzparams(self, params, bug_ids):\n    if 'amend_bz_params' in params:\n        amended_bugs = self.get_bugs(params['amend_bz_params'])\n        for bug in amended_bugs:\n            bug_id = str(bug['id'])\n            if bug_id not in bug_ids:\n                bug_ids.append(bug_id)\n    return True\n", "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        bug_ids = self.get_bugs()\n\n    if self.amend_bzparams.lower() == \"n\":\n        return False\n    elif self.amend_bzparams.lower() == \"y\":\n        return True\n    else:\n        return False\n", "def amend_bzparams(self, params, bug_ids):\n    if not isinstance(params, dict):\n        raise TypeError(\"amend_bzparams requires params to be a dict\")\n\n    for key, value in params.items():\n        if key in ['amend_to', 'comment', 'amend_status']:\n            if value == 'no':\n                params[key] = False\n            elif value == 'yes':\n                params[key] = True\n\n    if 'amend_to' in params and not params['amend_to']:\n        params.pop('amend_to')\n    if 'comment' in params and not params['comment']:\n        params.pop('comment')\n    if 'amend_status' in params and not params['amend_status']:\n        params.pop('amend_status')\n\n    for bug_id in bug_ids:\n        params['bugzilla_id'] = bug_id\n\n    return params\n", "def amend_bzparams(self, params, bug_ids):\n    if not isinstance(params, dict):\n        raise TypeError(\"amend_bzparams requires a parameter dictionary\")\n\n    if params.get('amend'):\n        for bz in bug_ids:\n            params['amend_status'] = 'MAYBE_CHANGED'\n\n    return params\n", "def amend_bzparams(self, params, bug_ids):\n    param_list = []\n    for key in params:\n        if key == 'ni':\n            param_list.append('<bz:ni>%s</bz:ni>' % params[key])\n        else:\n            param_list.append('<bz:param>%s</bz:param>' % params[key])\n    return param_list\n", "def amend_bzparams(self, params, bug_ids):\n    if 'amend_bz_params' in params:\n        amended_bug_ids = self.get_bug_ids_from_bz_params(params['amend_bz_params'])\n        bug_ids = bug_ids | amended_bug_ids\n\n    return bug_ids\n", "def amend_bzparams(self, params, bug_ids):\n    if 'amend_bz_product_ids' in params:\n        if isinstance(params['amend_bz_product_ids'], list):\n            params['amend_bz_product_ids'] = ','.join(str(i) for i in params['amend_bz_product_ids'])\n    return params\n", "def amend_bzparams(self, params, bug_ids):\n    if not self.amend_bzparams.should_run():\n        return\n\n    if self.dryrun:\n        return\n\n    if self.verbose:\n        print(\"AMENDING BZ PARAMS (NOT NEEDINFO)\")\n\n    params = self.get_bzparams(params, bug_ids)\n\n    for key in params:\n        if key not in self.ignore_meta:\n            self.cache[key] = params[key]\n", "def amend_bzparams(self, params, bug_ids):\n    if not isinstance(params, dict) or not isinstance(bug_ids, list):\n        raise TypeError(\"params and/or bug_ids must be type dict and/or list\")\n\n    if self.amend_bzparams.lower() not in ['true', 'false']:\n        raise ValueError(\"amend_bzparams must be either 'true' or 'false'\")\n\n    params = dict((k.lower(), v) for k, v in params.items())\n\n    for bug in bug_ids:\n        if not isinstance(bug, int):\n            raise TypeError(\"bug_ids must be ints\")\n        params['bug_id'] = bug\n\n    return params\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not isinstance(params, list):\n        params = [params]\n\n    amended_bugs = []\n    for param in params:\n        amended_bugs.extend(self.amend_bzparams_once(param, bug_ids))\n\n    return amended_bugs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if 'amend_bz_params' in params:\n        amended_bugs = self.get_bugs(params['amend_bz_params'])\n        for bug in amended_bugs:\n            bug_id = str(bug['id'])\n            if bug_id not in bug_ids:\n                bug_ids.append(bug_id)\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        bug_ids = self.get_bugs()\n\n    if self.amend_bzparams.lower() == \"n\":\n        return False\n    elif self.amend_bzparams.lower() == \"y\":\n        return True\n    else:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not isinstance(params, dict):\n        raise TypeError(\"amend_bzparams requires params to be a dict\")\n\n    for key, value in params.items():\n        if key in ['amend_to', 'comment', 'amend_status']:\n            if value == 'no':\n                params[key] = False\n            elif value == 'yes':\n                params[key] = True\n\n    if 'amend_to' in params and not params['amend_to']:\n        params.pop('amend_to')\n    if 'comment' in params and not params['comment']:\n        params.pop('comment')\n    if 'amend_status' in params and not params['amend_status']:\n        params.pop('amend_status')\n\n    for bug_id in bug_ids:\n        params['bugzilla_id'] = bug_id\n\n    return params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not isinstance(params, dict):\n        raise TypeError(\"amend_bzparams requires a parameter dictionary\")\n\n    if params.get('amend'):\n        for bz in bug_ids:\n            params['amend_status'] = 'MAYBE_CHANGED'\n\n    return params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    param_list = []\n    for key in params:\n        if key == 'ni':\n            param_list.append('<bz:ni>%s</bz:ni>' % params[key])\n        else:\n            param_list.append('<bz:param>%s</bz:param>' % params[key])\n    return param_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if 'amend_bz_params' in params:\n        amended_bug_ids = self.get_bug_ids_from_bz_params(params['amend_bz_params'])\n        bug_ids = bug_ids | amended_bug_ids\n\n    return bug_ids\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if 'amend_bz_product_ids' in params:\n        if isinstance(params['amend_bz_product_ids'], list):\n            params['amend_bz_product_ids'] = ','.join(str(i) for i in params['amend_bz_product_ids'])\n    return params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not self.amend_bzparams.should_run():\n        return\n\n    if self.dryrun:\n        return\n\n    if self.verbose:\n        print(\"AMENDING BZ PARAMS (NOT NEEDINFO)\")\n\n    params = self.get_bzparams(params, bug_ids)\n\n    for key in params:\n        if key not in self.ignore_meta:\n            self.cache[key] = params[key]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not isinstance(params, dict) or not isinstance(bug_ids, list):\n        raise TypeError(\"params and/or bug_ids must be type dict and/or list\")\n\n    if self.amend_bzparams.lower() not in ['true', 'false']:\n        raise ValueError(\"amend_bzparams must be either 'true' or 'false'\")\n\n    params = dict((k.lower(), v) for k, v in params.items())\n\n    for bug in bug_ids:\n        if not isinstance(bug, int):\n            raise TypeError(\"bug_ids must be ints\")\n        params['bug_id'] = bug\n\n    return params\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/atticmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "ques_id": "62b4567ed7d32e5b55cc83d9", "code": ["def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, list):\n            result.append(deep_merge_nodes(node))\n        else:\n            key = (node.tag, node.value)\n            if key in DELETED_NODE:\n                continue\n            if key in node.value:\n                node.value = [DELETED_NODE[key]]\n            else:\n                node.value = []\n            node.value.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n    merged = OrderedDict()\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in merged:\n                merged[node.value].append(node)\n            else:\n                merged[node.value] = [node]\n        else:\n            if node not in merged:\n                merged[node] = []\n            merged[node] += node\n    return merged\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, list):\n            result.append(deep_merge_nodes(node))\n        elif isinstance(node, dict):\n            merged_node = node.copy()\n            merged_node.update(node)\n            result.append(merged_node)\n        else:\n            result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n    if not nodes:\n        return []\n\n    logger.info('Deep merging nodes: %s', nodes)\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in DELETED_NODE:\n                logger.debug('Replacing node value %s with DELETED_NODE value %s', node, node.value)\n                node.value = DELETED_NODE[node.value]\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n\n    return result\n", "def deep_merge_nodes(nodes):\n    logger.debug('deep_merge_nodes(%s)', nodes)\n    result = []\n    for node in nodes:\n        if isinstance(node[1], MappingNode):\n            result.append((node[0], deep_merge_nodes(node[1])))\n        else:\n            result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n    logger.info('Deep merging nodes')\n    result = []\n    for node in nodes:\n        if isinstance(node[1], MappingNode):\n            if node[1].value in DELETED_NODE:\n                logger.warning('Deleting duplicate key: %s', node[1].value)\n            else:\n                result.append(node)\n                result.extend(deep_merge_nodes(node[1].value))\n        else:\n            result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n    return [(\n        _deep_merge(node[0], node[1]),\n        [node[2] for node in nodes if isinstance(node[2], MappingNode)],\n    ) for node in nodes]\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append((node.key, deep_merge_nodes(node.value)))\n        else:\n            result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n    logger.debug('deep_merge_nodes(%s)', nodes)\n    if not isinstance(nodes, list):\n        raise ValueError('nodes must be a list')\n\n    result = []\n    for node in nodes:\n        if isinstance(node, list):\n            result.append(deep_merge_nodes(node))\n        else:\n            result.append(node)\n    logger.debug('result: %s', result)\n    return result\n", "def deep_merge_nodes(nodes):\n    duplicates = []\n    for node in nodes:\n        if node[0] in DELETED_NODE:\n            if node[1] in DELETED_NODE:\n                logger.warning(\"Removing duplicate node '%s'\", node[0])\n                duplicates.append(node)\n        else:\n            node[1] = deep_merge_nodes(node[1].children)\n    for node in duplicates:\n        node[1].tag = node[0].tag\n    return duplicates\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, list):\n            result.append(deep_merge_nodes(node))\n        else:\n            key = (node.tag, node.value)\n            if key in DELETED_NODE:\n                continue\n            if key in node.value:\n                node.value = [DELETED_NODE[key]]\n            else:\n                node.value = []\n            node.value.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    merged = OrderedDict()\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in merged:\n                merged[node.value].append(node)\n            else:\n                merged[node.value] = [node]\n        else:\n            if node not in merged:\n                merged[node] = []\n            merged[node] += node\n    return merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, list):\n            result.append(deep_merge_nodes(node))\n        elif isinstance(node, dict):\n            merged_node = node.copy()\n            merged_node.update(node)\n            result.append(merged_node)\n        else:\n            result.append(node)\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n    if not nodes:\n        return []\n\n    logger.info('Deep merging nodes: %s', nodes)\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in DELETED_NODE:\n                logger.debug('Replacing node value %s with DELETED_NODE value %s', node, node.value)\n                node.value = DELETED_NODE[node.value]\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    logger.debug('deep_merge_nodes(%s)', nodes)\n    result = []\n    for node in nodes:\n        if isinstance(node[1], MappingNode):\n            result.append((node[0], deep_merge_nodes(node[1])))\n        else:\n            result.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    logger.info('Deep merging nodes')\n    result = []\n    for node in nodes:\n        if isinstance(node[1], MappingNode):\n            if node[1].value in DELETED_NODE:\n                logger.warning('Deleting duplicate key: %s', node[1].value)\n            else:\n                result.append(node)\n                result.extend(deep_merge_nodes(node[1].value))\n        else:\n            result.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    return [(\n        _deep_merge(node[0], node[1]),\n        [node[2] for node in nodes if isinstance(node[2], MappingNode)],\n    ) for node in nodes]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append((node.key, deep_merge_nodes(node.value)))\n        else:\n            result.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    logger.debug('deep_merge_nodes(%s)', nodes)\n    if not isinstance(nodes, list):\n        raise ValueError('nodes must be a list')\n\n    result = []\n    for node in nodes:\n        if isinstance(node, list):\n            result.append(deep_merge_nodes(node))\n        else:\n            result.append(node)\n    logger.debug('result: %s', result)\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n    duplicates = []\n    for node in nodes:\n        if node[0] in DELETED_NODE:\n            if node[1] in DELETED_NODE:\n                logger.warning(\"Removing duplicate node '%s'\", node[0])\n                duplicates.append(node)\n        else:\n            node[1] = deep_merge_nodes(node[1].children)\n    for node in duplicates:\n        node[1].tag = node[0].tag\n    return duplicates\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "ques_id": "62b4567ad7d32e5b55cc83af", "code": ["def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--config',\n        help='Path to a file with a default destination config file.',\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n    )\n    parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Run the script without writing out any changes.',\n    )\n    return parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n    return argparse.ArgumentParser(\n        description='Script to remove existing directories or files from the source directory.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Parse the content of the previous commit log '\n                    'and return the resulting arguments.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        '--config',\n        help='Path to the previously saved config file.',\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        required=False,\n        metavar='FILE',\n        type=argparse.FileType('r')\n    )\n    return parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Generate a new data source with a new project.'\n    )\n    parser.add_argument(\n        '--config', '-c',\n        type=argparse.FileType('r'),\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        help='The source configuration file.'\n    )\n    return parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Provide default destination configuration file.\")\n    parser.add_argument(\n        \"-d\", \"--destination-config-file\",\n        help=\"Configuration file to use.\")\n    parser.add_argument(\n        \"-v\", \"--version\", action=\"version\",\n        version=\"%(prog)s {}\".format(__version__))\n\n    return parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config-file')\n    return parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', '-c', default=DEFAULT_DESTINATION_CONFIG_FILENAME, required=False,\n                        help='Configuration file to use.')\n    parser.add_argument('--verbose', '-v', action='store_true', help='Be more verbose.')\n    parser.add_argument('--dry-run', '-n', action='store_true', help='Dry run.')\n    parser.add_argument('--overwrite', '-y', action='store_true', help='Overwrite existing files.')\n    parser.add_argument('--dest-dir', '-d', default=None, help='Destination directory.')\n    parser.add_argument('--skip', '-s', action='append', help='Skip renaming step.')\n    parser.add_argument('--no-overwrite', '-n', action='append', help='Skip renaming step.')\n    parser.add_argument('--rename-existing', '-r', action='append', help='Rename existing files.')\n    parser.add_argument('-", "def parse_arguments(*arguments):\n    return create_argument_parser(*arguments).parse_args()\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(description='Project Borgmatic deployment.')\n    parser.add_argument('--config', '-c', dest='config_file', default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n                        help='Configuration file to use (default: %(default)s)')\n    parser.add_argument('--src', '-s', dest='source', default=None,\n                        help='Source directory for project deployment (default: %(default)s)')\n    parser.add_argument('--dest', '-d', dest='dest', default=None,\n                        help='Destination directory for project deployment (default: %(default)s)')\n    parser.add_argument('--skip', '-s', dest='skip', default=False,\n                        action='store_true',\n                        help='Skip over git repositories (default: %(default)s)')\n    parser.add_argument('--update', '", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Convert CSV files to MongoDB database.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\n        '--destination-config',\n        required=False,\n        help='Path to destination configuration file.')\n\n    parser.add_argument(\n        '--source-config',\n        required=False,\n        help='Path to source configuration file.')\n\n    parser.add_argument(\n        '--source-collection',\n        required=False,\n        help='Collection name for the source database.')\n\n    parser.add_argument(\n        '--destination-collection',\n        required=False,\n        help='Collection name for the destination database.')\n\n    parser.add_argument(\n        '--source-username',\n        required=False,\n        help='Username for the source database.')\n\n    parser.add_argument(\n    "], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--config',\n        help='Path to a file with a default destination config file.',\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n    )\n    parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Run the script without writing out any changes.',\n    )\n    return parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    return argparse.ArgumentParser(\n        description='Script to remove existing directories or files from the source directory.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Parse the content of the previous commit log '\n                    'and return the resulting arguments.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        '--config',\n        help='Path to the previously saved config file.',\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        required=False,\n        metavar='FILE',\n        type=argparse.FileType('r')\n    )\n    return parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Generate a new data source with a new project.'\n    )\n    parser.add_argument(\n        '--config', '-c',\n        type=argparse.FileType('r'),\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        help='The source configuration file.'\n    )\n    return parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Provide default destination configuration file.\")\n    parser.add_argument(\n        \"-d\", \"--destination-config-file\",\n        help=\"Configuration file to use.\")\n    parser.add_argument(\n        \"-v\", \"--version\", action=\"version\",\n        version=\"%(prog)s {}\".format(__version__))\n\n    return parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config-file')\n    return parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', '-c', default=DEFAULT_DESTINATION_CONFIG_FILENAME, required=False,\n                        help='Configuration file to use.')\n    parser.add_argument('--verbose', '-v', action='store_true', help='Be more verbose.')\n    parser.add_argument('--dry-run', '-n', action='store_true', help='Dry run.')\n    parser.add_argument('--overwrite', '-y', action='store_true', help='Overwrite existing files.')\n    parser.add_argument('--dest-dir', '-d', default=None, help='Destination directory.')\n    parser.add_argument('--skip', '-s', action='append', help='Skip renaming step.')\n    parser.add_argument('--no-overwrite', '-n', action='append', help='Skip renaming step.')\n    parser.add_argument('--rename-existing', '-r', action='append', help='Rename existing files.')\n    parser.add_argument('-", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    return create_argument_parser(*arguments).parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(description='Project Borgmatic deployment.')\n    parser.add_argument('--config', '-c', dest='config_file', default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n                        help='Configuration file to use (default: %(default)s)')\n    parser.add_argument('--src', '-s', dest='source', default=None,\n                        help='Source directory for project deployment (default: %(default)s)')\n    parser.add_argument('--dest', '-d', dest='dest', default=None,\n                        help='Destination directory for project deployment (default: %(default)s)')\n    parser.add_argument('--skip', '-s', dest='skip', default=False,\n                        action='store_true',\n                        help='Skip over git repositories (default: %(default)s)')\n    parser.add_argument('--update', '", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Convert CSV files to MongoDB database.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\n        '--destination-config',\n        required=False,\n        help='Path to destination configuration file.')\n\n    parser.add_argument(\n        '--source-config',\n        required=False,\n        help='Path to source configuration file.')\n\n    parser.add_argument(\n        '--source-collection',\n        required=False,\n        help='Collection name for the source database.')\n\n    parser.add_argument(\n        '--destination-collection',\n        required=False,\n        help='Collection name for the destination database.')\n\n    parser.add_argument(\n        '--source-username',\n        required=False,\n        help='Username for the source database.')\n\n    parser.add_argument(\n    ", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/atticmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62b45679d7d32e5b55cc83a9", "code": ["def parser_flags(parser):\n    return''.join(\n        flag.name for flag in parser._get_argparser()._actions\n        if isinstance(flag, argparse._StoreTrueAction))\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.nargs == argparse.OPTIONAL:\n            flags +=''\n        elif arg.nargs == argparse.ZERO_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.ZERO_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.ZERO_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.ONE_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.OPTIONAL:\n            flags +=''\n        elif arg.nargs == argparse.ZERO_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.ONE_OR_MORE:\n            flags +=''\n        elif arg.", "def parser_flags(parser):\n    return''.join(str(f) for f in parser._flags)\n", "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if isinstance(arg, argparse.ArgumentParser):\n            flags.append(arg.default)\n        else:\n            flags.append(arg)\n    return''.join(flags)\n", "def parser_flags(parser):\n    return''.join([str(f) for f in parser.flags])\n", "def parser_flags(parser):\n    return''.join(str(flag) for flag in parser._flags)\n", "def parser_flags(parser):\n    flags = []\n    for argument in parser._actions:\n        if isinstance(argument, argparse.ArgumentParser):\n            flags.extend(argument.option_strings)\n        else:\n            flags.append(argument)\n    return''.join(flags)\n", "def parser_flags(parser):\n    return''.join(str(f) for f in parser._get_flags())\n", "def parser_flags(parser):\n    return''.join([\n        flag.description for flag in parser._get_args()\n    ])\n", "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if isinstance(arg, argparse.ArgumentParser):\n            flags.extend(arg.args)\n        elif isinstance(arg, (argparse.ArgumentDefaultsHelpFormatter,\n                              argparse.ArgumentAction)):\n            flags.append('--' + arg.default)\n    return''.join(flags)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n    return''.join(\n        flag.name for flag in parser._get_argparser()._actions\n        if isinstance(flag, argparse._StoreTrueAction))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.nargs == argparse.OPTIONAL:\n            flags +=''\n        elif arg.nargs == argparse.ZERO_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.ZERO_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.ZERO_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.ONE_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.OPTIONAL:\n            flags +=''\n        elif arg.nargs == argparse.ZERO_OR_MORE:\n            flags +=''\n        elif arg.nargs == argparse.ONE_OR_MORE:\n            flags +=''\n        elif arg.", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(str(f) for f in parser._flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if isinstance(arg, argparse.ArgumentParser):\n            flags.append(arg.default)\n        else:\n            flags.append(arg)\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join([str(f) for f in parser.flags])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(str(flag) for flag in parser._flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for argument in parser._actions:\n        if isinstance(argument, argparse.ArgumentParser):\n            flags.extend(argument.option_strings)\n        else:\n            flags.append(argument)\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(str(f) for f in parser._get_flags())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join([\n        flag.description for flag in parser._get_args()\n    ])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if isinstance(arg, argparse.ArgumentParser):\n            flags.extend(arg.args)\n        elif isinstance(arg, (argparse.ArgumentDefaultsHelpFormatter,\n                              argparse.ArgumentAction)):\n            flags.append('--' + arg.default)\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "ques_id": "62b45665d7d32e5b55cc8365", "code": ["def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    return dict((name, args) for name, args in parse_subparser_arguments(\n        unparsed_arguments,\n        parser.add_subparsers()\n    ))\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        if argument in SUBPARSER_ALIASES:\n            parser.add_argument(SUBPARSER_ALIASES[argument], **argument_kwargs(argument))\n        else:\n            parser.add_argument(argument)\n    return dict(parser.parse_known_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    return dict((name, args) for name, args in parse_subparser_arguments(parsed_arguments=parser.parse_args(unparsed_arguments)))\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    return dict((argument.name, argument.parse()) for argument in parser.parse_args(unparsed_arguments))\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for arg in unparsed_arguments:\n        if arg.startswith('--'):\n            name = arg[2:]\n            args = parser.parse_args(arg[3:])\n        else:\n            name = None\n            args = {}\n        if name not in SUBPARSER_ALIASES:\n            raise ValueError('Unknown subparser \"{}\"'.format(name))\n        args.update(vars(args))\n        yield name, args\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    parser.add_argument('-v', '--version', action='version',\n                        version='%(prog)s'+ __version__)\n    for argument in unparsed_arguments:\n        if not argument.startswith('--version'):\n            arguments = parser.parse_args(argument.split())\n            if isinstance(arguments, dict):\n                return arguments\n    return {}\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        if argument.startswith('--'):\n            name = argument[2:]\n            parser = make_parsers()[name]\n        else:\n            parser.add_argument(argument)\n    return vars(parser.parse_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    args = parser.parse_args(unparsed_arguments)\n    return vars(args)\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parsers()\n    arguments = vars(parser.parse_args(unparsed_arguments))\n    return arguments\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for argument in unparsed_arguments:\n        if argument in SUBPARSER_ALIASES:\n            parser = getattr(parser, SUBPARSER_ALIASES[argument])\n        else:\n            parser = getattr(parser, argument)\n    return vars(parser.parse_args(unparsed_arguments))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    return dict((name, args) for name, args in parse_subparser_arguments(\n        unparsed_arguments,\n        parser.add_subparsers()\n    ))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        if argument in SUBPARSER_ALIASES:\n            parser.add_argument(SUBPARSER_ALIASES[argument], **argument_kwargs(argument))\n        else:\n            parser.add_argument(argument)\n    return dict(parser.parse_known_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    return dict((name, args) for name, args in parse_subparser_arguments(parsed_arguments=parser.parse_args(unparsed_arguments)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    return dict((argument.name, argument.parse()) for argument in parser.parse_args(unparsed_arguments))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for arg in unparsed_arguments:\n        if arg.startswith('--'):\n            name = arg[2:]\n            args = parser.parse_args(arg[3:])\n        else:\n            name = None\n            args = {}\n        if name not in SUBPARSER_ALIASES:\n            raise ValueError('Unknown subparser \"{}\"'.format(name))\n        args.update(vars(args))\n        yield name, args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    parser.add_argument('-v', '--version', action='version',\n                        version='%(prog)s'+ __version__)\n    for argument in unparsed_arguments:\n        if not argument.startswith('--version'):\n            arguments = parser.parse_args(argument.split())\n            if isinstance(arguments, dict):\n                return arguments\n    return {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        if argument.startswith('--'):\n            name = argument[2:]\n            parser = make_parsers()[name]\n        else:\n            parser.add_argument(argument)\n    return vars(parser.parse_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    args = parser.parse_args(unparsed_arguments)\n    return vars(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parsers()\n    arguments = vars(parser.parse_args(unparsed_arguments))\n    return arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for argument in unparsed_arguments:\n        if argument in SUBPARSER_ALIASES:\n            parser = getattr(parser, SUBPARSER_ALIASES[argument])\n        else:\n            parser = getattr(parser, argument)\n    return vars(parser.parse_args(unparsed_arguments))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "ques_id": "62b45665d7d32e5b55cc8364", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        arguments.update(subparser.parse_arguments(unparsed_arguments))\n        remaining_arguments.extend(subparser.remaining_arguments)\n    return arguments, remaining_arguments, len(remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser_name, subparser in subparsers.items():\n        parse_arguments_for_subparser = parse_arguments(unparsed_arguments, subparser)\n        arguments.update(parse_arguments_for_subparser['arguments'])\n        remaining_arguments += parse_arguments_for_subparser['remaining_arguments']\n    return arguments, remaining_arguments, subparsers\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = dict(unparsed_arguments)\n    remaining_arguments = list()\n    for subparser in subparsers._subparserss:\n        if subparser.name in SUBPARSER_ALIASES:\n            subparser_arguments = parse_arguments(arguments.copy(), subparser)\n            remaining_arguments.extend(subparser_arguments)\n            del arguments[subparser.name]\n        else:\n            remaining_arguments.extend(parse_arguments(arguments.copy(), subparser))\n    return arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parser = make_parsers()\n    arguments = {}\n    remaining_arguments = []\n    for argname, argparse_action in subparsers.items():\n        if argparse_action.required:\n            parser.print_help()\n            sys.exit(1)\n        arguments[argname] = parser.parse_args(unparsed_arguments[argparse_action.name])\n        remaining_arguments.append(arguments[argname])\n    return arguments, remaining_arguments, parser.print_help()\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining = []\n    for subparser_name, subparser in subparsers.items():\n        if subparser_name in SUBPARSER_ALIASES:\n            subparser_name = SUBPARSER_ALIASES[subparser_name]\n        arguments[subparser_name] = parse_arguments(unparsed_arguments, subparser)\n        remaining.append(subparser)\n    return arguments, remaining, remaining!= []\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        parser = make_parser(subparser_name)\n        parser.add_arguments(unparsed_arguments)\n        arguments[subparser_name] = parser.parse_args()\n        remaining_arguments.extend(subparser.remaining_arguments)\n    return arguments, remaining_arguments, len(arguments) - len(remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining = []\n    for name in subparsers:\n        parser = SUBPARSER_ALIASES.get(name)\n        if parser is None:\n            raise RuntimeError(\"Unknown subparser %s\" % name)\n        arguments[name] = parser(unparsed_arguments, name)\n        remaining.append(name)\n    return arguments, remaining, []\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for name, parser in six.iteritems(subparsers):\n        if name in unparsed_arguments:\n            parser_args = unparsed_arguments[name]\n            parser_namespace = parser.parse_args(parser_args)\n            remaining_arguments.extend(parser_namespace.args)\n            unparsed_arguments.pop(name)\n        else:\n            parser_namespace = parser.parse_args()\n            parsed_arguments.update(parser_namespace.__dict__)\n            remaining_arguments.append(name)\n    return parsed_arguments, remaining_arguments, unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        parser = make_parsers()[subparser_name]\n        arguments[subparser_name] = parser.parse_args(unparsed_arguments)\n        remaining.append(subparser)\n    return (arguments, remaining, [])\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    return {\n        subparser_name: parse_arguments(unparsed_arguments, subparser)\n        for (subparser_name, subparser) in SUBPARSER_ALIASES.items()\n    }, []\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        arguments.update(subparser.parse_arguments(unparsed_arguments))\n        remaining_arguments.extend(subparser.remaining_arguments)\n    return arguments, remaining_arguments, len(remaining_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser_name, subparser in subparsers.items():\n        parse_arguments_for_subparser = parse_arguments(unparsed_arguments, subparser)\n        arguments.update(parse_arguments_for_subparser['arguments'])\n        remaining_arguments += parse_arguments_for_subparser['remaining_arguments']\n    return arguments, remaining_arguments, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = dict(unparsed_arguments)\n    remaining_arguments = list()\n    for subparser in subparsers._subparserss:\n        if subparser.name in SUBPARSER_ALIASES:\n            subparser_arguments = parse_arguments(arguments.copy(), subparser)\n            remaining_arguments.extend(subparser_arguments)\n            del arguments[subparser.name]\n        else:\n            remaining_arguments.extend(parse_arguments(arguments.copy(), subparser))\n    return arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parser = make_parsers()\n    arguments = {}\n    remaining_arguments = []\n    for argname, argparse_action in subparsers.items():\n        if argparse_action.required:\n            parser.print_help()\n            sys.exit(1)\n        arguments[argname] = parser.parse_args(unparsed_arguments[argparse_action.name])\n        remaining_arguments.append(arguments[argname])\n    return arguments, remaining_arguments, parser.print_help()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining = []\n    for subparser_name, subparser in subparsers.items():\n        if subparser_name in SUBPARSER_ALIASES:\n            subparser_name = SUBPARSER_ALIASES[subparser_name]\n        arguments[subparser_name] = parse_arguments(unparsed_arguments, subparser)\n        remaining.append(subparser)\n    return arguments, remaining, remaining!= []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        parser = make_parser(subparser_name)\n        parser.add_arguments(unparsed_arguments)\n        arguments[subparser_name] = parser.parse_args()\n        remaining_arguments.extend(subparser.remaining_arguments)\n    return arguments, remaining_arguments, len(arguments) - len(remaining_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining = []\n    for name in subparsers:\n        parser = SUBPARSER_ALIASES.get(name)\n        if parser is None:\n            raise RuntimeError(\"Unknown subparser %s\" % name)\n        arguments[name] = parser(unparsed_arguments, name)\n        remaining.append(name)\n    return arguments, remaining, []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for name, parser in six.iteritems(subparsers):\n        if name in unparsed_arguments:\n            parser_args = unparsed_arguments[name]\n            parser_namespace = parser.parse_args(parser_args)\n            remaining_arguments.extend(parser_namespace.args)\n            unparsed_arguments.pop(name)\n        else:\n            parser_namespace = parser.parse_args()\n            parsed_arguments.update(parser_namespace.__dict__)\n            remaining_arguments.append(name)\n    return parsed_arguments, remaining_arguments, unparsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        parser = make_parsers()[subparser_name]\n        arguments[subparser_name] = parser.parse_args(unparsed_arguments)\n        remaining.append(subparser)\n    return (arguments, remaining, [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    return {\n        subparser_name: parse_arguments(unparsed_arguments, subparser)\n        for (subparser_name, subparser) in SUBPARSER_ALIASES.items()\n    }, []\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "ques_id": "62b45665d7d32e5b55cc8363", "code": ["def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Test that a package is installed and can be installed.\")\n    for parser_name in SUBPARSER_ALIASES:\n        parser = parser_name(parser)\n    return parser, parser.add_subparsers()\n", "def make_parsers():\n    parser = argparse.ArgumentParser()\n    for alias in SUBPARSER_ALIASES:\n        parser.add_argument('--' + alias, action='store_true', help='Show details of a specific subcommand.')\n\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test parser for arguments.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    for subparser in SUBPARSER_ALIASES.values():\n        parser.add_subparsers(dest=subparser.dest, title=subparser.title)\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser()\n    for alias, parse_subparser in SUBPARSER_ALIASES.items():\n        parser.add_argument(alias, help=parse_subparser.__doc__)\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test Borgmatic parser that returns results from a list of files.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for parse_action in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(parse_action.replace('_', '-')),\n            nargs='+',\n            type=parse_action,\n            help='Test command line arguments.'\n        )\n\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Install packages with given name or version into a given '\n                    'directory.', formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    for command_name in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(command_name.replace('-', '_')),\n            metavar='PACKAGE',\n            required=True,\n            help='Package to install.',\n        )\n    for command_name in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}-dir'.format(command_name.replace('-', '_')),\n            metavar='PACKAGE_DIR',\n            required=True,\n            help='Directory into which the packages should be installed.',\n        )\n    return parser.parse_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Record a flight path to a stop detection and merge them to a single stop detection output.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for sp in SUBPARSER_ALIASES:\n        parser.add_argument('--%s' % sp, help='The stop detection to merge.')\n\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test subcommand parser.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for command_name in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(command_name),\n            action='store_true',\n            help='Show help for {}'.format(command_name)\n        )\n\n    return parser.parse_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test the installation of the Git repository.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    for argument in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(argument),\n            help='A description for the {} argument.'.format(argument)\n        )\n    return parser.parse_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Run the test suite.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for parser_name in SUBPARSER_ALIASES:\n        subparser = parser.add_subparsers(dest=parser_name)\n        for name in SUBPARSER_ALIASES[parser_name]:\n            parse_subparser_arguments(\n                subparser,\n                SUBPARSER_ALIASES[parser_name][name],\n                subparser,\n                name,\n            )\n    return parser.parse_args()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Test that a package is installed and can be installed.\")\n    for parser_name in SUBPARSER_ALIASES:\n        parser = parser_name(parser)\n    return parser, parser.add_subparsers()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser()\n    for alias in SUBPARSER_ALIASES:\n        parser.add_argument('--' + alias, action='store_true', help='Show details of a specific subcommand.')\n\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test parser for arguments.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    for subparser in SUBPARSER_ALIASES.values():\n        parser.add_subparsers(dest=subparser.dest, title=subparser.title)\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser()\n    for alias, parse_subparser in SUBPARSER_ALIASES.items():\n        parser.add_argument(alias, help=parse_subparser.__doc__)\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test Borgmatic parser that returns results from a list of files.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for parse_action in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(parse_action.replace('_', '-')),\n            nargs='+',\n            type=parse_action,\n            help='Test command line arguments.'\n        )\n\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Install packages with given name or version into a given '\n                    'directory.', formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    for command_name in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(command_name.replace('-', '_')),\n            metavar='PACKAGE',\n            required=True,\n            help='Package to install.',\n        )\n    for command_name in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}-dir'.format(command_name.replace('-', '_')),\n            metavar='PACKAGE_DIR',\n            required=True,\n            help='Directory into which the packages should be installed.',\n        )\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Record a flight path to a stop detection and merge them to a single stop detection output.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for sp in SUBPARSER_ALIASES:\n        parser.add_argument('--%s' % sp, help='The stop detection to merge.')\n\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test subcommand parser.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for command_name in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(command_name),\n            action='store_true',\n            help='Show help for {}'.format(command_name)\n        )\n\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test the installation of the Git repository.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    for argument in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(argument),\n            help='A description for the {} argument.'.format(argument)\n        )\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Run the test suite.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for parser_name in SUBPARSER_ALIASES:\n        subparser = parser.add_subparsers(dest=parser_name)\n        for name in SUBPARSER_ALIASES[parser_name]:\n            parse_subparser_arguments(\n                subparser,\n                SUBPARSER_ALIASES[parser_name][name],\n                subparser,\n                name,\n            )\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/borgmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "ques_id": "62b438ba66fea644fe22cca2", "code": ["def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if not node.value:\n                continue\n            value = node.value\n            if isinstance(value, list):\n                value = deep_merge_nodes(value)\n            elif isinstance(value, MappingNode):\n                value = dict(value)\n            node.value = value\n        elif isinstance(node, ScalarNode):\n            node.value = node.value\n        else:\n            logger.error('Node %s cannot be merged with %s', node, nodes)\n            raise TypeError('Input must be a list or MappingNode')\n        result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n    merged_nodes = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            next_node = deep_merge_nodes(node.value)\n            if next_node is None:\n                continue\n            else:\n                node.value = next_node\n        elif isinstance(node, ScalarNode):\n            merged_nodes.append(node)\n        elif isinstance(node, Node):\n            merged_nodes.append(node.value)\n        else:\n            logger.error('No node merge function defined for node type %s', type(node))\n            raise ValueError('No node merge function defined for node type %s', type(node))\n    return merged_nodes\n", "def deep_merge_nodes(nodes):\n    for node in nodes:\n        if isinstance(node[1], MappingNode):\n            new_node = deep_merge_nodes(node[1].value)\n            node[1] = new_node\n        if isinstance(node[1], ScalarNode):\n            if node[1].value!= node[0].value:\n                logger.warning('Overwriting node values with different types')\n            node[1] = node[0]\n    return nodes\n", "def deep_merge_nodes(nodes):\n    logger.info('deep merging nodes')\n    return [(_deep_merge(node_a, node_b), node_b) for node_a, node_b in nodes]\n", "def deep_merge_nodes(nodes):\n    def _merge_nodes(nodes_list, node):\n        if isinstance(node, MappingNode):\n            for key, value in node.items():\n                if key not in nodes_list:\n                    nodes_list.append((key, value))\n                elif isinstance(value, MappingNode):\n                    _merge_nodes(nodes_list, value)\n        elif isinstance(node, ScalarNode):\n            logger.debug(\"node %s is a scalar node, merging\", node)\n            nodes_list.append(node)\n\n    nodes_list = []\n    _merge_nodes(nodes, DELETED_NODE)\n    return nodes_list\n", "def deep_merge_nodes(nodes):\n\n    # First merge the nodes with the same tag value\n    merged_nodes = []\n    for node_list in nodes:\n        for node in node_list:\n            if node.tag == node.value:\n                node.value = node.value[:-1]\n            else:\n                merged_nodes.append(node)\n    return merged_nodes\n", "def deep_merge_nodes(nodes):\n    logger.debug('Deep merge nodes: %s', nodes)\n    return merge_nodes([_deep_merge(node) for node in nodes])\n", "def deep_merge_nodes(nodes):\n    for k, v in nodes:\n        if isinstance(v, MappingNode):\n            m = deep_merge_nodes(v)\n            if m is not None:\n                v.parent = k\n                v.value = m\n        elif isinstance(v, ScalarNode):\n            v.parent = k\n            v.value = v.value\n    return nodes\n", "def deep_merge_nodes(nodes):\n    merged = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if not node.value:\n                continue\n            next_node = deep_merge_nodes(node.value)\n            if next_node is None:\n                continue\n            else:\n                merged.append((node, next_node))\n        else:\n            merged.append(node)\n    return merged\n", "def deep_merge_nodes(nodes):\n    logger.debug('deep_merge_nodes(%s)', nodes)\n    for node in nodes:\n        if isinstance(node[0], MappingNode):\n            node_val = deep_merge_nodes(node[0].value)\n            if node_val:\n                node[0].value = node_val\n        if isinstance(node[0], ScalarNode):\n            if node[0].value == DELETED_NODE:\n                logger.debug('Removing duplicate scalar node %s', node)\n                node[0].value = None\n            else:\n                logger.debug('Adding scalar node %s', node)\n                node[0].value = node[0].value\n    return nodes\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if not node.value:\n                continue\n            value = node.value\n            if isinstance(value, list):\n                value = deep_merge_nodes(value)\n            elif isinstance(value, MappingNode):\n                value = dict(value)\n            node.value = value\n        elif isinstance(node, ScalarNode):\n            node.value = node.value\n        else:\n            logger.error('Node %s cannot be merged with %s', node, nodes)\n            raise TypeError('Input must be a list or MappingNode')\n        result.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    merged_nodes = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            next_node = deep_merge_nodes(node.value)\n            if next_node is None:\n                continue\n            else:\n                node.value = next_node\n        elif isinstance(node, ScalarNode):\n            merged_nodes.append(node)\n        elif isinstance(node, Node):\n            merged_nodes.append(node.value)\n        else:\n            logger.error('No node merge function defined for node type %s', type(node))\n            raise ValueError('No node merge function defined for node type %s', type(node))\n    return merged_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    for node in nodes:\n        if isinstance(node[1], MappingNode):\n            new_node = deep_merge_nodes(node[1].value)\n            node[1] = new_node\n        if isinstance(node[1], ScalarNode):\n            if node[1].value!= node[0].value:\n                logger.warning('Overwriting node values with different types')\n            node[1] = node[0]\n    return nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    logger.info('deep merging nodes')\n    return [(_deep_merge(node_a, node_b), node_b) for node_a, node_b in nodes]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    def _merge_nodes(nodes_list, node):\n        if isinstance(node, MappingNode):\n            for key, value in node.items():\n                if key not in nodes_list:\n                    nodes_list.append((key, value))\n                elif isinstance(value, MappingNode):\n                    _merge_nodes(nodes_list, value)\n        elif isinstance(node, ScalarNode):\n            logger.debug(\"node %s is a scalar node, merging\", node)\n            nodes_list.append(node)\n\n    nodes_list = []\n    _merge_nodes(nodes, DELETED_NODE)\n    return nodes_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\n    # First merge the nodes with the same tag value\n    merged_nodes = []\n    for node_list in nodes:\n        for node in node_list:\n            if node.tag == node.value:\n                node.value = node.value[:-1]\n            else:\n                merged_nodes.append(node)\n    return merged_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    logger.debug('Deep merge nodes: %s', nodes)\n    return merge_nodes([_deep_merge(node) for node in nodes])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    for k, v in nodes:\n        if isinstance(v, MappingNode):\n            m = deep_merge_nodes(v)\n            if m is not None:\n                v.parent = k\n                v.value = m\n        elif isinstance(v, ScalarNode):\n            v.parent = k\n            v.value = v.value\n    return nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    merged = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if not node.value:\n                continue\n            next_node = deep_merge_nodes(node.value)\n            if next_node is None:\n                continue\n            else:\n                merged.append((node, next_node))\n        else:\n            merged.append(node)\n    return merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    logger.debug('deep_merge_nodes(%s)', nodes)\n    for node in nodes:\n        if isinstance(node[0], MappingNode):\n            node_val = deep_merge_nodes(node[0].value)\n            if node_val:\n                node[0].value = node_val\n        if isinstance(node[0], ScalarNode):\n            if node[0].value == DELETED_NODE:\n                logger.debug('Removing duplicate scalar node %s', node)\n                node[0].value = None\n            else:\n                logger.debug('Adding scalar node %s', node)\n                node[0].value = node[0].value\n    return nodes\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "ques_id": "62b438b666fea644fe22cc78", "code": ["def parse_arguments(*arguments):\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('config_file')\n    parser.add_argument('destination')\n    parser.add_argument('-v', '--version', action='store_true')\n\n    return parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Generate and store required metadata from the \"\n        \"external metadata index.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        '--config', type=str, default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        help=(\n            \"Path to the config file to use when parsing the \"\n            \"external metadata index (default: {}). \"\n            \"The default is the relative path \"\n            \"$DESTINATION_CONFIG_FILENAME. \"\n            \"If you do not want to override the \"\n            \"default config file in the \"\n            \"$DESTINATION_CONFIG_FILENAME environment variable, \"\n            \"set the environment variable to the default \"\n            \"value (.txt).\"\n        ).format(DEFAULT_DESTINATION_CONFIG_FILENAME)\n    )\n   ", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('destination_config_file')\n    parser.add_argument('--output_file')\n    parser.add_argument('--overwrite', action='store_true')\n    return parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Generate shapefiles from a single csv file.')\n    parser.add_argument(\n        '-v', '--version',\n        action='version',\n        version='%(prog)s {}'.format(__version__))\n    parser.add_argument(\n        '-d', '--destination-config',\n        help='Path to the destination configuration file.')\n    parser.add_argument(\n        '-o', '--output-directory',\n        help='Directory to save the shapefiles in.')\n    parser.add_argument(\n        '-t', '--template-file',\n        help='Path to the template file.')\n    parser.add_argument(\n        '-s', '--shapefile-suffix',\n        help='Suffix to be appended to the filename. The suffix must be in the format [shapefile-suffix]')\n    parser.add_argument(\n        '-a', '--additional-files',\n  ", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', '-c', default=DEFAULT_DESTINATION_CONFIG_FILENAME)\n    return parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n\n    for argument in arguments:\n        parser.add_argument(*argument.split('=', 1), **argument_options(argument))\n\n    return parser\n", "def parse_arguments(*arguments):\n\n    parser = argparse.ArgumentParser(\n        description='Take actions related to archive metadata.'\n    )\n\n    parser.add_argument(\n        '--src',\n        help='Path to a source folder to the source archive.',\n        required=True\n    )\n\n    parser.add_argument(\n        '--dest',\n        help='Path to a destination folder to the destination folder for the archive metadata.',\n        required=True\n    )\n\n    parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Dry run mode, do not proceed without actual archive metadata.'\n    )\n\n    parser.add_argument(\n        '--version',\n        action='store_true',\n        help='Print the version number of the program.'\n    )\n\n    parser.add_argument(\n        '--debug',\n        action='store_true',\n        help='Print debugging informatio", "def parse_arguments(*arguments):\n    return argparse.ArgumentParser(\n        description='Borgmatic CLI tools',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'destination',\n        metavar='DEST',\n        type=str,\n        nargs='?',\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        help='File to store the destination configuration in.'\n    )\n    parser.add_argument(\n        '--overwrite',\n        action='store_true',\n        help='Overwrite the destination configuration if it exists.'\n    )\n    parser.add_argument(\n        '--overwrite-if-exists',\n        action='store_true',\n        help='Overwrite the destination configuration if it exists.'\n    )\n    parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Just print what would be written to the destination configuration.'\n    )\n    return parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test installation of a source package.\"\n    )\n    parser.add_argument(\n        \"-i\", \"--install-source-package\",\n        help=\"The source package to install.\"\n    )\n    parser.add_argument(\n        \"-v\", \"--verbose\", action=\"store_true\", help=\"Show more information.\"\n    )\n    parser.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Show debugging messages.\"\n    )\n    parser.add_argument(\n        \"-l\", \"--license\", action=\"store_true\", help=\"Show license messages.\"\n    )\n    parser.add_argument(\n        \"-a\", \"--all\", action=\"store_true\", help=\"Show all installed packages.\"\n    )\n\n    return parser.parse_args(*arguments)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('config_file')\n    parser.add_argument('destination')\n    parser.add_argument('-v', '--version', action='store_true')\n\n    return parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Generate and store required metadata from the \"\n        \"external metadata index.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        '--config', type=str, default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        help=(\n            \"Path to the config file to use when parsing the \"\n            \"external metadata index (default: {}). \"\n            \"The default is the relative path \"\n            \"$DESTINATION_CONFIG_FILENAME. \"\n            \"If you do not want to override the \"\n            \"default config file in the \"\n            \"$DESTINATION_CONFIG_FILENAME environment variable, \"\n            \"set the environment variable to the default \"\n            \"value (.txt).\"\n        ).format(DEFAULT_DESTINATION_CONFIG_FILENAME)\n    )\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('destination_config_file')\n    parser.add_argument('--output_file')\n    parser.add_argument('--overwrite', action='store_true')\n    return parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Generate shapefiles from a single csv file.')\n    parser.add_argument(\n        '-v', '--version',\n        action='version',\n        version='%(prog)s {}'.format(__version__))\n    parser.add_argument(\n        '-d', '--destination-config',\n        help='Path to the destination configuration file.')\n    parser.add_argument(\n        '-o', '--output-directory',\n        help='Directory to save the shapefiles in.')\n    parser.add_argument(\n        '-t', '--template-file',\n        help='Path to the template file.')\n    parser.add_argument(\n        '-s', '--shapefile-suffix',\n        help='Suffix to be appended to the filename. The suffix must be in the format [shapefile-suffix]')\n    parser.add_argument(\n        '-a', '--additional-files',\n  ", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', '-c', default=DEFAULT_DESTINATION_CONFIG_FILENAME)\n    return parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n\n    for argument in arguments:\n        parser.add_argument(*argument.split('=', 1), **argument_options(argument))\n\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\n    parser = argparse.ArgumentParser(\n        description='Take actions related to archive metadata.'\n    )\n\n    parser.add_argument(\n        '--src',\n        help='Path to a source folder to the source archive.',\n        required=True\n    )\n\n    parser.add_argument(\n        '--dest',\n        help='Path to a destination folder to the destination folder for the archive metadata.',\n        required=True\n    )\n\n    parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Dry run mode, do not proceed without actual archive metadata.'\n    )\n\n    parser.add_argument(\n        '--version',\n        action='store_true',\n        help='Print the version number of the program.'\n    )\n\n    parser.add_argument(\n        '--debug',\n        action='store_true',\n        help='Print debugging informatio", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    return argparse.ArgumentParser(\n        description='Borgmatic CLI tools',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'destination',\n        metavar='DEST',\n        type=str,\n        nargs='?',\n        default=DEFAULT_DESTINATION_CONFIG_FILENAME,\n        help='File to store the destination configuration in.'\n    )\n    parser.add_argument(\n        '--overwrite',\n        action='store_true',\n        help='Overwrite the destination configuration if it exists.'\n    )\n    parser.add_argument(\n        '--overwrite-if-exists',\n        action='store_true',\n        help='Overwrite the destination configuration if it exists.'\n    )\n    parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Just print what would be written to the destination configuration.'\n    )\n    return parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test installation of a source package.\"\n    )\n    parser.add_argument(\n        \"-i\", \"--install-source-package\",\n        help=\"The source package to install.\"\n    )\n    parser.add_argument(\n        \"-v\", \"--verbose\", action=\"store_true\", help=\"Show more information.\"\n    )\n    parser.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Show debugging messages.\"\n    )\n    parser.add_argument(\n        \"-l\", \"--license\", action=\"store_true\", help=\"Show license messages.\"\n    )\n    parser.add_argument(\n        \"-a\", \"--all\", action=\"store_true\", help=\"Show all installed packages.\"\n    )\n\n    return parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62b438b666fea644fe22cc72", "code": ["def parser_flags(parser):\n    flags = []\n    for action in parser._actions:\n        if action.dest is None:\n            flags.append(action.default)\n        else:\n            flags.append(action.take_flags(action))\n    return''.join(flags)\n", "def parser_flags(parser):\n    return''.join(\n        flag.description for flag in parser._actions\n        if isinstance(flag, argparse.ArgumentParser)\n    )\n", "def parser_flags(parser):\n    return''.join(\n        flag for flag in parser._get_flags() if flag.default\n    )\n", "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if hasattr(arg, 'dest'):\n            flags.append('--{}'.format(arg.dest))\n        elif hasattr(arg, 'flags'):\n            flags.extend(arg.flags)\n    return''.join(flags)\n", "def parser_flags(parser):\n    return''.join(flag.arg for flag in parser._flags)\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.default is not None and arg.default!= argparse.SUPPRESS:\n            flags +='{}'.format(arg.dest)\n            if arg.default == argparse.SUPPRESS:\n                break\n        if arg.choices is not None and arg.choices!= []:\n            flags +='{}'.format(','.join(arg.choices))\n        if arg.metavar is not None and arg.metavar!= []:\n            flags +='{}'.format(','.join(arg.metavar))\n    return flags\n", "def parser_flags(parser):\n    flags = []\n    for argument in parser._actions:\n        if argument.nargs == argparse.OPTIONAL:\n            flags.append('--')\n        elif argument.nargs == argparse.ZERO_OR_MORE:\n            flags.append('-')\n        elif argument.default is not argparse.SUPPRESS:\n            flags.append('{}={}'.format(argument.dest, argument.default))\n    return''.join(flags)\n", "def parser_flags(parser):\n    return''.join([a.help for a in parser._actions])\n", "def parser_flags(parser):\n    return''.join([str(flag) for flag in parser._flags])\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +='{}'.format(arg.default)\n        else:\n            flags +='{} {}'.format(arg.default, arg.dest)\n    return flags\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n    flags = []\n    for action in parser._actions:\n        if action.dest is None:\n            flags.append(action.default)\n        else:\n            flags.append(action.take_flags(action))\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(\n        flag.description for flag in parser._actions\n        if isinstance(flag, argparse.ArgumentParser)\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(\n        flag for flag in parser._get_flags() if flag.default\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if hasattr(arg, 'dest'):\n            flags.append('--{}'.format(arg.dest))\n        elif hasattr(arg, 'flags'):\n            flags.extend(arg.flags)\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(flag.arg for flag in parser._flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.default is not None and arg.default!= argparse.SUPPRESS:\n            flags +='{}'.format(arg.dest)\n            if arg.default == argparse.SUPPRESS:\n                break\n        if arg.choices is not None and arg.choices!= []:\n            flags +='{}'.format(','.join(arg.choices))\n        if arg.metavar is not None and arg.metavar!= []:\n            flags +='{}'.format(','.join(arg.metavar))\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for argument in parser._actions:\n        if argument.nargs == argparse.OPTIONAL:\n            flags.append('--')\n        elif argument.nargs == argparse.ZERO_OR_MORE:\n            flags.append('-')\n        elif argument.default is not argparse.SUPPRESS:\n            flags.append('{}={}'.format(argument.dest, argument.default))\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join([a.help for a in parser._actions])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join([str(flag) for flag in parser._flags])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +='{}'.format(arg.default)\n        else:\n            flags +='{} {}'.format(arg.default, arg.dest)\n    return flags\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "bash_completion", "docstring": "Return a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.", "ques_id": "62b438b566fea644fe22cc70", "code": ["def bash_completion():\n    return \"\\n\".join(\n        \"   --{} {}\".format(k, v) for k, v in parser_flags(parser()).items()\n    )\n", "def bash_completion():\n    parser = argparse.ArgumentParser(\n        description=UPGRADE_MESSAGE,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    for parser_flag in bash_completion_flags():\n        parser.add_argument(*parser_flag.flags, **parser_flag.kwargs)\n    return parser.parse_args()\n", "def bash_completion():\n    parser = argparse.ArgumentParser(\n        description=UPGRADE_MESSAGE,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\n        '--version',\n        action='version',\n        version='%(prog)s {}'.format(__version__)\n    )\n\n    parser_flags(parser)\n\n    return parser.format_help()\n", "def bash_completion():\n    return '\\n'.join(['  %s %s' % (k.replace('-', '_').capitalize(), v)\n                     for k, v in parser_flags().items()])\n", "def bash_completion():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--version',\n        action='store_true',\n        help='Show the version number and exit.',\n    )\n    parser_flags(parser)\n    return parser.format_help()\n", "def bash_completion():\n    return '\\n'.join([\n        '# Borgmatic command line arguments',\n        '', 'export BRANCH=$(git log --pretty=%h -n 1 --pretty=%s)',\n        '', 'export GIT_DIR=$(git rev-parse --abbrev-ref HEAD)',\n        '', '', UPGRADE_MESSAGE\n    ])\n", "def bash_completion():\n    return \"\\n\".join(\"-- %s %s\" % (k, v) for k, v in parser_flags().items())\n", "def bash_completion():\n    arguments = [\n        '-c',\n        '--bash-completion',\n        '--no-color',\n        '--no-breaks',\n        '--no-location',\n    ]\n    return arguments\n", "def bash_completion():\n    return '\\n'.join(['  --%s %s' % (k, v) for k, v in parser_flags().items()])\n", "def bash_completion():\n    parser = ArgumentParser(prog=\"bash-completion\")\n    for flag in [\"-h\", \"--help\"]:\n        parser.add_argument(flag, action=\"store_true\")\n    return parser.parse_args()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def bash_completion():\n    return \"\\n\".join(\n        \"   --{} {}\".format(k, v) for k, v in parser_flags(parser()).items()\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    parser = argparse.ArgumentParser(\n        description=UPGRADE_MESSAGE,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    for parser_flag in bash_completion_flags():\n        parser.add_argument(*parser_flag.flags, **parser_flag.kwargs)\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    parser = argparse.ArgumentParser(\n        description=UPGRADE_MESSAGE,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\n        '--version',\n        action='version',\n        version='%(prog)s {}'.format(__version__)\n    )\n\n    parser_flags(parser)\n\n    return parser.format_help()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return '\\n'.join(['  %s %s' % (k.replace('-', '_').capitalize(), v)\n                     for k, v in parser_flags().items()])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--version',\n        action='store_true',\n        help='Show the version number and exit.',\n    )\n    parser_flags(parser)\n    return parser.format_help()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return '\\n'.join([\n        '# Borgmatic command line arguments',\n        '', 'export BRANCH=$(git log --pretty=%h -n 1 --pretty=%s)',\n        '', 'export GIT_DIR=$(git rev-parse --abbrev-ref HEAD)',\n        '', '', UPGRADE_MESSAGE\n    ])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return \"\\n\".join(\"-- %s %s\" % (k, v) for k, v in parser_flags().items())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    arguments = [\n        '-c',\n        '--bash-completion',\n        '--no-color',\n        '--no-breaks',\n        '--no-location',\n    ]\n    return arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return '\\n'.join(['  --%s %s' % (k, v) for k, v in parser_flags().items()])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    parser = ArgumentParser(prog=\"bash-completion\")\n    for flag in [\"-h\", \"--help\"]:\n        parser.add_argument(flag, action=\"store_true\")\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "ques_id": "62b438a266fea644fe22cc2e", "code": ["def parse_arguments(*unparsed_arguments):\n    parser = make_parsers()\n    results = {}\n    for arg in unparsed_arguments:\n        argname = parser.parse_args(arg.split())\n        results[argname.name] = argname\n    return results\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        # Get the name of the subparser, or \"global\" if it's not specified\n        subparser_name = SUBPARSER_ALIASES.get(argument.dest, 'global')\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return dict(parser.parse_known_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    namespace = parser.parse_args(unparsed_arguments)\n    return namespace\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for argument in unparsed_arguments:\n        argument = argument.strip()\n        if argument:\n            name, default = argument.split('=')\n            args = parser.parse_args([name])\n            args.__dict__.update(parse_subparser_arguments(args.args, subparsers=SUBPARSER_ALIASES[name]))\n            args.__dict__['default'] = parse_arguments(default)\n            return args\n    return parser.parse_args()\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        if argument in SUBPARSER_ALIASES:\n            parser.add_argument(SUBPARSER_ALIASES[argument])\n        else:\n            parser.add_argument(argument)\n    return vars(parser.parse_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parsers()\n    return parser.parse_args(unparsed_arguments)\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for arg in unparsed_arguments:\n        try:\n            name = SUBPARSER_ALIASES[arg.__class__]\n        except KeyError:\n            raise ValueError(\"Unknown subparser '%s'\" % arg.__class__)\n        else:\n            setattr(args, name, arg)\n    return vars(parser.parse_args(unparsed_arguments))\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    parser.parse_args(unparsed_arguments)\n    return vars(parser.parse_args())\n", "def parse_arguments(*unparsed_arguments):\n    arguments = vars(get_parser().parse_args(unparsed_arguments))\n    return arguments\n", "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for arg in unparsed_arguments:\n        args = parser.parse_args(arg.split())\n        yield args.name, args\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parsers()\n    results = {}\n    for arg in unparsed_arguments:\n        argname = parser.parse_args(arg.split())\n        results[argname.name] = argname\n    return results\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        # Get the name of the subparser, or \"global\" if it's not specified\n        subparser_name = SUBPARSER_ALIASES.get(argument.dest, 'global')\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return dict(parser.parse_known_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    namespace = parser.parse_args(unparsed_arguments)\n    return namespace\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for argument in unparsed_arguments:\n        argument = argument.strip()\n        if argument:\n            name, default = argument.split('=')\n            args = parser.parse_args([name])\n            args.__dict__.update(parse_subparser_arguments(args.args, subparsers=SUBPARSER_ALIASES[name]))\n            args.__dict__['default'] = parse_arguments(default)\n            return args\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        if argument in SUBPARSER_ALIASES:\n            parser.add_argument(SUBPARSER_ALIASES[argument])\n        else:\n            parser.add_argument(argument)\n    return vars(parser.parse_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parsers()\n    return parser.parse_args(unparsed_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for arg in unparsed_arguments:\n        try:\n            name = SUBPARSER_ALIASES[arg.__class__]\n        except KeyError:\n            raise ValueError(\"Unknown subparser '%s'\" % arg.__class__)\n        else:\n            setattr(args, name, arg)\n    return vars(parser.parse_args(unparsed_arguments))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    parser.parse_args(unparsed_arguments)\n    return vars(parser.parse_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    arguments = vars(get_parser().parse_args(unparsed_arguments))\n    return arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = make_parser()\n    for arg in unparsed_arguments:\n        args = parser.parse_args(arg.split())\n        yield args.name, args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "ques_id": "62b438a266fea644fe22cc2d", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    not_parsed = []\n    for parser_name, parser in parse_arguments(unparsed_arguments, subparsers):\n        if parser_name in SUBPARSER_ALIASES:\n            parser_name = SUBPARSER_ALIASES[parser_name]\n        parsed_arguments[parser_name] = parser\n        not_parsed.append(parser)\n    return parsed_arguments, not_parsed, len(not_parsed)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for argname, parser in six.iteritems(subparsers):\n        if argname in unparsed_arguments:\n            parsed_arguments[argname] = unparsed_arguments[argname]\n        else:\n            remaining_arguments.append(argname)\n            parser.parse(parsed_arguments)\n    return (parsed_arguments, remaining_arguments, remaining_arguments!= [])\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    args = dict()\n    remaining = []\n    for name, parser in six.iteritems(subparsers):\n        parser_args = parser.parse_known_args(unparsed_arguments)\n        if parser_args:\n            args[name] = parser_args\n            remaining.append(name)\n            unparsed_arguments = unparsed_arguments[len(remaining):]\n    return (args, remaining, unparsed_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    result = {}\n    not_claimed = set()\n    for subparser in subparsers.values():\n        parser_arguments = parse_arguments(unparsed_arguments, subparser)\n        if parser_arguments:\n            result.update(parser_arguments)\n        else:\n            not_claimed.add(subparser)\n    return result, not_claimed, []\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    args = dict()\n    remaining_args = list()\n    for subparser in subparsers.values():\n        if subparser.name in SUBPARSER_ALIASES:\n            parser_name = SUBPARSER_ALIASES[subparser.name]\n            args[parser_name] = subparser.parse_args(unparsed_arguments)\n        else:\n            remaining_args.extend(subparser.parse_args(unparsed_arguments))\n    return args, remaining_args, len(remaining_args)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parser_arguments = dict()\n    remaining_arguments = list()\n    for name, parser in six.iteritems(subparsers):\n        parser_arguments[name] = parser.parse_known_args(unparsed_arguments)\n        remaining_arguments.append(name)\n    return parser_arguments, remaining_arguments, unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    result = {}\n    not_claimed_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        parser_args = getattr(subparser, 'parse_args', subparser.parse_args)\n        result[subparser_name] = parser_args\n        if hasattr(subparser, 'parse_args_unparsed'):\n            not_claimed_arguments.extend(getattr(subparser, 'parse_args_unparsed')())\n        else:\n            not_claimed_arguments.extend(getattr(subparser, 'parse_args')())\n    return result, not_claimed_arguments, not_claimed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_arguments = [s for s in unparsed_arguments if s is not None]\n    for subparser in subparsers.values():\n        if subparser.name in SUBPARSER_ALIASES:\n            parser_name = SUBPARSER_ALIASES[subparser.name]\n        else:\n            parser_name = subparser.name\n        parsed_arguments[parser_name] = parser_name(unparsed_arguments)\n    remaining_arguments = [s for s in unparsed_arguments if s is not None]\n    return parsed_arguments, remaining_arguments, unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        parsed_subparser = subparsers.add_parser(subparser_name)\n        parse_arguments(parsed_subparser, unparsed_arguments)\n        parsed_arguments.update(parsed_subparser.parse_known_args())\n        remaining_arguments += parsed_subparser.required_arguments\n    return parsed_arguments, remaining_arguments, parsed_arguments.get('--repository')\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        parser = make_parsers()\n        parser.add_argument('--%s' % subparser_name, action=subparser,\n                            required=True)\n        arguments[subparser_name] = parser\n        remaining_arguments.append(subparser_name)\n    return arguments, remaining_arguments, unparsed_arguments\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    not_parsed = []\n    for parser_name, parser in parse_arguments(unparsed_arguments, subparsers):\n        if parser_name in SUBPARSER_ALIASES:\n            parser_name = SUBPARSER_ALIASES[parser_name]\n        parsed_arguments[parser_name] = parser\n        not_parsed.append(parser)\n    return parsed_arguments, not_parsed, len(not_parsed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for argname, parser in six.iteritems(subparsers):\n        if argname in unparsed_arguments:\n            parsed_arguments[argname] = unparsed_arguments[argname]\n        else:\n            remaining_arguments.append(argname)\n            parser.parse(parsed_arguments)\n    return (parsed_arguments, remaining_arguments, remaining_arguments!= [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    args = dict()\n    remaining = []\n    for name, parser in six.iteritems(subparsers):\n        parser_args = parser.parse_known_args(unparsed_arguments)\n        if parser_args:\n            args[name] = parser_args\n            remaining.append(name)\n            unparsed_arguments = unparsed_arguments[len(remaining):]\n    return (args, remaining, unparsed_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    result = {}\n    not_claimed = set()\n    for subparser in subparsers.values():\n        parser_arguments = parse_arguments(unparsed_arguments, subparser)\n        if parser_arguments:\n            result.update(parser_arguments)\n        else:\n            not_claimed.add(subparser)\n    return result, not_claimed, []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    args = dict()\n    remaining_args = list()\n    for subparser in subparsers.values():\n        if subparser.name in SUBPARSER_ALIASES:\n            parser_name = SUBPARSER_ALIASES[subparser.name]\n            args[parser_name] = subparser.parse_args(unparsed_arguments)\n        else:\n            remaining_args.extend(subparser.parse_args(unparsed_arguments))\n    return args, remaining_args, len(remaining_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parser_arguments = dict()\n    remaining_arguments = list()\n    for name, parser in six.iteritems(subparsers):\n        parser_arguments[name] = parser.parse_known_args(unparsed_arguments)\n        remaining_arguments.append(name)\n    return parser_arguments, remaining_arguments, unparsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    result = {}\n    not_claimed_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        parser_args = getattr(subparser, 'parse_args', subparser.parse_args)\n        result[subparser_name] = parser_args\n        if hasattr(subparser, 'parse_args_unparsed'):\n            not_claimed_arguments.extend(getattr(subparser, 'parse_args_unparsed')())\n        else:\n            not_claimed_arguments.extend(getattr(subparser, 'parse_args')())\n    return result, not_claimed_arguments, not_claimed_arguments\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_arguments = [s for s in unparsed_arguments if s is not None]\n    for subparser in subparsers.values():\n        if subparser.name in SUBPARSER_ALIASES:\n            parser_name = SUBPARSER_ALIASES[subparser.name]\n        else:\n            parser_name = subparser.name\n        parsed_arguments[parser_name] = parser_name(unparsed_arguments)\n    remaining_arguments = [s for s in unparsed_arguments if s is not None]\n    return parsed_arguments, remaining_arguments, unparsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        parsed_subparser = subparsers.add_parser(subparser_name)\n        parse_arguments(parsed_subparser, unparsed_arguments)\n        parsed_arguments.update(parsed_subparser.parse_known_args())\n        remaining_arguments += parsed_subparser.required_arguments\n    return parsed_arguments, remaining_arguments, parsed_arguments.get('--repository')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = subparsers[subparser_name]\n        parser = make_parsers()\n        parser.add_argument('--%s' % subparser_name, action=subparser,\n                            required=True)\n        arguments[subparser_name] = parser\n        remaining_arguments.append(subparser_name)\n    return arguments, remaining_arguments, unparsed_arguments\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "ques_id": "62b438a266fea644fe22cc2c", "code": ["def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Simple command line tool that can be run on a single '\n                    'container.')\n    for command_alias in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(command_alias.replace('_', '-').capitalize()),\n            action='store_true',\n            help='Use {} subcommand to run.'.format(command_alias))\n    return parser.parse_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(description='Borgmatic command line tool.')\n    for alias in SUBPARSER_ALIASES:\n        parser.add_argument('--{}'.format(alias), action='store_true',\n                            help='Enable the {} subcommand.'.format(alias))\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Compute the difference between two sets of arguments.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    for argument_name in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--' + argument_name,\n            metavar='PATTERN',\n            choices=[\n                '<pattern>',\n                '<filename>',\n                '<lineno>',\n                '<prefix>',\n                '<suffix>',\n            ],\n            required=True,\n            help='The argument to compare.'\n        )\n\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test the local filesystem on the local computer.'\n    )\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = parser.add_subparsers(dest=subparser_name)\n        for parser_name in SUBPARSER_ALIASES[subparser_name]:\n            parser_method = getattr(parser, parser_name)\n            parser_method(subparser)\n    return parser.parse_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(description='Python script for extracting data from a Git repository.')\n    for argument in SUBPARSER_ALIASES:\n        parser.add_argument('--' + argument.replace('-', '_'), type=argument.replace('_', '-'),\n                            help='The name of a sub-parser for the sub-command.', required=False)\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Generate random passwords from a file.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for subparser in SUBPARSER_ALIASES:\n        subparser.add_argument(\n            'file',\n            metavar='FILE',\n            help='The file to read the passwords from.'\n        )\n\n    return parser.parse_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser()\n    for name in sorted(SUBPARSER_ALIASES):\n        parser.add_argument(name, help=SUBPARSER_ALIASES[name][\"help\"])\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser()\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = parser.add_subparsers(dest=subparser_name)\n        for subparser_alias in SUBPARSER_ALIASES[subparser_name]:\n            subparser.add_argument(\n                '--{}'.format(subparser_alias),\n                help='{} argument'.format(subparser_alias),\n                required=False,\n                type=argparse.Flag,\n                default=False,\n            )\n    return parser.parse_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Parse Git repositories from a submodule',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for argument in SUBPARSER_ALIASES:\n        parser.add_argument(argument)\n\n    return parser.parse_known_args()\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Project metadata exporter.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    for subparser in SUBPARSER_ALIASES:\n        subparser = parser.add_subparsers(dest=subparser.title.lower())\n        for command_name in SUBPARSER_ALIASES[subparser]:\n            make_subparser(subparser, command_name)\n\n    return parser.parse_args()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Simple command line tool that can be run on a single '\n                    'container.')\n    for command_alias in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--{}'.format(command_alias.replace('_', '-').capitalize()),\n            action='store_true',\n            help='Use {} subcommand to run.'.format(command_alias))\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(description='Borgmatic command line tool.')\n    for alias in SUBPARSER_ALIASES:\n        parser.add_argument('--{}'.format(alias), action='store_true',\n                            help='Enable the {} subcommand.'.format(alias))\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Compute the difference between two sets of arguments.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    for argument_name in SUBPARSER_ALIASES:\n        parser.add_argument(\n            '--' + argument_name,\n            metavar='PATTERN',\n            choices=[\n                '<pattern>',\n                '<filename>',\n                '<lineno>',\n                '<prefix>',\n                '<suffix>',\n            ],\n            required=True,\n            help='The argument to compare.'\n        )\n\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test the local filesystem on the local computer.'\n    )\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = parser.add_subparsers(dest=subparser_name)\n        for parser_name in SUBPARSER_ALIASES[subparser_name]:\n            parser_method = getattr(parser, parser_name)\n            parser_method(subparser)\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(description='Python script for extracting data from a Git repository.')\n    for argument in SUBPARSER_ALIASES:\n        parser.add_argument('--' + argument.replace('-', '_'), type=argument.replace('_', '-'),\n                            help='The name of a sub-parser for the sub-command.', required=False)\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Generate random passwords from a file.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for subparser in SUBPARSER_ALIASES:\n        subparser.add_argument(\n            'file',\n            metavar='FILE',\n            help='The file to read the passwords from.'\n        )\n\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser()\n    for name in sorted(SUBPARSER_ALIASES):\n        parser.add_argument(name, help=SUBPARSER_ALIASES[name][\"help\"])\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser()\n    for subparser_name in SUBPARSER_ALIASES:\n        subparser = parser.add_subparsers(dest=subparser_name)\n        for subparser_alias in SUBPARSER_ALIASES[subparser_name]:\n            subparser.add_argument(\n                '--{}'.format(subparser_alias),\n                help='{} argument'.format(subparser_alias),\n                required=False,\n                type=argparse.Flag,\n                default=False,\n            )\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Parse Git repositories from a submodule',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    for argument in SUBPARSER_ALIASES:\n        parser.add_argument(argument)\n\n    return parser.parse_known_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Project metadata exporter.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    for subparser in SUBPARSER_ALIASES:\n        subparser = parser.add_subparsers(dest=subparser.title.lower())\n        for command_name in SUBPARSER_ALIASES[subparser]:\n            make_subparser(subparser, command_name)\n\n    return parser.parse_args()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_10.py", "name": "paging", "docstring": "Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.", "ques_id": "62ece4982e6aefcf4aabbd5f", "code": ["def paging(response, max_results):\n    page = 1\n    if max_results == 0:\n        return response\n\n    while len(response) > max_results:\n        for item in response[page * max_results:]:\n            yield item\n\n        page += 1\n        if page >= max_results:\n            break\n", "def paging(response, max_results):\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]\n", "def paging(response, max_results):\n    page = 0\n    page_size = 100\n    offset = 0\n\n    while offset < max_results:\n        results = response[offset: offset + page_size]\n        for result in results:\n            yield result\n\n        offset += page_size\n        page += 1\n", "def paging(response, max_results):\n\n    page = 1\n    while page <= max_results:\n        for response_page in response:\n            yield response_page\n        page += 1\n", "def paging(response, max_results):\n    if max_results <= 0:\n        raise ValueError('Maximum number of objects to be returned must be greater than zero')\n\n    for page in range(1, max_results + 1):\n        for page_size in range(1, page + 1):\n            yield response[page * page_size: (page + 1) * page_size]\n", "def paging(response, max_results):\n    if len(response) <= max_results:\n        for result in response:\n            yield result\n    else:\n        return response[max_results:]\n", "def paging(response, max_results):\n    page = 1\n    page_size = 100\n    offset = 0\n    results = []\n\n    while offset < max_results:\n        results.extend(response[offset: (offset + page_size)])\n        offset += page_size\n\n        if len(results) >= max_results:\n            break\n\n    return results\n", "def paging(response, max_results):\n    for page in range(1, max_results + 1):\n        if page < 1:\n            continue\n\n        response_page = response[page - 1]\n        for item in response_page:\n            yield item\n", "def paging(response, max_results):\n    offset = 0\n    while True:\n        for item in response:\n            yield item\n        if len(response) >= max_results:\n            break\n        offset += max_results\n        response = response[offset:]\n", "def paging(response, max_results):\n    if len(response) < max_results:\n        for item in response:\n            yield item\n    else:\n        for i in range(0, max_results):\n            yield response[i:min(len(response), max_results)]\n"], "level": "self_contained", "generate_results": [{"generate_code": "def paging(response, max_results):\n    page = 1\n    if max_results == 0:\n        return response\n\n    while len(response) > max_results:\n        for item in response[page * max_results:]:\n            yield item\n\n        page += 1\n        if page >= max_results:\n            break\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n    page = 0\n    page_size = 100\n    offset = 0\n\n    while offset < max_results:\n        results = response[offset: offset + page_size]\n        for result in results:\n            yield result\n\n        offset += page_size\n        page += 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\n    page = 1\n    while page <= max_results:\n        for response_page in response:\n            yield response_page\n        page += 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    if max_results <= 0:\n        raise ValueError('Maximum number of objects to be returned must be greater than zero')\n\n    for page in range(1, max_results + 1):\n        for page_size in range(1, page + 1):\n            yield response[page * page_size: (page + 1) * page_size]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    if len(response) <= max_results:\n        for result in response:\n            yield result\n    else:\n        return response[max_results:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    page = 1\n    page_size = 100\n    offset = 0\n    results = []\n\n    while offset < max_results:\n        results.extend(response[offset: (offset + page_size)])\n        offset += page_size\n\n        if len(results) >= max_results:\n            break\n\n    return results\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    for page in range(1, max_results + 1):\n        if page < 1:\n            continue\n\n        response_page = response[page - 1]\n        for item in response_page:\n            yield item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    offset = 0\n    while True:\n        for item in response:\n            yield item\n        if len(response) >= max_results:\n            break\n        offset += max_results\n        response = response[offset:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    if len(response) < max_results:\n        for item in response:\n            yield item\n    else:\n        for i in range(0, max_results):\n            yield response[i:min(len(response), max_results)]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_20.py", "name": "size_to_bytes", "docstring": "Convert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000", "ques_id": "62ece4982e6aefcf4aabbd60", "code": ["def size_to_bytes(size: str) -> int:\n    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n        if abs(int(size)) < 1024:\n            return int(size)\n        size /= 1024\n    return size\n", "def size_to_bytes(size: str) -> int:\n    for x in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if len(size) < 10:\n            return int(size)\n        elif len(size) < 100:\n            return int(round(float(size)))\n        else:\n            return int(round(float(size) / 1024 ** 2))\n", "def size_to_bytes(size: str) -> int:\n    for factor, suffix in FILE_SIZE_TO_BYTES_FACTORS.items():\n        if size.endswith(suffix):\n            return int(size.rstrip(suffix)) * factor\n    raise ValueError(\"Unknown size: {}\".format(size))\n", "def size_to_bytes(size: str) -> int:\n    multiplier = 1024.0\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(unit):\n            return int(float(size.rstrip(unit)) * multiplier)\n    return int(float(size.rstrip('B')) * multiplier)\n", "def size_to_bytes(size: str) -> int:\n    for x in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            size = size[:-len(x)]\n            return int(size)\n    # fallback to using ascii for other values\n    return int(round(float(size)))\n", "def size_to_bytes(size: str) -> int:\n    for factor, suffix in FILE_UNITS:\n        if size.endswith(suffix):\n            size = size[:-len(suffix)]\n            break\n    else:\n        raise ValueError(\"Unknown file unit for {}\".format(size))\n\n    try:\n        return int(size) * factor\n    except ValueError:\n        raise ValueError(\"Unable to convert {} to bytes\".format(size))\n", "def size_to_bytes(size: str) -> int:\n    # https://stackoverflow.com/questions/3694835/python-how-can-i-convert-human-readable-file-size-to-bytes\n    # https://stackoverflow.com/questions/18072969/how-can-i-convert-human-readable-file-size-to-bytes-in-python\n    value = int(size.replace(\"B\", \"\").replace(\"K\", \"\").replace(\"M\", \"\"))\n    for i in [\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\"]:\n        if value < 1024:\n            return value\n        value /= 1024\n    return value\n", "def size_to_bytes(size: str) -> int:\n    value = 0\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\"]:\n        value += float(size.count(unit))\n    return value\n", "def size_to_bytes(size: str) -> int:\n    for x in [\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if size.endswith(x):\n            size = size[:-len(x)]\n            return int(size)\n    # fallback for other cases\n    return int(size)\n", "def size_to_bytes(size: str) -> int:\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            size = size[:-len(x)]\n            return int(size)\n\n    raise ValueError(\"File size {} not in accepted format\".format(size))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def size_to_bytes(size: str) -> int:\n    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n        if abs(int(size)) < 1024:\n            return int(size)\n        size /= 1024\n    return size\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for x in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if len(size) < 10:\n            return int(size)\n        elif len(size) < 100:\n            return int(round(float(size)))\n        else:\n            return int(round(float(size) / 1024 ** 2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for factor, suffix in FILE_SIZE_TO_BYTES_FACTORS.items():\n        if size.endswith(suffix):\n            return int(size.rstrip(suffix)) * factor\n    raise ValueError(\"Unknown size: {}\".format(size))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    multiplier = 1024.0\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(unit):\n            return int(float(size.rstrip(unit)) * multiplier)\n    return int(float(size.rstrip('B')) * multiplier)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for x in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            size = size[:-len(x)]\n            return int(size)\n    # fallback to using ascii for other values\n    return int(round(float(size)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for factor, suffix in FILE_UNITS:\n        if size.endswith(suffix):\n            size = size[:-len(suffix)]\n            break\n    else:\n        raise ValueError(\"Unknown file unit for {}\".format(size))\n\n    try:\n        return int(size) * factor\n    except ValueError:\n        raise ValueError(\"Unable to convert {} to bytes\".format(size))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    # https://stackoverflow.com/questions/3694835/python-how-can-i-convert-human-readable-file-size-to-bytes\n    # https://stackoverflow.com/questions/18072969/how-can-i-convert-human-readable-file-size-to-bytes-in-python\n    value = int(size.replace(\"B\", \"\").replace(\"K\", \"\").replace(\"M\", \"\"))\n    for i in [\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\"]:\n        if value < 1024:\n            return value\n        value /= 1024\n    return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    value = 0\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\"]:\n        value += float(size.count(unit))\n    return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for x in [\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if size.endswith(x):\n            size = size[:-len(x)]\n            return int(size)\n    # fallback for other cases\n    return int(size)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            size = size[:-len(x)]\n            return int(size)\n\n    raise ValueError(\"File size {} not in accepted format\".format(size))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_14.py", "name": "_dictsum", "docstring": "Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}", "ques_id": "62ece4982e6aefcf4aabbd61", "code": ["def _dictsum(dicts):\n    return dict(sum(d.items(), ()) for d in dicts)\n", "def _dictsum(dicts):\n    return dict(reduce(dict.__add__, map(sum, dicts)))\n", "def _dictsum(dicts):\n    return dict(sum(d.items(), ()) for d in dicts)\n", "def _dictsum(dicts):\n    return dict(sum(dicts, {}))\n", "def _dictsum(dicts):\n    return dict(sum(dicts, {}))\n", "def _dictsum(dicts):\n    return dict(chain.from_iterable(dicts))\n", "def _dictsum(dicts):\n    return dict(sum((d.items() if isinstance(d, dict) else d), {})\n                for d in dicts)\n", "def _dictsum(dicts):\n    return dict(reduce(sum, dicts))\n", "def _dictsum(dicts):\n    return dict(reduce(operator.add, map(dict.items, dicts)))\n", "def _dictsum(dicts):\n    return dict(sum(dicts, {}))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _dictsum(dicts):\n    return dict(sum(d.items(), ()) for d in dicts)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(reduce(dict.__add__, map(sum, dicts)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(sum(d.items(), ()) for d in dicts)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(sum(dicts, {}))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(sum(dicts, {}))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(chain.from_iterable(dicts))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(sum((d.items() if isinstance(d, dict) else d), {})\n                for d in dicts)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(reduce(sum, dicts))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(reduce(operator.add, map(dict.items, dicts)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(sum(dicts, {}))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_15.py", "name": "_replace_url_args", "docstring": "Replace any custom string URL items with values in args", "ques_id": "62ece4982e6aefcf4aabbd62", "code": ["def _replace_url_args(url, url_args):\n\n    # Remove any query strings\n    url = url[:url.find('?')]\n\n    # Replace any variables with their values\n    url = url.replace('{', '').replace('}', '')\n\n    # Add any URL encoded query strings\n    for key, value in url_args.items():\n        url = url.replace('{{{0}}}'.format(key), value)\n\n    return url\n", "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        if isinstance(value, str):\n            url[key] = value.format(**url)\n        else:\n            url[key] = value\n    return url\n", "def _replace_url_args(url, url_args):\n    return re.sub(r'{{(.*?)}}', lambda m: url_args.get(m.group(1), m.group(0)), url)\n", "def _replace_url_args(url, url_args):\n    return url.format(**url_args)\n", "def _replace_url_args(url, url_args):\n    return dict(\n        (key, url_args[key]) for key in url_args if key in url\n    )\n", "def _replace_url_args(url, url_args):\n    return dict((key, url_args.get(key)) for key in url_args)\n", "def _replace_url_args(url, url_args):\n    return {\n        k: v.format(**url_args) if isinstance(v, str) else v\n        for k, v in url.items()\n    }\n", "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        url = url.replace('{{' + key + '}}', value)\n    return url\n", "def _replace_url_args(url, url_args):\n    return dict(\n        (url_args[k], v) for k, v in url.items()\n        if k in url_args\n    )\n", "def _replace_url_args(url, url_args):\n    return url.format(**url_args)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_url_args(url, url_args):\n\n    # Remove any query strings\n    url = url[:url.find('?')]\n\n    # Replace any variables with their values\n    url = url.replace('{', '').replace('}', '')\n\n    # Add any URL encoded query strings\n    for key, value in url_args.items():\n        url = url.replace('{{{0}}}'.format(key), value)\n\n    return url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        if isinstance(value, str):\n            url[key] = value.format(**url)\n        else:\n            url[key] = value\n    return url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return re.sub(r'{{(.*?)}}', lambda m: url_args.get(m.group(1), m.group(0)), url)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return url.format(**url_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return dict(\n        (key, url_args[key]) for key in url_args if key in url\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return dict((key, url_args.get(key)) for key in url_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return {\n        k: v.format(**url_args) if isinstance(v, str) else v\n        for k, v in url.items()\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        url = url.replace('{{' + key + '}}', value)\n    return url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return dict(\n        (url_args[k], v) for k, v in url.items()\n        if k in url_args\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return url.format(**url_args)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_5.py", "name": "is_none_string", "docstring": "Check if a string represents a None value.", "ques_id": "62ece4982e6aefcf4aabbd63", "code": ["def is_none_string(val: str) -> bool:\n    return val is None or (val.lower() == 'none' and 'none' in VALID_STRINGS)\n", "def is_none_string(val: Any) -> bool:\n    return val is None or isinstance(val, str) and val == 'None'\n", "def is_none_string(val: str) -> bool:\n    return val is None or val == \"\"\n", "def is_none_string(val: str) -> bool:\n    return val is None or val == ''\n", "def is_none_string(val: str) -> bool:\n    return val is None or val == ''\n", "def is_none_string(val: Any) -> bool:\n    return val is None or val == \"\"\n", "def is_none_string(val: str) -> bool:\n    return val in ('None', 'null', 'none')\n", "def is_none_string(val: str) -> bool:\n    return val is None or val == \"\"\n", "def is_none_string(val: str) -> bool:\n    return val in (\"\", None)\n", "def is_none_string(val: Any) -> bool:\n    return val is None or val == \"\"\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_none_string(val: str) -> bool:\n    return val is None or (val.lower() == 'none' and 'none' in VALID_STRINGS)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: Any) -> bool:\n    return val is None or isinstance(val, str) and val == 'None'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: str) -> bool:\n    return val is None or val == \"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: str) -> bool:\n    return val is None or val == ''\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: str) -> bool:\n    return val is None or val == ''\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: Any) -> bool:\n    return val is None or val == \"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: str) -> bool:\n    return val in ('None', 'null', 'none')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: str) -> bool:\n    return val is None or val == \"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: str) -> bool:\n    return val in (\"\", None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: Any) -> bool:\n    return val is None or val == \"\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_1.py", "name": "strip_root", "docstring": "Remove root from path, throw exception on failure.", "ques_id": "62ece4982e6aefcf4aabbd64", "code": ["def strip_root(path, root):\n    if root and path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(\n            'Path does not start with root \"{}\"'.format(root))\n", "def strip_root(path, root):\n    if root:\n        return path.replace(root, '')\n    else:\n        raise Exception('Cannot remove root from path.')\n", "def strip_root(path, root):\n    if path == root:\n        return path\n    tail = path.rsplit(root, 1)[-1]\n    if not tail:\n        raise ValueError('Invalid root path %s' % root)\n    return path[:-len(tail)]\n", "def strip_root(path, root):\n    if not path:\n        return path\n\n    if root and path.startswith(root):\n        return path[len(root):]\n\n    raise Exception(\n        'Path {path} does not have root {root}'.format(\n            path=path, root=root))\n", "def strip_root(path, root):\n    if root:\n        path = path[len(root):]\n    return path\n", "def strip_root(path, root):\n    if root and path == root:\n        return path\n    if not root:\n        raise Exception(\"'{}' is not root\".format(root))\n    if path == root:\n        return ''\n    return path.rstrip(root)\n", "def strip_root(path, root):\n    if path == root:\n        return path\n    if path.startswith(root + os.sep):\n        return path[len(root) + 1:]\n    raise Exception(\n        'Invalid path: \"{}\" in \"{}\"'.format(path, root)\n    )\n", "def strip_root(path, root):\n    if not path:\n        raise exception.InvalidParameter(\n            reason=_(\"path is empty\"))\n\n    if path == root:\n        raise exception.InvalidParameter(\n            reason=_(\"path is root\"))\n\n    if path.startswith(root + \"/\"):\n        return path[len(root) + 1:]\n\n    return path\n", "def strip_root(path, root):\n    if path == root:\n        return path\n    if path.startswith(root):\n        return path[len(root):]\n    return ''\n", "def strip_root(path, root):\n    if root:\n        path = '{}/{}'.format(path, root)\n    return path\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def strip_root(path, root):\n    if root and path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(\n            'Path does not start with root \"{}\"'.format(root))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if root:\n        return path.replace(root, '')\n    else:\n        raise Exception('Cannot remove root from path.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if path == root:\n        return path\n    tail = path.rsplit(root, 1)[-1]\n    if not tail:\n        raise ValueError('Invalid root path %s' % root)\n    return path[:-len(tail)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if not path:\n        return path\n\n    if root and path.startswith(root):\n        return path[len(root):]\n\n    raise Exception(\n        'Path {path} does not have root {root}'.format(\n            path=path, root=root))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if root:\n        path = path[len(root):]\n    return path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if root and path == root:\n        return path\n    if not root:\n        raise Exception(\"'{}' is not root\".format(root))\n    if path == root:\n        return ''\n    return path.rstrip(root)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if path == root:\n        return path\n    if path.startswith(root + os.sep):\n        return path[len(root) + 1:]\n    raise Exception(\n        'Invalid path: \"{}\" in \"{}\"'.format(path, root)\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if not path:\n        raise exception.InvalidParameter(\n            reason=_(\"path is empty\"))\n\n    if path == root:\n        raise exception.InvalidParameter(\n            reason=_(\"path is root\"))\n\n    if path.startswith(root + \"/\"):\n        return path[len(root) + 1:]\n\n    return path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if path == root:\n        return path\n    if path.startswith(root):\n        return path[len(root):]\n    return ''\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if root:\n        path = '{}/{}'.format(path, root)\n    return path\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_8.py", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62ece4982e6aefcf4aabbd65", "code": ["def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.default is not None:\n            flags +='' + arg.dest + '=' + str(arg.default)\n        else:\n            flags +='' + arg.dest\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.default is not None and arg.default!= argparse.SUPPRESS:\n            flags +='{}'.format(arg.dest)\n    return flags\n", "def parser_flags(parser):\n    return''.join([str(f) for f in parser._flags])\n", "def parser_flags(parser):\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreTrueAction):\n            flags.append('--%s' % action.dest)\n        elif isinstance(action, argparse._StoreFalseAction):\n            flags.append('--false')\n        elif isinstance(action, argparse._StoreProtectedAction):\n            flags.append('--protected')\n        elif isinstance(action, argparse._StoreUnprotectedAction):\n            flags.append('--unprotected')\n        elif isinstance(action, argparse._StoreAllActionsAction):\n            flags.append('--all')\n        elif isinstance(action, argparse._StoreArgumentAction):\n            flags.append('--%s' % action.dest)\n    return''.join(flags)\n", "def parser_flags(parser):\n    return''.join(\n        flag.description for flag in parser._get_args()\n    )\n", "def parser_flags(parser):\n    return''.join(parser._flags)\n", "def parser_flags(parser):\n    flags = []\n    for argument in parser._actions:\n        if argument.dest == '--help':\n            flags.append('-h')\n            break\n        if argument.dest == '--version':\n            flags.append('--version')\n            break\n        flags.append(argument.default)\n    return''.join(flags)\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +='' + arg.default +''\n        else:\n            flags +='' + arg.default +'' + arg.dest +''\n    return flags.strip()\n", "def parser_flags(parser):\n    return''.join([flag.__name__ for flag in parser._option_classes])\n", "def parser_flags(parser):\n    return''.join(str(f) for f in parser.flags)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.default is not None:\n            flags +='' + arg.dest + '=' + str(arg.default)\n        else:\n            flags +='' + arg.dest\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.default is not None and arg.default!= argparse.SUPPRESS:\n            flags +='{}'.format(arg.dest)\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join([str(f) for f in parser._flags])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreTrueAction):\n            flags.append('--%s' % action.dest)\n        elif isinstance(action, argparse._StoreFalseAction):\n            flags.append('--false')\n        elif isinstance(action, argparse._StoreProtectedAction):\n            flags.append('--protected')\n        elif isinstance(action, argparse._StoreUnprotectedAction):\n            flags.append('--unprotected')\n        elif isinstance(action, argparse._StoreAllActionsAction):\n            flags.append('--all')\n        elif isinstance(action, argparse._StoreArgumentAction):\n            flags.append('--%s' % action.dest)\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(\n        flag.description for flag in parser._get_args()\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(parser._flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for argument in parser._actions:\n        if argument.dest == '--help':\n            flags.append('-h')\n            break\n        if argument.dest == '--version':\n            flags.append('--version')\n            break\n        flags.append(argument.default)\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +='' + arg.default +''\n        else:\n            flags +='' + arg.default +'' + arg.dest +''\n    return flags.strip()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join([flag.__name__ for flag in parser._option_classes])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(str(f) for f in parser.flags)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_19.py", "name": "was_processed", "docstring": "Check if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.", "ques_id": "62ece4982e6aefcf4aabbd66", "code": ["def was_processed(processed, path_name, verbose):\n\n    if path_name in processed:\n        if verbose:\n            print('Issue in recursion in {}'.format(path_name))\n        return True\n\n    processed.add(path_name)\n    if verbose:\n        print('Added {} to the set'.format(path_name))\n    return False\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"[!] Issue processing pathname: {}\".format(path_name))\n        return False\n    processed.add(path_name)\n    if verbose:\n        print(\"[+] Entry processed pathname: {}\".format(path_name))\n    return True\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"Issue in recursive expansion on {}\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Adding {} to processed set\".format(path_name))\n        return False\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"Issue processing path '{}' in set '{}'.\".format(path_name, processed))\n        return False\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Added '{}' to set '{}'.\".format(path_name, processed))\n    return True\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"[!] Already processed: {}\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"[+] Adding {} to set\".format(path_name))\n    return False\n", "def was_processed(processed, path_name, verbose):\n\n    if path_name in processed:\n        if verbose:\n            print(\"Recursion detected. Entry already processed: '{}'\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n\n    return False\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"Path '{}' already processed.\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Adding '{}' to processed set.\".format(path_name))\n        return False\n", "def was_processed(processed, path_name, verbose):\n\n    if path_name in processed:\n        if verbose:\n            print(\"Issue in processing: {}\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Added: {}\".format(path_name))\n\n    return False\n", "def was_processed(processed, path_name, verbose):\n\n    if verbose:\n        print(\"Checking if path {} already processed\".format(path_name))\n\n    if path_name in processed:\n        if verbose:\n            print(\"Issue report for path {} already processed.\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Added path {} to processed set.\".format(path_name))\n        return False\n", "def was_processed(processed, path_name, verbose):\n\n    if path_name in processed:\n        if verbose:\n            print('Issue processing path {} already processed.'\n                 .format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n\n    return False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def was_processed(processed, path_name, verbose):\n\n    if path_name in processed:\n        if verbose:\n            print('Issue in recursion in {}'.format(path_name))\n        return True\n\n    processed.add(path_name)\n    if verbose:\n        print('Added {} to the set'.format(path_name))\n    return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"[!] Issue processing pathname: {}\".format(path_name))\n        return False\n    processed.add(path_name)\n    if verbose:\n        print(\"[+] Entry processed pathname: {}\".format(path_name))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"Issue in recursive expansion on {}\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Adding {} to processed set\".format(path_name))\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"Issue processing path '{}' in set '{}'.\".format(path_name, processed))\n        return False\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Added '{}' to set '{}'.\".format(path_name, processed))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"[!] Already processed: {}\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"[+] Adding {} to set\".format(path_name))\n    return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\n    if path_name in processed:\n        if verbose:\n            print(\"Recursion detected. Entry already processed: '{}'\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n\n    return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"Path '{}' already processed.\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Adding '{}' to processed set.\".format(path_name))\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\n    if path_name in processed:\n        if verbose:\n            print(\"Issue in processing: {}\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Added: {}\".format(path_name))\n\n    return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\n    if verbose:\n        print(\"Checking if path {} already processed\".format(path_name))\n\n    if path_name in processed:\n        if verbose:\n            print(\"Issue report for path {} already processed.\".format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Added path {} to processed set.\".format(path_name))\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\n    if path_name in processed:\n        if verbose:\n            print('Issue processing path {} already processed.'\n                 .format(path_name))\n        return True\n    else:\n        processed.add(path_name)\n\n    return False\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_9.py", "name": "vertex3tuple", "docstring": "return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices", "ques_id": "62ece4982e6aefcf4aabbd67", "code": ["def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        raise ValueError(\"vertices must contain at least 3 coordinates\")\n\n    if len(vertices) == 3:\n        return vertices\n\n    a, b, c = vertices\n\n    return tuple([(a, b, c), (b, c, a), (c, a, b)])\n", "def vertex3tuple(vertices):\n\n    return tuple([(vertices[0], vertices[2], vertices[1]),\n                  (vertices[2], vertices[0], vertices[1]),\n                  (vertices[1], vertices[2], vertices[0])])\n", "def vertex3tuple(vertices):\n    return tuple([vertices[0], vertices[1], vertices[2]])\n", "def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        return None\n\n    vertices = list(vertices)\n    if len(vertices) == 2:\n        return (vertices[0], vertices[1], vertices[0])\n\n    abcd = []\n    bcd = []\n    cda = []\n\n    # vertices = vertices[0:2]\n    for i in range(2, len(vertices)):\n        dx = vertices[i][0] - vertices[i - 1][0]\n        dy = vertices[i][1] - vertices[i - 1][1]\n        dz = vertices[i][2] - vertices[i - 1][2]\n\n        abcd.append(dx)\n        bcd.append(dy)\n        cda.append(dz)\n\n    abcd.reverse()\n    bcd.reverse()\n    cda.reverse()\n\n    return tuple([tuple(v) for v in zip(abcd, bcd, cda)])\n", "def vertex3tuple(vertices):\n\n    return tuple(v3 + v2 for v1, v2 in it.combinations(vertices, 2))\n", "def vertex3tuple(vertices):\n\n    return tuple([vertices[0], vertices[1], vertices[2]])\n", "def vertex3tuple(vertices):\n\n    A = vertices[0]\n    B = vertices[1]\n    C = vertices[2]\n\n    ABCD = A + C\n    CDAB = B + C\n    CABC = A + B\n    CABC = B + A\n\n    return (CABC, ABCD, CDAB)\n", "def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        return None\n\n    abcd = vertices[0]\n    bcd = vertices[1]\n    abd = vertices[2]\n\n    return ((abcd[0], abd[1], bcd[1]),\n            (abcd[1], abd[0], bcd[0]),\n            (bcd[1], abd[0], bcd[0]),\n            (bcd[0], bcd[1], abd[1]),\n            (abd[0], abd[1], bcd[1]),\n            (abd[0], bcd[0], abcd[1]))\n", "def vertex3tuple(vertices):\n\n    return tuple(sorted((vertices[0], vertices[1], vertices[2])))\n", "def vertex3tuple(vertices):\n\n    return tuple([vertices[0], vertices[1], vertices[2]])\n"], "level": "self_contained", "generate_results": [{"generate_code": "def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        raise ValueError(\"vertices must contain at least 3 coordinates\")\n\n    if len(vertices) == 3:\n        return vertices\n\n    a, b, c = vertices\n\n    return tuple([(a, b, c), (b, c, a), (c, a, b)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    return tuple([(vertices[0], vertices[2], vertices[1]),\n                  (vertices[2], vertices[0], vertices[1]),\n                  (vertices[1], vertices[2], vertices[0])])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n    return tuple([vertices[0], vertices[1], vertices[2]])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        return None\n\n    vertices = list(vertices)\n    if len(vertices) == 2:\n        return (vertices[0], vertices[1], vertices[0])\n\n    abcd = []\n    bcd = []\n    cda = []\n\n    # vertices = vertices[0:2]\n    for i in range(2, len(vertices)):\n        dx = vertices[i][0] - vertices[i - 1][0]\n        dy = vertices[i][1] - vertices[i - 1][1]\n        dz = vertices[i][2] - vertices[i - 1][2]\n\n        abcd.append(dx)\n        bcd.append(dy)\n        cda.append(dz)\n\n    abcd.reverse()\n    bcd.reverse()\n    cda.reverse()\n\n    return tuple([tuple(v) for v in zip(abcd, bcd, cda)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    return tuple(v3 + v2 for v1, v2 in it.combinations(vertices, 2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    return tuple([vertices[0], vertices[1], vertices[2]])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    A = vertices[0]\n    B = vertices[1]\n    C = vertices[2]\n\n    ABCD = A + C\n    CDAB = B + C\n    CABC = A + B\n    CABC = B + A\n\n    return (CABC, ABCD, CDAB)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        return None\n\n    abcd = vertices[0]\n    bcd = vertices[1]\n    abd = vertices[2]\n\n    return ((abcd[0], abd[1], bcd[1]),\n            (abcd[1], abd[0], bcd[0]),\n            (bcd[1], abd[0], bcd[0]),\n            (bcd[0], bcd[1], abd[1]),\n            (abd[0], abd[1], bcd[1]),\n            (abd[0], bcd[0], abcd[1]))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    return tuple(sorted((vertices[0], vertices[1], vertices[2])))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    return tuple([vertices[0], vertices[1], vertices[2]])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_6.py", "name": "int_to_string", "docstring": "Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.", "ques_id": "62ece4982e6aefcf4aabbd68", "code": ["def int_to_string(number: int, alphabet: List[str]) -> str:\n    s = []\n    base = len(alphabet)\n\n    while number > 0:\n        s.append(alphabet[number % base])\n        number //= base\n    s.reverse()\n    return \"\".join(s)\n", "def int_to_string(number: int, alphabet: str) -> str:\n    if number == 0:\n        return alphabet[0]\n    arr = []\n    base = len(alphabet)\n    while number!= 0:\n        number, rem = divmod(number, base)\n        arr.append(alphabet[rem])\n    arr.reverse()\n    return ''.join(arr)\n", "def int_to_string(number: int, alphabet: List[str]) -> str:\n    s = ''\n    base = len(alphabet)\n    while number:\n        s = alphabet[number % base] + s\n        number //= base\n    return s\n", "def int_to_string(number: int, alphabet: str) -> str:\n    s = ''\n    base = len(alphabet)\n    while number!= 0:\n        remainder = number % base\n        s = alphabet[remainder] + s\n        number = int(number // base)\n    return s\n", "def int_to_string(number: int, alphabet: str) -> str:\n    base = len(alphabet)\n    str_number = \"{:0\" + str(base) + \"d}\"\n    return str_number.format(number)\n", "def int_to_string(number: int, alphabet: str) -> str:\n    s = \"\"\n    while number:\n        number, remainder = divmod(number, len(alphabet))\n        s = alphabet[remainder] + s\n    return s\n", "def int_to_string(number: int, alphabet: str) -> str:\n    if number == 0:\n        return '0'\n    arr = []\n    base = len(alphabet)\n    while number:\n        rem = number % base\n        arr.append(alphabet[rem])\n        number //= base\n    arr.reverse()\n    return ''.join(arr)\n", "def int_to_string(number: int, alphabet: str) -> str:\n    if number == 0:\n        return '0'\n    arr = []\n    base = len(alphabet)\n    while number:\n        number, remainder = divmod(number, base)\n        arr.append(alphabet[remainder])\n    arr.reverse()\n    return ''.join(arr)\n", "def int_to_string(number: int, alphabet: str) -> str:\n    if number < 0:\n        raise ValueError(\"Cannot convert negative numbers to string\")\n\n    if number == 0:\n        return alphabet[0]\n\n    s = []\n    base = len(alphabet)\n    while number:\n        number, remainder = divmod(number, base)\n        s.append(alphabet[remainder])\n    return ''.join(reversed(s))\n", "def int_to_string(number: int, alphabet: str) -> str:\n    result = ''\n    base = len(alphabet)\n\n    while number > 0:\n        result = alphabet[number % base] + result\n        number = number // base\n\n    return result\n"], "level": "self_contained", "generate_results": [{"generate_code": "def int_to_string(number: int, alphabet: List[str]) -> str:\n    s = []\n    base = len(alphabet)\n\n    while number > 0:\n        s.append(alphabet[number % base])\n        number //= base\n    s.reverse()\n    return \"\".join(s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: str) -> str:\n    if number == 0:\n        return alphabet[0]\n    arr = []\n    base = len(alphabet)\n    while number!= 0:\n        number, rem = divmod(number, base)\n        arr.append(alphabet[rem])\n    arr.reverse()\n    return ''.join(arr)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str]) -> str:\n    s = ''\n    base = len(alphabet)\n    while number:\n        s = alphabet[number % base] + s\n        number //= base\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: str) -> str:\n    s = ''\n    base = len(alphabet)\n    while number!= 0:\n        remainder = number % base\n        s = alphabet[remainder] + s\n        number = int(number // base)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: str) -> str:\n    base = len(alphabet)\n    str_number = \"{:0\" + str(base) + \"d}\"\n    return str_number.format(number)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: str) -> str:\n    s = \"\"\n    while number:\n        number, remainder = divmod(number, len(alphabet))\n        s = alphabet[remainder] + s\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: str) -> str:\n    if number == 0:\n        return '0'\n    arr = []\n    base = len(alphabet)\n    while number:\n        rem = number % base\n        arr.append(alphabet[rem])\n        number //= base\n    arr.reverse()\n    return ''.join(arr)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: str) -> str:\n    if number == 0:\n        return '0'\n    arr = []\n    base = len(alphabet)\n    while number:\n        number, remainder = divmod(number, base)\n        arr.append(alphabet[remainder])\n    arr.reverse()\n    return ''.join(arr)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: str) -> str:\n    if number < 0:\n        raise ValueError(\"Cannot convert negative numbers to string\")\n\n    if number == 0:\n        return alphabet[0]\n\n    s = []\n    base = len(alphabet)\n    while number:\n        number, remainder = divmod(number, base)\n        s.append(alphabet[remainder])\n    return ''.join(reversed(s))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: str) -> str:\n    result = ''\n    base = len(alphabet)\n\n    while number > 0:\n        result = alphabet[number % base] + result\n        number = number // base\n\n    return result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_12.py", "name": "_replace_register", "docstring": "Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number", "ques_id": "62ece4982e6aefcf4aabbd69", "code": ["def _replace_register(flow_params, register_number, register_value):\n    try:\n        flow_params['registers'][register_number] = flow_params['registers'][register_number].replace(\n            '{register_value}', str(register_value))\n    except KeyError as ex:\n        raise KeyError(\"'registers' key not found in given flow params: {}\".format(str(ex)))\n", "def _replace_register(flow_params, register_number, register_value):\n    for key, val in flow_params.items():\n        if val['reg'] == register_number:\n            flow_params[key]['value'] = register_value\n", "def _replace_register(flow_params, register_number, register_value):\n    for key, value in flow_params.items():\n        if key == 'input_value':\n            flow_params[key] = value\n            continue\n        if key == 'output_value':\n            flow_params[key] = value\n            continue\n        if key =='source_address':\n            flow_params[key] = value\n            continue\n        if key == 'destination_address':\n            flow_params[key] = value\n            continue\n        if key =='max_value':\n            flow_params[key] = value\n            continue\n        if key =='min_value':\n            flow_params[key] = value\n            continue\n        if key == 'input_data':\n            flow_params[key] = value\n            continue\n        if key == 'output_data':\n        ", "def _replace_register(flow_params, register_number, register_value):\n    if'registers' in flow_params.keys():\n        flow_params['registers'][register_number] = flow_params['registers'][register_value]\n    else:\n        flow_params['registers'] = {register_number: flow_params['registers'][register_value]}\n", "def _replace_register(flow_params, register_number, register_value):\n    for k, v in flow_params.items():\n        if v[\"register_number\"] == register_number:\n            v[\"register_value\"] = register_value\n", "def _replace_register(flow_params, register_number, register_value):\n    if register_number not in flow_params:\n        flow_params[register_number] = {}\n    flow_params[register_number][flow_params['command']] = register_value\n", "def _replace_register(flow_params, register_number, register_value):\n    if'registers' in flow_params:\n        flow_params['registers'][register_number] = flow_params['registers'][register_value]\n    else:\n        flow_params['registers'] = {register_number: flow_params['registers'][register_value]}\n", "def _replace_register(flow_params, register_number, register_value):\n    if register_value not in flow_params['registers'][register_number]:\n        raise ValueError(\"Value {} is not a register number\".format(register_value))\n\n    flow_params['registers'][register_number].remove(register_value)\n    flow_params['registers'][register_number].append(register_value)\n", "def _replace_register(flow_params, register_number, register_value):\n    for flow in flow_params['flows']:\n        if flow['reg_num'] == register_number:\n            flow['reg_value'] = register_value\n", "def _replace_register(flow_params, register_number, register_value):\n    for key, value in flow_params.items():\n        if key == \"registers\":\n            if value == register_number:\n                flow_params[key] = register_value\n        elif key == \"input_value\":\n            if value == register_value:\n                flow_params[key] = flow_params[\"input_value\"]\n        elif key == \"output_value\":\n            if value == register_value:\n                flow_params[key] = flow_params[\"output_value\"]\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    try:\n        flow_params['registers'][register_number] = flow_params['registers'][register_number].replace(\n            '{register_value}', str(register_value))\n    except KeyError as ex:\n        raise KeyError(\"'registers' key not found in given flow params: {}\".format(str(ex)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for key, val in flow_params.items():\n        if val['reg'] == register_number:\n            flow_params[key]['value'] = register_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for key, value in flow_params.items():\n        if key == 'input_value':\n            flow_params[key] = value\n            continue\n        if key == 'output_value':\n            flow_params[key] = value\n            continue\n        if key =='source_address':\n            flow_params[key] = value\n            continue\n        if key == 'destination_address':\n            flow_params[key] = value\n            continue\n        if key =='max_value':\n            flow_params[key] = value\n            continue\n        if key =='min_value':\n            flow_params[key] = value\n            continue\n        if key == 'input_data':\n            flow_params[key] = value\n            continue\n        if key == 'output_data':\n        ", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    if'registers' in flow_params.keys():\n        flow_params['registers'][register_number] = flow_params['registers'][register_value]\n    else:\n        flow_params['registers'] = {register_number: flow_params['registers'][register_value]}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for k, v in flow_params.items():\n        if v[\"register_number\"] == register_number:\n            v[\"register_value\"] = register_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    if register_number not in flow_params:\n        flow_params[register_number] = {}\n    flow_params[register_number][flow_params['command']] = register_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    if'registers' in flow_params:\n        flow_params['registers'][register_number] = flow_params['registers'][register_value]\n    else:\n        flow_params['registers'] = {register_number: flow_params['registers'][register_value]}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    if register_value not in flow_params['registers'][register_number]:\n        raise ValueError(\"Value {} is not a register number\".format(register_value))\n\n    flow_params['registers'][register_number].remove(register_value)\n    flow_params['registers'][register_number].append(register_value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for flow in flow_params['flows']:\n        if flow['reg_num'] == register_number:\n            flow['reg_value'] = register_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for key, value in flow_params.items():\n        if key == \"registers\":\n            if value == register_number:\n                flow_params[key] = register_value\n        elif key == \"input_value\":\n            if value == register_value:\n                flow_params[key] = flow_params[\"input_value\"]\n        elif key == \"output_value\":\n            if value == register_value:\n                flow_params[key] = flow_params[\"output_value\"]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_2.py", "name": "replace_dots", "docstring": "Replaces all values of '.' to arg from the given string", "ques_id": "62ece4982e6aefcf4aabbd6a", "code": ["def replace_dots(value, arg):\n    return value.replace(\".\", arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def replace_dots(value, arg):\n    return value.replace(\".\", arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_3.py", "name": "subclasses", "docstring": "Return all subclasses of a class, recursively", "ques_id": "62ece4982e6aefcf4aabbd6b", "code": ["def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__()\n                                   for g in subclasses(s)]\n", "def subclasses(cls):\n    return [subcls for subcls in cls.__subclasses__()\n            if issubclass(subcls, cls)]\n", "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "def subclasses(cls):\n    return [subclass for subclass in cls.__subclasses__() if issubclass(cls, subclass)]\n", "def subclasses(cls):\n    return [subclass for subclass in cls.__subclasses__() if issubclass(cls, subclass)]\n", "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "def subclasses(cls):\n    return [\n        x for x in cls.__subclasses__()\n        if issubclass(x, cls)\n    ]\n", "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "def subclasses(cls):\n    all_subclasses = []\n    for subclass in cls.__subclasses__():\n        all_subclasses.append(subclass)\n        all_subclasses.extend(subclass.subclasses())\n    return all_subclasses\n", "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n"], "level": "self_contained", "generate_results": [{"generate_code": "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__()\n                                   for g in subclasses(s)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    return [subcls for subcls in cls.__subclasses__()\n            if issubclass(subcls, cls)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    return [subclass for subclass in cls.__subclasses__() if issubclass(cls, subclass)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    return [subclass for subclass in cls.__subclasses__() if issubclass(cls, subclass)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    return [\n        x for x in cls.__subclasses__()\n        if issubclass(x, cls)\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    all_subclasses = []\n    for subclass in cls.__subclasses__():\n        all_subclasses.append(subclass)\n        all_subclasses.extend(subclass.subclasses())\n    return all_subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_7.py", "name": "string_to_int", "docstring": "Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.", "ques_id": "62ece4982e6aefcf4aabbd6d", "code": ["def string_to_int(string: str, alphabet: str) -> int:\n    numeric_value = 0\n    for c in string:\n        numeric_value *= len(alphabet)\n        numeric_value += alphabet.index(c)\n    return numeric_value\n", "def string_to_int(string: str, alphabet: List[str] = None) -> int:\n    if alphabet is None:\n        alphabet = get_alphabet()\n    if len(string) == 1:\n        return alphabet[0].index(string)\n    return sum([alphabet.index(c) for c in string]) - 1\n", "def string_to_int(string: str, alphabet: str) -> int:\n    return sum((alphabet.index(x) for x in string))\n", "def string_to_int(string: str, alphabet: str) -> int:\n    if not alphabet:\n        raise ValueError('Alphabet cannot be empty')\n\n    result = 0\n    for char in string:\n        result = alphabet.index(char) * len(alphabet) + result\n    return result\n", "def string_to_int(string: str, alphabet: str) -> int:\n    return sum([alphabet.index(char) for char in string])\n", "def string_to_int(string: str, alphabet: str) -> int:\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "def string_to_int(string: str, alphabet: str) -> int:\n    if not all([isinstance(s, str) for s in [string, alphabet]]):\n        raise TypeError(\"input string must be of type str and have the most \"\n                        \"significant digits first\")\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "def string_to_int(string: str, alphabet: str = '0123456789') -> int:\n    out = 0\n    for c in string:\n        out *= len(alphabet)\n        out += alphabet.index(c)\n    return out\n", "def string_to_int(string: str, alphabet: List[str] = None) -> int:\n    if alphabet is None:\n        alphabet = list(string)\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "def string_to_int(string: str, alphabet: str) -> int:\n    base = len(alphabet)\n    converted_str = 0\n\n    for char in string:\n        converted_str += alphabet.index(char) * (base ** (len(alphabet) - 1))\n\n    return converted_str\n"], "level": "self_contained", "generate_results": [{"generate_code": "def string_to_int(string: str, alphabet: str) -> int:\n    numeric_value = 0\n    for c in string:\n        numeric_value *= len(alphabet)\n        numeric_value += alphabet.index(c)\n    return numeric_value\n", "is_pass": true, "return_code": 0}, {"generate_code": "def string_to_int(string: str, alphabet: List[str] = None) -> int:\n    if alphabet is None:\n        alphabet = get_alphabet()\n    if len(string) == 1:\n        return alphabet[0].index(string)\n    return sum([alphabet.index(c) for c in string]) - 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: str) -> int:\n    return sum((alphabet.index(x) for x in string))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def string_to_int(string: str, alphabet: str) -> int:\n    if not alphabet:\n        raise ValueError('Alphabet cannot be empty')\n\n    result = 0\n    for char in string:\n        result = alphabet.index(char) * len(alphabet) + result\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: str) -> int:\n    return sum([alphabet.index(char) for char in string])\n", "is_pass": true, "return_code": 0}, {"generate_code": "def string_to_int(string: str, alphabet: str) -> int:\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def string_to_int(string: str, alphabet: str) -> int:\n    if not all([isinstance(s, str) for s in [string, alphabet]]):\n        raise TypeError(\"input string must be of type str and have the most \"\n                        \"significant digits first\")\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: str = '0123456789') -> int:\n    out = 0\n    for c in string:\n        out *= len(alphabet)\n        out += alphabet.index(c)\n    return out\n", "is_pass": true, "return_code": 0}, {"generate_code": "def string_to_int(string: str, alphabet: List[str] = None) -> int:\n    if alphabet is None:\n        alphabet = list(string)\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: str) -> int:\n    base = len(alphabet)\n    converted_str = 0\n\n    for char in string:\n        converted_str += alphabet.index(char) * (base ** (len(alphabet) - 1))\n\n    return converted_str\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_24.py", "name": "get_repo_archive", "docstring": "Given an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.", "ques_id": "62ece4982e6aefcf4aabbd6f", "code": ["def get_repo_archive(url: str, destination_path: Path) -> Path:\n    if not destination_path.is_dir():\n        destination_path.mkdir()\n\n    archive_name = destination_path / \"desc\"\n    if not archive_name.is_file():\n        req = requests.get(url, stream=True)\n        with archive_name.open(\"wb\") as f:\n            for chunk in req.iter_content(chunk_size=1024):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n\n    return destination_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    with tarfile.open(str(destination_path), \"r:gz\") as tar:\n        tar.extractall(path=str(destination_path))\n\n    return destination_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    # TODO: Add unique name to archive path to prevent multiple repos\n    #       from being downloaded at the same time.\n    archive_path = destination_path / Path(url.split(\"/\")[-1])\n    if not archive_path.exists():\n        response = requests.get(url)\n        archive_path.mkdir()\n        with archive_path.open(\"wb\") as f:\n            f.write(response.content)\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_name = url.split('/')[-1]\n    if not destination_path.exists():\n        destination_path.mkdir()\n    archive_path = destination_path / archive_name\n    response = requests.get(url)\n    if response.status_code!= 200:\n        raise Exception('Status code is {}, but response is {}'.format(\n            response.status_code, url\n        ))\n    with archive_path.open('wb') as f:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    # TODO: when https://github.com/scivision/scivision-python/issues/23 is\n    #       merged, we should make sure that the archive has a.tar.gz extension.\n    res = requests.get(url, allow_redirects=False, stream=True)\n    archive_path = destination_path / Path(res.url.split(\"/\")[-1])\n    archive_path.parent.mkdir(exist_ok=True, parents=True)\n    with archive_path.open(\"wb\") as f:\n        for chunk in res.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    # TODO: test on windows\n    if not destination_path.exists():\n        destination_path.mkdir()\n    archive_name = destination_path / f\"{url.split('/')[-1]}.tar.gz\"\n    with urlopen(url) as response:\n        with archive_name.open(\"wb\") as out_file:\n            shutil.copyfileobj(response, out_file)\n    return destination_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    r = requests.get(url, allow_redirects=True)\n    if r.status_code!= 200:\n        raise Exception('Error getting repo archive.')\n    archive_path = destination_path / f'{url.split(\"/\")[-1]}.tar.gz'\n    archive_path.parent.mkdir(exist_ok=True)\n    with archive_path.open('wb') as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: str) -> Path:\n    archive_path = destination_path / \"desc\"\n    if not archive_path.exists():\n        logging.debug(f\"Downloading package descriptions from {url}\")\n        r = requests.get(url)\n        archive_path.mkdir()\n        with archive_path.open(\"wb\") as f:\n            f.write(r.content)\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_name = url.split('/')[-1]\n    repo_path = destination_path / archive_name\n\n    if not repo_path.exists():\n        response = requests.get(url)\n        if response.status_code == 404:\n            raise NotFoundError(\"No such archive\")\n\n        repo_path.mkdir()\n        with repo_path.open('wb') as f:\n            f.write(response.content)\n\n    return repo_path\n", "def get_repo_archive(url: str, destination_path: str) -> Path:\n    response = requests.get(url, stream=True)\n    with tarfile.open(fileobj=response.raw, mode='r:gz') as archive:\n        archive.extractall(destination_path)\n    return destination_path\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    if not destination_path.is_dir():\n        destination_path.mkdir()\n\n    archive_name = destination_path / \"desc\"\n    if not archive_name.is_file():\n        req = requests.get(url, stream=True)\n        with archive_name.open(\"wb\") as f:\n            for chunk in req.iter_content(chunk_size=1024):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n\n    return destination_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    with tarfile.open(str(destination_path), \"r:gz\") as tar:\n        tar.extractall(path=str(destination_path))\n\n    return destination_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    # TODO: Add unique name to archive path to prevent multiple repos\n    #       from being downloaded at the same time.\n    archive_path = destination_path / Path(url.split(\"/\")[-1])\n    if not archive_path.exists():\n        response = requests.get(url)\n        archive_path.mkdir()\n        with archive_path.open(\"wb\") as f:\n            f.write(response.content)\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_name = url.split('/')[-1]\n    if not destination_path.exists():\n        destination_path.mkdir()\n    archive_path = destination_path / archive_name\n    response = requests.get(url)\n    if response.status_code!= 200:\n        raise Exception('Status code is {}, but response is {}'.format(\n            response.status_code, url\n        ))\n    with archive_path.open('wb') as f:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    # TODO: when https://github.com/scivision/scivision-python/issues/23 is\n    #       merged, we should make sure that the archive has a.tar.gz extension.\n    res = requests.get(url, allow_redirects=False, stream=True)\n    archive_path = destination_path / Path(res.url.split(\"/\")[-1])\n    archive_path.parent.mkdir(exist_ok=True, parents=True)\n    with archive_path.open(\"wb\") as f:\n        for chunk in res.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    # TODO: test on windows\n    if not destination_path.exists():\n        destination_path.mkdir()\n    archive_name = destination_path / f\"{url.split('/')[-1]}.tar.gz\"\n    with urlopen(url) as response:\n        with archive_name.open(\"wb\") as out_file:\n            shutil.copyfileobj(response, out_file)\n    return destination_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    r = requests.get(url, allow_redirects=True)\n    if r.status_code!= 200:\n        raise Exception('Error getting repo archive.')\n    archive_path = destination_path / f'{url.split(\"/\")[-1]}.tar.gz'\n    archive_path.parent.mkdir(exist_ok=True)\n    with archive_path.open('wb') as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: str) -> Path:\n    archive_path = destination_path / \"desc\"\n    if not archive_path.exists():\n        logging.debug(f\"Downloading package descriptions from {url}\")\n        r = requests.get(url)\n        archive_path.mkdir()\n        with archive_path.open(\"wb\") as f:\n            f.write(r.content)\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_name = url.split('/')[-1]\n    repo_path = destination_path / archive_name\n\n    if not repo_path.exists():\n        response = requests.get(url)\n        if response.status_code == 404:\n            raise NotFoundError(\"No such archive\")\n\n        repo_path.mkdir()\n        with repo_path.open('wb') as f:\n            f.write(response.content)\n\n    return repo_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: str) -> Path:\n    response = requests.get(url, stream=True)\n    with tarfile.open(fileobj=response.raw, mode='r:gz') as archive:\n        archive.extractall(destination_path)\n    return destination_path\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_20.py", "name": "os_is_mac", "docstring": "Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool", "ques_id": "62ece4982e6aefcf4aabbd70", "code": ["def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n", "def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n", "def os_is_mac():\n    return sys.platform.startswith('darwin')\n", "def os_is_mac():\n\n    return sys.platform.startswith('darwin')\n", "def os_is_mac():\n\n    return sys.platform.startswith(\"darwin\")\n", "def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n", "def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "def os_is_mac():\n    return sys.platform.startswith('darwin')\n", "def os_is_mac():\n    return sys.platform.startswith('darwin')\n", "def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n    return sys.platform.startswith('darwin')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\n    return sys.platform.startswith('darwin')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\n    return sys.platform.startswith(\"darwin\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n    return sys.platform.startswith('darwin')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n    return sys.platform.startswith('darwin')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_14.py", "name": "regex_dict", "docstring": "Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes", "ques_id": "62ece4982e6aefcf4aabbd71", "code": ["def regex_dict(item):\n    return {k.replace('*.', '.*'): v for k, v in item.items()}\n", "def regex_dict(item):\n    return {k.replace('*.', '.*'): v for k, v in item.items()}\n", "def regex_dict(item):\n    converted = {}\n    for key, value in item.items():\n        wildcard_key = '*.' + key\n        converted[wildcard_key] = value\n    return converted\n", "def regex_dict(item):\n    return {re.escape(key): value for key, value in item.items()}\n", "def regex_dict(item):\n    return dict((k.replace('*.', '.*'), v) for k, v in item.items())\n", "def regex_dict(item):\n    return {re.sub(wildcard_match, '[^\\\\w]+', key): value\n            for key, value in item.items()}\n", "def regex_dict(item):\n    return {k: re.compile(v) for k, v in item.items()}\n", "def regex_dict(item):\n    return {k: re.compile(v) for k, v in item.items()}\n", "def regex_dict(item):\n    return {k: re.compile(v) for k, v in item.items()}\n", "def regex_dict(item):\n    return {re.sub(r'\\.cpp$', '', k): v for k, v in item.items()}\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def regex_dict(item):\n    return {k.replace('*.', '.*'): v for k, v in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {k.replace('*.', '.*'): v for k, v in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    converted = {}\n    for key, value in item.items():\n        wildcard_key = '*.' + key\n        converted[wildcard_key] = value\n    return converted\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {re.escape(key): value for key, value in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return dict((k.replace('*.', '.*'), v) for k, v in item.items())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {re.sub(wildcard_match, '[^\\\\w]+', key): value\n            for key, value in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {k: re.compile(v) for k, v in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {k: re.compile(v) for k, v in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {k: re.compile(v) for k, v in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {re.sub(r'\\.cpp$', '', k): v for k, v in item.items()}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_4.py", "name": "unquote", "docstring": "Remove quote from the given name.", "ques_id": "62ece4982e6aefcf4aabbd72", "code": ["def unquote(name):\n    if name[0] in \"\"\"\\'\"\"\":\n        return name[1:-1]\n    return name[1:-1].replace('\\\\\"', '\"')\n", "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name and name[0] in '\"\\'':\n        return name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == \"'\" and name[1:2]!= \"'\":\n        return name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def unquote(name):\n    if name[0] in \"\"\"\\'\"\"\":\n        return name[1:-1]\n    return name[1:-1].replace('\\\\\"', '\"')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name and name[0] in '\"\\'':\n        return name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == \"'\" and name[1:2]!= \"'\":\n        return name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_21.py", "name": "split", "docstring": "Multi-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)", "ques_id": "62ece4982e6aefcf4aabbd73", "code": ["def split(s, platform='this'):\n    if platform == 'posix':\n        pattern = r'(\\\\w)\\s+(\\d)'\n    elif platform == 'windows':\n        pattern = r'([A-Z])\\s+(\\d)'\n    else:\n        pattern = r'([a-z])\\s+(\\d)'\n    return re.split(pattern, s)\n", "def split(s, platform='this'):\n    if platform not in ('this', 1, 0):\n        raise ValueError('platform must be one of (\"this\", 1, 0)')\n    if platform == 1:\n        s = re.sub(r'(\\\\(?!^)', r'\\\\\\1', s)\n    elif platform == 0:\n        s = re.sub(r'(\\\\)', r'\\\\\\0', s)\n    return shlex.split(s)\n", "def split(s, platform='this'):\n    if platform == 'this':\n        pattern = r'([\\w\\-]+)'\n    elif platform == 'posix':\n        pattern = r'([\\w\\-]+)'\n    elif platform == 'cmd':\n        pattern = r'([\\w\\-]+)'\n    else:\n        raise ValueError('unknown platform: %r' % platform)\n\n    return re.split(pattern, s)\n", "def split(s, platform='this'):\n    if platform == '':\n        platform = os.name\n    if platform == 'nt':\n        s = s.replace('\\\\', '\\\\\\\\\\\\')\n        s = re.sub(r'([:/])', r'\\1', s)\n    lex = shlex.shlex(s, posix=True)\n    lex.whitespace += '.,\\t\\n'\n    lex.whitespace_split = True\n    lex.whitespace = ''\n    lex.wordchars += '/.+-():'\n\n    result = []\n    buf = ''\n    for tok in lex:\n        if tok.isalnum():\n            buf += tok\n        else:\n            result.append(buf)\n            buf = ''\n    if buf:\n        result.append(buf)\n    return result\n", "def split(s, platform='this'):\n\n    if platform == 'this':\n        return shlex.split(s)\n    if platform == 'posix':\n        if ';' in s:\n            return shlex.split(s)\n        else:\n            return [s]\n    if platform == 'cmd':\n        if ';' in s:\n            return shlex.split(s.replace(';',''))\n        else:\n            return [s]\n    raise ValueError('invalid platform specified: %r' % platform)\n", "def split(s, platform='this'):\n\n    if platform == 'c':\n        return shlex.split(s)\n    elif platform == 'posix':\n        import posix\n        return posix.split(s)\n    elif platform == 'cmd':\n        import subprocess\n        return [s for s in subprocess.Popen(['which', 'cmd'],\n                                            stdin=subprocess.PIPE,\n                                            stdout=subprocess.PIPE,\n                                            stderr=subprocess.PIPE).communicate()[0].split('\\0')]\n    else:\n        return shlex.split(s)\n", "def split(s, platform='this'):\n    if platform == 'auto':\n        # POSIX systems\n        platform = os.name == 'posix'\n    if platform == 1:\n        # Windows\n        s = re.split(r'\\\\(?!/)', s)\n        s = [s[0]] + ['/' + i for i in s[1:]]\n    elif platform == 0:\n        # Posix\n        s = re.split(r'\\\\(?!:)', s)\n        s = [i for i in s if ':' not in i]\n    return s\n", "def split(s, platform='this'):\n    if platform not in ('this', 1):\n        raise ValueError('unsupported platform: %r' % platform)\n    if s.startswith('\"') and s.endswith('\"'):\n        s = s[1:-1]\n    if s.startswith('\\\\\\\\') and s.endswith('\\\\\\\\'):\n        s = s[2:-2]\n    return shlex.split(s)\n", "def split(s, platform='this'):\n    if platform == 'posix':\n        pattern = r'\\A[^\\W\\d_]+\\Z'\n    elif platform == 'windows':\n        pattern = r'(?:\\\\\\\\)*(?:[A-Za-z]|(?<!\\\\):)(?!\\\\\\\\)*'\n    elif platform == 'cmd':\n        pattern = r'[^\\w\\d_]+'\n    else:\n        raise ValueError('Unknown platform: %r' % platform)\n    return re.split(pattern, s)\n", "def split(s, platform='this'):\n    if platform == 'posix':\n        split_re = re.compile(r'\\A\\-?\\w+\\Z')\n    elif platform == 'windows':\n        split_re = re.compile(r'\\A\\-?\\w+\\Z', re.M)\n    else:\n        split_re = re.compile(r'\\A(\\d+)\\Z')\n    return split_re.split(s)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def split(s, platform='this'):\n    if platform == 'posix':\n        pattern = r'(\\\\w)\\s+(\\d)'\n    elif platform == 'windows':\n        pattern = r'([A-Z])\\s+(\\d)'\n    else:\n        pattern = r'([a-z])\\s+(\\d)'\n    return re.split(pattern, s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform not in ('this', 1, 0):\n        raise ValueError('platform must be one of (\"this\", 1, 0)')\n    if platform == 1:\n        s = re.sub(r'(\\\\(?!^)', r'\\\\\\1', s)\n    elif platform == 0:\n        s = re.sub(r'(\\\\)', r'\\\\\\0', s)\n    return shlex.split(s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'this':\n        pattern = r'([\\w\\-]+)'\n    elif platform == 'posix':\n        pattern = r'([\\w\\-]+)'\n    elif platform == 'cmd':\n        pattern = r'([\\w\\-]+)'\n    else:\n        raise ValueError('unknown platform: %r' % platform)\n\n    return re.split(pattern, s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == '':\n        platform = os.name\n    if platform == 'nt':\n        s = s.replace('\\\\', '\\\\\\\\\\\\')\n        s = re.sub(r'([:/])', r'\\1', s)\n    lex = shlex.shlex(s, posix=True)\n    lex.whitespace += '.,\\t\\n'\n    lex.whitespace_split = True\n    lex.whitespace = ''\n    lex.wordchars += '/.+-():'\n\n    result = []\n    buf = ''\n    for tok in lex:\n        if tok.isalnum():\n            buf += tok\n        else:\n            result.append(buf)\n            buf = ''\n    if buf:\n        result.append(buf)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\n    if platform == 'this':\n        return shlex.split(s)\n    if platform == 'posix':\n        if ';' in s:\n            return shlex.split(s)\n        else:\n            return [s]\n    if platform == 'cmd':\n        if ';' in s:\n            return shlex.split(s.replace(';',''))\n        else:\n            return [s]\n    raise ValueError('invalid platform specified: %r' % platform)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\n    if platform == 'c':\n        return shlex.split(s)\n    elif platform == 'posix':\n        import posix\n        return posix.split(s)\n    elif platform == 'cmd':\n        import subprocess\n        return [s for s in subprocess.Popen(['which', 'cmd'],\n                                            stdin=subprocess.PIPE,\n                                            stdout=subprocess.PIPE,\n                                            stderr=subprocess.PIPE).communicate()[0].split('\\0')]\n    else:\n        return shlex.split(s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'auto':\n        # POSIX systems\n        platform = os.name == 'posix'\n    if platform == 1:\n        # Windows\n        s = re.split(r'\\\\(?!/)', s)\n        s = [s[0]] + ['/' + i for i in s[1:]]\n    elif platform == 0:\n        # Posix\n        s = re.split(r'\\\\(?!:)', s)\n        s = [i for i in s if ':' not in i]\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform not in ('this', 1):\n        raise ValueError('unsupported platform: %r' % platform)\n    if s.startswith('\"') and s.endswith('\"'):\n        s = s[1:-1]\n    if s.startswith('\\\\\\\\') and s.endswith('\\\\\\\\'):\n        s = s[2:-2]\n    return shlex.split(s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'posix':\n        pattern = r'\\A[^\\W\\d_]+\\Z'\n    elif platform == 'windows':\n        pattern = r'(?:\\\\\\\\)*(?:[A-Za-z]|(?<!\\\\):)(?!\\\\\\\\)*'\n    elif platform == 'cmd':\n        pattern = r'[^\\w\\d_]+'\n    else:\n        raise ValueError('Unknown platform: %r' % platform)\n    return re.split(pattern, s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'posix':\n        split_re = re.compile(r'\\A\\-?\\w+\\Z')\n    elif platform == 'windows':\n        split_re = re.compile(r'\\A\\-?\\w+\\Z', re.M)\n    else:\n        split_re = re.compile(r'\\A(\\d+)\\Z')\n    return split_re.split(s)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_15.py", "name": "prepare_repository_from_archive", "docstring": "Given an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.", "ques_id": "62ece4982e6aefcf4aabbd74", "code": ["def prepare_repository_from_archive(\n    archive_path: Path, remote_name: str = \"origin\", preserve_filename: bool = False\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it. Returns a file repo url which can be used\n    as origin url. This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    # TODO: make sure the archive exists and is a directory.\n    if not archive_path.is_file():\n        raise ValueError(\"archive path is not a file\")\n\n    # TODO: detect remote name\n    repo_url = archive_path.with_suffix(\".tar.gz\")\n\n    if preserve_filename:\n        repo_url = repo_url.with_suffix(\".gz\")\n\n    return repo_url\n", "def prepare_repository_from_archive(\n    archive_path: Path, version: Optional[str] = None\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.  See ``prepare_repository_from_archive``\n    for details.\n    \"\"\"\n    # Prepare repository url.\n    archive = ZipFile(archive_path)\n    archive_contents = archive.read(str(version) + \".tar.gz\")\n    repo_url = archive_contents.decoded_content.decode().split(\"://\")[1].split(\"/\")[0]\n\n    # Remove.tar.gz extension and add.git extension.\n    repo_url = repo_url[:-4] if repo_url.endswith(\".tar.gz\") else repo_url + \".git\"\n\n    return repo_url\n", "def prepare_repository_from_archive(\n    archive_path: Path, output_path: Path, force: bool = False\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url which\n    can be used as origin url. This does not deal with the case where the archive\n    passed along does not exist.\n    \"\"\"\n    repo_url = archive_path.with_suffix(\".tar.gz\")\n    repo_url.unlink()\n\n    if force:\n        output_path.touch()\n    else:\n        output_path.unlink()\n\n    if subprocess.call(\n        [\n            \"tar\",\n            \"-zcvf\",\n            str(archive_path),\n            str(output_path),\n        ],\n        stdout=DEVNULL,\n        stderr=DEVNULL,\n    )!= 0:\n        raise Exception(\"Could not extract tar file\")\n\n    if not repo_url.is_file():\n        raise Exception(\"Unable to extract repo url from archive\")\n\n    return repo", "def prepare_repository_from_archive(\n    archive_path: Path, force: bool = False\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url which\n    can be used as origin url.\n    \"\"\"\n    repo_url = archive_path.with_suffix(\".tar.gz\")\n    if force:\n        repo_url.unlink()\n    else:\n        repo_url.mkdir()\n        repo_url.joinpath(\"HEAD\").write_clean(\n            \"{}\\n\".format(archive_path.name), lock_=False\n        )\n    return repo_url.as_uri()\n", "def prepare_repository_from_archive(\n    archive_path: Path, dest_dir: Path, dry_run: bool = False\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case where the\n    archive passed along does not exist.\n    \"\"\"\n    # TODO: support dry_run mode\n    # TODO: support zip files in a separate directory and then uncompress them\n    # TODO: need to be able to handle zip files in a separate directory\n    with archive_path.open(\"rb\") as inp:\n        # TODO: support unicode in the repo url\n        # TODO: support zip files in a separate directory and then uncompress them\n        # TODO: need to be able to handle zip files in a separate directory\n        repo_url = \"https://raw.githubusercontent.com/{name}/v{version}/{name}.tar.gz\"\n        repo_url = repo_url.format(name=archive_path.name, version=archive_path.stat().st_size)\n    ", "def prepare_repository_from_archive(\n    archive_path: pathlib.Path,\n) -> Tuple[Optional[str], str]:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case where the\n    archive passed along does not exist.\n    \"\"\"\n    archive_path = str(archive_path)\n    if not archive_path.is_file():\n        raise IOError(f\"{archive_path} is not a file\")\n\n    zip_file = zipfile.ZipFile(archive_path)\n    try:\n        zip_file.extractall(str(archive_path))\n    except zipfile.BadZipfile as e:\n        raise IOError(\n            f\"Unable to unzip archive {archive_path!r}: {e}\"\n        ) from e\n    except (zipfile.LengthError, UnicodeDecodeError, KeyError) as e:\n        raise IOError(f\"Unable to extract archive {archive_path!r}: {e}\") from e\n    archive_path = zip_file.filename\n    repo_url = f\"https://github.com/{archi", "def prepare_repository_from_archive(\n    archive_path: Path, repo_name: str = \"pypi\", url: str = \"https://raw.githubusercontent.com/pypi/{}/{}.tar.gz\".format(repo_name, repo_name)\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    This does not deal with the case where the archive passed along\n    does not exist.\n    \"\"\"\n    try:\n        archive = tarfile.open(archive_path, \"r:gz\")\n    except tarfile.TarError:\n        raise ArchiveNotFound(f\"Could not find archive {archive_path}\")\n\n    # Get the first member of the archive.\n    archive_member = archive.getmembers()[0]\n\n    # Uncompress the archive using gzip.\n    with archive.extractfile(archive_member) as archive_file:\n        repo_url = archive_file.read().decode(\"utf-8\")\n\n    # Remove the file extension from the repo url.\n    repo_url = repo_url.split(\"/\")[-1]\n\n    # Remove the.git extension from the repo", "def prepare_repository_from_archive(\n    archive_path: pathlib.Path, repository: str\n) -> Tuple[str, str]:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case where\n    the archive passed along does not exist.\n    \"\"\"\n    archive = archive_path.with_suffix(\".tar.gz\")\n    with archive.open(mode=\"r\") as archive_file:\n        return archive_repository_from_archive_bytes(archive_file, repository)\n", "def prepare_repository_from_archive(\n    archive_path: Path, version: str, tar_name: str = \"dist.tar.gz\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case where the\n    archive passed along does not exist.\n    \"\"\"\n    repo_url = archive_path.with_suffix(f\".{version}.tar.gz\")\n    repo_url.unlink()\n    with tarfile.open(repo_url, mode=\"r:gz\") as archive:\n        archive.extractall(archive_path)\n    archive.close()\n    repo_url.unlink()\n    return repo_url.as_url()\n", "def prepare_repository_from_archive(\n    archive_path: Path, allow_bare: bool = False, allow_mirrors: bool = False\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url which\n    can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not\n    exist.\n    \"\"\"\n    if not archive_path.exists():\n        raise RuntimeError(\n            \"Unable to prepare repository without a previous archive at {}\".format(str(archive_path))\n        )\n    repo_url = archive_path.with_suffix(\".tar.gz\")\n    if allow_bare:\n        repo_url = repo_url.with_suffix(\".tar.bz2\")\n    if allow_mirrors:\n        repo_url = repo_url.with_suffix(\".tar.gz\")\n    repo_url.unlink()\n    return repo_url.as_uri()\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def prepare_repository_from_archive(\n    archive_path: Path, remote_name: str = \"origin\", preserve_filename: bool = False\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it. Returns a file repo url which can be used\n    as origin url. This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    # TODO: make sure the archive exists and is a directory.\n    if not archive_path.is_file():\n        raise ValueError(\"archive path is not a file\")\n\n    # TODO: detect remote name\n    repo_url = archive_path.with_suffix(\".tar.gz\")\n\n    if preserve_filename:\n        repo_url = repo_url.with_suffix(\".gz\")\n\n    return repo_url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: Path, version: Optional[str] = None\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.  See ``prepare_repository_from_archive``\n    for details.\n    \"\"\"\n    # Prepare repository url.\n    archive = ZipFile(archive_path)\n    archive_contents = archive.read(str(version) + \".tar.gz\")\n    repo_url = archive_contents.decoded_content.decode().split(\"://\")[1].split(\"/\")[0]\n\n    # Remove.tar.gz extension and add.git extension.\n    repo_url = repo_url[:-4] if repo_url.endswith(\".tar.gz\") else repo_url + \".git\"\n\n    return repo_url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: Path, output_path: Path, force: bool = False\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url which\n    can be used as origin url. This does not deal with the case where the archive\n    passed along does not exist.\n    \"\"\"\n    repo_url = archive_path.with_suffix(\".tar.gz\")\n    repo_url.unlink()\n\n    if force:\n        output_path.touch()\n    else:\n        output_path.unlink()\n\n    if subprocess.call(\n        [\n            \"tar\",\n            \"-zcvf\",\n            str(archive_path),\n            str(output_path),\n        ],\n        stdout=DEVNULL,\n        stderr=DEVNULL,\n    )!= 0:\n        raise Exception(\"Could not extract tar file\")\n\n    if not repo_url.is_file():\n        raise Exception(\"Unable to extract repo url from archive\")\n\n    return repo", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: Path, force: bool = False\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url which\n    can be used as origin url.\n    \"\"\"\n    repo_url = archive_path.with_suffix(\".tar.gz\")\n    if force:\n        repo_url.unlink()\n    else:\n        repo_url.mkdir()\n        repo_url.joinpath(\"HEAD\").write_clean(\n            \"{}\\n\".format(archive_path.name), lock_=False\n        )\n    return repo_url.as_uri()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: Path, dest_dir: Path, dry_run: bool = False\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case where the\n    archive passed along does not exist.\n    \"\"\"\n    # TODO: support dry_run mode\n    # TODO: support zip files in a separate directory and then uncompress them\n    # TODO: need to be able to handle zip files in a separate directory\n    with archive_path.open(\"rb\") as inp:\n        # TODO: support unicode in the repo url\n        # TODO: support zip files in a separate directory and then uncompress them\n        # TODO: need to be able to handle zip files in a separate directory\n        repo_url = \"https://raw.githubusercontent.com/{name}/v{version}/{name}.tar.gz\"\n        repo_url = repo_url.format(name=archive_path.name, version=archive_path.stat().st_size)\n    ", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: pathlib.Path,\n) -> Tuple[Optional[str], str]:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case where the\n    archive passed along does not exist.\n    \"\"\"\n    archive_path = str(archive_path)\n    if not archive_path.is_file():\n        raise IOError(f\"{archive_path} is not a file\")\n\n    zip_file = zipfile.ZipFile(archive_path)\n    try:\n        zip_file.extractall(str(archive_path))\n    except zipfile.BadZipfile as e:\n        raise IOError(\n            f\"Unable to unzip archive {archive_path!r}: {e}\"\n        ) from e\n    except (zipfile.LengthError, UnicodeDecodeError, KeyError) as e:\n        raise IOError(f\"Unable to extract archive {archive_path!r}: {e}\") from e\n    archive_path = zip_file.filename\n    repo_url = f\"https://github.com/{archi", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: Path, repo_name: str = \"pypi\", url: str = \"https://raw.githubusercontent.com/pypi/{}/{}.tar.gz\".format(repo_name, repo_name)\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    This does not deal with the case where the archive passed along\n    does not exist.\n    \"\"\"\n    try:\n        archive = tarfile.open(archive_path, \"r:gz\")\n    except tarfile.TarError:\n        raise ArchiveNotFound(f\"Could not find archive {archive_path}\")\n\n    # Get the first member of the archive.\n    archive_member = archive.getmembers()[0]\n\n    # Uncompress the archive using gzip.\n    with archive.extractfile(archive_member) as archive_file:\n        repo_url = archive_file.read().decode(\"utf-8\")\n\n    # Remove the file extension from the repo url.\n    repo_url = repo_url.split(\"/\")[-1]\n\n    # Remove the.git extension from the repo", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: pathlib.Path, repository: str\n) -> Tuple[str, str]:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case where\n    the archive passed along does not exist.\n    \"\"\"\n    archive = archive_path.with_suffix(\".tar.gz\")\n    with archive.open(mode=\"r\") as archive_file:\n        return archive_repository_from_archive_bytes(archive_file, repository)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: Path, version: str, tar_name: str = \"dist.tar.gz\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case where the\n    archive passed along does not exist.\n    \"\"\"\n    repo_url = archive_path.with_suffix(f\".{version}.tar.gz\")\n    repo_url.unlink()\n    with tarfile.open(repo_url, mode=\"r:gz\") as archive:\n        archive.extractall(archive_path)\n    archive.close()\n    repo_url.unlink()\n    return repo_url.as_url()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: Path, allow_bare: bool = False, allow_mirrors: bool = False\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url which\n    can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not\n    exist.\n    \"\"\"\n    if not archive_path.exists():\n        raise RuntimeError(\n            \"Unable to prepare repository without a previous archive at {}\".format(str(archive_path))\n        )\n    repo_url = archive_path.with_suffix(\".tar.gz\")\n    if allow_bare:\n        repo_url = repo_url.with_suffix(\".tar.bz2\")\n    if allow_mirrors:\n        repo_url = repo_url.with_suffix(\".tar.gz\")\n    repo_url.unlink()\n    return repo_url.as_uri()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_5.py", "name": "addignored", "docstring": "Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.", "ques_id": "62ece4982e6aefcf4aabbd75", "code": ["def addignored(ignored):\n    ignored = subprocess.Popen(['git', 'ls-files', '-z'], stdin=subprocess.PIPE, stdout=subprocess.PIPE).communicate()[0].strip().decode('utf-8')\n    ignored = ignored.split('\\n')\n    ignored.sort()\n    return ignored\n", "def addignored(ignored):\n\n    ignored = subprocess.check_output(\n        ['git', 'ls-files', '-z', '--ignored'],\n        universal_newlines=True\n    ).split(os.linesep)\n    ignored.sort()\n    return ignored\n", "def addignored(ignored):\n    ignored = subprocess.Popen(['git', 'ls-files', '-z'],\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE).communicate()[0].split('\\n')\n    ignored = [f for f in ignored if not f.startswith('ignore')]\n    ignored.sort()\n    return ','.join(ignored)\n", "def addignored(ignored):\n    ignored = subprocess.Popen([\"git\", \"diff\", \"--name-only\", ignored],\n                                stdout=subprocess.PIPE).communicate()[0]\n    ignored = ignored.decode(\"utf-8\").split(\"\\n\")\n    ignored.sort()\n    return ignored\n", "def addignored(ignored):\n    ignored = subprocess.check_output(['git', 'ls-files', '-z']).split(b'\\x00'.decode('utf-8'))\n    ignored.sort()\n    return ignored\n", "def addignored(ignored):\n    ignored = subprocess.check_output(['git', 'ls-files', '-z']).decode('utf-8')\n    ignored = ignored.split('\\n')\n    ignored.sort()\n    return ignored\n", "def addignored(ignored):\n    ignored = subprocess.check_output(['git', 'ls-files', '-z']).split('\\n')\n    ignored.sort()\n    return ','.join(ignored)\n", "def addignored(ignored):\n    ignore_files = subprocess.check_output([\"git\", \"ls-files\", \"-z\"]).decode(\"utf-8\").split(\"\\n\")\n    ignored_files = []\n    for ignore in ignore_files:\n        if ignore not in ignored:\n            ignored_files.append(ignore)\n    ignored_files.sort()\n    return \", \".join(ignored_files)\n", "def addignored(ignored):\n    ignored = subprocess.check_output(['git', 'ls-files', '-z'],\n                                       universal_newlines=True)\n    ignored = ignored.split('\\n')\n    ignored = [f.strip() for f in ignored]\n    ignored.sort()\n    return ','.join(ignored)\n", "def addignored(ignored):\n    ignored = subprocess.check_output([\"git\", \"ls-files\"]).decode(\"utf-8\").split(\",\")\n    ignored.sort()\n    return \",\".join(ignored)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def addignored(ignored):\n    ignored = subprocess.Popen(['git', 'ls-files', '-z'], stdin=subprocess.PIPE, stdout=subprocess.PIPE).communicate()[0].strip().decode('utf-8')\n    ignored = ignored.split('\\n')\n    ignored.sort()\n    return ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\n    ignored = subprocess.check_output(\n        ['git', 'ls-files', '-z', '--ignored'],\n        universal_newlines=True\n    ).split(os.linesep)\n    ignored.sort()\n    return ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.Popen(['git', 'ls-files', '-z'],\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE).communicate()[0].split('\\n')\n    ignored = [f for f in ignored if not f.startswith('ignore')]\n    ignored.sort()\n    return ','.join(ignored)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.Popen([\"git\", \"diff\", \"--name-only\", ignored],\n                                stdout=subprocess.PIPE).communicate()[0]\n    ignored = ignored.decode(\"utf-8\").split(\"\\n\")\n    ignored.sort()\n    return ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.check_output(['git', 'ls-files', '-z']).split(b'\\x00'.decode('utf-8'))\n    ignored.sort()\n    return ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.check_output(['git', 'ls-files', '-z']).decode('utf-8')\n    ignored = ignored.split('\\n')\n    ignored.sort()\n    return ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.check_output(['git', 'ls-files', '-z']).split('\\n')\n    ignored.sort()\n    return ','.join(ignored)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignore_files = subprocess.check_output([\"git\", \"ls-files\", \"-z\"]).decode(\"utf-8\").split(\"\\n\")\n    ignored_files = []\n    for ignore in ignore_files:\n        if ignore not in ignored:\n            ignored_files.append(ignore)\n    ignored_files.sort()\n    return \", \".join(ignored_files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.check_output(['git', 'ls-files', '-z'],\n                                       universal_newlines=True)\n    ignored = ignored.split('\\n')\n    ignored = [f.strip() for f in ignored]\n    ignored.sort()\n    return ','.join(ignored)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.check_output([\"git\", \"ls-files\"]).decode(\"utf-8\").split(\",\")\n    ignored.sort()\n    return \",\".join(ignored)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_11.py", "name": "match", "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported", "ques_id": "62ece4982e6aefcf4aabbd76", "code": ["def match(filename):\n    return ((filename.endswith(\".cpp\") or filename.endswith(\".c\")) and\n            filename.startswith(\"gcc\") and\n            os.path.splitext(filename)[1] in [\".h\", \".cpp\", \".c\"])\n", "def match(filename):\n    return (\n        filename.endswith(\".txt\")\n        or filename.endswith(\".yml\")\n        or filename.endswith(\".json\")\n        or filename.endswith(\".yaml\")\n    )\n", "def match(filename):\n    return ((filename.endswith('.xml') or filename.endswith('.xml.gz')) and\n            filename.rsplit('.', 1)[1] in ['xml', 'gz'])\n", "def match(filename):\n    return ('.' in filename and\n            filename.rsplit('.', 1)[1].lower() in SUPPORTED_EXTENSIONS)\n", "def match(filename):\n    if not filename.endswith(\".xml\"):\n        return False\n    return not filename.startswith(\"src/\")\n", "def match(filename):\n    if filename.endswith(\".png\"):\n        return False\n\n    return True\n", "def match(filename):\n    return (\n        filename.endswith(\".py\") or\n        filename.endswith(\".pyc\") or\n        filename.endswith(\".pyo\") or\n        not filename.startswith(\"lib.\")\n    )\n", "def match(filename):\n    return (filename.lower().endswith((\".c\", \".cpp\")) and\n            not filename.startswith(\"cpp\"))\n", "def match(filename):\n    return ('.' in filename and\n            filename.rsplit('.', 1)[1].lower() in SUPPORTED_EXTENSIONS)\n", "def match(filename):\n    return (\n        filename.endswith(\".c\")\n        or filename.endswith(\".cpp\")\n        or filename.endswith(\".h\")\n        or filename.endswith(\"$Fortran\")\n    )\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match(filename):\n    return ((filename.endswith(\".cpp\") or filename.endswith(\".c\")) and\n            filename.startswith(\"gcc\") and\n            os.path.splitext(filename)[1] in [\".h\", \".cpp\", \".c\"])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return (\n        filename.endswith(\".txt\")\n        or filename.endswith(\".yml\")\n        or filename.endswith(\".json\")\n        or filename.endswith(\".yaml\")\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return ((filename.endswith('.xml') or filename.endswith('.xml.gz')) and\n            filename.rsplit('.', 1)[1] in ['xml', 'gz'])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return ('.' in filename and\n            filename.rsplit('.', 1)[1].lower() in SUPPORTED_EXTENSIONS)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    if not filename.endswith(\".xml\"):\n        return False\n    return not filename.startswith(\"src/\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    if filename.endswith(\".png\"):\n        return False\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return (\n        filename.endswith(\".py\") or\n        filename.endswith(\".pyc\") or\n        filename.endswith(\".pyo\") or\n        not filename.startswith(\"lib.\")\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return (filename.lower().endswith((\".c\", \".cpp\")) and\n            not filename.startswith(\"cpp\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return ('.' in filename and\n            filename.rsplit('.', 1)[1].lower() in SUPPORTED_EXTENSIONS)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return (\n        filename.endswith(\".c\")\n        or filename.endswith(\".cpp\")\n        or filename.endswith(\".h\")\n        or filename.endswith(\"$Fortran\")\n    )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_1.py", "name": "parse_frequency", "docstring": "Given a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.", "ques_id": "62ece4982e6aefcf4aabbd77", "code": ["def parse_frequency(frequency):\n    try:\n        return FREQ_NAMES[frequency.upper()]\n    except KeyError:\n        raise ValueError(\"Frequency '%s' is not valid.\" % frequency)\n", "def parse_frequency(frequency):\n\n    if frequency in (\"always\", \"on-time\"):\n        return datetime.timedelta(days=1)\n\n    if frequency in (\"days\", \"weeks\", \"months\"):\n        return datetime.timedelta(days=7)\n\n    if frequency == \"weekly\":\n        return datetime.timedelta(weeks=52)\n\n    if frequency == \"monthly\":\n        return datetime.timedelta(months=12)\n\n    if frequency == \"yearly\":\n        return datetime.timedelta(years=1)\n\n    raise ValueError(\"Frequency {} is not supported.\".format(frequency))\n", "def parse_frequency(frequency):\n    if frequency is None:\n        return None\n\n    if frequency.lower() == \"always\":\n        return datetime.timedelta(weeks=1)\n\n    frequency = frequency.upper()\n    if frequency in _FREQ_TO_DELTA:\n        return datetime.timedelta(**_FREQ_TO_DELTA[frequency])\n\n    raise ValueError(\"Unrecognized frequency: %r\" % frequency)\n", "def parse_frequency(frequency):\n\n    if frequency is None or frequency.lower() == 'always':\n        return datetime.timedelta(days=1)\n\n    frequency = frequency.lower()\n\n    if frequency in ['weeks', 'week']:\n        return datetime.timedelta(weeks=1)\n\n    if frequency in ['days', 'day']:\n        return datetime.timedelta(days=1)\n\n    if frequency in ['hours', 'hour']:\n        return datetime.timedelta(hours=1)\n\n    if frequency in ['minutes','minute']:\n        return datetime.timedelta(minutes=1)\n\n    if frequency in ['seconds','second']:\n        return datetime.timedelta(seconds=1)\n\n    raise ValueError('Unknown frequency \"{0}\"'.format(frequency))\n", "def parse_frequency(frequency):\n\n    frequency = frequency.lower()\n    if frequency == 'always':\n        return datetime.timedelta(days=1)\n    elif frequency in FREQUENCIES:\n        return datetime.timedelta(days=FREQUENCIES[frequency])\n    else:\n        raise ValueError('frequency must be one of {0}'.format(FREQUENCIES))\n", "def parse_frequency(frequency):\n    frequency = frequency.strip()\n    if frequency in (\"always\", \"days\", \"weeks\", \"months\", \"years\"):\n        return timedelta(**{frequency.upper(): 1})\n    raise ValueError(\"invalid frequency %r\" % frequency)\n", "def parse_frequency(frequency):\n\n    frequency = frequency.lower()\n\n    if frequency == \"always\":\n        return datetime.timedelta(days=7)\n\n    if frequency == \"never\":\n        return datetime.timedelta(days=0)\n\n    raise ValueError(\"Unknown frequency '%s'\" % frequency)\n", "def parse_frequency(frequency):\n\n    frequency = frequency.lower()\n\n    if frequency in [\"days\", \"weeks\", \"hours\", \"minutes\", \"seconds\"]:\n        return timedelta(days=int(frequency))\n    elif frequency == \"always\":\n        return None\n    else:\n        raise ValueError(\"Invalid frequency {!r}\".format(frequency))\n", "def parse_frequency(frequency):\n\n    if frequency in ('always', 'never'):\n        return None\n\n    frequency = frequency.lower()\n\n    if frequency in ('weeks', 'week'):\n        return datetime.timedelta(days=7)\n\n    raise ValueError('Unrecognized frequency: %s' % frequency)\n", "def parse_frequency(frequency):\n    if frequency == \"always\":\n        return datetime.timedelta(days=1)\n    elif frequency == \"weekly\":\n        return datetime.timedelta(days=7)\n    elif frequency == \"monthly\":\n        return datetime.timedelta(days=30)\n    elif frequency == \"daily\":\n        return datetime.timedelta(days=365)\n    elif frequency == \"yearly\":\n        return datetime.timedelta(days=365.25)\n    else:\n        raise ValueError(\"Unknown frequency: %s\" % frequency)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def parse_frequency(frequency):\n    try:\n        return FREQ_NAMES[frequency.upper()]\n    except KeyError:\n        raise ValueError(\"Frequency '%s' is not valid.\" % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\n    if frequency in (\"always\", \"on-time\"):\n        return datetime.timedelta(days=1)\n\n    if frequency in (\"days\", \"weeks\", \"months\"):\n        return datetime.timedelta(days=7)\n\n    if frequency == \"weekly\":\n        return datetime.timedelta(weeks=52)\n\n    if frequency == \"monthly\":\n        return datetime.timedelta(months=12)\n\n    if frequency == \"yearly\":\n        return datetime.timedelta(years=1)\n\n    raise ValueError(\"Frequency {} is not supported.\".format(frequency))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency is None:\n        return None\n\n    if frequency.lower() == \"always\":\n        return datetime.timedelta(weeks=1)\n\n    frequency = frequency.upper()\n    if frequency in _FREQ_TO_DELTA:\n        return datetime.timedelta(**_FREQ_TO_DELTA[frequency])\n\n    raise ValueError(\"Unrecognized frequency: %r\" % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\n    if frequency is None or frequency.lower() == 'always':\n        return datetime.timedelta(days=1)\n\n    frequency = frequency.lower()\n\n    if frequency in ['weeks', 'week']:\n        return datetime.timedelta(weeks=1)\n\n    if frequency in ['days', 'day']:\n        return datetime.timedelta(days=1)\n\n    if frequency in ['hours', 'hour']:\n        return datetime.timedelta(hours=1)\n\n    if frequency in ['minutes','minute']:\n        return datetime.timedelta(minutes=1)\n\n    if frequency in ['seconds','second']:\n        return datetime.timedelta(seconds=1)\n\n    raise ValueError('Unknown frequency \"{0}\"'.format(frequency))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\n    frequency = frequency.lower()\n    if frequency == 'always':\n        return datetime.timedelta(days=1)\n    elif frequency in FREQUENCIES:\n        return datetime.timedelta(days=FREQUENCIES[frequency])\n    else:\n        raise ValueError('frequency must be one of {0}'.format(FREQUENCIES))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    frequency = frequency.strip()\n    if frequency in (\"always\", \"days\", \"weeks\", \"months\", \"years\"):\n        return timedelta(**{frequency.upper(): 1})\n    raise ValueError(\"invalid frequency %r\" % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\n    frequency = frequency.lower()\n\n    if frequency == \"always\":\n        return datetime.timedelta(days=7)\n\n    if frequency == \"never\":\n        return datetime.timedelta(days=0)\n\n    raise ValueError(\"Unknown frequency '%s'\" % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\n    frequency = frequency.lower()\n\n    if frequency in [\"days\", \"weeks\", \"hours\", \"minutes\", \"seconds\"]:\n        return timedelta(days=int(frequency))\n    elif frequency == \"always\":\n        return None\n    else:\n        raise ValueError(\"Invalid frequency {!r}\".format(frequency))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\n    if frequency in ('always', 'never'):\n        return None\n\n    frequency = frequency.lower()\n\n    if frequency in ('weeks', 'week'):\n        return datetime.timedelta(days=7)\n\n    raise ValueError('Unrecognized frequency: %s' % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency == \"always\":\n        return datetime.timedelta(days=1)\n    elif frequency == \"weekly\":\n        return datetime.timedelta(days=7)\n    elif frequency == \"monthly\":\n        return datetime.timedelta(days=30)\n    elif frequency == \"daily\":\n        return datetime.timedelta(days=365)\n    elif frequency == \"yearly\":\n        return datetime.timedelta(days=365.25)\n    else:\n        raise ValueError(\"Unknown frequency: %s\" % frequency)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_18.py", "name": "is_local", "docstring": "Checks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost", "ques_id": "62ece4992e6aefcf4aabbd78", "code": ["def is_local(host):\n    return host.startswith('127.0.0.1')\n", "def is_local(host):\n    return host == \"127.0.0.1\"\n", "def is_local(host):\n    return host == '127.0.0.1'\n", "def is_local(host):\n    return host == '127.0.0.1'\n", "def is_local(host):\n    return host == '127.0.0.1'\n", "def is_local(host):\n    if host == '127.0.0.1':\n        return True\n    return False\n", "def is_local(host):\n    return host.startswith('127.0.0.1')\n", "def is_local(host):\n    return host == '127.0.0.1'\n", "def is_local(host):\n    return host == '127.0.0.1'\n", "def is_local(host):\n    return host == '127.0.0.1'\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_local(host):\n    return host.startswith('127.0.0.1')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == \"127.0.0.1\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    if host == '127.0.0.1':\n        return True\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host.startswith('127.0.0.1')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_8.py", "name": "make_find_paths", "docstring": "Given a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']", "ques_id": "62ece4992e6aefcf4aabbd79", "code": ["def make_find_paths(find_paths):\n    return [f\"sh:**/*{f}*/**\" for f in find_paths]\n", "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "def make_find_paths(find_paths):\n    return [f'sh:**/*{p}*/**' for p in find_paths]\n", "def make_find_paths(find_paths):\n    return [f\"sh:**/*{f}*/**\" for f in find_paths]\n", "def make_find_paths(find_paths):\n    return [f'sh:**/*{p}*/**' for p in find_paths]\n", "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def make_find_paths(find_paths):\n    return [f\"sh:**/*{f}*/**\" for f in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [f'sh:**/*{p}*/**' for p in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [f\"sh:**/*{f}*/**\" for f in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [f'sh:**/*{p}*/**' for p in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [f\"sh:**/*{p}*/**\" for p in find_paths]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_19.py", "name": "is_gitbash", "docstring": "returns True if you run in a Windows gitbash\n\n:return: True if gitbash", "ques_id": "62ece4992e6aefcf4aabbd7a", "code": ["def is_gitbash():\n    return os.name == \"nt\"\n", "def is_gitbash():\n    # If running in git bash we know we are running in git\n    return os.name == 'nt'\n", "def is_gitbash():\n    return os.name == \"nt\"\n", "def is_gitbash():\n    return os.name == \"nt\"\n", "def is_gitbash():\n    return os.name == \"nt\"\n", "def is_gitbash():\n    return os.name == 'nt'\n", "def is_gitbash():\n    return os.name == \"nt\"\n", "def is_gitbash():\n    return os.name == 'nt'\n", "def is_gitbash():\n    return os.name == 'nt'\n", "def is_gitbash():\n    try:\n        subprocess.check_output(['git','rev-parse', '--symbolic'])\n    except subprocess.CalledProcessError:\n        return False\n    return True\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_gitbash():\n    return os.name == \"nt\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    # If running in git bash we know we are running in git\n    return os.name == 'nt'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    return os.name == \"nt\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    return os.name == \"nt\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    return os.name == \"nt\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    return os.name == 'nt'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    return os.name == \"nt\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    return os.name == 'nt'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    return os.name == 'nt'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    try:\n        subprocess.check_output(['git','rev-parse', '--symbolic'])\n    except subprocess.CalledProcessError:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_9.py", "name": "write_configuration", "docstring": "Given a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.", "ques_id": "62ece4992e6aefcf4aabbd7b", "code": ["def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    # If the filename doesn't exist, make it.\n    if not os.path.exists(os.path.dirname(config_filename)):\n        os.makedirs(os.path.dirname(config_filename))\n\n    # Write the config to a temporary file.\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n\n    # Create directories to put the config file in.\n    temp_dir = tempfile.mkdtemp()\n\n    try:\n        # Write the config to the temporary directory.\n        with open(config_filename, 'a') as file:\n            file.write('\\n')\n            file.write(config_filename)\n            file.write('\\n')\n            file.write(temp_dir)\n\n        # Write the rendered config to the temporary directory.\n        with open(config_filename, 'w') as file:\n            file.write(rendered_config)\n    finally:\n        # Close the file.\n        if overwrite or not os.path.exists(temp_di", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directories = [os.path.dirname(config_filename), os.path.dirname(rendered_config)]\n    for directory in directories:\n        if not overwrite and os.path.exists(directory):\n            print(\"ERROR: Config directory '%s' already exists. Overwriting it anyway.\" % directory)\n            continue\n        elif not overwrite and not os.path.exists(directory):\n            os.makedirs(directory)\n        elif overwrite and not os.path.exists(directory):\n            print(\"ERROR: Config directory '%s' does not exist.\" % directory)\n            continue\n        with open(directory, mode) as configfile:\n            configfile.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\n    # If the file does not exist yet, make it\n    if not os.path.isfile(config_filename) or overwrite:\n        os.makedirs(os.path.dirname(config_filename), exist_ok=True)\n        with open(config_filename, 'w') as outfile:\n            outfile.write(rendered_config)\n        return\n\n    # If the file already exists, just overwrite it\n    elif overwrite:\n        os.remove(config_filename)\n        with open(config_filename, 'w') as outfile:\n            outfile.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        print('\\nConfig file already exists; skipping overwriting.\\n')\n        return\n    if directory:\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n    with open(config_filename, 'w') as f:\n        f.write(yaml.dump(rendered_config, default_flow_style=mode))\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.exists(config_filename):\n        print('Refusing to overwrite existing file {}.'.format(config_filename))\n        return\n\n    dirname = os.path.dirname(config_filename)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with open(config_filename, mode) as f:\n        f.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.isfile(config_filename):\n        abort('Configuration file {} already exists. Aborting.'.format(config_filename))\n\n    if not os.path.isdir(os.path.dirname(config_filename)):\n        os.makedirs(os.path.dirname(config_filename))\n\n    with open(config_filename, 'w') as f:\n        f.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.exists(config_filename):\n        abort(\"Configuration file '{}' already exists\".format(config_filename))\n\n    directory = os.path.dirname(config_filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    with open(config_filename, mode) as outfile:\n        outfile.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    dirname = os.path.dirname(config_filename)\n\n    if not overwrite and os.path.exists(config_filename):\n        sys.exit('\\n** Configuration file already exists: {}\\n** Aborting.\\n'.format(config_filename))\n\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with open(config_filename, 'w') as f:\n        yaml.safe_dump(rendered_config, f, default_flow_style=False, allow_unicode=True, indent=mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        print(\"Refusing to overwrite existing file {}\".format(config_filename))\n        return\n\n    # Make sure the directory exists.\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    with open(config_filename, mode) as outfile:\n        outfile.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n\n    # make sure the directory exists\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # write the config\n    with open(config_filename, mode) as outfile:\n        outfile.write(rendered_config)\n\n    # make sure the directory is empty\n    if not overwrite:\n        shutil.rmtree(directory)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    # If the filename doesn't exist, make it.\n    if not os.path.exists(os.path.dirname(config_filename)):\n        os.makedirs(os.path.dirname(config_filename))\n\n    # Write the config to a temporary file.\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n\n    # Create directories to put the config file in.\n    temp_dir = tempfile.mkdtemp()\n\n    try:\n        # Write the config to the temporary directory.\n        with open(config_filename, 'a') as file:\n            file.write('\\n')\n            file.write(config_filename)\n            file.write('\\n')\n            file.write(temp_dir)\n\n        # Write the rendered config to the temporary directory.\n        with open(config_filename, 'w') as file:\n            file.write(rendered_config)\n    finally:\n        # Close the file.\n        if overwrite or not os.path.exists(temp_di", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directories = [os.path.dirname(config_filename), os.path.dirname(rendered_config)]\n    for directory in directories:\n        if not overwrite and os.path.exists(directory):\n            print(\"ERROR: Config directory '%s' already exists. Overwriting it anyway.\" % directory)\n            continue\n        elif not overwrite and not os.path.exists(directory):\n            os.makedirs(directory)\n        elif overwrite and not os.path.exists(directory):\n            print(\"ERROR: Config directory '%s' does not exist.\" % directory)\n            continue\n        with open(directory, mode) as configfile:\n            configfile.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\n    # If the file does not exist yet, make it\n    if not os.path.isfile(config_filename) or overwrite:\n        os.makedirs(os.path.dirname(config_filename), exist_ok=True)\n        with open(config_filename, 'w') as outfile:\n            outfile.write(rendered_config)\n        return\n\n    # If the file already exists, just overwrite it\n    elif overwrite:\n        os.remove(config_filename)\n        with open(config_filename, 'w') as outfile:\n            outfile.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        print('\\nConfig file already exists; skipping overwriting.\\n')\n        return\n    if directory:\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n    with open(config_filename, 'w') as f:\n        f.write(yaml.dump(rendered_config, default_flow_style=mode))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.exists(config_filename):\n        print('Refusing to overwrite existing file {}.'.format(config_filename))\n        return\n\n    dirname = os.path.dirname(config_filename)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with open(config_filename, mode) as f:\n        f.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.isfile(config_filename):\n        abort('Configuration file {} already exists. Aborting.'.format(config_filename))\n\n    if not os.path.isdir(os.path.dirname(config_filename)):\n        os.makedirs(os.path.dirname(config_filename))\n\n    with open(config_filename, 'w') as f:\n        f.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.exists(config_filename):\n        abort(\"Configuration file '{}' already exists\".format(config_filename))\n\n    directory = os.path.dirname(config_filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    with open(config_filename, mode) as outfile:\n        outfile.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    dirname = os.path.dirname(config_filename)\n\n    if not overwrite and os.path.exists(config_filename):\n        sys.exit('\\n** Configuration file already exists: {}\\n** Aborting.\\n'.format(config_filename))\n\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with open(config_filename, 'w') as f:\n        yaml.safe_dump(rendered_config, f, default_flow_style=False, allow_unicode=True, indent=mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        print(\"Refusing to overwrite existing file {}\".format(config_filename))\n        return\n\n    # Make sure the directory exists.\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    with open(config_filename, mode) as outfile:\n        outfile.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n\n    # make sure the directory exists\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # write the config\n    with open(config_filename, mode) as outfile:\n        outfile.write(rendered_config)\n\n    # make sure the directory is empty\n    if not overwrite:\n        shutil.rmtree(directory)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_22.py", "name": "oneline", "docstring": "converts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:", "ques_id": "62ece4992e6aefcf4aabbd7c", "code": ["def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator)[1:])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator)[1:])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator))\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator)[1:])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([shlex.quote(i) for i in script.split(\"\\n\")])\n", "def oneline(script, seperator=\" && \"):\n    return \"{0}{1}\".format(seperator, script)\n", "def oneline(script, seperator=\" && \"):\n    return \"{0}{1}\".format(seperator, script)\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh\", script])\n", "def oneline(script, seperator=\" && \"):\n    return \"{}{}\".format(seperator, script)\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.splitlines())\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator)[1:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator)[1:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator)[1:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([shlex.quote(i) for i in script.split(\"\\n\")])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return \"{0}{1}\".format(seperator, script)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return \"{0}{1}\".format(seperator, script)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh\", script])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return \"{}{}\".format(seperator, script)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.splitlines())\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_16.py", "name": "subprocess_run_helper", "docstring": "Run a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.", "ques_id": "62ece4992e6aefcf4aabbd7d", "code": ["def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    # type: (Callable, *str, dict[str, str]) -> None\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.\n        It must be in a module that is imported.\n        Its name must match the function name.\n\n    *args : str\n        Any additional command line arguments to be passed in the first\n        argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    command = [sys.executable, '-c', func] + list(args)\n    if extra_env:\n        command.extend(extra_env.items())\n    proc = subprocess.Popen(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env={}\n    )\n    out, err = proc.communicate()\n    if proc.returncode:\n   ", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    env['GIT_TERMINAL_PROMPT'] = '0'\n    p = subprocess.Popen(\n        [sys.executable, '-m', func.__module__] + list(args),\n        env=env,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        timeout=timeout)\n    return p.communicate()\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = dict(os.environ)\n    env.update(extra_env or {})\n    p = subprocess.Popen(\n        [sys.executable, '-m', func.__module__] + list(args),\n        env=env)\n    p.wait(timeout)\n    return p.returncode == 0\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n\n    subprocess.check_call([sys.executable, \"-m\", func] + list(args),\n                          env=env, timeout=timeout)\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    proc = subprocess.Popen(\n        args,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env,\n        timeout=timeout,\n    )\n    proc.stdin.close()\n    return proc.wait()\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    extra_env[\"PYTHONPATH\"] = os.pathsep.join(sys.path)\n    with Timer(func.__name__):\n        return subprocess.run(\n            args, env=extra_env, timeout=timeout, check=True,\n        )\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    extra_env['PYTHONPATH'] = os.pathsep.join(sys.path)\n    proc = subprocess.Popen(\n        args,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=extra_env,\n        timeout=timeout,\n    )\n    out, err = proc.communicate()\n    if proc.wait()!= 0:\n        print(out)\n        print(err)\n        raise RuntimeError('Command failed: {}'.format(' '.join(args)))\n    return out.decode('utf-8'), err.decode('utf-8')\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    args = list(args)\n    args.append('--timeout={}s'.format(timeout))\n    if extra_env:\n        for k, v in extra_env.items():\n            args.append('--env-{}={}'.format(k, v))\n    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = proc.communicate()\n    if proc.returncode:\n        raise subprocess.CalledProcessError(proc.returncode, args, output=out, error=err)\n    return out.decode('utf-8')\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    with tempfile.NamedTemporaryFile(suffix='.py') as temp:\n        with open(temp.name, 'w') as f:\n            f.write(\"def {}(*args):\".format(func.__name__))\n            for k, v in extra_env.items():\n                f.write(\"    {} = {}\".format(k, v))\n            f.write(\"    return {}(*args)\".format(func.__name__))\n        command = [sys.executable, temp.name] + list(args)\n        print(command)\n        p = subprocess.Popen(command, stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE, env=extra_env)\n        out, err = p.communicate()\n        return out.decode(), err.decode()\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = {} if extra_env is None else extra_env\n    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n        f.write('{}={}'.format(args[0], args[1]))\n        f.close()\n        f.flush()\n        cmd = [sys.executable, f.name]\n        cmd.extend(args[2:])\n        cmd.append(timeout)\n        proc = subprocess.Popen(\n            cmd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE)\n        stdout, stderr = proc.communicate()\n    return proc.returncode, stdout, stderr\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    # type: (Callable, *str, dict[str, str]) -> None\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.\n        It must be in a module that is imported.\n        Its name must match the function name.\n\n    *args : str\n        Any additional command line arguments to be passed in the first\n        argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    command = [sys.executable, '-c', func] + list(args)\n    if extra_env:\n        command.extend(extra_env.items())\n    proc = subprocess.Popen(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env={}\n    )\n    out, err = proc.communicate()\n    if proc.returncode:\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    env['GIT_TERMINAL_PROMPT'] = '0'\n    p = subprocess.Popen(\n        [sys.executable, '-m', func.__module__] + list(args),\n        env=env,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        timeout=timeout)\n    return p.communicate()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = dict(os.environ)\n    env.update(extra_env or {})\n    p = subprocess.Popen(\n        [sys.executable, '-m', func.__module__] + list(args),\n        env=env)\n    p.wait(timeout)\n    return p.returncode == 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n\n    subprocess.check_call([sys.executable, \"-m\", func] + list(args),\n                          env=env, timeout=timeout)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    proc = subprocess.Popen(\n        args,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env,\n        timeout=timeout,\n    )\n    proc.stdin.close()\n    return proc.wait()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    extra_env[\"PYTHONPATH\"] = os.pathsep.join(sys.path)\n    with Timer(func.__name__):\n        return subprocess.run(\n            args, env=extra_env, timeout=timeout, check=True,\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    extra_env['PYTHONPATH'] = os.pathsep.join(sys.path)\n    proc = subprocess.Popen(\n        args,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=extra_env,\n        timeout=timeout,\n    )\n    out, err = proc.communicate()\n    if proc.wait()!= 0:\n        print(out)\n        print(err)\n        raise RuntimeError('Command failed: {}'.format(' '.join(args)))\n    return out.decode('utf-8'), err.decode('utf-8')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    args = list(args)\n    args.append('--timeout={}s'.format(timeout))\n    if extra_env:\n        for k, v in extra_env.items():\n            args.append('--env-{}={}'.format(k, v))\n    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = proc.communicate()\n    if proc.returncode:\n        raise subprocess.CalledProcessError(proc.returncode, args, output=out, error=err)\n    return out.decode('utf-8')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    with tempfile.NamedTemporaryFile(suffix='.py') as temp:\n        with open(temp.name, 'w') as f:\n            f.write(\"def {}(*args):\".format(func.__name__))\n            for k, v in extra_env.items():\n                f.write(\"    {} = {}\".format(k, v))\n            f.write(\"    return {}(*args)\".format(func.__name__))\n        command = [sys.executable, temp.name] + list(args)\n        print(command)\n        p = subprocess.Popen(command, stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE, env=extra_env)\n        out, err = p.communicate()\n        return out.decode(), err.decode()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = {} if extra_env is None else extra_env\n    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n        f.write('{}={}'.format(args[0], args[1]))\n        f.close()\n        f.flush()\n        cmd = [sys.executable, f.name]\n        cmd.extend(args[2:])\n        cmd.append(timeout)\n        proc = subprocess.Popen(\n            cmd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE)\n        stdout, stderr = proc.communicate()\n    return proc.returncode, stdout, stderr\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_6.py", "name": "_resolve_string", "docstring": "Get the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.", "ques_id": "62ece4992e6aefcf4aabbd7e", "code": ["def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2)\n\n    if default is None:\n        raise Error(\"Environment variable '{}' not set\".format(name))\n    elif default == '':\n        raise Error(\"Default value for environment variable '{}' is an empty string\".format(name))\n    elif default == []:\n        raise Error(\"Default value for environment variable '{}' is an empty list\".format(name))\n\n    return os.environ.get(name, default)\n", "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) if matcher.group(2) is not None else None\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is not None:\n            return default\n        raise Error(\"Environment variable '{}' is not defined\".format(name))\n", "def _resolve_string(matcher):\n    name = matcher.group('name')\n    default = matcher.group('default')\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is None:\n            raise Error('Missing environment variable {0}'.format(name))\n        return os.getenv(name, default)\n", "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) or ''\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default:\n            return os.environ.get(default)\n        else:\n            raise Error('Environment variable \"{0}\" is not set.'\n                       .format(name))\n", "def _resolve_string(matcher):\n    name = matcher.group('name')\n    default = matcher.group('default')\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is None:\n            raise Error('Missing environment variable \"{}\"'.format(name))\n        else:\n            return default\n", "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = None\n    value = os.environ.get(name)\n    if value is None:\n        try:\n            default = int(os.environ[name + '_DEFAULT'])\n        except KeyError:\n            pass\n    return name, default, value\n", "def _resolve_string(matcher):\n    name, default = matcher.group(1, 2)\n    if default is None:\n        raise EnvironmentError(\"Missing default value for '%s'\" % name)\n    return os.environ.get(name, default)\n", "def _resolve_string(matcher):\n    name = matcher.group(1)\n    if name in os.environ:\n        return os.environ[name]\n\n    if name in ['True', 'False']:\n        return bool(int(os.environ[name]))\n\n    raise Error(\"Environment variable '{}' does not exist or is not boolean\".format(name))\n", "def _resolve_string(matcher):\n    name = matcher.group('name')\n    default = matcher.group('default')\n\n    if default is None:\n        raise Error('Missing default value for environment variable \"%s\".' % name)\n    elif default not in os.environ:\n        raise Error('Environment variable \"%s\" does not define default value \"%s\".' % (name, default))\n\n    return os.environ[name]\n", "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2)\n\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is None:\n            raise\n        return os.environ.get(name, default)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2)\n\n    if default is None:\n        raise Error(\"Environment variable '{}' not set\".format(name))\n    elif default == '':\n        raise Error(\"Default value for environment variable '{}' is an empty string\".format(name))\n    elif default == []:\n        raise Error(\"Default value for environment variable '{}' is an empty list\".format(name))\n\n    return os.environ.get(name, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) if matcher.group(2) is not None else None\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is not None:\n            return default\n        raise Error(\"Environment variable '{}' is not defined\".format(name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group('name')\n    default = matcher.group('default')\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is None:\n            raise Error('Missing environment variable {0}'.format(name))\n        return os.getenv(name, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) or ''\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default:\n            return os.environ.get(default)\n        else:\n            raise Error('Environment variable \"{0}\" is not set.'\n                       .format(name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group('name')\n    default = matcher.group('default')\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is None:\n            raise Error('Missing environment variable \"{}\"'.format(name))\n        else:\n            return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = None\n    value = os.environ.get(name)\n    if value is None:\n        try:\n            default = int(os.environ[name + '_DEFAULT'])\n        except KeyError:\n            pass\n    return name, default, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name, default = matcher.group(1, 2)\n    if default is None:\n        raise EnvironmentError(\"Missing default value for '%s'\" % name)\n    return os.environ.get(name, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    if name in os.environ:\n        return os.environ[name]\n\n    if name in ['True', 'False']:\n        return bool(int(os.environ[name]))\n\n    raise Error(\"Environment variable '{}' does not exist or is not boolean\".format(name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group('name')\n    default = matcher.group('default')\n\n    if default is None:\n        raise Error('Missing default value for environment variable \"%s\".' % name)\n    elif default not in os.environ:\n        raise Error('Environment variable \"%s\" does not define default value \"%s\".' % (name, default))\n\n    return os.environ[name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2)\n\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is None:\n            raise\n        return os.environ.get(name, default)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_2.py", "name": "_parse_image_ref", "docstring": "Parse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:", "ques_id": "62ece4992e6aefcf4aabbd7f", "code": ["def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    try:\n        netloc = parsed.netloc\n    except AttributeError:\n        netloc = parsed.path\n    use_ssl = (parsed.scheme == \"https\")\n    image_id = parsed.path.split(\"/\")[-1]\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_id = image_href.split(\"/\")[-1]\n    netloc = image_href.split(\":\")[0]\n    use_ssl = image_href.split(\":\")[1] == \"on\"\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    if parsed.netloc.startswith('//'):\n        # keep the path for backward compatibility\n        return parsed.path, parsed.netloc, False\n\n    netloc = parsed.netloc.rsplit(':', 1)[0]\n    use_ssl = bool(parsed.scheme in ('http', 'https'))\n    return parsed.path, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_href = image_href.strip('/')\n    if '//' in image_href:\n        netloc, image_id = image_href.split('//', 1)\n    else:\n        netloc = image_href\n        image_id = image_href\n\n    use_ssl = False\n    if ':' in image_id:\n        use_ssl = image_id.startswith('https://')\n        image_id = image_id.split(':', 1)[1]\n\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    netloc = parsed.netloc\n    use_ssl = parsed.scheme!= \"\"\n\n    if not use_ssl:\n        netloc, image_id = netloc.rsplit(\":\", 1)\n\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    if image_href.startswith('/'):\n        image_href = image_href[1:]\n    if not image_href.startswith('/'):\n        raise ValueError('image_href must be a relative path')\n    image_href = image_href[1:]\n\n    if '://' in image_href:\n        # we assume we are dealing with an image reference\n        return image_href, '', True\n\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = False\n    if 'https' in image_href:\n        use_ssl = True\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> tuple:\n    parsed = urlparse(image_href)\n    if not parsed.netloc:\n        raise ValueError('Invalid image reference %s' % image_href)\n    image_id = parsed.path.split('/')[-1]\n    netloc = parsed.netloc.split(':')[0]\n    use_ssl = bool(parsed.scheme == 'https')\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_id = image_href.split(\"/\")[-1]\n    if \":\" in image_href:\n        netloc, use_ssl = image_href.split(\":\", 1)\n    else:\n        netloc = image_href\n        use_ssl = False\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    if not parsed.netloc:\n        raise ValueError('Invalid image href: {}'.format(image_href))\n    image_id = parsed.path.split('/')[-1]\n    netloc = parsed.netloc.split(':')[0]\n    use_ssl = bool(parsed.scheme == 'https')\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    if not parsed.netloc:\n        raise ValueError('URL {} does not include a netloc'.format(image_href))\n    netloc = parsed.netloc\n    if not parsed.path:\n        raise ValueError('URL {} does not include a path'.format(image_href))\n    image_id = parsed.path.lstrip('/')\n    return image_id, netloc, False\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    try:\n        netloc = parsed.netloc\n    except AttributeError:\n        netloc = parsed.path\n    use_ssl = (parsed.scheme == \"https\")\n    image_id = parsed.path.split(\"/\")[-1]\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_id = image_href.split(\"/\")[-1]\n    netloc = image_href.split(\":\")[0]\n    use_ssl = image_href.split(\":\")[1] == \"on\"\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    if parsed.netloc.startswith('//'):\n        # keep the path for backward compatibility\n        return parsed.path, parsed.netloc, False\n\n    netloc = parsed.netloc.rsplit(':', 1)[0]\n    use_ssl = bool(parsed.scheme in ('http', 'https'))\n    return parsed.path, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_href = image_href.strip('/')\n    if '//' in image_href:\n        netloc, image_id = image_href.split('//', 1)\n    else:\n        netloc = image_href\n        image_id = image_href\n\n    use_ssl = False\n    if ':' in image_id:\n        use_ssl = image_id.startswith('https://')\n        image_id = image_id.split(':', 1)[1]\n\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    netloc = parsed.netloc\n    use_ssl = parsed.scheme!= \"\"\n\n    if not use_ssl:\n        netloc, image_id = netloc.rsplit(\":\", 1)\n\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    if image_href.startswith('/'):\n        image_href = image_href[1:]\n    if not image_href.startswith('/'):\n        raise ValueError('image_href must be a relative path')\n    image_href = image_href[1:]\n\n    if '://' in image_href:\n        # we assume we are dealing with an image reference\n        return image_href, '', True\n\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = False\n    if 'https' in image_href:\n        use_ssl = True\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> tuple:\n    parsed = urlparse(image_href)\n    if not parsed.netloc:\n        raise ValueError('Invalid image reference %s' % image_href)\n    image_id = parsed.path.split('/')[-1]\n    netloc = parsed.netloc.split(':')[0]\n    use_ssl = bool(parsed.scheme == 'https')\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_id = image_href.split(\"/\")[-1]\n    if \":\" in image_href:\n        netloc, use_ssl = image_href.split(\":\", 1)\n    else:\n        netloc = image_href\n        use_ssl = False\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    if not parsed.netloc:\n        raise ValueError('Invalid image href: {}'.format(image_href))\n    image_id = parsed.path.split('/')[-1]\n    netloc = parsed.netloc.split(':')[0]\n    use_ssl = bool(parsed.scheme == 'https')\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n    if not parsed.netloc:\n        raise ValueError('URL {} does not include a netloc'.format(image_href))\n    netloc = parsed.netloc\n    if not parsed.path:\n        raise ValueError('URL {} does not include a path'.format(image_href))\n    image_id = parsed.path.lstrip('/')\n    return image_id, netloc, False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_13.py", "name": "remove_ending_os_sep", "docstring": "Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError", "ques_id": "62ece4992e6aefcf4aabbd80", "code": ["def remove_ending_os_sep(input_list):\n    if len(input_list) > 1:\n        for idx, item in enumerate(input_list):\n            if len(item) > 1 and item[-1] == os.sep:\n                input_list[idx] = item[:-1]\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n    if len(input_list) == 1:\n        return input_list\n    if input_list[-1] == os.sep:\n        input_list.pop()\n        return input_list\n    if input_list[-1]!= os.sep:\n        return input_list[:-1]\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n    if len(input_list) == 1:\n        return input_list\n\n    if len(input_list[-1]) > 1:\n        return input_list[:-1]\n\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n    if len(input_list) == 1:\n        return input_list\n    last_char = input_list[-1]\n    if len(last_char) > 1 and last_char.endswith(os.sep):\n        return input_list[:-1]\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    # remove trailing os seperator\n    if len(input_list) > 1:\n        if input_list[-1] == os.sep:\n            input_list.pop()\n\n    # remove seperator character\n    return [item.strip() for item in input_list if item.strip()!= os.sep]\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n\n    if len(input_list[0]) > 1:\n        return input_list\n\n    last_char = input_list[0][-1]\n    if last_char == os.sep:\n        return input_list[0:len(input_list[0]) - 1]\n    else:\n        return input_list\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n\n    if len(input_list[0]) > 1:\n        return input_list\n\n    sep = input_list[0][-1]\n    if sep!= '/':\n        return input_list\n\n    return [item[:-1] for item in input_list]\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) <= 1:\n        return input_list\n\n    return [\n        item[:-1] if item.endswith(os.sep) else item\n        for item in input_list\n    ]\n", "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError('Input must be a list')\n    if len(input_list) < 2:\n        raise TypeError('Input list must contain at least two items')\n    if not input_list[-1] == os.sep:\n        input_list.pop()\n    if not input_list[0] == os.sep:\n        input_list[0] = os.sep\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) <= 1:\n        return input_list\n    else:\n        return input_list[:-1]\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) > 1:\n        for idx, item in enumerate(input_list):\n            if len(item) > 1 and item[-1] == os.sep:\n                input_list[idx] = item[:-1]\n    return input_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n    if len(input_list) == 1:\n        return input_list\n    if input_list[-1] == os.sep:\n        input_list.pop()\n        return input_list\n    if input_list[-1]!= os.sep:\n        return input_list[:-1]\n    return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n    if len(input_list) == 1:\n        return input_list\n\n    if len(input_list[-1]) > 1:\n        return input_list[:-1]\n\n    return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n    if len(input_list) == 1:\n        return input_list\n    last_char = input_list[-1]\n    if len(last_char) > 1 and last_char.endswith(os.sep):\n        return input_list[:-1]\n    return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    # remove trailing os seperator\n    if len(input_list) > 1:\n        if input_list[-1] == os.sep:\n            input_list.pop()\n\n    # remove seperator character\n    return [item.strip() for item in input_list if item.strip()!= os.sep]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n\n    if len(input_list[0]) > 1:\n        return input_list\n\n    last_char = input_list[0][-1]\n    if last_char == os.sep:\n        return input_list[0:len(input_list[0]) - 1]\n    else:\n        return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) == 0:\n        return input_list\n\n    if len(input_list[0]) > 1:\n        return input_list\n\n    sep = input_list[0][-1]\n    if sep!= '/':\n        return input_list\n\n    return [item[:-1] for item in input_list]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) <= 1:\n        return input_list\n\n    return [\n        item[:-1] if item.endswith(os.sep) else item\n        for item in input_list\n    ]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError('Input must be a list')\n    if len(input_list) < 2:\n        raise TypeError('Input list must contain at least two items')\n    if not input_list[-1] == os.sep:\n        input_list.pop()\n    if not input_list[0] == os.sep:\n        input_list[0] = os.sep\n    return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) <= 1:\n        return input_list\n    else:\n        return input_list[:-1]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_17.py", "name": "get_pattern", "docstring": "This method converts the given string to regex pattern", "ques_id": "62ece4992e6aefcf4aabbd82", "code": ["def get_pattern(pattern, strip=True):\n    if type(pattern) is not str:\n        raise ValueError('The pattern must be a string')\n    pattern = re.compile(pattern)\n    if strip:\n        return re.sub('\\W', '', pattern)\n    else:\n        return pattern\n", "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.VERBOSE | re.UNICODE | re.UNICODE_S) if strip else re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n    regex_pattern = re.compile(pattern)\n    if strip:\n        return regex_pattern.strip()\n    return regex_pattern\n", "def get_pattern(pattern, strip=True):\n    if strip:\n        pattern = re.sub('\\W', '', pattern)\n    return re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.VERBOSE | re.UNICODE | re.MULTILINE | re.DOTALL) if strip else re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.VERBOSE | re.UNICODE | re.UNICODE_EXP) \\\n        if strip else re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.VERBOSE | re.UNICODE | re.UNICODE_EXP) if strip else re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.UNICODE | re.VERBOSE | re.MULTILINE | re.DOTALL) if strip else re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n    if strip:\n        pattern = re.sub(r'\\W', '', pattern)\n    pattern = re.sub(r'\\s+', '\\\\s+', pattern)\n    return re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n    pattern = re.compile(pattern)\n    if strip:\n        return re.sub(r'\\s+', '\\\\s+', pattern)\n    return pattern\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_pattern(pattern, strip=True):\n    if type(pattern) is not str:\n        raise ValueError('The pattern must be a string')\n    pattern = re.compile(pattern)\n    if strip:\n        return re.sub('\\W', '', pattern)\n    else:\n        return pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.VERBOSE | re.UNICODE | re.UNICODE_S) if strip else re.compile(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    regex_pattern = re.compile(pattern)\n    if strip:\n        return regex_pattern.strip()\n    return regex_pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    if strip:\n        pattern = re.sub('\\W', '', pattern)\n    return re.compile(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.VERBOSE | re.UNICODE | re.MULTILINE | re.DOTALL) if strip else re.compile(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.VERBOSE | re.UNICODE | re.UNICODE_EXP) \\\n        if strip else re.compile(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.VERBOSE | re.UNICODE | re.UNICODE_EXP) if strip else re.compile(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.UNICODE | re.VERBOSE | re.MULTILINE | re.DOTALL) if strip else re.compile(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    if strip:\n        pattern = re.sub(r'\\W', '', pattern)\n    pattern = re.sub(r'\\s+', '\\\\s+', pattern)\n    return re.compile(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    pattern = re.compile(pattern)\n    if strip:\n        return re.sub(r'\\s+', '\\\\s+', pattern)\n    return pattern\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_7.py", "name": "run_command", "docstring": "Call the given command(s).", "ques_id": "62ece4992e6aefcf4aabbd83", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_4.py", "name": "is_ipv4", "docstring": "Test if IPv4 address or not\n    ", "ques_id": "62ece4992e6aefcf4aabbd84", "code": ["def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_11.py", "name": "find_roots", "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader", "ques_id": "62ece4992e6aefcf4aabbd85", "code": ["def find_roots(\n    graph: Graph,\n    rdflib_graph: rdflib.Graph,\n    triple: Tuple[Tuple[str, str, str], Tuple[str, str], Tuple[str, str]],\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n\n    triple = triple[::-1]\n    rdf_triple = rdflib.RDFS.skos.to_SKOS(triple)\n    triple_graph = rdflib_graph.triples((rdf_triple,))\n\n    return set(graph.prefixes(triple_graph, include_self=True))\n", "def find_roots(\n    graph: Graph,\n    rdflib_subclass: Union[\n        List[Tuple[str, str, str]], Tuple[str, str, str]\n    ],\n    rdflib_property: Union[\n        List[Tuple[str, str, str]], Tuple[str, str, str]\n    ],\n    strict: bool = True,\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if strict:\n        return set(\n            k for k, v in graph.triples(\n                (graph.skos_pref(rdflib_subclass), rdflib_property, None)\n            )\n            if k in graph\n        )\n\n    return set(graph.prefixes(graph.skos_pref(rdflib_subclass)))\n", "def find_roots(\n    graph: Graph,\n    subclasses: Union[str, Sequence[str]],\n    prop: str\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy. find_roots(graph, rdflib.RDFS.subClassOf) will return a set\n    of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if isinstance(subclasses, str):\n        subclasses = (subclasses,)\n    roots = set()\n    for c in subclasses:\n        subclasses = graph.prefixes(c, prop)\n        for p in subclasses:\n            roots.add(p)\n    return roots\n", "def find_roots(\n    graph: Graph, subclass_of: Tuple[str, str, str]\n) -> Set[Tuple[str, str, str]]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    roots = set()\n    for (child, prop, parent) in graph.triples((subclass_of, None, None)):\n        if prop == RDF.type and (\n            parent is None\n            or graph.value(parent, RDFS.subClassOf) == subclass_of\n        ):\n            roots.add((child, prop, parent))\n    return roots\n", "def find_roots(\n    graph: Graph,\n    subclass: Union[\n        Class, Tuple[Union[Class, str], str, str]\n    ],\n    prop: str = SKOS.subClassOf,\n    parent: Optional[str] = None,\n    relations: Optional[Dict[str, str]] = None,\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    result = set()\n    # This is a hackish way of doing this without a loop.  We need to do this\n    # for deterministic output of the results, because the order of triples\n    # in the results depends on the order of the roots in the triples list.\n    for subclasses in (graph, subclass, relations):\n        for c in subclasses:\n            result.update(c.su", "def find_roots(\n    graph: Graph,\n    triple: Tuple[str, str, str],\n    rdflib_subclass: Optional[Graph] = None,\n    rdflib_broader: Optional[Graph] = None,\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form\n    (child, prop, parent), i.e. the direction of RDFS.subClassOf or\n    SKOS.broader\n    \"\"\"\n    # if triple is None:\n    #     return set()\n    # (child, prop, parent) = triple\n    # if rdflib_subclass and rdflib_subclass == triple:\n    #     return {parent}\n    # if rdflib_broader and rdflib_broader == triple:\n    #     return {parent}\n    # return set()\n    if graph is None:\n        graph = rdflib.Graph()\n    graph.parse(data=triple, format=\"trig\")\n    s = set()\n    for triple in graph.triples(", "def find_roots(\n    graph: Graph,\n    rdflib_subclass: Union[str, URIRef]\n) -> Set[URIRef]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if isinstance(rdflib_subclass, str):\n        rdflib_subclass = URIRef(rdflib_subclass)\n    if isinstance(graph, Graph):\n        for s, p, o in graph.triples((rdflib_subclass, None, None)):\n            yield o\n    else:\n        for s, p, o in graph:\n            yield from find_roots(graph[s], p)\n            yield from find_roots(graph[o], p)\n", "def find_roots(\n    graph: Graph, subclass_of: Union[None, RDFClass, RDFSEntity]\n) -> Set[Tuple[Literal, str, str]]:\n    \"\"\"Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    roots = set()\n    s = set(graph.subjects(subclass_of, predicate=RDF.type))\n    for c in s:\n        s2 = set(graph.objects(c, RDFS.subClassOf))\n        for p in s2:\n            p2 = set(graph.predicates(p, RDFS.subClassOf))\n            roots.update(\n                (\n                    (c, p, p2),\n                    (c, p, p | p2),\n                    (c, p, p2 | p2),\n     ", "def find_roots(\n    graph: Graph,\n    rdflib_subclasses: Set[Tuple[str, str, str]],\n    rdflib_broaders: Set[Tuple[str, str, str]],\n    triple: Tuple[str, str, str],\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    # We don't want to traverse multiple levels of the hierarchy\n    if triple[0] not in rdflib_subclasses:\n        return set()\n\n    # This is the triple from the root to the end of the triple\n    triple_prefix = triple[0] + '|'\n\n    # This is the triple from the start to the end of the triple\n    triple_suffix = triple[1] + '|'\n\n    # This is the triple from the start to the end of the triple\n    triple_tail = tri", "def find_roots(\n    graph: Graph,\n    rdflib_subclass: Union[\n        Tuple[str, str, str],\n        Tuple[str, str, str],\n        Tuple[str, str, str],\n    ],\n    rdflib_broader: Union[\n        Tuple[str, str, str],\n        Tuple[str, str, str],\n    ],\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n\n    \"\"\"\n    roots = set()\n    for (child, prop, parent), rdf_subclass in zip(\n            graph.subjects(RDF.type, SKOS.broader),\n            (rdflib_subclass, rdflib_broader)):\n        if child == parent:\n            continue\n        if rdf_subclass in graph[child][prop]:\n        "], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_roots(\n    graph: Graph,\n    rdflib_graph: rdflib.Graph,\n    triple: Tuple[Tuple[str, str, str], Tuple[str, str], Tuple[str, str]],\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n\n    triple = triple[::-1]\n    rdf_triple = rdflib.RDFS.skos.to_SKOS(triple)\n    triple_graph = rdflib_graph.triples((rdf_triple,))\n\n    return set(graph.prefixes(triple_graph, include_self=True))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: Graph,\n    rdflib_subclass: Union[\n        List[Tuple[str, str, str]], Tuple[str, str, str]\n    ],\n    rdflib_property: Union[\n        List[Tuple[str, str, str]], Tuple[str, str, str]\n    ],\n    strict: bool = True,\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if strict:\n        return set(\n            k for k, v in graph.triples(\n                (graph.skos_pref(rdflib_subclass), rdflib_property, None)\n            )\n            if k in graph\n        )\n\n    return set(graph.prefixes(graph.skos_pref(rdflib_subclass)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: Graph,\n    subclasses: Union[str, Sequence[str]],\n    prop: str\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy. find_roots(graph, rdflib.RDFS.subClassOf) will return a set\n    of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if isinstance(subclasses, str):\n        subclasses = (subclasses,)\n    roots = set()\n    for c in subclasses:\n        subclasses = graph.prefixes(c, prop)\n        for p in subclasses:\n            roots.add(p)\n    return roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: Graph, subclass_of: Tuple[str, str, str]\n) -> Set[Tuple[str, str, str]]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    roots = set()\n    for (child, prop, parent) in graph.triples((subclass_of, None, None)):\n        if prop == RDF.type and (\n            parent is None\n            or graph.value(parent, RDFS.subClassOf) == subclass_of\n        ):\n            roots.add((child, prop, parent))\n    return roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: Graph,\n    subclass: Union[\n        Class, Tuple[Union[Class, str], str, str]\n    ],\n    prop: str = SKOS.subClassOf,\n    parent: Optional[str] = None,\n    relations: Optional[Dict[str, str]] = None,\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    result = set()\n    # This is a hackish way of doing this without a loop.  We need to do this\n    # for deterministic output of the results, because the order of triples\n    # in the results depends on the order of the roots in the triples list.\n    for subclasses in (graph, subclass, relations):\n        for c in subclasses:\n            result.update(c.su", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: Graph,\n    triple: Tuple[str, str, str],\n    rdflib_subclass: Optional[Graph] = None,\n    rdflib_broader: Optional[Graph] = None,\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form\n    (child, prop, parent), i.e. the direction of RDFS.subClassOf or\n    SKOS.broader\n    \"\"\"\n    # if triple is None:\n    #     return set()\n    # (child, prop, parent) = triple\n    # if rdflib_subclass and rdflib_subclass == triple:\n    #     return {parent}\n    # if rdflib_broader and rdflib_broader == triple:\n    #     return {parent}\n    # return set()\n    if graph is None:\n        graph = rdflib.Graph()\n    graph.parse(data=triple, format=\"trig\")\n    s = set()\n    for triple in graph.triples(", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: Graph,\n    rdflib_subclass: Union[str, URIRef]\n) -> Set[URIRef]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if isinstance(rdflib_subclass, str):\n        rdflib_subclass = URIRef(rdflib_subclass)\n    if isinstance(graph, Graph):\n        for s, p, o in graph.triples((rdflib_subclass, None, None)):\n            yield o\n    else:\n        for s, p, o in graph:\n            yield from find_roots(graph[s], p)\n            yield from find_roots(graph[o], p)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: Graph, subclass_of: Union[None, RDFClass, RDFSEntity]\n) -> Set[Tuple[Literal, str, str]]:\n    \"\"\"Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    roots = set()\n    s = set(graph.subjects(subclass_of, predicate=RDF.type))\n    for c in s:\n        s2 = set(graph.objects(c, RDFS.subClassOf))\n        for p in s2:\n            p2 = set(graph.predicates(p, RDFS.subClassOf))\n            roots.update(\n                (\n                    (c, p, p2),\n                    (c, p, p | p2),\n                    (c, p, p2 | p2),\n     ", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: Graph,\n    rdflib_subclasses: Set[Tuple[str, str, str]],\n    rdflib_broaders: Set[Tuple[str, str, str]],\n    triple: Tuple[str, str, str],\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    # We don't want to traverse multiple levels of the hierarchy\n    if triple[0] not in rdflib_subclasses:\n        return set()\n\n    # This is the triple from the root to the end of the triple\n    triple_prefix = triple[0] + '|'\n\n    # This is the triple from the start to the end of the triple\n    triple_suffix = triple[1] + '|'\n\n    # This is the triple from the start to the end of the triple\n    triple_tail = tri", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: Graph,\n    rdflib_subclass: Union[\n        Tuple[str, str, str],\n        Tuple[str, str, str],\n        Tuple[str, str, str],\n    ],\n    rdflib_broader: Union[\n        Tuple[str, str, str],\n        Tuple[str, str, str],\n    ],\n) -> Set[str]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n\n    \"\"\"\n    roots = set()\n    for (child, prop, parent), rdf_subclass in zip(\n            graph.subjects(RDF.type, SKOS.broader),\n            (rdflib_subclass, rdflib_broader)):\n        if child == parent:\n            continue\n        if rdf_subclass in graph[child][prop]:\n        ", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_1.py", "name": "_dump_string", "docstring": "Dump to a py2-unicode or py3-string", "ques_id": "62ece4992e6aefcf4aabbd86", "code": ["def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.dump(obj)\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.represent_scalar(u'tag:yaml.org,2002:str', str(obj))\n", "def _dump_string(obj, dumper=None):\n\n    if dumper is None:\n        dumper = yaml.SafeDumper\n\n    return dumper.represent_scalar(u'![{}]'.format(obj.__class__.__name__),\n                                                                     obj)\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.dump(obj, default_flow_style=False)\n", "def _dump_string(obj, dumper=None):\n\n    if dumper is None:\n        dumper = yaml.SafeDumper\n\n    if isinstance(obj, (bytes, str)):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj)\n    elif isinstance(obj, list):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj)\n    elif isinstance(obj, dict):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', dict(obj))\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', str(obj))\n", "def _dump_string(obj, dumper=None):\n    if not dumper:\n        dumper = yaml.SafeDumper\n\n    if isinstance(obj, six.text_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj)\n    elif isinstance(obj, six.binary_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj)\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:dict', obj)\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    if isinstance(obj, str):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj)\n    elif isinstance(obj, bytes):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj)\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', str(obj))\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.dump(obj, default_flow_style=False)\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    if isinstance(obj, unicode):\n        return dumper.represent_unicode(obj)\n    elif isinstance(obj, str):\n        return dumper.represent_str(obj)\n    else:\n        raise TypeError('Object %s is not of type %s' % (str(obj), str(unicode)))\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.dump(obj, Dumper=dumper, default_flow_style=False)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.dump(obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.represent_scalar(u'tag:yaml.org,2002:str', str(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\n    if dumper is None:\n        dumper = yaml.SafeDumper\n\n    return dumper.represent_scalar(u'![{}]'.format(obj.__class__.__name__),\n                                                                     obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.dump(obj, default_flow_style=False)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\n    if dumper is None:\n        dumper = yaml.SafeDumper\n\n    if isinstance(obj, (bytes, str)):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj)\n    elif isinstance(obj, list):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj)\n    elif isinstance(obj, dict):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', dict(obj))\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', str(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if not dumper:\n        dumper = yaml.SafeDumper\n\n    if isinstance(obj, six.text_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj)\n    elif isinstance(obj, six.binary_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj)\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:dict', obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    if isinstance(obj, str):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj)\n    elif isinstance(obj, bytes):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj)\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', str(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.dump(obj, default_flow_style=False)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    if isinstance(obj, unicode):\n        return dumper.represent_unicode(obj)\n    elif isinstance(obj, str):\n        return dumper.represent_str(obj)\n    else:\n        raise TypeError('Object %s is not of type %s' % (str(obj), str(unicode)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    return dumper.dump(obj, Dumper=dumper, default_flow_style=False)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_8.py", "name": "build_app_logger", "docstring": "General purpose application logger. Useful mainly for debugging", "ques_id": "62ece4992e6aefcf4aabbd87", "code": ["def build_app_logger(name='app', logfile='app.log', debug=True):\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.INFO)\n\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    handler.setFormatter(formatter)\n\n    app_logger.addHandler(handler)\n\n    if debug:\n        app_logger.setLevel(logging.DEBUG)\n\n    return app_logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    if debug:\n        handler = logging.FileHandler(logfile, mode='w')\n        handler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter('%(levelname)s - %(message)s')\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n\n    # create console handler and set level to debug\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n\n    # create formatter\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    # add formatter to ch\n    ch.setFormatter(formatter)\n\n    # add ch to logger\n    app_logger.addHandler(ch)\n\n    if debug:\n        app_logger.debug('app logger ready')\n\n    return app_logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    log = logging.getLogger(name)\n    log.setLevel(logging.DEBUG if debug else logging.INFO)\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.INFO)\n\n    formatter = logging.Formatter('%(levelname)s - %(message)s')\n    ch.setFormatter(formatter)\n\n    log.addHandler(ch)\n\n    return log\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n\n    # create console handler and set level to debug\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n\n    # create formatter\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    # add formatter to ch\n    ch.setFormatter(formatter)\n\n    # add ch to logger\n    app_logger.addHandler(ch)\n\n    # add logger to logger list\n    app_logger.debug(logfile)\n    app_logger.info('Logger started.')\n\n    if debug:\n        return app_logger\n    else:\n        return True\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    handler = logging.FileHandler(logfile)\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    if debug:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    handler = logging.FileHandler(logfile, 'w')\n    handler.setLevel(logging.INFO)\n\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n\n    if debug:\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.DEBUG)\n        ch.setFormatter(formatter)\n\n    logger.addHandler(handler)\n    logger.addHandler(ch)\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    # Setup logging\n    if debug:\n        level = logging.DEBUG\n    else:\n        level = logging.INFO\n\n    # Setup logger\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(level)\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(name)s - %(message)s'\n    )\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    # Setup logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n    # Setup formatter\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(message)s'\n    )\n\n    # Add formatter to logger\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatter)\n\n    # Add handler to logger\n    logger.addHandler(console_handler)\n\n    # Log file\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setFormatter(formatter)\n\n    # Add handler to logger\n    logger.addHandler(file_handler)\n\n    return logger\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.INFO)\n\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    handler.setFormatter(formatter)\n\n    app_logger.addHandler(handler)\n\n    if debug:\n        app_logger.setLevel(logging.DEBUG)\n\n    return app_logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    if debug:\n        handler = logging.FileHandler(logfile, mode='w')\n        handler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter('%(levelname)s - %(message)s')\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n    return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n\n    # create console handler and set level to debug\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n\n    # create formatter\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    # add formatter to ch\n    ch.setFormatter(formatter)\n\n    # add ch to logger\n    app_logger.addHandler(ch)\n\n    if debug:\n        app_logger.debug('app logger ready')\n\n    return app_logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    log = logging.getLogger(name)\n    log.setLevel(logging.DEBUG if debug else logging.INFO)\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.INFO)\n\n    formatter = logging.Formatter('%(levelname)s - %(message)s')\n    ch.setFormatter(formatter)\n\n    log.addHandler(ch)\n\n    return log\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n\n    # create console handler and set level to debug\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n\n    # create formatter\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    # add formatter to ch\n    ch.setFormatter(formatter)\n\n    # add ch to logger\n    app_logger.addHandler(ch)\n\n    # add logger to logger list\n    app_logger.debug(logfile)\n    app_logger.info('Logger started.')\n\n    if debug:\n        return app_logger\n    else:\n        return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    handler = logging.FileHandler(logfile)\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    if debug:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    handler = logging.FileHandler(logfile, 'w')\n    handler.setLevel(logging.INFO)\n\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n\n    if debug:\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.DEBUG)\n        ch.setFormatter(formatter)\n\n    logger.addHandler(handler)\n    logger.addHandler(ch)\n    return logger\n", "is_pass": true, "return_code": 0}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    # Setup logging\n    if debug:\n        level = logging.DEBUG\n    else:\n        level = logging.INFO\n\n    # Setup logger\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(level)\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(name)s - %(message)s'\n    )\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    # Setup logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n    # Setup formatter\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(message)s'\n    )\n\n    # Add formatter to logger\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatter)\n\n    # Add handler to logger\n    logger.addHandler(console_handler)\n\n    # Log file\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setFormatter(formatter)\n\n    # Add handler to logger\n    logger.addHandler(file_handler)\n\n    return logger\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_6.py", "name": "make_array", "docstring": "Function to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create", "ques_id": "62ece4992e6aefcf4aabbd88", "code": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.asarray(np.random.rand(*shape), dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.asarray(np.random.rand(*shape), dtype=dtype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype=dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "plib_runnable/test_2.py", "name": "gaussian", "docstring": "Gaussian centered around 0.2 with a sigma of 0.1.", "ques_id": "62ece4992e6aefcf4aabbd89", "code": ["def gaussian(x):\n    return np.exp(-(x-0.2)**2/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x - 0.2)**2/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x-0.2)**2/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x - 0.2)**2/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def gaussian(x):\n    return np.exp(-(x-0.2)**2/(2*0.1**2))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x - 0.2)**2/(2*0.1**2))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x-0.2)**2/(2*0.1**2))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x - 0.2)**2/(2*0.1**2))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_13.py", "name": "load_configurations", "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.", "ques_id": "62ece4992e6aefcf4aabbd8a", "code": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    parsers = [_load_parser(fn, resolve_env) for fn in config_filenames]\n    return _validate_configurations(parsers, overrides)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n\n    for config_filename in config_filenames:\n        try:\n            parsed_config = load_config_file(config_filename, resolve_env)\n            if parsed_config is not None:\n                configs[config_filename] = parsed_config\n            else:\n                errors.append('Invalid configuration file: %s' % config_filename)\n        except IOError as e:\n            errors.append('Unable to load configuration file: %s' % e)\n        except ParseError as e:\n            errors.append('Error parsing configuration file: %s' % e)\n\n    if overrides is not None:\n        configs.update(overrides)\n\n    return (configs, errors)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n    for config_filename in config_filenames:\n        try:\n            config = load_config_file(config_filename, resolve_env)\n        except IOError as e:\n            errors.append(ConfigError(\n                'Error while parsing configuration file {0}: {1}'\n               .format(config_filename, str(e))))\n            continue\n        except YAMLError as e:\n            errors.append(ConfigError(\n                'Error while parsing YAML configuration file {0}: {1}'\n               .format(config_filename, str(e))))\n            continue\n\n        if overrides:\n            overrides.update(config)\n        else:\n            configs.update(config)\n\n        configs[config_filename] = config\n\n    return (configs, errors)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n\n    for config_filename in config_filenames:\n        try:\n            config = load_config_file(config_filename, resolve_env=resolve_env)\n        except IOError as exc:\n            errors.append(exc)\n            config = {}\n\n        if overrides:\n            config.update(overrides)\n\n        # check for valid configuration\n        if not is_valid_config(config):\n            errors.append(ConfigurationError(\n                'Invalid configuration file \"{}\". '\n                'Please check your configuration file.'.format(config_filename)\n            ))\n            continue\n\n        # check for valid configuration file (no errors)\n        if not is_valid_config_file(config, config_filename):\n            errors.append(ConfigurationError(\n                'Invalid configuration file \"{}\". '\n              ", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n    for filename in config_filenames:\n        try:\n            with open(filename) as f:\n                configs[filename] = load_config(f, resolve_env=resolve_env)\n        except IOError as e:\n            errors.append((filename, e))\n            continue\n\n        if overrides:\n            for key, val in overrides.items():\n                if hasattr(configs[filename], key) and getattr(configs[filename], key)!= val:\n                    errors.append((filename, \"Conflicting setting {}={}\".format(key, val)))\n\n        if configs[filename].parse_errors:\n            errors.append((filename, configs[filename].parse_errors))\n\n    return configs, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    parser = create_parser()\n    options = parser.parse_args(config_filenames)\n\n    # Convert overrides to a dictionary\n    if overrides:\n        options.overrides = dict(overrides)\n\n    # Resolve environment variable settings\n    if resolve_env:\n        options.resolve_env = True\n\n    # Load the configurations and validate them\n    configs = load_configurations_from_options(options)\n\n    errors = []\n    for config_filename, parsed_config in configs.items():\n        parsed_config = validate_configuration(config_filename, parsed_config)\n        if parsed_config is not None:\n            errors.append((config_filename, parsed_config))\n\n    return configs, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    validate_config_filenames(config_filenames)\n    results = {}\n    errors = []\n    for config_filename in config_filenames:\n        config = load_configuration(config_filename, overrides, resolve_env)\n        results[config_filename] = config\n        logging.getLogger(\"configuration\").info(\"Loaded configuration: %s\", config)\n        errors.append(config.validate())\n    return results, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n    for filename in config_filenames:\n        try:\n            configs[filename] = load_config_from_file(\n                filename,\n                overrides=overrides,\n                resolve_env=resolve_env,\n            )\n        except ConfigFileError as error:\n            errors.append(error)\n            continue\n        except IOError as error:\n            errors.append(error)\n            continue\n        except UnknownConfigFileError as error:\n            errors.append(error)\n            continue\n        except UnknownConfigFileNameError as error:\n            errors.append(error)\n            continue\n        except UnknownFileError as error:\n            errors.append(error)\n            continue\n\n        if configs[filename].validate():\n ", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            config = load_configuration(config_filename,\n                                       overrides=overrides,\n                                       resolve_env=resolve_env)\n        except IOError as e:\n            logs.append(e.args[1])\n        else:\n            configs[config_filename] = config\n\n    return (configs, logs)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n    for config_filename in config_filenames:\n        try:\n            config = load_config_file(config_filename, resolve_env=resolve_env)\n        except Exception as e:\n            errors.append(str(e))\n            config = {}\n        else:\n            if overrides:\n                config = override_config(config, overrides)\n            configs[config_filename] = config\n    return (configs, errors)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    parsers = [_load_parser(fn, resolve_env) for fn in config_filenames]\n    return _validate_configurations(parsers, overrides)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n\n    for config_filename in config_filenames:\n        try:\n            parsed_config = load_config_file(config_filename, resolve_env)\n            if parsed_config is not None:\n                configs[config_filename] = parsed_config\n            else:\n                errors.append('Invalid configuration file: %s' % config_filename)\n        except IOError as e:\n            errors.append('Unable to load configuration file: %s' % e)\n        except ParseError as e:\n            errors.append('Error parsing configuration file: %s' % e)\n\n    if overrides is not None:\n        configs.update(overrides)\n\n    return (configs, errors)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n    for config_filename in config_filenames:\n        try:\n            config = load_config_file(config_filename, resolve_env)\n        except IOError as e:\n            errors.append(ConfigError(\n                'Error while parsing configuration file {0}: {1}'\n               .format(config_filename, str(e))))\n            continue\n        except YAMLError as e:\n            errors.append(ConfigError(\n                'Error while parsing YAML configuration file {0}: {1}'\n               .format(config_filename, str(e))))\n            continue\n\n        if overrides:\n            overrides.update(config)\n        else:\n            configs.update(config)\n\n        configs[config_filename] = config\n\n    return (configs, errors)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n\n    for config_filename in config_filenames:\n        try:\n            config = load_config_file(config_filename, resolve_env=resolve_env)\n        except IOError as exc:\n            errors.append(exc)\n            config = {}\n\n        if overrides:\n            config.update(overrides)\n\n        # check for valid configuration\n        if not is_valid_config(config):\n            errors.append(ConfigurationError(\n                'Invalid configuration file \"{}\". '\n                'Please check your configuration file.'.format(config_filename)\n            ))\n            continue\n\n        # check for valid configuration file (no errors)\n        if not is_valid_config_file(config, config_filename):\n            errors.append(ConfigurationError(\n                'Invalid configuration file \"{}\". '\n              ", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n    for filename in config_filenames:\n        try:\n            with open(filename) as f:\n                configs[filename] = load_config(f, resolve_env=resolve_env)\n        except IOError as e:\n            errors.append((filename, e))\n            continue\n\n        if overrides:\n            for key, val in overrides.items():\n                if hasattr(configs[filename], key) and getattr(configs[filename], key)!= val:\n                    errors.append((filename, \"Conflicting setting {}={}\".format(key, val)))\n\n        if configs[filename].parse_errors:\n            errors.append((filename, configs[filename].parse_errors))\n\n    return configs, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    parser = create_parser()\n    options = parser.parse_args(config_filenames)\n\n    # Convert overrides to a dictionary\n    if overrides:\n        options.overrides = dict(overrides)\n\n    # Resolve environment variable settings\n    if resolve_env:\n        options.resolve_env = True\n\n    # Load the configurations and validate them\n    configs = load_configurations_from_options(options)\n\n    errors = []\n    for config_filename, parsed_config in configs.items():\n        parsed_config = validate_configuration(config_filename, parsed_config)\n        if parsed_config is not None:\n            errors.append((config_filename, parsed_config))\n\n    return configs, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    validate_config_filenames(config_filenames)\n    results = {}\n    errors = []\n    for config_filename in config_filenames:\n        config = load_configuration(config_filename, overrides, resolve_env)\n        results[config_filename] = config\n        logging.getLogger(\"configuration\").info(\"Loaded configuration: %s\", config)\n        errors.append(config.validate())\n    return results, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n    for filename in config_filenames:\n        try:\n            configs[filename] = load_config_from_file(\n                filename,\n                overrides=overrides,\n                resolve_env=resolve_env,\n            )\n        except ConfigFileError as error:\n            errors.append(error)\n            continue\n        except IOError as error:\n            errors.append(error)\n            continue\n        except UnknownConfigFileError as error:\n            errors.append(error)\n            continue\n        except UnknownConfigFileNameError as error:\n            errors.append(error)\n            continue\n        except UnknownFileError as error:\n            errors.append(error)\n            continue\n\n        if configs[filename].validate():\n ", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            config = load_configuration(config_filename,\n                                       overrides=overrides,\n                                       resolve_env=resolve_env)\n        except IOError as e:\n            logs.append(e.args[1])\n        else:\n            configs[config_filename] = config\n\n    return (configs, logs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n    for config_filename in config_filenames:\n        try:\n            config = load_config_file(config_filename, resolve_env=resolve_env)\n        except Exception as e:\n            errors.append(str(e))\n            config = {}\n        else:\n            if overrides:\n                config = override_config(config, overrides)\n            configs[config_filename] = config\n    return (configs, errors)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_3.py", "name": "force_string", "docstring": "This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. ", "ques_id": "62ece4992e6aefcf4aabbd8b", "code": ["def force_string(obj):\n    if isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    return obj\n", "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, str):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"force_string only accepts bytes or str.\")\n", "def force_string(obj):\n    return obj.encode(\"utf-8\") if isinstance(obj, str) else obj\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    return obj.encode('utf-8')\n", "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    else:\n        return obj\n", "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def force_string(obj):\n    if isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    return obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, str):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"force_string only accepts bytes or str.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    return obj.encode(\"utf-8\") if isinstance(obj, str) else obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    return obj.encode('utf-8')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    else:\n        return obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "from_ticks", "docstring": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)", "ques_id": "62e60723d76274f8a4026b76", "code": ["def from_ticks(cls, ticks, tz=None):\n    if isinstance(ticks, datetime):\n        ticks = [ticks]\n    return super(Date, cls).__new__(cls, *(int(x) for x in ticks))\n", "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(\n        ticks, tz=tz, normalize=True, strict=False\n    )\n", "def from_ticks(cls, ticks, tz=None):\n    if isinstance(ticks, datetime.datetime):\n        ticks = Timestamp(ticks)\n\n    if ticks.tzinfo and ticks.tzinfo!= tz:\n        ticks = ticks.astimezone(tz)\n\n    return cls.from_datetime(ticks)\n", "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(\n        cls.from_iso_format(ticks, tz=tz), tz=tz)\n", "def from_ticks(cls, ticks, tz=None):\n    if ticks is None:\n        return cls()\n    if isinstance(ticks, datetime.timedelta):\n        ticks = TimeDelta.from_timedelta(ticks)\n    return super(Timezone, cls).from_ticks(ticks, tz)\n", "def from_ticks(cls, ticks, tz=None):\n    if ticks == 0:\n        return Date(0)\n\n    return DateTime(\n        year=int(ticks / NANO_SECONDS),\n        month=int((ticks % NANO_SECONDS) / 12),\n        day=int(ticks % NANO_SECONDS / 14),\n        hour=int((ticks % NANO_SECONDS) / 14),\n        minute=int((ticks % NANO_SECONDS) % 14),\n        second=int((ticks % NANO_SECONDS) % 14),\n        tzinfo=tz or get_default_timezone(),\n    )\n", "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(\n        ticks, strict=False, tzinfo=tz or get_default_timezone())\n", "def from_ticks(cls, ticks, tz=None):\n\n    if not HAS_NUMPY:\n        raise NumpyUnsupported()\n\n    if isinstance(ticks, datetime.timedelta):\n        ticks = timedelta_to_nanos(ticks)\n\n    if isinstance(ticks, datetime.datetime):\n        ticks = time.mktime(ticks.timetuple())\n\n    if tz is None:\n        tz = get_current_timezone()\n\n    return cls.from_datetime(datetime.datetime.utcfromtimestamp(ticks), tz)\n", "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(\n        cls._get_both_normalized_ticks(ticks, strict=False), tz=tz\n    )\n", "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(ticks, strict=False)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if isinstance(ticks, datetime):\n        ticks = [ticks]\n    return super(Date, cls).__new__(cls, *(int(x) for x in ticks))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(\n        ticks, tz=tz, normalize=True, strict=False\n    )\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if isinstance(ticks, datetime.datetime):\n        ticks = Timestamp(ticks)\n\n    if ticks.tzinfo and ticks.tzinfo!= tz:\n        ticks = ticks.astimezone(tz)\n\n    return cls.from_datetime(ticks)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(\n        cls.from_iso_format(ticks, tz=tz), tz=tz)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if ticks is None:\n        return cls()\n    if isinstance(ticks, datetime.timedelta):\n        ticks = TimeDelta.from_timedelta(ticks)\n    return super(Timezone, cls).from_ticks(ticks, tz)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if ticks == 0:\n        return Date(0)\n\n    return DateTime(\n        year=int(ticks / NANO_SECONDS),\n        month=int((ticks % NANO_SECONDS) / 12),\n        day=int(ticks % NANO_SECONDS / 14),\n        hour=int((ticks % NANO_SECONDS) / 14),\n        minute=int((ticks % NANO_SECONDS) % 14),\n        second=int((ticks % NANO_SECONDS) % 14),\n        tzinfo=tz or get_default_timezone(),\n    )\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(\n        ticks, strict=False, tzinfo=tz or get_default_timezone())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\n    if not HAS_NUMPY:\n        raise NumpyUnsupported()\n\n    if isinstance(ticks, datetime.timedelta):\n        ticks = timedelta_to_nanos(ticks)\n\n    if isinstance(ticks, datetime.datetime):\n        ticks = time.mktime(ticks.timetuple())\n\n    if tz is None:\n        tz = get_current_timezone()\n\n    return cls.from_datetime(datetime.datetime.utcfromtimestamp(ticks), tz)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(\n        cls._get_both_normalized_ticks(ticks, strict=False), tz=tz\n    )\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    return cls.from_iso_format(ticks, strict=False)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_async/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "ques_id": "62e60873d76274f8a4026bd8", "code": ["def protocol_handlers(cls, protocol_version=None):\n    return cls._protocol_handlers.get(protocol_version, [])\n", "def protocol_handlers(cls, protocol_version=None):\n    return cls.PROTOCOL_HANDLERS_CLS.get(\n        (protocol_version or cls.PROTOCOL_VERSION),\n        (None, cls.PROTOCOL_HANDLERS_CLS[(protocol_version or cls.PROTOCOL_VERSION)])\n    )\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        protocol_version = cls.PROTOCOL_VERSION\n    return cls._protocol_handlers.get(protocol_version, [])\n", "def protocol_handlers(cls, protocol_version=None):\n    return cls.HANDLER_CLS.protocol_handlers(\n        cls.PROTOCOL_VERSION, protocol_version)\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        protocol_version = API_VERSION\n    if protocol_version == 1:\n        return (\n            (neo4j.Protocol.USER_AGENT, neo4j.Protocol.USER_METHOD),\n            neo4j.Protocol.SERVICE_METHOD,\n            neo4j.Protocol.CLIENT_METHOD,\n        )\n    else:\n        raise ValueError('unsupported protocol version {}'.format(protocol_version))\n", "def protocol_handlers(cls, protocol_version=None):\n    \"Protocol handlers\"\n    return cls._protocol_handlers.get(protocol_version, [])\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        version = cls.PROTOCOL_VERSION\n    else:\n        version = protocol_version\n    return cls.HANDLER_CLS.get(version, Handler)\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        protocol_version = cls.PROTOCOL_VERSION\n    return cls.HANDLER_CLS.get(protocol_version, None)\n", "def protocol_handlers(cls, protocol_version=None):\n    return cls.HANDLER_CLS.protocol_handlers(\n        cls, protocol_version=protocol_version)\n", "def protocol_handlers(cls, protocol_version=None):\n    return cls.UNPACKER_CLS.protocol_handlers(\n        cls, protocol_version=protocol_version)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    return cls._protocol_handlers.get(protocol_version, [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    return cls.PROTOCOL_HANDLERS_CLS.get(\n        (protocol_version or cls.PROTOCOL_VERSION),\n        (None, cls.PROTOCOL_HANDLERS_CLS[(protocol_version or cls.PROTOCOL_VERSION)])\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        protocol_version = cls.PROTOCOL_VERSION\n    return cls._protocol_handlers.get(protocol_version, [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    return cls.HANDLER_CLS.protocol_handlers(\n        cls.PROTOCOL_VERSION, protocol_version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        protocol_version = API_VERSION\n    if protocol_version == 1:\n        return (\n            (neo4j.Protocol.USER_AGENT, neo4j.Protocol.USER_METHOD),\n            neo4j.Protocol.SERVICE_METHOD,\n            neo4j.Protocol.CLIENT_METHOD,\n        )\n    else:\n        raise ValueError('unsupported protocol version {}'.format(protocol_version))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    \"Protocol handlers\"\n    return cls._protocol_handlers.get(protocol_version, [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        version = cls.PROTOCOL_VERSION\n    else:\n        version = protocol_version\n    return cls.HANDLER_CLS.get(version, Handler)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        protocol_version = cls.PROTOCOL_VERSION\n    return cls.HANDLER_CLS.get(protocol_version, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    return cls.HANDLER_CLS.protocol_handlers(\n        cls, protocol_version=protocol_version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    return cls.UNPACKER_CLS.protocol_handlers(\n        cls, protocol_version=protocol_version)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/api.py", "project": "neo4j/neo4j-python-driver", "name": "from_raw_values", "docstring": "Create a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]", "ques_id": "62e60e3bd76274f8a4026d1a", "code": ["def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/split.py", "project": "ynikitenko/lena", "name": "_get_seq_with_type", "docstring": "Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).", "ques_id": "62b87b199a0c4fa8b80b354c", "code": ["def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        # We need to copy the sequence because if we are passed a sequence\n        # object, we will lose the type information.\n        if bufsize is None:\n            seq_type = seq.__class__\n        else:\n            seq_type = getattr(seq, '__class__', None)\n        return seq._get_seq_with_type(bufsize), seq_type\n    else:\n        return seq, None\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, types.GeneratorType):\n        # We weren't passed an iterator or a sequence -- we want to make a tuple\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return tuple(_get_seq_with_type(x, bufsize) for x in seq)\n    elif isinstance(seq, (bytes, bytearray, memoryview)):\n        # We weren't passed a bytes or bytearray -- we want to make a tuple\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return tuple((seq, bufsize))\n    elif isinstance(seq, memoryview):\n        # We weren't passed an memoryview -- we want to make a tuple\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return tuple((seq, bufsize))\n    elif isinstance(", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return seq, SequenceType.Sequence\n    elif isinstance(seq, (tuple, list)):\n        if len(seq) > bufsize:\n            raise ValueError(\"Sequence is too long to be stored. \"\n                             \"Size of sequence is %d\" % len(seq))\n        type_ = seq[0].__class__\n        if issubclass(type_, SequenceType):\n            return seq[:bufsize], type_\n    raise TypeError(\"Unsupported type %r\" % type(seq))\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            return seq, type(seq)\n        else:\n            return seq, type(seq)(bufsize)\n    else:\n        return seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        for i, element in enumerate(seq):\n            seq[i:i + bufsize] = (element,)\n        return seq, Sequence\n    else:\n        return seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        return (seq, seq.__class__)\n    if isinstance(seq, SequenceType):\n        if bufsize is None:\n            bufsize = sys.getrefcount(seq)\n        return (Sequence(seq), SequenceType(bufsize))\n    raise TypeError(\"expected Sequence or SequenceType, got %s\" % type(seq))\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (seq._seq, seq._type), bufsize\n    else:\n        return seq, bufsize\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            return seq, type(seq)\n        else:\n            return seq, type(seq)(bufsize)\n    else:\n        return seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getrecursionlimit() or 1024\n        return seq, type(seq)\n    else:\n        return seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        return (seq._get_seq_with_type(bufsize=bufsize), seq.__class__)\n    else:\n        return seq, type(seq)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        # We need to copy the sequence because if we are passed a sequence\n        # object, we will lose the type information.\n        if bufsize is None:\n            seq_type = seq.__class__\n        else:\n            seq_type = getattr(seq, '__class__', None)\n        return seq._get_seq_with_type(bufsize), seq_type\n    else:\n        return seq, None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, types.GeneratorType):\n        # We weren't passed an iterator or a sequence -- we want to make a tuple\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return tuple(_get_seq_with_type(x, bufsize) for x in seq)\n    elif isinstance(seq, (bytes, bytearray, memoryview)):\n        # We weren't passed a bytes or bytearray -- we want to make a tuple\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return tuple((seq, bufsize))\n    elif isinstance(seq, memoryview):\n        # We weren't passed an memoryview -- we want to make a tuple\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return tuple((seq, bufsize))\n    elif isinstance(", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return seq, SequenceType.Sequence\n    elif isinstance(seq, (tuple, list)):\n        if len(seq) > bufsize:\n            raise ValueError(\"Sequence is too long to be stored. \"\n                             \"Size of sequence is %d\" % len(seq))\n        type_ = seq[0].__class__\n        if issubclass(type_, SequenceType):\n            return seq[:bufsize], type_\n    raise TypeError(\"Unsupported type %r\" % type(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            return seq, type(seq)\n        else:\n            return seq, type(seq)(bufsize)\n    else:\n        return seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        for i, element in enumerate(seq):\n            seq[i:i + bufsize] = (element,)\n        return seq, Sequence\n    else:\n        return seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        return (seq, seq.__class__)\n    if isinstance(seq, SequenceType):\n        if bufsize is None:\n            bufsize = sys.getrefcount(seq)\n        return (Sequence(seq), SequenceType(bufsize))\n    raise TypeError(\"expected Sequence or SequenceType, got %s\" % type(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (seq._seq, seq._type), bufsize\n    else:\n        return seq, bufsize\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            return seq, type(seq)\n        else:\n            return seq, type(seq)(bufsize)\n    else:\n        return seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getrecursionlimit() or 1024\n        return seq, type(seq)\n    else:\n        return seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        return (seq._get_seq_with_type(bufsize=bufsize), seq.__class__)\n    else:\n        return seq, type(seq)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.", "ques_id": "62b87b4f9a0c4fa8b80b3581", "code": ["def scale(self, other=None, recompute=False):\n\n    if recompute or (self.scale is None and other is not None):\n        if other is None:\n            self._update_context({'scale': 1.0 / self.bins })\n        else:\n            self._update_context({'scale': (self.edges[1:] - self.edges[:-1]) * other + self.edges[:-1] })\n\n    if self.scale is None:\n        raise LenaValueError('scale not computed yet')\n\n    return self.scale\n", "def scale(self, other=None, recompute=False):\n\n    if not recompute and self._scale is not None:\n        return self._scale\n\n    if other is not None:\n        try:\n            self._update_context(context=copy.copy(other.context))\n        except TypeError:\n            raise LenaValueError('Unable to recompute scale of histogram'+\n                                 'with type %s' % type(other))\n\n    # compute the scale\n    self._update_context()\n    self._scale = np.sum(self._context.get_values())\n\n    return self._scale\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        # compute scale\n        self._update_context({'scale': stats.linregress(self.edges, self.hist.filled_values)[0]})\n\n        return self._update_context({'scale': self.scale})\n\n    if recompute:\n        # compute scale only after changing (filling) histo\n        self._update_context({'scale': other})\n\n        return other._update_context({'scale': self.scale})\n\n    self._update_context({'scale': copy.copy(other.scale)})\n    return other\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self._scale is None:\n            if recompute:\n                self._scale = self._compute_scale()\n            else:\n                return self._scale\n        else:\n            raise LenaValueError(\"Can't set scale to None\")\n\n    else:\n        if recompute:\n            self._scale = other._compute_scale()\n        else:\n            self._scale = other\n\n    self.ranges = []\n    self._update_context(self.scale.ranges)\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        return self._scale\n\n    if recompute:\n        self._update_context({\"scale\": copy.copy(other.scale)})\n    else:\n        try:\n            nbins = self.bins - (self.bins % 2) + 1\n            self._scale = (self.edges[1:] + self.edges[:-1]) / nbins\n        except ZeroDivisionError:\n            raise LenaValueError(\"scale cannot be computed without recompute\")\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self._scale is None:\n            self._scale = self.ranges.sum() / self.nbins\n        return self._scale\n\n    if recompute:\n        self._update_context({'scale': other})\n    else:\n        try:\n            other = float(other)\n        except TypeError:\n            raise LenaValueError('Cannot compute scale of histogram with '\n                                 'input value of type {0!r}'\n                                .format(type(other).__name__))\n        else:\n            if self._scale is None:\n                self._scale = other\n            else:\n                self._scale = self.scale(other, recompute=True)\n\n    return self._scale\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if recompute:\n            self._update_context({'scale': 0.0})\n            return self\n        else:\n            raise LenaValueError('Cannot recompute scale of histogram because scale has not been computed yet.')\n\n    self.bins = self.edges + other\n    self.ranges = (self.edges[0], self.edges[-1])\n\n    return self\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self._scale is None:\n            raise LenaValueError('scale not computed before')\n        else:\n            other = self._scale\n    else:\n        if recompute:\n            self._update_context(copy.copy(other))\n\n    self._scale = numpy.trapz(self.bins * other, self.edges)\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self._scale is None:\n            raise LenaValueError('Cannot rescale histogram to scale 0.')\n        else:\n            other = self._scale\n\n    if recompute or (isinstance(other, numbers.Number) and other == 0):\n        self._scale = other * self.bins\n\n    return self._scale\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        # compute it before setting scale\n        self._update_context({'scale': self.scale})\n        return self.scale\n\n    if recompute:\n        # recompute scale if it was not computed before\n        self._update_context({'scale': self.scale})\n        self.bins = other\n        self.ranges = None\n        self.edges = None\n    else:\n        # otherwise compute it and store it\n        self._update_context({'scale': other})\n        return other\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None, recompute=False):\n\n    if recompute or (self.scale is None and other is not None):\n        if other is None:\n            self._update_context({'scale': 1.0 / self.bins })\n        else:\n            self._update_context({'scale': (self.edges[1:] - self.edges[:-1]) * other + self.edges[:-1] })\n\n    if self.scale is None:\n        raise LenaValueError('scale not computed yet')\n\n    return self.scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if not recompute and self._scale is not None:\n        return self._scale\n\n    if other is not None:\n        try:\n            self._update_context(context=copy.copy(other.context))\n        except TypeError:\n            raise LenaValueError('Unable to recompute scale of histogram'+\n                                 'with type %s' % type(other))\n\n    # compute the scale\n    self._update_context()\n    self._scale = np.sum(self._context.get_values())\n\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        # compute scale\n        self._update_context({'scale': stats.linregress(self.edges, self.hist.filled_values)[0]})\n\n        return self._update_context({'scale': self.scale})\n\n    if recompute:\n        # compute scale only after changing (filling) histo\n        self._update_context({'scale': other})\n\n        return other._update_context({'scale': self.scale})\n\n    self._update_context({'scale': copy.copy(other.scale)})\n    return other\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self._scale is None:\n            if recompute:\n                self._scale = self._compute_scale()\n            else:\n                return self._scale\n        else:\n            raise LenaValueError(\"Can't set scale to None\")\n\n    else:\n        if recompute:\n            self._scale = other._compute_scale()\n        else:\n            self._scale = other\n\n    self.ranges = []\n    self._update_context(self.scale.ranges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        return self._scale\n\n    if recompute:\n        self._update_context({\"scale\": copy.copy(other.scale)})\n    else:\n        try:\n            nbins = self.bins - (self.bins % 2) + 1\n            self._scale = (self.edges[1:] + self.edges[:-1]) / nbins\n        except ZeroDivisionError:\n            raise LenaValueError(\"scale cannot be computed without recompute\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self._scale is None:\n            self._scale = self.ranges.sum() / self.nbins\n        return self._scale\n\n    if recompute:\n        self._update_context({'scale': other})\n    else:\n        try:\n            other = float(other)\n        except TypeError:\n            raise LenaValueError('Cannot compute scale of histogram with '\n                                 'input value of type {0!r}'\n                                .format(type(other).__name__))\n        else:\n            if self._scale is None:\n                self._scale = other\n            else:\n                self._scale = self.scale(other, recompute=True)\n\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if recompute:\n            self._update_context({'scale': 0.0})\n            return self\n        else:\n            raise LenaValueError('Cannot recompute scale of histogram because scale has not been computed yet.')\n\n    self.bins = self.edges + other\n    self.ranges = (self.edges[0], self.edges[-1])\n\n    return self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self._scale is None:\n            raise LenaValueError('scale not computed before')\n        else:\n            other = self._scale\n    else:\n        if recompute:\n            self._update_context(copy.copy(other))\n\n    self._scale = numpy.trapz(self.bins * other, self.edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self._scale is None:\n            raise LenaValueError('Cannot rescale histogram to scale 0.')\n        else:\n            other = self._scale\n\n    if recompute or (isinstance(other, numbers.Number) and other == 0):\n        self._scale = other * self.bins\n\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        # compute it before setting scale\n        self._update_context({'scale': self.scale})\n        return self.scale\n\n    if recompute:\n        # recompute scale if it was not computed before\n        self._update_context({'scale': self.scale})\n        self.bins = other\n        self.ranges = None\n        self.edges = None\n    else:\n        # otherwise compute it and store it\n        self._update_context({'scale': other})\n        return other\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.", "ques_id": "62b87b519a0c4fa8b80b3583", "code": ["def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if self._scale is None:\n        if self._coord_names is not None:\n            self._scale = 1.0 / self._coord_names[-1]\n        else:\n            self._scale = 1.0\n    if self._scale!= other:\n        msg = 'Graph scale mismatch. Expected %s, got %s.' % (self.field_names, other)\n        raise LenaValueError(msg)\n    return self._scale\n", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (int, float)):\n        raise TypeError('scale must be a number')\n    if other!= 0:\n        self._update_context({'scale': other})\n    return self._scale\n", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (numbers.Number, np.number)):\n        raise TypeError('other must be a number')\n    if not self._coord_names == other.shape:\n        raise ValueError('graph coordinates must match')\n    self._scale = other\n", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if isinstance(other, (int, float)):\n        self._scale = other\n        self._update_context({'scale': other})\n    else:\n        raise LenaValueError(\"Unknown scale of a graph.\")\n", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (int, float)):\n        raise TypeError(\"Unsupported type '{0}'\".format(type(other)))\n    if other < 0:\n        raise ValueError(\"Unsupported scale value '{0}'\".format(other))\n    self._scale = other\n", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if isinstance(other, (tuple, list)):\n        if len(other) == 2:\n            other = (other[0], other[1])\n        elif len(other) == 3:\n            other = (other[0], other[1], other[2])\n    try:\n        self._scale = float(other[0])\n    except ValueError:\n        pass\n    try:\n        self._scale = float(other[1])\n    except ValueError:\n        pass\n    try:\n        self._scale = float(other[2])\n    except ValueError:\n        pass\n", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (int, float)):\n        raise TypeError('scale must be a number')\n    if self._scale is None:\n        self._scale = other\n    else:\n        raise ValueError('graph already has scale')\n", "def scale(self, other=None):\n    if other is None:\n        other = self._scale\n    else:\n        try:\n            self._scale = float(other)\n        except ValueError:\n            raise LenaValueError('Unsupported scale {}'.format(other))\n    return self\n", "def scale(self, other=None):\n\n    if other is not None:\n        if isinstance(other, tuple):\n            self._check_scale_other_coords(other)\n            other = np.array(other)\n        elif not isinstance(other, np.ndarray):\n            raise TypeError(\"Unsupported type for rescaling: %r\" % type(other))\n        self._check_scale_other_coords(other)\n\n    self._scale = other if other is not None else np.ones(self.dim)\n    self._update_context({'scale': self._scale})\n    return self\n", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (int, float, complex)):\n        raise TypeError('scale values must be numbers')\n    if isinstance(other, complex):\n        self._update_context({'scale': other.real, 'z': other.imag})\n    else:\n        self._update_context({'scale': other})\n    self._scale = other\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if self._scale is None:\n        if self._coord_names is not None:\n            self._scale = 1.0 / self._coord_names[-1]\n        else:\n            self._scale = 1.0\n    if self._scale!= other:\n        msg = 'Graph scale mismatch. Expected %s, got %s.' % (self.field_names, other)\n        raise LenaValueError(msg)\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (int, float)):\n        raise TypeError('scale must be a number')\n    if other!= 0:\n        self._update_context({'scale': other})\n    return self._scale\n", "is_pass": true, "return_code": 0}, {"generate_code": "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (numbers.Number, np.number)):\n        raise TypeError('other must be a number')\n    if not self._coord_names == other.shape:\n        raise ValueError('graph coordinates must match')\n    self._scale = other\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if isinstance(other, (int, float)):\n        self._scale = other\n        self._update_context({'scale': other})\n    else:\n        raise LenaValueError(\"Unknown scale of a graph.\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (int, float)):\n        raise TypeError(\"Unsupported type '{0}'\".format(type(other)))\n    if other < 0:\n        raise ValueError(\"Unsupported scale value '{0}'\".format(other))\n    self._scale = other\n", "is_pass": true, "return_code": 0}, {"generate_code": "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if isinstance(other, (tuple, list)):\n        if len(other) == 2:\n            other = (other[0], other[1])\n        elif len(other) == 3:\n            other = (other[0], other[1], other[2])\n    try:\n        self._scale = float(other[0])\n    except ValueError:\n        pass\n    try:\n        self._scale = float(other[1])\n    except ValueError:\n        pass\n    try:\n        self._scale = float(other[2])\n    except ValueError:\n        pass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (int, float)):\n        raise TypeError('scale must be a number')\n    if self._scale is None:\n        self._scale = other\n    else:\n        raise ValueError('graph already has scale')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def scale(self, other=None):\n    if other is None:\n        other = self._scale\n    else:\n        try:\n            self._scale = float(other)\n        except ValueError:\n            raise LenaValueError('Unsupported scale {}'.format(other))\n    return self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\n    if other is not None:\n        if isinstance(other, tuple):\n            self._check_scale_other_coords(other)\n            other = np.array(other)\n        elif not isinstance(other, np.ndarray):\n            raise TypeError(\"Unsupported type for rescaling: %r\" % type(other))\n        self._check_scale_other_coords(other)\n\n    self._scale = other if other is not None else np.ones(self.dim)\n    self._update_context({'scale': self._scale})\n    return self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    if not isinstance(other, (int, float, complex)):\n        raise TypeError('scale values must be numbers')\n    if isinstance(other, complex):\n        self._update_context({'scale': other.real, 'z': other.imag})\n    else:\n        self._update_context({'scale': other})\n    self._scale = other\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "hist_to_graph", "docstring": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.", "ques_id": "62b87b869a0c4fa8b80b35e1", "code": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n\n    For a *make_value* above that would be used to create a graph\n    with errors from a histogram bin will be used.  It can be \"left\"\n    (default), \"right\" and \"middle\".\n\n    *field_names* set field names of the graph.  This can be \"left\"\n    (default), \"right\" and \"middle\".\n\n    *scale* becomes the graph's scale (unknown by default).\n\n    If it is ``True``, it uses the histogram scale.\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate* make_value\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * make_value\n    >>> make_value = lambda bin_: (\"x\", ", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For a *make_value*\n    above that would be used to create a graph with errors from a histogram\n    bin will be \"left\" (default), \"right\" and \"middle\".\n\n    For a *make_value* above that would be used to create a numeric graph.\n\n    *field_names* set field names of the graph. The number must be the same\n    as the dimension of the result. This can be \"left\" (default), \"right\" and\n    \"middle\".\n\n    *scale* becomes the graph's scale (unknown by default). If it is ``True``,\n    it uses the histogram scale.\n\n    Return the resulting graph.\n    \"\"\"\n    if scale is None:\n        scale = hist.get_scale()\n    return hist.make_grap", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n    For example, to create a graph with errors from a histogram which\n    bins contain a named tuple with fields *mean*, *mean_error* and\n    a context one would use\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = lambd", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n    For example, to create a graph with errors from a histogram where\n    bins contain a named tuple with fields *mean*, *mean_error`, and\n    a context one would use\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".                                                                                                                                     ", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = l", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For a *make_value*\n    above that would be use the histogram scale. If it is ``True``, it uses the\n    histogram scale. \n\n    *field_names* set field names of the graph. The number must be the same as\n    the dimension of the result. For a *make_value* above that would be *(\"x\",\n    \"y_mean\", \"y_mean_error\")*. \n\n    *scale* behaves the same as the histogram scale. If it is ``True``, it uses\n    the histogram scale. \n\n    Return the resulting graph. \n    \"\"\"\n    return hist.to_graph(make_value=make_value,\n                        get_coordinate=get_coordinate,\n               ", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point. By\n    default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example, to\n    create a graph with errors from a histogram where bins contain a\n    named tuple with fields *mean*, *mean_error` and a context one would be\n    used.\n\n    For a *make_value* above that would be used to create a graph with\n    errors from a histogram which will be \"left\" (default), \"right\" and \"middle\".\n\n    *field_names* set field names of the graph. The number must be the same\n    as the dimension of the result. For a *make_value* above that would be\n    *(\"x\", \"y_mean\", \"y_mean_error\")*.  This can be used to create a graph\n    with errors from a histogram where bins contain only numeric bins\n    (without context) or *make_valu", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For a\n    ``make_value* above that would be ``left``, this can be used to\n    create a graph with errors from a histogram bin will be set.\n\n    For a *make_value* above that would be ``left``, this would be\n    \"middle\".  *field_names* set field names of the graph. This can\n    be \"left\" (default), \"right\" and \"middle\".         \n\n    *scale* behaves the same as the histogram scale.        \n\n    Return the resulting graph.\n    \"\"\"\n    return hist_to_graph_with_context(hist, make_value, get_coordinate,\n                                    field_names, scale)\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n\n    For a *make_value* above that would be use the histogram scale.\n    This can be used to create a graph with errors from a histogram\n    where bins contain a named tuple with fields *mean*, *mean_error*\n    and a context one would be used. For a *make_value* above\n    that would be use the histogram scale.\n\n    *field_names* set field names of the graph. The number must be\n    the same as the dimension of the result.  For a *make_value*\n    above that would be *(\"x\", \"y_mean\", \"y_mean_error\")*.  If it\n    is ``True``, it uses the histogram scale.\n\n    *scale* becomes the graph's scale.  If it is ``True", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n\n    For a *make_value* above that would be used to create a graph\n    with errors from a histogram bin will be \"left\" (default), \"right\" and\n    \"middle\".\n\n    For a *field_names* set field names of the graph.\n\n    However, to remove context when creating a numeric graph, use the\n    histogram's scale (unknown by default).\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n    >>> make_value(100) # mean = 100\n    (100, 50)\n    >>> make_value(200) # mean = 200\n    (200, 50)\n    >>> make_value(25"], "level": "project_runnable", "generate_results": [{"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n\n    For a *make_value* above that would be used to create a graph\n    with errors from a histogram bin will be used.  It can be \"left\"\n    (default), \"right\" and \"middle\".\n\n    *field_names* set field names of the graph.  This can be \"left\"\n    (default), \"right\" and \"middle\".\n\n    *scale* becomes the graph's scale (unknown by default).\n\n    If it is ``True``, it uses the histogram scale.\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate* make_value\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * make_value\n    >>> make_value = lambda bin_: (\"x\", ", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For a *make_value*\n    above that would be used to create a graph with errors from a histogram\n    bin will be \"left\" (default), \"right\" and \"middle\".\n\n    For a *make_value* above that would be used to create a numeric graph.\n\n    *field_names* set field names of the graph. The number must be the same\n    as the dimension of the result. This can be \"left\" (default), \"right\" and\n    \"middle\".\n\n    *scale* becomes the graph's scale (unknown by default). If it is ``True``,\n    it uses the histogram scale.\n\n    Return the resulting graph.\n    \"\"\"\n    if scale is None:\n        scale = hist.get_scale()\n    return hist.make_grap", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n    For example, to create a graph with errors from a histogram which\n    bins contain a named tuple with fields *mean*, *mean_error* and\n    a context one would use\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = lambd", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n    For example, to create a graph with errors from a histogram where\n    bins contain a named tuple with fields *mean*, *mean_error`, and\n    a context one would use\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".                                                                                                                                     ", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    define what the coordinate of a graph point created from a histogram\n    bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = l", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For a *make_value*\n    above that would be use the histogram scale. If it is ``True``, it uses the\n    histogram scale. \n\n    *field_names* set field names of the graph. The number must be the same as\n    the dimension of the result. For a *make_value* above that would be *(\"x\",\n    \"y_mean\", \"y_mean_error\")*. \n\n    *scale* behaves the same as the histogram scale. If it is ``True``, it uses\n    the histogram scale. \n\n    Return the resulting graph. \n    \"\"\"\n    return hist.to_graph(make_value=make_value,\n                        get_coordinate=get_coordinate,\n               ", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point. By\n    default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example, to\n    create a graph with errors from a histogram where bins contain a\n    named tuple with fields *mean*, *mean_error` and a context one would be\n    used.\n\n    For a *make_value* above that would be used to create a graph with\n    errors from a histogram which will be \"left\" (default), \"right\" and \"middle\".\n\n    *field_names* set field names of the graph. The number must be the same\n    as the dimension of the result. For a *make_value* above that would be\n    *(\"x\", \"y_mean\", \"y_mean_error\")*.  This can be used to create a graph\n    with errors from a histogram where bins contain only numeric bins\n    (without context) or *make_valu", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For a\n    ``make_value* above that would be ``left``, this can be used to\n    create a graph with errors from a histogram bin will be set.\n\n    For a *make_value* above that would be ``left``, this would be\n    \"middle\".  *field_names* set field names of the graph. This can\n    be \"left\" (default), \"right\" and \"middle\".         \n\n    *scale* behaves the same as the histogram scale.        \n\n    Return the resulting graph.\n    \"\"\"\n    return hist_to_graph_with_context(hist, make_value, get_coordinate,\n                                    field_names, scale)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n\n    For a *make_value* above that would be use the histogram scale.\n    This can be used to create a graph with errors from a histogram\n    where bins contain a named tuple with fields *mean*, *mean_error*\n    and a context one would be used. For a *make_value* above\n    that would be use the histogram scale.\n\n    *field_names* set field names of the graph. The number must be\n    the same as the dimension of the result.  For a *make_value*\n    above that would be *(\"x\", \"y_mean\", \"y_mean_error\")*.  If it\n    is ``True``, it uses the histogram scale.\n\n    *scale* becomes the graph's scale.  If it is ``True", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n\n    For a *make_value* above that would be used to create a graph\n    with errors from a histogram bin will be \"left\" (default), \"right\" and\n    \"middle\".\n\n    For a *field_names* set field names of the graph.\n\n    However, to remove context when creating a numeric graph, use the\n    histogram's scale (unknown by default).\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n    >>> make_value(100) # mean = 100\n    (100, 50)\n    >>> make_value(200) # mean = 200\n    (200, 50)\n    >>> make_value(25", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "_verify", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "ques_id": "62b8b4baeb7e40a82d2d1136", "code": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative=tentative)\n    if vtype is not None:\n        verifyObject(iface, candidate, vtype)\n    return True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative)\n    if vtype is None:\n        vtype = candidate.__class__\n    if not issubclass(vtype, InterfaceClass):\n        return False\n    return True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if vtype is None:\n        vtype = iface.providedBy\n    try:\n        return vtype.verify(candidate)\n    except TypeError:\n        return False\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative)\n    if vtype is not None:\n        verifyObject(iface, candidate, vtype)\n    return True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if not vtype:\n        vtype = iface.providedBy(candidate)\n\n    if tentative and not vtype:\n        # Candidate has no interface but we need to provide it.\n        return True\n\n    # Check that the candidate claims that it provides the interface.\n    if not vtype and not candidate:\n        return False\n\n    # Check that the candidate defines the interface.\n    if not _verify_element(iface, 'providedBy', candidate, vtype):\n        raise Invalid(_MSG_TOO_MANY)\n\n    # Check that the candidate defines the interface.\n    if not _verify_element(iface, 'provides', candidate, vtype):\n        raise Invalid(_MSG_INCOMPATIBLE)\n\n    # Check that the candidate defines all the required methods.\n    if not _verify_element(iface,'methods', candidate, vtype):\n        raise Invalid(_MSG_NO_METHODS)\n\n    # Check that the candidate defines all the required attributes.\n    if not _verify_element(iface, 'attributes", "def _verify(iface, candidate, tentative=False, vtype=None):\n    try:\n        verifyClass(iface, candidate, tentative)\n    except zope.interface.Invalid as e:\n        if e.args[0]!= _MSG_TOO_MANY:\n            raise\n        return False\n    return True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative)\n    if not isinstance(candidate, zope.interface.implementer.InterfaceClass):\n        raise zope.interface.Invalid(_MSG_NOT_INTERFACE)\n    if vtype is not None:\n        verifyObject(iface, candidate, vtype)\n    return True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative=tentative)\n    verifyObject(iface, candidate, tentative=tentative)\n\n    try:\n        # Check that *candidate* provides the interface.\n        providedBy(iface, candidate)\n    except zope.interface.Invalid as e:\n        # If the candidate's class declares that it *provides* the interface,\n        # then raise the error.\n        if e.args[0] is not None:\n            raise e.args[0]\n        if vtype is not None:\n            # If the candidate's class declares that it *provides* the interface,\n            # then raise the error.\n            if e.args[1] is not None:\n                raise e.args[1]\n        raise\n    return True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative)\n    return _verify_element(iface, 'providedBy', candidate, vtype)\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative=tentative)\n    return _verify_element(iface, 'providedBy', candidate, vtype)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative=tentative)\n    if vtype is not None:\n        verifyObject(iface, candidate, vtype)\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative)\n    if vtype is None:\n        vtype = candidate.__class__\n    if not issubclass(vtype, InterfaceClass):\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if vtype is None:\n        vtype = iface.providedBy\n    try:\n        return vtype.verify(candidate)\n    except TypeError:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative)\n    if vtype is not None:\n        verifyObject(iface, candidate, vtype)\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if not vtype:\n        vtype = iface.providedBy(candidate)\n\n    if tentative and not vtype:\n        # Candidate has no interface but we need to provide it.\n        return True\n\n    # Check that the candidate claims that it provides the interface.\n    if not vtype and not candidate:\n        return False\n\n    # Check that the candidate defines the interface.\n    if not _verify_element(iface, 'providedBy', candidate, vtype):\n        raise Invalid(_MSG_TOO_MANY)\n\n    # Check that the candidate defines the interface.\n    if not _verify_element(iface, 'provides', candidate, vtype):\n        raise Invalid(_MSG_INCOMPATIBLE)\n\n    # Check that the candidate defines all the required methods.\n    if not _verify_element(iface,'methods', candidate, vtype):\n        raise Invalid(_MSG_NO_METHODS)\n\n    # Check that the candidate defines all the required attributes.\n    if not _verify_element(iface, 'attributes", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    try:\n        verifyClass(iface, candidate, tentative)\n    except zope.interface.Invalid as e:\n        if e.args[0]!= _MSG_TOO_MANY:\n            raise\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative)\n    if not isinstance(candidate, zope.interface.implementer.InterfaceClass):\n        raise zope.interface.Invalid(_MSG_NOT_INTERFACE)\n    if vtype is not None:\n        verifyObject(iface, candidate, vtype)\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative=tentative)\n    verifyObject(iface, candidate, tentative=tentative)\n\n    try:\n        # Check that *candidate* provides the interface.\n        providedBy(iface, candidate)\n    except zope.interface.Invalid as e:\n        # If the candidate's class declares that it *provides* the interface,\n        # then raise the error.\n        if e.args[0] is not None:\n            raise e.args[0]\n        if vtype is not None:\n            # If the candidate's class declares that it *provides* the interface,\n            # then raise the error.\n            if e.args[1] is not None:\n                raise e.args[1]\n        raise\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative)\n    return _verify_element(iface, 'providedBy', candidate, vtype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    verifyClass(iface, candidate, tentative=tentative)\n    return _verify_element(iface, 'providedBy', candidate, vtype)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyObject", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "ques_id": "62b8b4baeb7e40a82d2d1137", "code": ["def verifyObject(iface, candidate, tentative=False):\n    _verify(\n        iface.providedBy, candidate, tentative=tentative, vtype=iface.providesContent)\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative=tentative)\n    verifyObject(iface, candidate, tentative=tentative)\n    if not hasattr(candidate, 'providedBy'):\n        return False\n    return _verify(iface, candidate, tentative=tentative)\n", "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    if not hasattr(candidate, 'providedBy'):\n        return _incompat(required=[], implemented=[], tentative=[])\n    provided = candidate.providedBy(iface)\n    if provided is None:\n        return _MSG_NO_PROVIDED\n    elif not isinstance(provided, zope.interface.providedBy):\n        return _MSG_NOT_PROVIDED\n    elif provided.__module__!= iface.__module__:\n        return _MSG_NOT_PROVIDED\n    elif provided.__name__!= iface.__name__:\n        return _MSG_NOT_PROVIDED\n    elif not hasattr(provided, 'provides'):\n        return _MSG_NOT_PROVIDED\n    elif not hasattr(provided, '__call__'):\n        return _MSG_NOT_PROVIDED\n    elif not callable(provided.provides):\n        return _MSG_NOT_PROVIDED\n    elif not hasattr(provided, '__call__'):\n        return _MSG_NOT_PROVIDE", "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    provided = iface.providedBy(candidate)\n    verify(provided, [Candidate])\n    verifyObject(provided, candidate, tentative)\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    if not candidate.providedBy(iface):\n        raise Invalid(\n            u\"Candidate '%s' does not provide '%s'\" % (candidate, iface))\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    provided = iface.providedBy(candidate)\n    if provided is False:\n        return False\n    elif provided is True:\n        return verifyObject(iface.context, candidate, tentative)\n    else:\n        raise Invalid(\n            u'Candidate {0} must provide {1}'.format(\n                candidate, u', '.join(sorted(provided))))\n", "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative=tentative)\n    if not hasattr(candidate, 'providedBy'):\n        return _incompat(_MSG_NO_PROVIDEBY, candidate, 'candidate', 'object')\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    if not verifyClass(iface, candidate, tentative):\n        return False\n\n    # Make sure that the candidate claims it provides the interface.\n    if _verify_element(iface, 'providedBy', candidate, True):\n        return True\n\n    # Make sure the candidate declares its interface as a provided\n    # interface.\n    if _verify_element(iface, 'providedBy', candidate, False):\n        return True\n\n    # Make sure the candidate defines the interface as a provided\n    # interface.\n    if not _verify(iface, candidate, tentative, True):\n        raise Invalid(\n            'Candidate does not provide interface \"%s\". '\n            'This is probably a bug in the zope.interface module.'\n            % iface.__name__)\n\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n\n    try:\n        verifyObject(iface.providedBy(candidate), candidate, tentative)\n    except Invalid as e:\n        if e.args and e.args[0]:\n            # If the candidate is not a concrete class, then there was no\n            # specific way to do that.\n            raise\n        return False\n    else:\n        return True\n", "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    try:\n        verifyObject(iface, candidate, tentative)\n    except zope.interface.Invalid as e:\n        if not e.args:\n            # No candidate provides the interface\n            return False\n        # Some candidate does not provide the interface\n        return True\n    return True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    _verify(\n        iface.providedBy, candidate, tentative=tentative, vtype=iface.providesContent)\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative=tentative)\n    verifyObject(iface, candidate, tentative=tentative)\n    if not hasattr(candidate, 'providedBy'):\n        return False\n    return _verify(iface, candidate, tentative=tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    if not hasattr(candidate, 'providedBy'):\n        return _incompat(required=[], implemented=[], tentative=[])\n    provided = candidate.providedBy(iface)\n    if provided is None:\n        return _MSG_NO_PROVIDED\n    elif not isinstance(provided, zope.interface.providedBy):\n        return _MSG_NOT_PROVIDED\n    elif provided.__module__!= iface.__module__:\n        return _MSG_NOT_PROVIDED\n    elif provided.__name__!= iface.__name__:\n        return _MSG_NOT_PROVIDED\n    elif not hasattr(provided, 'provides'):\n        return _MSG_NOT_PROVIDED\n    elif not hasattr(provided, '__call__'):\n        return _MSG_NOT_PROVIDED\n    elif not callable(provided.provides):\n        return _MSG_NOT_PROVIDED\n    elif not hasattr(provided, '__call__'):\n        return _MSG_NOT_PROVIDE", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    provided = iface.providedBy(candidate)\n    verify(provided, [Candidate])\n    verifyObject(provided, candidate, tentative)\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    if not candidate.providedBy(iface):\n        raise Invalid(\n            u\"Candidate '%s' does not provide '%s'\" % (candidate, iface))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    provided = iface.providedBy(candidate)\n    if provided is False:\n        return False\n    elif provided is True:\n        return verifyObject(iface.context, candidate, tentative)\n    else:\n        raise Invalid(\n            u'Candidate {0} must provide {1}'.format(\n                candidate, u', '.join(sorted(provided))))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative=tentative)\n    if not hasattr(candidate, 'providedBy'):\n        return _incompat(_MSG_NO_PROVIDEBY, candidate, 'candidate', 'object')\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if not verifyClass(iface, candidate, tentative):\n        return False\n\n    # Make sure that the candidate claims it provides the interface.\n    if _verify_element(iface, 'providedBy', candidate, True):\n        return True\n\n    # Make sure the candidate declares its interface as a provided\n    # interface.\n    if _verify_element(iface, 'providedBy', candidate, False):\n        return True\n\n    # Make sure the candidate defines the interface as a provided\n    # interface.\n    if not _verify(iface, candidate, tentative, True):\n        raise Invalid(\n            'Candidate does not provide interface \"%s\". '\n            'This is probably a bug in the zope.interface module.'\n            % iface.__name__)\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n\n    try:\n        verifyObject(iface.providedBy(candidate), candidate, tentative)\n    except Invalid as e:\n        if e.args and e.args[0]:\n            # If the candidate is not a concrete class, then there was no\n            # specific way to do that.\n            raise\n        return False\n    else:\n        return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    verifyClass(iface, candidate, tentative)\n    try:\n        verifyObject(iface, candidate, tentative)\n    except zope.interface.Invalid as e:\n        if not e.args:\n            # No candidate provides the interface\n            return False\n        # Some candidate does not provide the interface\n        return True\n    return True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyClass", "docstring": "Verify that the *candidate* might correctly provide *iface*.", "ques_id": "62b8b4c1eb7e40a82d2d1139", "code": ["def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n    verifyObject(iface, candidate, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n    verifyObject(iface, candidate, tentative)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyObject(iface, candidate, tentative)\n    verifyObject(iface, candidate, tentative)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "determineMetaclass", "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__", "ques_id": "62b8b559eb7e40a82d2d11f6", "code": ["def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = []\n    for base in bases:\n        if issubclass(base, type) and base not in explicit_mc:\n            return base\n        elif issubclass(base, type) and base in explicit_mc:\n            return base\n        elif issubclass(base, types.ModuleType) and base not in explicit_mc:\n            return base\n        elif not issubclass(base, type):\n            return base\n        explicit_mc.append(base)\n\n    if len(explicit_mc) == 1:\n        return explicit_mc[0]\n    else:\n        raise TypeError(\"Too many bases of metaclass %s\" % str(bases))\n", "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = getFrameInfo(sys._getframe(1))[0]\n    metaclass = sys.modules[__name__]\n    for base in bases:\n        if issubclass(base, explicit_mc):\n            return metaclass\n        elif issubclass(base, type):\n            return metaclass\n        elif issubclass(base, metaclass):\n            return base\n    return metaclass\n", "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = []\n\n    metaclass = None\n    for base in bases:\n        if isClassAdvisor(base):\n            if metaclass is None:\n                metaclass = base\n            else:\n                assert metaclass is base, (\n                    \"Different metaclasses in bases %s and %s\" %\n                    (metaclass, base))\n        elif isFrameInfo(base) and base not in explicit_mc:\n            assert metaclass is None, (\n                \"Base %s is not an explicit metaclass\" % base)\n            metaclass = base\n    if metaclass is None:\n        raise TypeError(\"Cannot determine metaclass from %s\" % bases)\n    return metaclass\n", "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = getFrameInfo(frame=1)\n    for base in bases:\n        if not isClassAdvisor(base):\n            continue\n        for advisor in base.__subclasses__():\n            if advisor == explicit_mc:\n                return advisor\n            if advisor.__module__ == explicit_mc.__module__:\n                return advisor\n    return type\n", "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = []\n\n    metaclass = None\n    for base in bases:\n        if issubclass(base, Advisor) and base not in explicit_mc:\n            metaclass = base\n            break\n    if metaclass is None:\n        metaclass = type\n    return metaclass\n", "def determineMetaclass(bases, explicit_mc=None):\n    metaclass = None\n    for base in bases:\n        if isClassAdvisor(base):\n            metaclass = base\n            break\n        if isMetaclass(base) and explicit_mc is None:\n            metaclass = base\n            break\n    return metaclass\n", "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = []\n    for base in bases:\n        if isClassAdvisor(base):\n            explicit_mc.append(base)\n            continue\n        metaclass = getFrameInfo(base)[0]\n        if metaclass is not None and metaclass not in explicit_mc:\n            yield metaclass\n            explicit_mc.append(metaclass)\n", "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = getFrameInfo(sys._getframe(1))\n\n    if not issubclass(explicit_mc, type):\n        raise TypeError(\"explicit_mc must be a subclass of type\")\n\n    metaclass = None\n    for base in bases:\n        if issubclass(base, explicit_mc):\n            metaclass = base\n            break\n\n    if metaclass is None:\n        raise TypeError(\"cannot determine metaclass from %s\" % str(bases))\n\n    return metaclass\n", "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = getFrameInfo(sys._getframe(1))[2]\n    classes = [b[0] for b in bases]\n    for c in classes:\n        if c == \"__metaclass__\":\n            return explicit_mc\n        elif isClassAdvisor(c):\n            return c\n", "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = type\n    classes = []\n    for base in bases:\n        if issubclass(base, explicit_mc):\n            classes.append(base)\n            classes.extend(determineMetaclass(classes, base.__metaclass__))\n        else:\n            classes.extend([base])\n    return classes\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = []\n    for base in bases:\n        if issubclass(base, type) and base not in explicit_mc:\n            return base\n        elif issubclass(base, type) and base in explicit_mc:\n            return base\n        elif issubclass(base, types.ModuleType) and base not in explicit_mc:\n            return base\n        elif not issubclass(base, type):\n            return base\n        explicit_mc.append(base)\n\n    if len(explicit_mc) == 1:\n        return explicit_mc[0]\n    else:\n        raise TypeError(\"Too many bases of metaclass %s\" % str(bases))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = getFrameInfo(sys._getframe(1))[0]\n    metaclass = sys.modules[__name__]\n    for base in bases:\n        if issubclass(base, explicit_mc):\n            return metaclass\n        elif issubclass(base, type):\n            return metaclass\n        elif issubclass(base, metaclass):\n            return base\n    return metaclass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = []\n\n    metaclass = None\n    for base in bases:\n        if isClassAdvisor(base):\n            if metaclass is None:\n                metaclass = base\n            else:\n                assert metaclass is base, (\n                    \"Different metaclasses in bases %s and %s\" %\n                    (metaclass, base))\n        elif isFrameInfo(base) and base not in explicit_mc:\n            assert metaclass is None, (\n                \"Base %s is not an explicit metaclass\" % base)\n            metaclass = base\n    if metaclass is None:\n        raise TypeError(\"Cannot determine metaclass from %s\" % bases)\n    return metaclass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = getFrameInfo(frame=1)\n    for base in bases:\n        if not isClassAdvisor(base):\n            continue\n        for advisor in base.__subclasses__():\n            if advisor == explicit_mc:\n                return advisor\n            if advisor.__module__ == explicit_mc.__module__:\n                return advisor\n    return type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = []\n\n    metaclass = None\n    for base in bases:\n        if issubclass(base, Advisor) and base not in explicit_mc:\n            metaclass = base\n            break\n    if metaclass is None:\n        metaclass = type\n    return metaclass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    metaclass = None\n    for base in bases:\n        if isClassAdvisor(base):\n            metaclass = base\n            break\n        if isMetaclass(base) and explicit_mc is None:\n            metaclass = base\n            break\n    return metaclass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = []\n    for base in bases:\n        if isClassAdvisor(base):\n            explicit_mc.append(base)\n            continue\n        metaclass = getFrameInfo(base)[0]\n        if metaclass is not None and metaclass not in explicit_mc:\n            yield metaclass\n            explicit_mc.append(metaclass)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = getFrameInfo(sys._getframe(1))\n\n    if not issubclass(explicit_mc, type):\n        raise TypeError(\"explicit_mc must be a subclass of type\")\n\n    metaclass = None\n    for base in bases:\n        if issubclass(base, explicit_mc):\n            metaclass = base\n            break\n\n    if metaclass is None:\n        raise TypeError(\"cannot determine metaclass from %s\" % str(bases))\n\n    return metaclass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = getFrameInfo(sys._getframe(1))[2]\n    classes = [b[0] for b in bases]\n    for c in classes:\n        if c == \"__metaclass__\":\n            return explicit_mc\n        elif isClassAdvisor(c):\n            return c\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    if explicit_mc is None:\n        explicit_mc = type\n    classes = []\n    for base in bases:\n        if issubclass(base, explicit_mc):\n            classes.append(base)\n            classes.extend(determineMetaclass(classes, base.__metaclass__))\n        else:\n            classes.extend([base])\n    return classes\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "pop", "docstring": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.", "ques_id": "62b8d22a48ba5a41d1c3f47d", "code": ["def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    elif default is False:\n        raise ValueError(\"default value for pop is False\")\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is self.__marker:\n            return default\n        return default\n", "def pop(self, key, default=__marker):\n    try:\n        value = dict.pop(self, key)\n    except KeyError:\n        if default is not __marker:\n            return default\n        raise\n    else:\n        if default is not __marker:\n            return default\n        del self[key]\n        return value\n", "def pop(self, key, default=__marker):\n    if default!= __marker:\n        return super().pop(key, default)\n\n    try:\n        return self[key]\n    except KeyError:\n        if default is self.__marker:\n            raise\n        return default\n", "def pop(self, key, default=__marker):\n    if default is __marker:\n        default = self[-1][2]\n    return dict.pop(self, key.lower(), default)\n", "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    elif default is False:\n        raise KeyError(key)\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is self.__marker:\n            return default\n        return default\n", "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = self[key] = self[key]\n    return default.pop(key)\n", "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    value = super(DefaultDict, self).pop(key, default)\n    if value is self.__marker:\n        return default\n    else:\n        self.__setitem__(key, value)\n        return value\n", "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    value = dict.pop(self, key, default)\n    if value is not default:\n        self._dirty = True\n    return value\n", "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n\n    value = dict.pop(self, key, default)\n    if value is default:\n        if default is not None:\n            del self[key]\n\n    return value\n", "def pop(self, key, default=__marker):\n    if default is __marker:\n        default = self[-1][1]\n    return super(Documents, self).pop(key, default)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    elif default is False:\n        raise ValueError(\"default value for pop is False\")\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is self.__marker:\n            return default\n        return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    try:\n        value = dict.pop(self, key)\n    except KeyError:\n        if default is not __marker:\n            return default\n        raise\n    else:\n        if default is not __marker:\n            return default\n        del self[key]\n        return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default!= __marker:\n        return super().pop(key, default)\n\n    try:\n        return self[key]\n    except KeyError:\n        if default is self.__marker:\n            raise\n        return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default is __marker:\n        default = self[-1][2]\n    return dict.pop(self, key.lower(), default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    elif default is False:\n        raise KeyError(key)\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is self.__marker:\n            return default\n        return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = self[key] = self[key]\n    return default.pop(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    value = super(DefaultDict, self).pop(key, default)\n    if value is self.__marker:\n        return default\n    else:\n        self.__setitem__(key, value)\n        return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    value = dict.pop(self, key, default)\n    if value is not default:\n        self._dirty = True\n    return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n\n    value = dict.pop(self, key, default)\n    if value is default:\n        if default is not None:\n            del self[key]\n\n    return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default is __marker:\n        default = self[-1][1]\n    return super(Documents, self).pop(key, default)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lfu.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least frequently used.", "ques_id": "62b8d23748ba5a41d1c3f497", "code": ["def popitem(self):\n    try:\n        key = next(iter(self))\n    except StopIteration:\n        raise KeyError('Popping from an empty collection')\n    value = self.pop(key)\n    return (key, value)\n", "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('pop from empty cache')\n", "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('pop from empty cache')\n", "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except KeyError:\n        raise ValueError('Cannot pop from an empty cache.')\n", "def popitem(self):\n    try:\n        key = self.__counter.next()\n        value = self.pop(key)\n        del self[key]\n        return (key, value)\n    except StopIteration:\n        raise KeyError('pop from an empty collection')\n", "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('pop from an empty collection')\n", "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except KeyError:\n        raise KeyError('Pop from an empty collection')\n", "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n", "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n", "def popitem(self):\n    return self.__counter.popitem()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n    except StopIteration:\n        raise KeyError('Popping from an empty collection')\n    value = self.pop(key)\n    return (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('pop from empty cache')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('pop from empty cache')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except KeyError:\n        raise ValueError('Cannot pop from an empty cache.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = self.__counter.next()\n        value = self.pop(key)\n        del self[key]\n        return (key, value)\n    except StopIteration:\n        raise KeyError('pop from an empty collection')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('pop from an empty collection')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except KeyError:\n        raise KeyError('Pop from an empty collection')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__counter.popitem()\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__counter.popitem()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least recently used.", "ques_id": "62b8d23a48ba5a41d1c3f499", "code": ["def popitem(self):\n    return self.__order.popitem()\n", "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n", "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('pop from empty collection')\n", "def popitem(self):\n    return self.__popitem()[0]\n", "def popitem(self):\n    value = self.pop()\n    self.__order.remove(value)\n    return value\n", "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('pop from empty collection')\n", "def popitem(self):\n    values = dict.popitem(self)\n    self.__update(values[0])\n    return values\n", "def popitem(self):\n    return self.__order.popitem()\n", "def popitem(self):\n    return self.__order.pop()\n", "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    return self.__order.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('pop from empty collection')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__popitem()[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    value = self.pop()\n    self.__order.remove(value)\n    return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('pop from empty collection')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    values = dict.popitem(self)\n    self.__update(values[0])\n    return values\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__order.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__order.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/mru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair most recently used.", "ques_id": "62b8d23c48ba5a41d1c3f49b", "code": ["def popitem(self):\n    return self.__order.pop()\n", "def popitem(self):\n    return self.__order.pop()\n", "def popitem(self):\n    return self.__order.pop()\n", "def popitem(self):\n    return self.__order.pop()\n", "def popitem(self):\n    result = dict.popitem(self)\n    self.__order.remove(result[0])\n    return result\n", "def popitem(self):\n    return dict.popitem(self)[0]\n", "def popitem(self):\n    return self.__order.pop()\n", "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n", "def popitem(self):\n    try:\n        return dict.popitem(self)\n    except KeyError as e:\n        raise exceptions.CacheNotFound(str(e))\n", "def popitem(self):\n    return self.__order.popitem()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    return self.__order.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__order.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__order.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__order.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    result = dict.popitem(self)\n    self.__order.remove(result[0])\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return dict.popitem(self)[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__order.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.__order.pop()\n    except IndexError:\n        raise KeyError('popitem(): cache is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return dict.popitem(self)\n    except KeyError as e:\n        raise exceptions.CacheNotFound(str(e))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__order.popitem()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/rr.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return a random `(key, value)` pair.", "ques_id": "62b8d23e48ba5a41d1c3f49e", "code": ["def popitem(self):\n    return self.__choice.popitem()\n", "def popitem(self):\n    try:\n        key = random.choice(self.pop())\n        value = self.pop(key)\n        self[key].remove(value)\n        return key, value\n    except IndexError:\n        raise KeyError('pop from empty set')\n", "def popitem(self):\n    try:\n        key = self.__choice()\n    except IndexError:\n        raise KeyError('Cannot pop from an empty priority queue.')\n    else:\n        return (key, self.pop(key))\n", "def popitem(self):\n    return self.__choice.popitem()\n", "def popitem(self):\n    try:\n        key = self.__choice()\n    except IndexError:\n        raise KeyError('popitem(): Cannot pop from empty cache.')\n    else:\n        return key, self.pop(key)\n", "def popitem(self):\n    return self.__choice.popitem()\n", "def popitem(self):\n    return self.__choice.popitem()[0]\n", "def popitem(self):\n    return self.__choice.popitem()\n", "def popitem(self):\n    try:\n        key = self.__choice()\n        value = dict.pop(self, key)\n    except IndexError:\n        raise KeyError('pop from an empty cache')\n    return key, value\n", "def popitem(self):\n    try:\n        return self.pop()\n    except IndexError:\n        raise KeyError('Popping from an empty set')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    return self.__choice.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = random.choice(self.pop())\n        value = self.pop(key)\n        self[key].remove(value)\n        return key, value\n    except IndexError:\n        raise KeyError('pop from empty set')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = self.__choice()\n    except IndexError:\n        raise KeyError('Cannot pop from an empty priority queue.')\n    else:\n        return (key, self.pop(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__choice.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = self.__choice()\n    except IndexError:\n        raise KeyError('popitem(): Cannot pop from empty cache.')\n    else:\n        return key, self.pop(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__choice.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__choice.popitem()[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__choice.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = self.__choice()\n        value = dict.pop(self, key)\n    except IndexError:\n        raise KeyError('pop from an empty cache')\n    return key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return self.pop()\n    except IndexError:\n        raise KeyError('Popping from an empty set')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_in_regex", "docstring": "Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).", "ques_id": "62b43425903eeb48555d3ea1", "code": ["def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "def _create_in_regex(self) -> Pattern:\n    if self._in_obj:\n        return self._in_regex\n    if self._in_style:\n        return self._in_regex | self._in_style.pattern\n    return self._in_regex\n", "def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "def _create_in_regex(self) -> Pattern:\n    if self._in_obj is not None:\n        return self._in_obj.pattern(self._escape_char)\n    return self._in_regex\n", "def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "def _create_in_regex(self) -> Pattern:\n\n    in_regex = self.in_style._regex\n    if in_regex is not None:\n        return re.compile(in_regex, re.IGNORECASE)\n\n    # If we don't know what to do with the in-style parameter, use the\n    # standard parameter regex.\n    return self.in_style._regex  # type: ignore\n", "def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "def _create_in_regex(self) -> Pattern:\n    in_regex = self._in_obj.in_regex or self._in_obj.escape_regex\n    return re.compile(\n        in_regex, re.IGNORECASE | self._in_obj.out_style, re.UNICODE\n    )\n", "def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "def _create_in_regex(self) -> Pattern:\n    return self._in_obj.pattern\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    if self._in_obj:\n        return self._in_regex\n    if self._in_style:\n        return self._in_regex | self._in_style.pattern\n    return self._in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    if self._in_obj is not None:\n        return self._in_obj.pattern(self._escape_char)\n    return self._in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\n    in_regex = self.in_style._regex\n    if in_regex is not None:\n        return re.compile(in_regex, re.IGNORECASE)\n\n    # If we don't know what to do with the in-style parameter, use the\n    # standard parameter regex.\n    return self.in_style._regex  # type: ignore\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    in_regex = self._in_obj.in_regex or self._in_obj.escape_regex\n    return re.compile(\n        in_regex, re.IGNORECASE | self._in_obj.out_style, re.UNICODE\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    return self._in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    return self._in_obj.pattern\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_converter", "docstring": "Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).", "ques_id": "62b43426903eeb48555d3ea2", "code": ["def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "def _create_converter(self) -> _converting._Converter:\n\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "def _create_converter(self) -> _converting._Converter:\n\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "def _create_converter(self) -> _converting._Converter:\n\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "def _create_converter(self) -> _converting._Converter:\n    converter = _converters.get(self._in_style, _converters.get(\"noop\", _converters.get(\"default\")))\n    converter.escape_char = self._escape_char\n    return converter\n", "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char, self._expand_tuples)\n", "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char, self._expand_tuples)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    converter = _converters.get(self._in_style, _converters.get(\"noop\", _converters.get(\"default\")))\n    converter.escape_char = self._escape_char\n    return converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char, self._expand_tuples)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._in_obj, self._out_obj, self._escape_char, self._expand_tuples)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/isoparser.py", "project": "pexip/os-python-dateutil", "name": "isoparse", "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0", "ques_id": "62b8966c755ee91dce50a154", "code": ["def isoparse(self, dt_str):\n    dt_str = dt_str.strip()\n    if dt_str:\n        if dt_str[-1] not in self._TIME_SEP:\n            raise ValueError('not a datetime string: %r' % dt_str)\n        dt_str = dt_str[:-1]\n        return self._parse_isodate(dt_str)\n", "def isoparse(self, dt_str):\n    try:\n        return datetime.datetime.strptime(dt_str, self._ISO8601_PATTERN).replace(\n            tzinfo=self._parse_tzstr(dt_str)\n        )\n    except ValueError:\n        msg = \"{!r} is not a valid datetime string.\".format(dt_str)\n        raise ValueError(msg)\n", "def isoparse(self, dt_str):\n    if isinstance(dt_str, datetime.datetime):\n        return dt_str\n    elif isinstance(dt_str, six.string_types):\n        dt_str = dt_str.strip()\n        if self._DATE_SEP not in dt_str:\n            dt_str = dt_str + self._DATE_SEP\n        parsed = self._parse_isodate(dt_str)\n        if parsed is None:\n            parsed = self._parse_isotime(dt_str)\n        return parsed\n    else:\n        raise ValueError('expected datetime string or datetime object')\n", "def isoparse(self, dt_str):\n    return self._parse_isodate_common(dt_str)\n", "def isoparse(self, dt_str):\n    return self._parse_isodate(dt_str)\n", "def isoparse(self, dt_str):\n    # Note that the following call will not work if the string is empty.\n    # The ``dt_str`` parameter is ignored when parsing the regex.\n    return datetime.datetime.strptime(dt_str, self._ISO8601_PATTERN).replace(\n        tzinfo=self._parse_tzstr(dt_str)\n    )\n", "def isoparse(self, dt_str):\n    return self._parse_isodate(dt_str)\n", "def isoparse(self, dt_str):\n    parsed_date = self._parse_isodate(dt_str)\n    if parsed_date is None:\n        parsed_time = self._parse_isotime(dt_str)\n        if parsed_time is None:\n            parsed_date = self._parse_isodate_uncommon(dt_str)\n            if parsed_date is None:\n                raise ValueError('Can not parse datetime string \"{}\"'.format(dt_str))\n        else:\n            parsed_time = self._parse_isodate_common(dt_str)\n            if parsed_time is None:\n                parsed_date = parsed_date + parsed_time\n            else:\n                parsed_date = parsed_date + parsed_time + self._TIME_SEP\n    return parsed_date\n", "def isoparse(self, dt_str):\n    if dt_str is None:\n        return None\n\n    dt = self._parse_isodate(dt_str)\n\n    if dt:\n        return dt + self._DATE_SEP if self._DATE_SEP in dt_str else dt\n", "def isoparse(self, dt_str):\n    if not dt_str:\n        return None\n    if dt_str.count(self._DATE_SEP) == 1:\n        return datetime.strptime(dt_str, self._DATE_FORMAT).replace(\n            tzinfo=self._parse_tzstr(dt_str))\n    else:\n        return datetime.strptime(dt_str, self._TIME_FORMAT).replace(\n            tzinfo=self._parse_tzstr(dt_str))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def isoparse(self, dt_str):\n    dt_str = dt_str.strip()\n    if dt_str:\n        if dt_str[-1] not in self._TIME_SEP:\n            raise ValueError('not a datetime string: %r' % dt_str)\n        dt_str = dt_str[:-1]\n        return self._parse_isodate(dt_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    try:\n        return datetime.datetime.strptime(dt_str, self._ISO8601_PATTERN).replace(\n            tzinfo=self._parse_tzstr(dt_str)\n        )\n    except ValueError:\n        msg = \"{!r} is not a valid datetime string.\".format(dt_str)\n        raise ValueError(msg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    if isinstance(dt_str, datetime.datetime):\n        return dt_str\n    elif isinstance(dt_str, six.string_types):\n        dt_str = dt_str.strip()\n        if self._DATE_SEP not in dt_str:\n            dt_str = dt_str + self._DATE_SEP\n        parsed = self._parse_isodate(dt_str)\n        if parsed is None:\n            parsed = self._parse_isotime(dt_str)\n        return parsed\n    else:\n        raise ValueError('expected datetime string or datetime object')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    return self._parse_isodate_common(dt_str)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def isoparse(self, dt_str):\n    return self._parse_isodate(dt_str)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def isoparse(self, dt_str):\n    # Note that the following call will not work if the string is empty.\n    # The ``dt_str`` parameter is ignored when parsing the regex.\n    return datetime.datetime.strptime(dt_str, self._ISO8601_PATTERN).replace(\n        tzinfo=self._parse_tzstr(dt_str)\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    return self._parse_isodate(dt_str)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def isoparse(self, dt_str):\n    parsed_date = self._parse_isodate(dt_str)\n    if parsed_date is None:\n        parsed_time = self._parse_isotime(dt_str)\n        if parsed_time is None:\n            parsed_date = self._parse_isodate_uncommon(dt_str)\n            if parsed_date is None:\n                raise ValueError('Can not parse datetime string \"{}\"'.format(dt_str))\n        else:\n            parsed_time = self._parse_isodate_common(dt_str)\n            if parsed_time is None:\n                parsed_date = parsed_date + parsed_time\n            else:\n                parsed_date = parsed_date + parsed_time + self._TIME_SEP\n    return parsed_date\n", "is_pass": true, "return_code": 0}, {"generate_code": "def isoparse(self, dt_str):\n    if dt_str is None:\n        return None\n\n    dt = self._parse_isodate(dt_str)\n\n    if dt:\n        return dt + self._DATE_SEP if self._DATE_SEP in dt_str else dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    if not dt_str:\n        return None\n    if dt_str.count(self._DATE_SEP) == 1:\n        return datetime.strptime(dt_str, self._DATE_FORMAT).replace(\n            tzinfo=self._parse_tzstr(dt_str))\n    else:\n        return datetime.strptime(dt_str, self._TIME_FORMAT).replace(\n            tzinfo=self._parse_tzstr(dt_str))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/_parser.py", "project": "pexip/os-python-dateutil", "name": "parse", "docstring": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.", "ques_id": "62b896de755ee91dce50a183", "code": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse string time values\n    \"\"\"\n    if ignoretz and timestr.endswith('Z'):\n        timestr = timestr[:-1]\n        tzinfos = ()\n\n    if tzinfos is None:\n        tzinfos = self._ampm_valid\n\n    res = self._parse(timestr, **kwargs)\n    if self._could_be_tzname(res):\n        return res\n\n    if res is None:\n        return default\n\n    return self._assign_tzname(res, tzinfos, tzinfos)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse a string with time information.\n    \"\"\"\n    if not timestr:\n        return default\n    return self._parse(timestr, ignoretz, **kwargs)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse string `timestr` according to rules defined in the `rules`\n    attribute of this class.\n    \"\"\"\n    if self._ampm_valid and ignoretz and timestr.count('+') == 1:\n        # No tzinfo means no timezone information\n        timestr = timestr.replace('+', '')\n    if not tzinfos:\n        tzinfos = self.info.get('timezone', None)\n    res = self._parse(timestr, **kwargs)\n    if tzinfos:\n        res = res.tzinfo if res else None\n    return res\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse a string using the current locale context.\n    \"\"\"\n    return self._parse(timestr, ignoretz, **kwargs).replace(\n        tzinfo=self.info.get('tzinfo', None) or default)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse a string and return a :class:`datetime.datetime` object\n    or ``None`` if the parse fails.\n    \"\"\"\n    try:\n        return self._parse(timestr, ignoretz, tzinfos)\n    except ValueError:\n        return default\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse string\n    \"\"\"\n    tokens = self._parse_numeric(timestr)\n    if tokens is None:\n        return default\n\n    hms = self._assign_hms(tokens, default)\n    if hms is None:\n        return default\n\n    if ignoretz and hms.tzname() == 'UTC':\n        hms = hms.replace(tzinfo=None)\n\n    res = self._ampm_valid(hms, **kwargs)\n    if res is None:\n        return self._parse_numeric(timestr)\n    else:\n        return res\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse string using local time\n    \"\"\"\n    if not tzinfos:\n        tzinfos = self.info.get('tzinfos', {})\n    if ignoretz and not timestr.endswith('Z'):\n        timestr += 'Z'\n    result = self._parse(timestr, default, **kwargs)\n    if result is None:\n        return default\n    elif tzinfos and self._assign_tzname:\n        result = self._assign_tzname(result, tzinfos)\n    return result\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"parse(timestr, default=None,\n                   ignoretz=False, tzinfos=None, **kwargs)\n    \n    parse(timestr, default=None,\n               ignoretz=False, tzinfos=None, **kwargs)\n        -> object\n\n    Parse a string and return a datetime object.\n\n    This function assumes that the input string is valid and\n    could be parsed by any of the 3 grammars:\n\n        - parse('%H:%M', '12:34', 'h:mm')\n        - parse('%H:%M', '12:34', 'h:mm', tzinfo='US/Eastern')\n        - parse('%H:%M', '12:34', 'h:mm', tzinfo='US/Eastern', tzinfos='US/Eastern')\n    \"\"\"\n    if ignoretz:\n        self._build_tzaware(naive=False, **kwargs)\n    else:\n        self._build_tzinfo(tzinfos, kwargs)\n\n    return self._parse(timestr, **kwargs)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse a string and return a datetime object.\n    \"\"\"\n    if tzinfos is None:\n        tzinfos = []\n    if self._ampm_valid(**kwargs):\n        hms = self._parse_hms(idx=None, tokens=timestr.split(),\n                               info=self.info,\n                               hms_idx=self._ampm_idx)\n        res = self._parse_numeric(val=hms, tokens=timestr.split(),\n                                  info=self.info,\n                                  hms_idx=self._ampm_idx)\n    else:\n        res = super(AMPMixin, self).parse(timestr, default,\n                                          ignoretz, tzinfos, **kwargs)\n    return res\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the given time string.  Default is to assume UTC if no\n    timezone is given.\n    \"\"\"\n    if not timestr:\n        return default\n\n    if self._ampm_valid:\n        res = self._parse_hms(idx=0, tokens=self._parse_hms(idx=0,\n                                                             tokens=timestr,\n                                                             info=self.info,\n                                                             hms_idx=0),\n                               info=self.info,\n                               hms_idx=0)\n    else:\n        res = self._parse_numeric(timestr, **kwargs)\n    return"], "level": "project_runnable", "generate_results": [{"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse string time values\n    \"\"\"\n    if ignoretz and timestr.endswith('Z'):\n        timestr = timestr[:-1]\n        tzinfos = ()\n\n    if tzinfos is None:\n        tzinfos = self._ampm_valid\n\n    res = self._parse(timestr, **kwargs)\n    if self._could_be_tzname(res):\n        return res\n\n    if res is None:\n        return default\n\n    return self._assign_tzname(res, tzinfos, tzinfos)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse a string with time information.\n    \"\"\"\n    if not timestr:\n        return default\n    return self._parse(timestr, ignoretz, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse string `timestr` according to rules defined in the `rules`\n    attribute of this class.\n    \"\"\"\n    if self._ampm_valid and ignoretz and timestr.count('+') == 1:\n        # No tzinfo means no timezone information\n        timestr = timestr.replace('+', '')\n    if not tzinfos:\n        tzinfos = self.info.get('timezone', None)\n    res = self._parse(timestr, **kwargs)\n    if tzinfos:\n        res = res.tzinfo if res else None\n    return res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse a string using the current locale context.\n    \"\"\"\n    return self._parse(timestr, ignoretz, **kwargs).replace(\n        tzinfo=self.info.get('tzinfo', None) or default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse a string and return a :class:`datetime.datetime` object\n    or ``None`` if the parse fails.\n    \"\"\"\n    try:\n        return self._parse(timestr, ignoretz, tzinfos)\n    except ValueError:\n        return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse string\n    \"\"\"\n    tokens = self._parse_numeric(timestr)\n    if tokens is None:\n        return default\n\n    hms = self._assign_hms(tokens, default)\n    if hms is None:\n        return default\n\n    if ignoretz and hms.tzname() == 'UTC':\n        hms = hms.replace(tzinfo=None)\n\n    res = self._ampm_valid(hms, **kwargs)\n    if res is None:\n        return self._parse_numeric(timestr)\n    else:\n        return res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse string using local time\n    \"\"\"\n    if not tzinfos:\n        tzinfos = self.info.get('tzinfos', {})\n    if ignoretz and not timestr.endswith('Z'):\n        timestr += 'Z'\n    result = self._parse(timestr, default, **kwargs)\n    if result is None:\n        return default\n    elif tzinfos and self._assign_tzname:\n        result = self._assign_tzname(result, tzinfos)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"parse(timestr, default=None,\n                   ignoretz=False, tzinfos=None, **kwargs)\n    \n    parse(timestr, default=None,\n               ignoretz=False, tzinfos=None, **kwargs)\n        -> object\n\n    Parse a string and return a datetime object.\n\n    This function assumes that the input string is valid and\n    could be parsed by any of the 3 grammars:\n\n        - parse('%H:%M', '12:34', 'h:mm')\n        - parse('%H:%M', '12:34', 'h:mm', tzinfo='US/Eastern')\n        - parse('%H:%M', '12:34', 'h:mm', tzinfo='US/Eastern', tzinfos='US/Eastern')\n    \"\"\"\n    if ignoretz:\n        self._build_tzaware(naive=False, **kwargs)\n    else:\n        self._build_tzinfo(tzinfos, kwargs)\n\n    return self._parse(timestr, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse a string and return a datetime object.\n    \"\"\"\n    if tzinfos is None:\n        tzinfos = []\n    if self._ampm_valid(**kwargs):\n        hms = self._parse_hms(idx=None, tokens=timestr.split(),\n                               info=self.info,\n                               hms_idx=self._ampm_idx)\n        res = self._parse_numeric(val=hms, tokens=timestr.split(),\n                                  info=self.info,\n                                  hms_idx=self._ampm_idx)\n    else:\n        res = super(AMPMixin, self).parse(timestr, default,\n                                          ignoretz, tzinfos, **kwargs)\n    return res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the given time string.  Default is to assume UTC if no\n    timezone is given.\n    \"\"\"\n    if not timestr:\n        return default\n\n    if self._ampm_valid:\n        res = self._parse_hms(idx=0, tokens=self._parse_hms(idx=0,\n                                                             tokens=timestr,\n                                                             info=self.info,\n                                                             hms_idx=0),\n                               info=self.info,\n                               hms_idx=0)\n    else:\n        res = self._parse_numeric(timestr, **kwargs)\n    return", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "ques_id": "62b8a4a4755ee91dce50a3d3", "code": ["def fromutc(self, dt):\n    return dt.tzinfo.fromutc()\n", "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt.replace(tzinfo=self)\n    else:\n        return dt\n", "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fromutc(dt)\n", "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return self._fold(dt)\n    else:\n        return dt\n", "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fold(dt)\n", "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fromutc(dt)\n", "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fromutc(dt)\n", "def fromutc(self, dt):\n    return self._fold(dt)\n", "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fold(dt)\n", "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt.replace(tzinfo=self)\n    else:\n        return dt\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fromutc(self, dt):\n    return dt.tzinfo.fromutc()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt.replace(tzinfo=self)\n    else:\n        return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fromutc(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return self._fold(dt)\n    else:\n        return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fold(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fromutc(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fromutc(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    return self._fold(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt\n\n    return self._fold(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self.is_ambiguous(dt):\n        return dt.replace(tzinfo=self)\n    else:\n        return dt\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/utils.py", "project": "pexip/os-python-dateutil", "name": "default_tzinfo", "docstring": "Sets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.", "ques_id": "62b8a7b2755ee91dce50a4a7", "code": ["def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    if dt.tzinfo is not tzinfo:\n        dt = dt.astimezone(tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if isinstance(dt, datetime):\n        dt = dt.replace(tzinfo=tzinfo)\n    elif isinstance(dt, date):\n        dt = datetime(dt.year, dt.month, dt.day)\n    if not is_aware(dt):\n        dt = dt.astimezone(tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if isinstance(dt, datetime):\n        dt = dt.replace(tzinfo=tzinfo)\n    elif not isinstance(dt, date):\n        raise TypeError('dt must be a datetime or date, not %s' % dt.__class__.__name__)\n\n    return dt - tzinfo.utcoffset(dt)\n", "def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n\n    result = dt.replace(tzinfo=tzinfo)\n\n    # If the datetime has a tzinfo, we can just return it without modification.\n    # (Also in this case, ``dflt_tzinfo`` is already a simple boolean value.)\n    if result.tzinfo is not None:\n        return result\n\n    return result.astimezone(tzinfo)\n", "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n\n    return dt\n"], "level": "self_contained", "generate_results": [{"generate_code": "def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    if dt.tzinfo is not tzinfo:\n        dt = dt.astimezone(tzinfo)\n    return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if isinstance(dt, datetime):\n        dt = dt.replace(tzinfo=tzinfo)\n    elif isinstance(dt, date):\n        dt = datetime(dt.year, dt.month, dt.day)\n    if not is_aware(dt):\n        dt = dt.astimezone(tzinfo)\n    return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if isinstance(dt, datetime):\n        dt = dt.replace(tzinfo=tzinfo)\n    elif not isinstance(dt, date):\n        raise TypeError('dt must be a datetime or date, not %s' % dt.__class__.__name__)\n\n    return dt - tzinfo.utcoffset(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n\n    result = dt.replace(tzinfo=tzinfo)\n\n    # If the datetime has a tzinfo, we can just return it without modification.\n    # (Also in this case, ``dflt_tzinfo`` is already a simple boolean value.)\n    if result.tzinfo is not None:\n        return result\n\n    return result.astimezone(tzinfo)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n\n    return dt\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/lithium/testcases.py", "project": "MozillaSecurity/lithium", "name": "set_cut_chars", "docstring": "Set the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.", "ques_id": "6305f9991d275c6667163c50", "code": ["def set_cut_chars(self, before: str, after: str) -> None:\n    if before is None:\n        before = ''\n    if after is None:\n        after = ''\n    self.cut_before = before\n    self.cut_after = after\n", "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_chars = (before, after)\n", "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_chars = (before, after)\n", "def set_cut_chars(self, before: str, after: str) -> None:\n    self._cutter.set_delimiter(before)\n    self._cutter.set_delimiter(after)\n", "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_before = before\n    self.cut_after = after\n", "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_chars = f'{before}{after}'\n", "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_chars = (before, after)\n", "def set_cut_chars(self, before: str ='', after: str ='') -> None:\n    self.cut_chars = before + ',' + after + ','\n", "def set_cut_chars(self, before: str, after: str) -> None:\n    self._cutter = slice(*[int(c) for c in before.split(self.parts)])\n    self._cutter.stop = slice(*[int(c) for c in after.split(self.parts)])\n", "def set_cut_chars(self, before: str, after: str):\n    self.cut_before = before\n    self.cut_after = after\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def set_cut_chars(self, before: str, after: str) -> None:\n    if before is None:\n        before = ''\n    if after is None:\n        after = ''\n    self.cut_before = before\n    self.cut_after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_chars = (before, after)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_chars = (before, after)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: str, after: str) -> None:\n    self._cutter.set_delimiter(before)\n    self._cutter.set_delimiter(after)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_before = before\n    self.cut_after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_chars = f'{before}{after}'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: str, after: str) -> None:\n    self.cut_chars = (before, after)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: str ='', after: str ='') -> None:\n    self.cut_chars = before + ',' + after + ','\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: str, after: str) -> None:\n    self._cutter = slice(*[int(c) for c in before.split(self.parts)])\n    self._cutter.stop = slice(*[int(c) for c in after.split(self.parts)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: str, after: str):\n    self.cut_before = before\n    self.cut_after = after\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/protocols/diaspora/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.", "ques_id": "6306292052e177c0ba469f09", "code": ["def identify_request(request: Dict[str, Any]) -> Tuple[bool, str]:\n    request_type = request.get('type')\n    if request_type == 'legacy':\n        return True, request['legacy_payload']\n\n    public_message_id = request.get('public_message_id')\n    if public_message_id is not None:\n        return True, identify_id(public_message_id)\n\n    private_message_id = request.get('private_message_id')\n    if private_message_id is not None:\n        return False, identify_id(private_message_id)\n\n    return False, None\n", "def identify_request(request: Request) -> bool:\n    message = request.message\n    if message.is_legacy_request:\n        logger.info(\"Found legacy request\")\n        return True\n    if message.private:\n        logger.info(\"Found private request\")\n        return False\n    return identify_id(message.request)\n", "def identify_request(request: Request) -> bool:\n    if request.message.is_legacy_request:\n        return True\n\n    public_message = request.message.public_request\n    if public_message:\n        return identify_id(public_message.identifier)\n\n    legacy_payload = request.message.legacy_request\n    if legacy_payload:\n        return identify_id(legacy_payload.identifier)\n\n    return False\n", "def identify_request(request: dict) -> bool:\n    if request[\"method\"] == \"POST\" and request[\"uri\"] == \"/legacy\":\n        return True\n    return identify_id(request[\"message_id\"]) and identify_request(request[\"request\"])\n", "def identify_request(request: Request) -> bool:\n    msg_payload = request.message.get(\"payload\")\n    if msg_payload is None:\n        return False\n    legacy_payload = msg_payload.get(\"legacyPayload\")\n    if legacy_payload is None:\n        return False\n    return True\n", "def identify_request(request: Request) -> Tuple[bool, str]:\n    if request.message.is_legacy_payload():\n        return True, request.message.payload.name\n\n    return identify_id(request.message.id), request.message.payload.name\n", "def identify_request(request: Request) -> bool:\n    msg = request.message\n    if msg.is_legacy_request():\n        return identify_request(msg.request)\n\n    return identify_request(msg.request.payload)\n", "def identify_request(request: Element) -> bool:\n    public_msg = request.find(\".//{%s}publicMessage\" % PROTOCOL_NS)\n    private_msg = request.find(\".//{%s}privateMessage\" % PROTOCOL_NS)\n\n    if public_msg is None or private_msg is None:\n        return False\n\n    if public_msg.text == MAGIC_ENV_TAG:\n        return True\n    else:\n        logger.debug(\n            \"Received a legacy request with public message, but it is not a Diaspora request.\"\n        )\n        return False\n", "def identify_request(request: Element) -> bool:\n    return (\n        isinstance(request, (Request, RequestDelegation))\n        and request.type == RequestType.PUBLIC_MESSAGE\n        and request.message.find(MAGIC_ENV_TAG) > -1\n    )\n", "def identify_request(request: Request) -> bool:\n    try:\n        return request.message.is_legacy_request\n    except AttributeError:\n        return request.message.public_id is not None\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def identify_request(request: Dict[str, Any]) -> Tuple[bool, str]:\n    request_type = request.get('type')\n    if request_type == 'legacy':\n        return True, request['legacy_payload']\n\n    public_message_id = request.get('public_message_id')\n    if public_message_id is not None:\n        return True, identify_id(public_message_id)\n\n    private_message_id = request.get('private_message_id')\n    if private_message_id is not None:\n        return False, identify_id(private_message_id)\n\n    return False, None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    message = request.message\n    if message.is_legacy_request:\n        logger.info(\"Found legacy request\")\n        return True\n    if message.private:\n        logger.info(\"Found private request\")\n        return False\n    return identify_id(message.request)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    if request.message.is_legacy_request:\n        return True\n\n    public_message = request.message.public_request\n    if public_message:\n        return identify_id(public_message.identifier)\n\n    legacy_payload = request.message.legacy_request\n    if legacy_payload:\n        return identify_id(legacy_payload.identifier)\n\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: dict) -> bool:\n    if request[\"method\"] == \"POST\" and request[\"uri\"] == \"/legacy\":\n        return True\n    return identify_id(request[\"message_id\"]) and identify_request(request[\"request\"])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    msg_payload = request.message.get(\"payload\")\n    if msg_payload is None:\n        return False\n    legacy_payload = msg_payload.get(\"legacyPayload\")\n    if legacy_payload is None:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> Tuple[bool, str]:\n    if request.message.is_legacy_payload():\n        return True, request.message.payload.name\n\n    return identify_id(request.message.id), request.message.payload.name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    msg = request.message\n    if msg.is_legacy_request():\n        return identify_request(msg.request)\n\n    return identify_request(msg.request.payload)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Element) -> bool:\n    public_msg = request.find(\".//{%s}publicMessage\" % PROTOCOL_NS)\n    private_msg = request.find(\".//{%s}privateMessage\" % PROTOCOL_NS)\n\n    if public_msg is None or private_msg is None:\n        return False\n\n    if public_msg.text == MAGIC_ENV_TAG:\n        return True\n    else:\n        logger.debug(\n            \"Received a legacy request with public message, but it is not a Diaspora request.\"\n        )\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Element) -> bool:\n    return (\n        isinstance(request, (Request, RequestDelegation))\n        and request.type == RequestType.PUBLIC_MESSAGE\n        and request.message.find(MAGIC_ENV_TAG) > -1\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    try:\n        return request.message.is_legacy_request\n    except AttributeError:\n        return request.message.public_id is not None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/matrix/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Matrix request", "ques_id": "6306292152e177c0ba469f0d", "code": ["def identify_request(request: Request) -> bool:\n    return (\n        isinstance(request, types.POST)\n        and \"javascript\" in request.get_header(\"content-type\")\n    )\n", "def identify_request(request: Request) -> bool:\n    return isinstance(request, MatrixRequest)\n", "def identify_request(request: Dict) -> bool:\n    return bool(request.get(\"method\", \"\") == \"matrix\")\n", "def identify_request(request: Request) -> bool:\n    return request.method == \"POST\" and request.path == \"/matrix/identify\"\n", "def identify_request(request: Dict[str, Any]) -> bool:\n    return request[\"request\"].get(\"type\", \"\").lower() == \"matrix\"\n", "def identify_request(request: Dict) -> bool:\n    return request.get('method') =='matrix'\n", "def identify_request(request: Request) -> bool:\n    if isinstance(request, dict):\n        if \"method\" in request and request[\"method\"] == \"Matrix\":\n            return True\n        if \"params\" in request:\n            return True\n        if \"id\" in request:\n            return True\n    return False\n", "def identify_request(request: Request) -> bool:\n    return request.method == \"POST\" and request.path == \"/matrix/v1/identify\"\n", "def identify_request(request: Request) -> bool:\n    return request.method == \"GET\" and request.url.startswith(\"/matrix/\")\n", "def identify_request(request: Request) -> bool:\n    if request.method == \"POST\":\n        return True\n    return False\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def identify_request(request: Request) -> bool:\n    return (\n        isinstance(request, types.POST)\n        and \"javascript\" in request.get_header(\"content-type\")\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    return isinstance(request, MatrixRequest)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Dict) -> bool:\n    return bool(request.get(\"method\", \"\") == \"matrix\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    return request.method == \"POST\" and request.path == \"/matrix/identify\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Dict[str, Any]) -> bool:\n    return request[\"request\"].get(\"type\", \"\").lower() == \"matrix\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Dict) -> bool:\n    return request.get('method') =='matrix'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    if isinstance(request, dict):\n        if \"method\" in request and request[\"method\"] == \"Matrix\":\n            return True\n        if \"params\" in request:\n            return True\n        if \"id\" in request:\n            return True\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    return request.method == \"POST\" and request.path == \"/matrix/v1/identify\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    return request.method == \"GET\" and request.url.startswith(\"/matrix/\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: Request) -> bool:\n    if request.method == \"POST\":\n        return True\n    return False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/utils.py", "project": "jaywink/federation", "name": "format_dt", "docstring": "Format a datetime in the way that D* nodes expect.", "ques_id": "6306292252e177c0ba469f11", "code": ["def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%d %H:%M:%S %z\")\n", "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%d %H:%M\")\n", "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "def format_dt(dt):\n    if isinstance(dt, datetime.datetime):\n        dt = dt.replace(tzinfo=pytz.utc)\n    return dt.isoformat()\n", "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%d %H:%M:%S %z\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%d %H:%M\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    if isinstance(dt, datetime.datetime):\n        dt = dt.replace(tzinfo=pytz.utc)\n    return dt.isoformat()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "find_tags", "docstring": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.", "ques_id": "6306292352e177c0ba469f1d", "code": ["def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    return set(\n        decode_if_bytes(tag)\n        for tag in find_tags_iter(text)\n        if (replacer is None or replacer(tag))\n    )\n", "def find_tags(text: str, replacer=None) -> Tuple[Set[str], str]:\n    text = decode_if_bytes(text)\n    finder = TagFinder()\n    finder.feed(text)\n    return finder.get_tags(), replacer(finder.get_body())\n", "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    if not text:\n        return set()\n\n    def replacer_or_str(match: Match) -> str:\n        return replacer(match.group(0))\n\n    def encode_if_text(match: Match) -> str:\n        return encode_if_bytes(match.group(0))\n\n    pattern = r'(?<!\\\\)(?P<tag>(?:(?:(?:\\\\[^\\\\])|(?:(?:(?:\\\\(?:\\\\(?:)|(?:(?:[^\\\\])|(?:[^\\\\])))\\\\)))\\\\(?:(?:[^\\\\])|(?:[^\\\\]))))?))'\n    replacer_pattern = re.compile(pattern, re.UNICODE | re.MULTILINE)\n    found_tags = replacer_pattern.findall(text)\n\n    if replacer_or_str is not None:\n        found_tags = set(replacer_or_str(match) for match in found_tags)\n\n    return found_tags\n", "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Tuple[Set[str], str]:\n    handle = find_tags(text, replacer)\n    try:\n        text = handle.read()\n    except AttributeError:\n        pass\n    return process_text_links(text), handle.name\n", "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    url = get_path_from_url(text)\n\n    if url is None:\n        return set()\n\n    tags = set()\n    for match in re.finditer(r\"(?i)(\\[\\[(.*?)\\]\\])\", text, flags=re.UNICODE):\n        tag = match.group(1)\n        if replacer is not None:\n            tag = replacer(tag)\n        tags.add(tag)\n\n    return tags\n", "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Tuple[Set[str], str]:\n    text = decode_if_bytes(text)\n    if text is None:\n        return {}, \"\"\n\n    def ignore_code(match):\n        return match.group(1) not in ILLEGAL_TAG_CHARS\n\n    def replacer(match):\n        return replacer(match.group(0))\n\n    ignore_code_matches = find_tags(text, ignore_code)\n    return {match.group(0) for match in ignore_code_matches}, text.replace(\"</code>\", \"</code><pre>\").replace(\"</pre>\", \"</pre><code>\")\n", "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    return set(\n        decode_if_bytes(tag) if isinstance(tag, bytes) else tag\n        for tag in find_tags_iter(text, replacer)\n    )\n", "def find_tags(text: str, replacer=None) -> Set[str]:\n    if replacer is None:\n        return set([])\n\n    def repl(match):\n        return replacer(match.group(1))\n\n    return re.sub(ILLEGAL_TAG_CHARS, repl, text)\n", "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Tuple[Set[str], str]:\n    # Try to find all code blocks\n    try:\n        text = re.sub(r\"<!--[\\s\\S]*-->\", \"\", text)\n    except TypeError:\n        pass\n\n    # Try to find all tags inside code\n    try:\n        text = re.sub(r\"<!\\[CDATA\\[[\\s\\S]*?\\]\\]>\", \"\", text)\n    except TypeError:\n        pass\n\n    # Try to find all tags with regex\n    try:\n        text = re.sub(r\"<!\\[\\[(.*?)\\]\\]>\", replacer, text)\n    except TypeError:\n        pass\n\n    return find_tags(text, replacer)\n", "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    text = decode_if_bytes(text)\n    finder = TagFinder()\n    finder.feed(text)\n    return finder.get_tags()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    return set(\n        decode_if_bytes(tag)\n        for tag in find_tags_iter(text)\n        if (replacer is None or replacer(tag))\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer=None) -> Tuple[Set[str], str]:\n    text = decode_if_bytes(text)\n    finder = TagFinder()\n    finder.feed(text)\n    return finder.get_tags(), replacer(finder.get_body())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    if not text:\n        return set()\n\n    def replacer_or_str(match: Match) -> str:\n        return replacer(match.group(0))\n\n    def encode_if_text(match: Match) -> str:\n        return encode_if_bytes(match.group(0))\n\n    pattern = r'(?<!\\\\)(?P<tag>(?:(?:(?:\\\\[^\\\\])|(?:(?:(?:\\\\(?:\\\\(?:)|(?:(?:[^\\\\])|(?:[^\\\\])))\\\\)))\\\\(?:(?:[^\\\\])|(?:[^\\\\]))))?))'\n    replacer_pattern = re.compile(pattern, re.UNICODE | re.MULTILINE)\n    found_tags = replacer_pattern.findall(text)\n\n    if replacer_or_str is not None:\n        found_tags = set(replacer_or_str(match) for match in found_tags)\n\n    return found_tags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Tuple[Set[str], str]:\n    handle = find_tags(text, replacer)\n    try:\n        text = handle.read()\n    except AttributeError:\n        pass\n    return process_text_links(text), handle.name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    url = get_path_from_url(text)\n\n    if url is None:\n        return set()\n\n    tags = set()\n    for match in re.finditer(r\"(?i)(\\[\\[(.*?)\\]\\])\", text, flags=re.UNICODE):\n        tag = match.group(1)\n        if replacer is not None:\n            tag = replacer(tag)\n        tags.add(tag)\n\n    return tags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Tuple[Set[str], str]:\n    text = decode_if_bytes(text)\n    if text is None:\n        return {}, \"\"\n\n    def ignore_code(match):\n        return match.group(1) not in ILLEGAL_TAG_CHARS\n\n    def replacer(match):\n        return replacer(match.group(0))\n\n    ignore_code_matches = find_tags(text, ignore_code)\n    return {match.group(0) for match in ignore_code_matches}, text.replace(\"</code>\", \"</code><pre>\").replace(\"</pre>\", \"</pre><code>\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    return set(\n        decode_if_bytes(tag) if isinstance(tag, bytes) else tag\n        for tag in find_tags_iter(text, replacer)\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer=None) -> Set[str]:\n    if replacer is None:\n        return set([])\n\n    def repl(match):\n        return replacer(match.group(1))\n\n    return re.sub(ILLEGAL_TAG_CHARS, repl, text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Tuple[Set[str], str]:\n    # Try to find all code blocks\n    try:\n        text = re.sub(r\"<!--[\\s\\S]*-->\", \"\", text)\n    except TypeError:\n        pass\n\n    # Try to find all tags inside code\n    try:\n        text = re.sub(r\"<!\\[CDATA\\[[\\s\\S]*?\\]\\]>\", \"\", text)\n    except TypeError:\n        pass\n\n    # Try to find all tags with regex\n    try:\n        text = re.sub(r\"<!\\[\\[(.*?)\\]\\]>\", replacer, text)\n    except TypeError:\n        pass\n\n    return find_tags(text, replacer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: Callable[[str], str] = None) -> Set[str]:\n    text = decode_if_bytes(text)\n    finder = TagFinder()\n    finder.feed(text)\n    return finder.get_tags()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "process_text_links", "docstring": "Process links in text, adding some attributes and linkifying textual links.", "ques_id": "6306292352e177c0ba469f1e", "code": ["def process_text_links(text):\n    text = linkify(text)\n    text = remove_html_links(text)\n    return text\n", "def process_text_links(text):\n    text = bleach.clean(\n        text,\n        tags=ALLOWED_TAGS,\n        attributes=ALLOWED_ATTRIBUTES,\n        linkify=True,\n        strip_comments=True,\n    )\n\n    return text\n", "def process_text_links(text):\n    linkify_text = bleach.linkify(text, **linkify_options)\n\n    return linkify_text\n", "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "def process_text_links(text):\n    text = linkify(text)\n    text = add_images(text)\n    text = remove_html(text)\n    return text\n", "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "def process_text_links(text):\n    linkified = bleach.linkify(text,\n                                output_format='html',\n                                quote=True,\n                                strip_comments=True,\n                                remove_anchors=True,\n                                encoding='utf8',\n                                allow_images=True,\n                                allow_emails=True,\n                                allow_favicon=True,\n                                extra_params={'rel':'stylesheet'})\n\n    return linkified\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def process_text_links(text):\n    text = linkify(text)\n    text = remove_html_links(text)\n    return text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    text = bleach.clean(\n        text,\n        tags=ALLOWED_TAGS,\n        attributes=ALLOWED_ATTRIBUTES,\n        linkify=True,\n        strip_comments=True,\n    )\n\n    return text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    linkify_text = bleach.linkify(text, **linkify_options)\n\n    return linkify_text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    text = linkify(text)\n    text = add_images(text)\n    text = remove_html(text)\n    return text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    linker = Linker()\n    return linker.linkify(text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    linkified = bleach.linkify(text,\n                                output_format='html',\n                                quote=True,\n                                strip_comments=True,\n                                remove_anchors=True,\n                                encoding='utf8',\n                                allow_images=True,\n                                allow_emails=True,\n                                allow_favicon=True,\n                                extra_params={'rel':'stylesheet'})\n\n    return linkified\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "fetch_content_type", "docstring": "Fetch the HEAD of the remote url to determine the content type.", "ques_id": "6306292652e177c0ba469f34", "code": ["def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url)\n    except requests.exceptions.ConnectionError:\n        logger.warning(\"Couldn't get remote document %s\", url)\n        return None\n    content_type = response.headers.get(\"content-type\", \"\")\n    logger.debug(\"Content-Type: %s\", content_type)\n    return content_type\n", "def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url)\n    except requests.exceptions.InvalidSchema as e:\n        raise FetcherError(str(e))\n\n    # The response is always a valid response - so we can just return the\n    # content type\n    return response.headers['content-type']\n", "def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url, allow_redirects=True)\n        return response.headers['content-type']\n    except requests.exceptions.RequestException:\n        return None\n", "def fetch_content_type(url: str) -> str:\n    # TODO: split into multiple requests to determine the content type\n    response = requests.head(url, allow_redirects=True)\n    return response.headers.get('content-type')\n", "def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url, allow_redirects=True)\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Failed to retrieve {url}: {e}\")\n        raise FetcherException(f\"Failed to retrieve {url}\")\n\n    if response.status_code!= 200:\n        logger.error(f\"Failed to retrieve {url}\")\n        raise FetcherException(f\"Failed to retrieve {url}\")\n\n    return response.headers[\"content-type\"]\n", "def fetch_content_type(url: str) -> str:\n    response = requests.head(url)\n    return response.headers[\"content-type\"]\n", "def fetch_content_type(url: str) -> str:\n    try:\n        req = requests.head(url)\n    except requests.exceptions.RequestException:\n        raise FetchFailed('Unable to reach {}'.format(url))\n\n    if req.status_code == 200:\n        return req.headers['content-type']\n    else:\n        raise FetchFailed('Unable to determine content type of {}'.format(url))\n", "def fetch_content_type(url: str) -> str:\n    with requests.get(url, stream=True) as response:\n        response.raise_for_status()\n        return response.headers['content-type']\n", "def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url, allow_redirects=False, headers={\"User-Agent\": USER_AGENT})\n        return response.headers[\"content-type\"]\n    except RequestException as e:\n        raise FetcherException(e)\n", "def fetch_content_type(url: str) -> str:\n    response = requests.head(url)\n    return fetch_http_date(response.headers[\"last-modified\"])\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url)\n    except requests.exceptions.ConnectionError:\n        logger.warning(\"Couldn't get remote document %s\", url)\n        return None\n    content_type = response.headers.get(\"content-type\", \"\")\n    logger.debug(\"Content-Type: %s\", content_type)\n    return content_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url)\n    except requests.exceptions.InvalidSchema as e:\n        raise FetcherError(str(e))\n\n    # The response is always a valid response - so we can just return the\n    # content type\n    return response.headers['content-type']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url, allow_redirects=True)\n        return response.headers['content-type']\n    except requests.exceptions.RequestException:\n        return None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> str:\n    # TODO: split into multiple requests to determine the content type\n    response = requests.head(url, allow_redirects=True)\n    return response.headers.get('content-type')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url, allow_redirects=True)\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Failed to retrieve {url}: {e}\")\n        raise FetcherException(f\"Failed to retrieve {url}\")\n\n    if response.status_code!= 200:\n        logger.error(f\"Failed to retrieve {url}\")\n        raise FetcherException(f\"Failed to retrieve {url}\")\n\n    return response.headers[\"content-type\"]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> str:\n    response = requests.head(url)\n    return response.headers[\"content-type\"]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> str:\n    try:\n        req = requests.head(url)\n    except requests.exceptions.RequestException:\n        raise FetchFailed('Unable to reach {}'.format(url))\n\n    if req.status_code == 200:\n        return req.headers['content-type']\n    else:\n        raise FetchFailed('Unable to determine content type of {}'.format(url))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> str:\n    with requests.get(url, stream=True) as response:\n        response.raise_for_status()\n        return response.headers['content-type']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> str:\n    try:\n        response = requests.head(url, allow_redirects=False, headers={\"User-Agent\": USER_AGENT})\n        return response.headers[\"content-type\"]\n    except RequestException as e:\n        raise FetcherException(e)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> str:\n    response = requests.head(url)\n    return fetch_http_date(response.headers[\"last-modified\"])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "test_tag", "docstring": "Test a word whether it could be accepted as a tag.", "ques_id": "6306292a52e177c0ba469f41", "code": ["def test_tag(tag: str) -> bool:\n    return (\n        not ALLOWED_TAGS.intersection(set(tag.split())) and not tag.startswith(\"_\")\n    )\n", "def test_tag(tag: str) -> bool:\n    return test_tag(encode_if_text(tag))\n", "def test_tag(tag: str) -> bool:\n    return test_tag(encode_if_text(tag))\n", "def test_tag(tag: str) -> bool:\n    return bool(ILLEGAL_TAG_CHARS.match(tag))\n", "def test_tag(tag: str) -> bool:\n    return bool(get_path_from_url(tag))\n", "def test_tag(tag: str) -> bool:\n    return all((\n        # Try to guess if the tag is a string or not\n        decode_if_bytes(tag) is not None,\n        # Try to guess if the tag is a number\n        decode_if_bytes(tag) is not False,\n        # Try to guess if the tag is a URI\n        decode_if_bytes(tag) is not None,\n    ))\n", "def test_tag(tag: str) -> bool:\n    if len(tag) > 1:\n        return False\n    return ((test_tag(t) or test_tag(t.upper())) and\n            (test_tag(t.lower()) or test_tag(t.capitalize())))\n", "def test_tag(tag: str) -> bool:\n    return (\n        not tag.startswith(ILLEGAL_TAG_CHARS)\n        and not re.match(r\"^[a-zA-Z_][a-zA-Z0-9_]*$\", tag)\n        and not re.match(r\"^[a-zA-Z0-9_][a-zA-Z0-9_]*$\", get_path_from_url(tag))\n    )\n", "def test_tag(tag: str) -> bool:\n    return (\n        tag.startswith(\"/\") and not tag.endswith(\"/\") and not any(\n            re.match(pattern, tag) for pattern in ILLEGAL_TAG_CHARS\n        )\n    )\n", "def test_tag(tag: str) -> bool:\n    return tag in ILLEGAL_TAG_CHARS\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def test_tag(tag: str) -> bool:\n    return (\n        not ALLOWED_TAGS.intersection(set(tag.split())) and not tag.startswith(\"_\")\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return test_tag(encode_if_text(tag))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return test_tag(encode_if_text(tag))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return bool(ILLEGAL_TAG_CHARS.match(tag))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return bool(get_path_from_url(tag))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return all((\n        # Try to guess if the tag is a string or not\n        decode_if_bytes(tag) is not None,\n        # Try to guess if the tag is a number\n        decode_if_bytes(tag) is not False,\n        # Try to guess if the tag is a URI\n        decode_if_bytes(tag) is not None,\n    ))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n    if len(tag) > 1:\n        return False\n    return ((test_tag(t) or test_tag(t.upper())) and\n            (test_tag(t.lower()) or test_tag(t.capitalize())))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return (\n        not tag.startswith(ILLEGAL_TAG_CHARS)\n        and not re.match(r\"^[a-zA-Z_][a-zA-Z0-9_]*$\", tag)\n        and not re.match(r\"^[a-zA-Z0-9_][a-zA-Z0-9_]*$\", get_path_from_url(tag))\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return (\n        tag.startswith(\"/\") and not tag.endswith(\"/\") and not any(\n            re.match(pattern, tag) for pattern in ILLEGAL_TAG_CHARS\n        )\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return tag in ILLEGAL_TAG_CHARS\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "xml_children_as_dict", "docstring": "Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.", "ques_id": "6306298b52e177c0ba469fdc", "code": ["def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "def xml_children_as_dict(node):\n    children = node.getchildren()\n    return {child.tag: child.text for child in children}\n", "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "def xml_children_as_dict(node):\n    children = {}\n    for child in node:\n        tag = child.tag\n        if tag in children:\n            children[tag] += [child]\n        else:\n            children[tag] = [child]\n    return children\n", "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "def xml_children_as_dict(node):\n    children = {}\n    for child in node:\n        tag = child.tag\n        children[tag] = child\n    return children\n", "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "def xml_children_as_dict(node):\n    children = [c for c in node.childNodes\n                if c.nodeType == c.CDATA_SECTION_NODE]\n    return dict([(c.tagName, c.firstChild.data)\n                 for c in children])\n", "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "def xml_children_as_dict(node):\n    children = {}\n    for child in node.childNodes:\n        if child.nodeType == child.TEXT_NODE:\n            children[child.tag] = child.strip()\n    return children\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    children = node.getchildren()\n    return {child.tag: child.text for child in children}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    children = {}\n    for child in node:\n        tag = child.tag\n        if tag in children:\n            children[tag] += [child]\n        else:\n            children[tag] = [child]\n    return children\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    children = {}\n    for child in node:\n        tag = child.tag\n        children[tag] = child\n    return children\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    children = [c for c in node.childNodes\n                if c.nodeType == c.CDATA_SECTION_NODE]\n    return dict([(c.tagName, c.firstChild.data)\n                 for c in children])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    children = {}\n    for child in node.childNodes:\n        if child.nodeType == child.TEXT_NODE:\n            children[child.tag] = child.strip()\n    return children\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "check_sender_and_entity_handle_match", "docstring": "Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.", "ques_id": "6306299052e177c0ba469fe8", "code": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender_is_who_said = sender_handle.startswith(\"<@\")\n    entity_is_who_said = entity_handle.startswith(\"<@\")\n\n    if sender_is_who_said and entity_is_who_said:\n        return True\n    elif not sender_is_who_said and not entity_is_who_said:\n        logger.debug(\"Sender handle %s is not who said %s\", sender_handle, entity_handle)\n        return False\n\n    return True\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    # type: (str, str) -> None\n    \"\"\"Ensure that sender and entity handles match.\n\n    Similar to the sender handle, we've already verified that the sender\n    is whom they say when receiving the payload. However, the sender\n    might be trying to set another author in the payload itself, so\n    we must ensure they're the same.\n    \"\"\"\n    sender = get_outbound_entity(entity, private_key=sender_handle)\n    entity = get_outbound_entity(entity, private_key=entity_handle)\n    if sender is None or entity is None:\n        return\n\n    if sender.name!= entity.name:\n        logger.warning(\n            \"Sender and entity names do not match, sender %s!= entity %s\",\n            sender.name,\n            entity.name,\n        )\n        raise SenderMismatch()\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender_key = get_outbound_entity(\n        entity=sender_handle, private_key=False\n    ).key\n    entity_key = get_outbound_entity(\n        entity=entity_handle, private_key=True\n    ).key\n    if sender_key!= entity_key:\n        logger.error(\n            \"Sender handle '{sender_handle}' does not match \"\n            \"entity handle '{entity_handle}'\".format(\n                sender_handle=sender_key,\n                entity_handle=entity_key,\n            )\n        )\n        raise InvalidMessageError(\n            message=(\n                \"Sender handle '{sender_handle}' does not match \"\n                \"entity handle '{entity_handle}'\".format(\n                    sender_handle=sender_key,\n                    entity_handle=entity_key,\n                )\n         ", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    return sender_handle == entity_handle\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        logger.debug(\"Sender handle doesn't match the entity handle\")\n        raise SenderHandleMismatch()\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        logger.warning(\n           'sender handle does not match the entity handle: %s!= %s',\n            sender_handle,\n            entity_handle,\n        )\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender = get_outbound_entity(entity=sender_handle, private_key=None)\n    entity = get_outbound_entity(entity=entity_handle, private_key=None)\n    if sender!= entity:\n        logger.warning(\n            \"Sender handle mismatch %s!= %s\",\n            sender_handle,\n            entity_handle,\n        )\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender = xml_children_as_dict(sender_handle)\n    entity = xml_children_as_dict(entity_handle)\n\n    if sender[\"@type\"]!= entity[\"@type\"]:\n        logger.debug(\"Sender handle mismatch %s!= %s\", sender_handle, entity_handle)\n        return False\n    return True\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender_email = Email(sender_handle)\n    entity_email = Email(entity_handle)\n\n    if sender_email!= entity_email:\n        logger.debug(\n            'Sender handle mismatch for sender: %s and entity: %s',\n            sender_handle,\n            entity_handle,\n        )\n        return False\n\n    return True\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        logger.warning(\n            \"Sender handle does not match the object's received header. \"\n            \"Sender: %s, Received: %s\",\n            sender_handle,\n            entity_handle,\n        )\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender_is_who_said = sender_handle.startswith(\"<@\")\n    entity_is_who_said = entity_handle.startswith(\"<@\")\n\n    if sender_is_who_said and entity_is_who_said:\n        return True\n    elif not sender_is_who_said and not entity_is_who_said:\n        logger.debug(\"Sender handle %s is not who said %s\", sender_handle, entity_handle)\n        return False\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    # type: (str, str) -> None\n    \"\"\"Ensure that sender and entity handles match.\n\n    Similar to the sender handle, we've already verified that the sender\n    is whom they say when receiving the payload. However, the sender\n    might be trying to set another author in the payload itself, so\n    we must ensure they're the same.\n    \"\"\"\n    sender = get_outbound_entity(entity, private_key=sender_handle)\n    entity = get_outbound_entity(entity, private_key=entity_handle)\n    if sender is None or entity is None:\n        return\n\n    if sender.name!= entity.name:\n        logger.warning(\n            \"Sender and entity names do not match, sender %s!= entity %s\",\n            sender.name,\n            entity.name,\n        )\n        raise SenderMismatch()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender_key = get_outbound_entity(\n        entity=sender_handle, private_key=False\n    ).key\n    entity_key = get_outbound_entity(\n        entity=entity_handle, private_key=True\n    ).key\n    if sender_key!= entity_key:\n        logger.error(\n            \"Sender handle '{sender_handle}' does not match \"\n            \"entity handle '{entity_handle}'\".format(\n                sender_handle=sender_key,\n                entity_handle=entity_key,\n            )\n        )\n        raise InvalidMessageError(\n            message=(\n                \"Sender handle '{sender_handle}' does not match \"\n                \"entity handle '{entity_handle}'\".format(\n                    sender_handle=sender_key,\n                    entity_handle=entity_key,\n                )\n         ", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    return sender_handle == entity_handle\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        logger.debug(\"Sender handle doesn't match the entity handle\")\n        raise SenderHandleMismatch()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        logger.warning(\n           'sender handle does not match the entity handle: %s!= %s',\n            sender_handle,\n            entity_handle,\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender = get_outbound_entity(entity=sender_handle, private_key=None)\n    entity = get_outbound_entity(entity=entity_handle, private_key=None)\n    if sender!= entity:\n        logger.warning(\n            \"Sender handle mismatch %s!= %s\",\n            sender_handle,\n            entity_handle,\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender = xml_children_as_dict(sender_handle)\n    entity = xml_children_as_dict(entity_handle)\n\n    if sender[\"@type\"]!= entity[\"@type\"]:\n        logger.debug(\"Sender handle mismatch %s!= %s\", sender_handle, entity_handle)\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    sender_email = Email(sender_handle)\n    entity_email = Email(entity_handle)\n\n    if sender_email!= entity_email:\n        logger.debug(\n            'Sender handle mismatch for sender: %s and entity: %s',\n            sender_handle,\n            entity_handle,\n        )\n        return False\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        logger.warning(\n            \"Sender handle does not match the object's received header. \"\n            \"Sender: %s, Received: %s\",\n            sender_handle,\n            entity_handle,\n        )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/hostmeta/generators.py", "project": "jaywink/federation", "name": "get_nodeinfo_well_known_document", "docstring": "Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict", "ques_id": "630629b952e177c0ba46a043", "code": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    nodeinfo = generate_nodeinfo2_document()\n\n    if document_path is not None:\n        nodeinfo[\"@id\"] = os.path.basename(document_path)\n\n    return {\"@id\": os.path.basename(url), \"nodeinfo\": nodeinfo}\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    return generate_nodeinfo2_document(url, document_path)\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = NODEINFO_DOCUMENT_PATH\n\n    with open(document_path, \"w\") as file:\n        file.write(generate_nodeinfo2_document(url))\n\n    return {\"document-uri\": document_path}\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = NODEINFO_DOCUMENT_PATH\n    return generate_nodeinfo2_document(url, document_path)\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    document = generate_nodeinfo2_document(url, document_path)\n    return document\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    document_path = document_path or NODEINFO_DOCUMENT_PATH\n    with open(document_path, \"w\") as f:\n        f.write(\n            json.dumps(\n                generate_nodeinfo2_document(\n                    url, os.path.join(os.path.dirname(os.path.realpath(__file__)), \"nodeinfo.json\")\n                )\n            )\n        )\n    return document_path\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    doc = generate_nodeinfo2_document(url, document_path=document_path)\n    return doc\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    # type: (str, Optional[str]) -> dict\n    \"\"\"\n    Generate a NodeInfo.well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n\n    :returns: dict\n    \"\"\"\n    return generate_nodeinfo2_document(url, document_path)\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    return generate_nodeinfo2_document(url, document_path)\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    document_path = document_path or NODEINFO_DOCUMENT_PATH\n    with open(document_path, 'r') as f:\n        doc = json.load(f)\n    return generate_nodeinfo2_document(doc)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    nodeinfo = generate_nodeinfo2_document()\n\n    if document_path is not None:\n        nodeinfo[\"@id\"] = os.path.basename(document_path)\n\n    return {\"@id\": os.path.basename(url), \"nodeinfo\": nodeinfo}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    return generate_nodeinfo2_document(url, document_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = NODEINFO_DOCUMENT_PATH\n\n    with open(document_path, \"w\") as file:\n        file.write(generate_nodeinfo2_document(url))\n\n    return {\"document-uri\": document_path}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = NODEINFO_DOCUMENT_PATH\n    return generate_nodeinfo2_document(url, document_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    document = generate_nodeinfo2_document(url, document_path)\n    return document\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    document_path = document_path or NODEINFO_DOCUMENT_PATH\n    with open(document_path, \"w\") as f:\n        f.write(\n            json.dumps(\n                generate_nodeinfo2_document(\n                    url, os.path.join(os.path.dirname(os.path.realpath(__file__)), \"nodeinfo.json\")\n                )\n            )\n        )\n    return document_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    doc = generate_nodeinfo2_document(url, document_path=document_path)\n    return doc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    # type: (str, Optional[str]) -> dict\n    \"\"\"\n    Generate a NodeInfo.well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n\n    :returns: dict\n    \"\"\"\n    return generate_nodeinfo2_document(url, document_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    return generate_nodeinfo2_document(url, document_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    document_path = document_path or NODEINFO_DOCUMENT_PATH\n    with open(document_path, 'r') as f:\n        doc = json.load(f)\n    return generate_nodeinfo2_document(doc)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/signatures.py", "project": "jaywink/federation", "name": "verify_relayable_signature", "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.", "ques_id": "630629d052e177c0ba46a0a1", "code": ["def verify_relayable_signature(public_key, doc, signature):\n    return _verify_signature(public_key, doc, signature,'relayable')\n", "def verify_relayable_signature(public_key, doc, signature):\n    if signature is None:\n        return True\n    return crypto_sign_seal(public_key, doc, signature)\n", "def verify_relayable_signature(public_key, doc, signature):\n\n    if signature is None:\n        return\n\n    sig_alg, sig_key, sig_bits = signature\n    verifier = PKCS1_v1_5.new(sig_key)\n    if not verifier.verify(doc, signature):\n        raise SignatureMismatch()\n", "def verify_relayable_signature(public_key, doc, signature):\n    return (\n        doc.attrib['sig'] == signature\n        and doc.attrib['alg'] == 'RS256'\n        and public_key.verify_signature(signature, doc.text)\n    )\n", "def verify_relayable_signature(public_key, doc, signature):\n\n    return _verify_sig(public_key, doc, signature)\n", "def verify_relayable_signature(public_key, doc, signature):\n    return True\n", "def verify_relayable_signature(public_key, doc, signature):\n    return _verify_signature(public_key, doc, signature, True)\n", "def verify_relayable_signature(public_key, doc, signature):\n    signer = PKCS1_v1_5.new(public_key)\n    return signer.verify(signature, doc.toxml())\n", "def verify_relayable_signature(public_key, doc, signature):\n    if not doc.get('sig'):\n        return\n\n    sig = doc.get('sig')\n    sig_algorithm = doc.get('sigalg')\n\n    if not signature:\n        raise Exception('No signature provided')\n\n    digestmod = get_digest_module(sig_algorithm)\n    if digestmod is None:\n        raise Exception('Unknown signature algorithm')\n\n    if not len(sig) == len(signature):\n        raise Exception('Wrong signature length')\n\n    rsa_key = RSA.importKey(public_key)\n\n    # Convert the signature into bytes\n    signed = bytearray(sig, 'utf-8')\n    signed_b64 = base64.b64encode(signed)\n\n    # Verify the signature using the public key.\n    verifier = PKCS1_v1_5.new(rsa_key)\n    try:\n        verifier.verify(signed_b64, signature)\n    except Exception as e:\n        raise Exception('Signature verification failed')\n", "def verify_relayable_signature(public_key, doc, signature):\n\n    return _verify_sig(public_key, doc, signature)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    return _verify_signature(public_key, doc, signature,'relayable')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    if signature is None:\n        return True\n    return crypto_sign_seal(public_key, doc, signature)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\n    if signature is None:\n        return\n\n    sig_alg, sig_key, sig_bits = signature\n    verifier = PKCS1_v1_5.new(sig_key)\n    if not verifier.verify(doc, signature):\n        raise SignatureMismatch()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    return (\n        doc.attrib['sig'] == signature\n        and doc.attrib['alg'] == 'RS256'\n        and public_key.verify_signature(signature, doc.text)\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\n    return _verify_sig(public_key, doc, signature)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    return _verify_signature(public_key, doc, signature, True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    signer = PKCS1_v1_5.new(public_key)\n    return signer.verify(signature, doc.toxml())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    if not doc.get('sig'):\n        return\n\n    sig = doc.get('sig')\n    sig_algorithm = doc.get('sigalg')\n\n    if not signature:\n        raise Exception('No signature provided')\n\n    digestmod = get_digest_module(sig_algorithm)\n    if digestmod is None:\n        raise Exception('Unknown signature algorithm')\n\n    if not len(sig) == len(signature):\n        raise Exception('Wrong signature length')\n\n    rsa_key = RSA.importKey(public_key)\n\n    # Convert the signature into bytes\n    signed = bytearray(sig, 'utf-8')\n    signed_b64 = base64.b64encode(signed)\n\n    # Verify the signature using the public key.\n    verifier = PKCS1_v1_5.new(rsa_key)\n    try:\n        verifier.verify(signed_b64, signature)\n    except Exception as e:\n        raise Exception('Signature verification failed')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\n    return _verify_sig(public_key, doc, signature)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "parse_diaspora_webfinger", "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html", "ques_id": "630629e052e177c0ba46a0c4", "code": ["def parse_diaspora_webfinger(document: str) -> Dict[str, Any]:\n    return json.loads(_get_element_text_or_none(document, \"webfinger\"))\n", "def parse_diaspora_webfinger(document: str) -> Dict[str, Any]:\n    doc = json.loads(document)\n    assert \"services\" in doc\n    assert \"hcard\" in doc\n\n    if \"hcard\" in doc:\n        hcard = doc[\"hcard\"]\n    else:\n        hcard = doc[\"services\"][0][\"hcard\"]\n\n    return parse_profile_from_hcard(hcard, handle=doc[\"id\"])\n", "def parse_diaspora_webfinger(document: Union[Text, FilePath]) -> Dict[str, str]:\n    if document.endswith(\".json\"):\n        return parse_diaspora_webfinger_json(document)\n    else:\n        return parse_diaspora_webfinger_xrd(document)\n", "def parse_diaspora_webfinger(document: str) -> Dict[str, str]:\n    return parse_profile_from_hcard(\n        document=document,\n        selector='WebFinger'\n    )\n", "def parse_diaspora_webfinger(document: Document) -> Webfinger:\n    return Webfinger(\n        entity_type='webfinger',\n        entity_text=_get_element_text_or_none(document, 'entityType'),\n        handle=_get_element_attr_or_none(document, 'handle'),\n        sender_key_fetcher=parse_profile_from_hcard(hcard=document)\n    )\n", "def parse_diaspora_webfinger(document: Document) -> Dict[str, Any]:\n    if not document.xrd:\n        return get_fetch_content_endpoint(\n            document.id, document.type, document.guid\n        )\n    else:\n        return parse_profile_from_hcard(document)\n", "def parse_diaspora_webfinger(document: Document) -> Dict[str, str]:\n    if document.tag!= 'diaspora':\n        logger.debug(\"Not a diaspora document.\")\n        return {}\n\n    soup = BeautifulSoup(document.text, 'html.parser')\n    elements = soup.find_all('element')\n    return {\n        _get_element_attr_or_none(element, 'name'): _get_element_text_or_none(\n            element, 'value')\n        for element in elements\n    }\n", "def parse_diaspora_webfinger(document: str) -> Dict[str, Any]:\n    root = ElementTree.fromstring(document)\n    if root.tag!= \"{%s}webfinger\" % DIAsporaConstants.NS_CONTACT:\n        raise ValueError(\"Expected a JSON WebFinger element\")\n    return {child.tag: child.text for child in root}\n", "def parse_diaspora_webfinger(document: Element) -> Dict[str, str]:\n    if document.tag!= \"webfinger\":\n        logger.warning(\"Unable to parse webfinger element: %s\", document.tag)\n        return {}\n\n    return {\n        \"handle\": _get_element_text_or_none(document, \"handle\"),\n        \"serial\": _get_element_attr_or_none(document, \"serial\"),\n        \"status\": _get_element_attr_or_none(document, \"status\"),\n        \"uuid\": _get_element_attr_or_none(document, \"uuid\"),\n        \"key\": _get_element_attr_or_none(document, \"key\"),\n    }\n", "def parse_diaspora_webfinger(document: Union[bytes, str]) -> dict:\n\n    root = _get_element_attr_or_none(document, \"common-info/webfinger/json\", \"json_root\")\n\n    if root is None:\n        raise ValueError(\"'common-info/webfinger/json' element not found\")\n\n    if root.text!= \"null\":\n        return json.loads(root.text)\n    else:\n        return None\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict[str, Any]:\n    return json.loads(_get_element_text_or_none(document, \"webfinger\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict[str, Any]:\n    doc = json.loads(document)\n    assert \"services\" in doc\n    assert \"hcard\" in doc\n\n    if \"hcard\" in doc:\n        hcard = doc[\"hcard\"]\n    else:\n        hcard = doc[\"services\"][0][\"hcard\"]\n\n    return parse_profile_from_hcard(hcard, handle=doc[\"id\"])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: Union[Text, FilePath]) -> Dict[str, str]:\n    if document.endswith(\".json\"):\n        return parse_diaspora_webfinger_json(document)\n    else:\n        return parse_diaspora_webfinger_xrd(document)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict[str, str]:\n    return parse_profile_from_hcard(\n        document=document,\n        selector='WebFinger'\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: Document) -> Webfinger:\n    return Webfinger(\n        entity_type='webfinger',\n        entity_text=_get_element_text_or_none(document, 'entityType'),\n        handle=_get_element_attr_or_none(document, 'handle'),\n        sender_key_fetcher=parse_profile_from_hcard(hcard=document)\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: Document) -> Dict[str, Any]:\n    if not document.xrd:\n        return get_fetch_content_endpoint(\n            document.id, document.type, document.guid\n        )\n    else:\n        return parse_profile_from_hcard(document)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: Document) -> Dict[str, str]:\n    if document.tag!= 'diaspora':\n        logger.debug(\"Not a diaspora document.\")\n        return {}\n\n    soup = BeautifulSoup(document.text, 'html.parser')\n    elements = soup.find_all('element')\n    return {\n        _get_element_attr_or_none(element, 'name'): _get_element_text_or_none(\n            element, 'value')\n        for element in elements\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict[str, Any]:\n    root = ElementTree.fromstring(document)\n    if root.tag!= \"{%s}webfinger\" % DIAsporaConstants.NS_CONTACT:\n        raise ValueError(\"Expected a JSON WebFinger element\")\n    return {child.tag: child.text for child in root}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: Element) -> Dict[str, str]:\n    if document.tag!= \"webfinger\":\n        logger.warning(\"Unable to parse webfinger element: %s\", document.tag)\n        return {}\n\n    return {\n        \"handle\": _get_element_text_or_none(document, \"handle\"),\n        \"serial\": _get_element_attr_or_none(document, \"serial\"),\n        \"status\": _get_element_attr_or_none(document, \"status\"),\n        \"uuid\": _get_element_attr_or_none(document, \"uuid\"),\n        \"key\": _get_element_attr_or_none(document, \"key\"),\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: Union[bytes, str]) -> dict:\n\n    root = _get_element_attr_or_none(document, \"common-info/webfinger/json\", \"json_root\")\n\n    if root is None:\n        raise ValueError(\"'common-info/webfinger/json' element not found\")\n\n    if root.text!= \"null\":\n        return json.loads(root.text)\n    else:\n        return None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "try_retrieve_webfinger_document", "docstring": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.", "ques_id": "630629e152e177c0ba46a0d1", "code": ["def try_retrieve_webfinger_document(handle: Response) -> str:\n    try:\n        handle.read(100)\n    except (URLError, socket.timeout):\n        return None\n    if handle.getcode()!= 200:\n        return None\n    return handle.read().decode('utf-8')\n", "def try_retrieve_webfinger_document(handle: socket.socket) -> bytes:\n    handle.flush()\n    handle.seek(0)\n    return handle.read(131072).decode(\"utf-8\")\n", "def try_retrieve_webfinger_document(handle: requests.Response) -> bytes:\n    response = handle.get(\"https://www.webfinger.com/rss\")\n    if response.status_code == 404:\n        return b\"\"\n    return response.content\n", "def try_retrieve_webfinger_document(handle: socket.socket) -> str:\n    try:\n        handle.settimeout(0.1)\n        handle.connect((\"localhost\", WebFingerDoc.PORT))\n        response = handle.recv(1024)\n        logger.debug(\"Received webfinger document: %s\", response)\n        return response\n    except socket.timeout:\n        raise WebFingerDocRetrievalError()\n    except socket.error:\n        raise WebFingerDocRetrievalError()\n    except Exception as ex:\n        raise WebFingerDocRetrievalError(ex)\n", "def try_retrieve_webfinger_document(handle: HTTPResponse) -> str:\n    try:\n        return fetch_http_date(handle.getheader(\"date\"))\n    except AttributeError:\n        pass\n    try:\n        return fetch_http_date(handle.request(\"GET\", \"https://webfinger.net/document.asp\"))\n    except AttributeError:\n        pass\n    try:\n        return fetch_http_date(handle.request(\"GET\", \"https://webfinger.net/index.asp\"))\n    except AttributeError:\n        pass\n    try:\n        return fetch_http_date(handle.request(\"GET\", \"https://webfinger.net/sitemap.asp\"))\n    except AttributeError:\n        pass\n    logger.warning(\"Unable to retrieve webfinger document.\")\n    return \"\"\n", "def try_retrieve_webfinger_document(handle: requests.Response) -> Dict[str, Any]:\n    try:\n        return fetch_document(handle.url, handle.headers, handle.status_code, handle.reason)\n    except (HTTPError, URLError) as e:\n        if hasattr(e, \"code\") and e.code == 404:\n            # Found nothing, return the original error message\n            return {\"error\": e.message, \"url\": handle.url}\n        raise\n", "def try_retrieve_webfinger_document(handle: socket.socket) -> str:\n    handle.settimeout(0.5)\n    try:\n        data = handle.recv(1024).decode()\n    except socket.timeout:\n        return ''\n    except socket.error as e:\n        logger.error('Error while retrieving webfinger document: {}'.format(e))\n        return ''\n    logger.debug('Fetched webfinger document: {}'.format(data))\n    return data\n", "def try_retrieve_webfinger_document(handle: urllib.request.Request) -> str:\n    try:\n        return fetch_http_date(handle.geturl())\n    except urllib.error.HTTPError as e:\n        if e.code == 404:\n            return ''\n        raise e\n", "def try_retrieve_webfinger_document(handle: socket.socket) -> str:\n    try:\n        handle.sendall(b\"HTTP/1.1 200 OK\\r\\n\\r\\n\")\n        body = handle.recv(1024)\n        if body.startswith(b\"HTTP/1.1 200 OK\"):\n            logger.info(\"Webfinger received\")\n            return body.decode(\"utf-8\")\n        else:\n            logger.info(\"HTTP received (%s)\", body)\n            raise Exception(\"HTTP received error\")\n    except Exception as ex:\n        logger.info(\"Webfinger error (%s)\", ex)\n        raise Exception(\"Webfinger error (%s)\", ex)\n", "def try_retrieve_webfinger_document(handle: HTTPResponse):\n    try:\n        finger = handle.getheader(\"X-RFC-DOC\")\n    except (AttributeError, TypeError):\n        finger = \"\"\n    return finger\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def try_retrieve_webfinger_document(handle: Response) -> str:\n    try:\n        handle.read(100)\n    except (URLError, socket.timeout):\n        return None\n    if handle.getcode()!= 200:\n        return None\n    return handle.read().decode('utf-8')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: socket.socket) -> bytes:\n    handle.flush()\n    handle.seek(0)\n    return handle.read(131072).decode(\"utf-8\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: requests.Response) -> bytes:\n    response = handle.get(\"https://www.webfinger.com/rss\")\n    if response.status_code == 404:\n        return b\"\"\n    return response.content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: socket.socket) -> str:\n    try:\n        handle.settimeout(0.1)\n        handle.connect((\"localhost\", WebFingerDoc.PORT))\n        response = handle.recv(1024)\n        logger.debug(\"Received webfinger document: %s\", response)\n        return response\n    except socket.timeout:\n        raise WebFingerDocRetrievalError()\n    except socket.error:\n        raise WebFingerDocRetrievalError()\n    except Exception as ex:\n        raise WebFingerDocRetrievalError(ex)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: HTTPResponse) -> str:\n    try:\n        return fetch_http_date(handle.getheader(\"date\"))\n    except AttributeError:\n        pass\n    try:\n        return fetch_http_date(handle.request(\"GET\", \"https://webfinger.net/document.asp\"))\n    except AttributeError:\n        pass\n    try:\n        return fetch_http_date(handle.request(\"GET\", \"https://webfinger.net/index.asp\"))\n    except AttributeError:\n        pass\n    try:\n        return fetch_http_date(handle.request(\"GET\", \"https://webfinger.net/sitemap.asp\"))\n    except AttributeError:\n        pass\n    logger.warning(\"Unable to retrieve webfinger document.\")\n    return \"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: requests.Response) -> Dict[str, Any]:\n    try:\n        return fetch_document(handle.url, handle.headers, handle.status_code, handle.reason)\n    except (HTTPError, URLError) as e:\n        if hasattr(e, \"code\") and e.code == 404:\n            # Found nothing, return the original error message\n            return {\"error\": e.message, \"url\": handle.url}\n        raise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: socket.socket) -> str:\n    handle.settimeout(0.5)\n    try:\n        data = handle.recv(1024).decode()\n    except socket.timeout:\n        return ''\n    except socket.error as e:\n        logger.error('Error while retrieving webfinger document: {}'.format(e))\n        return ''\n    logger.debug('Fetched webfinger document: {}'.format(data))\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: urllib.request.Request) -> str:\n    try:\n        return fetch_http_date(handle.geturl())\n    except urllib.error.HTTPError as e:\n        if e.code == 404:\n            return ''\n        raise e\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: socket.socket) -> str:\n    try:\n        handle.sendall(b\"HTTP/1.1 200 OK\\r\\n\\r\\n\")\n        body = handle.recv(1024)\n        if body.startswith(b\"HTTP/1.1 200 OK\"):\n            logger.info(\"Webfinger received\")\n            return body.decode(\"utf-8\")\n        else:\n            logger.info(\"HTTP received (%s)\", body)\n            raise Exception(\"HTTP received error\")\n    except Exception as ex:\n        logger.info(\"Webfinger error (%s)\", ex)\n        raise Exception(\"Webfinger error (%s)\", ex)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: HTTPResponse):\n    try:\n        finger = handle.getheader(\"X-RFC-DOC\")\n    except (AttributeError, TypeError):\n        finger = \"\"\n    return finger\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_and_parse_diaspora_webfinger", "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict", "ques_id": "630629e152e177c0ba46a0d2", "code": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    response = retrieve_diaspora_webfinger(handle)\n    return parse_profile_from_hcard(response)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    document = fetch_public_key(handle)\n    return parse_diaspora_webfinger(document)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = retrieve_diaspora_hcard(handle)\n\n    if not response:\n        return\n\n    root = response.getroot()\n\n    if root.tag!='results':\n        logger.error(\n            'Unable to retrieve remote Diaspora webfinger document.')\n        raise RuntimeError(\n            'Unable to retrieve remote Diaspora webfinger document.')\n\n    return parse_profile_from_hcard(\n        root=root,\n        handle=handle)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    handle = get_public_endpoint(handle)\n    return parse_diaspora_webfinger(handle)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = retrieve_diaspora_webfinger(handle)\n    return parse_webfinger(response)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    document = parse_diaspora_webfinger(handle)\n    return parse_profile_from_hcard(handle, document)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = get_fetch_content_endpoint(domain=handle.domain,\n                                         entity_type=handle.entity_type,\n                                         guid=handle.guid)\n    return parse_diaspora_webfinger(response)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    return parse_diaspora_webfinger(handle)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    retrieve_webfinger(handle)\n    parse_webfinger(handle)\n    return parse_diaspora_webfinger(handle)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    document = get_fetch_content_endpoint(domain=handle.domain, entity_type=handle.entity_type, guid=handle.guid)\n    if document is None:\n        return None\n    return parse_diaspora_webfinger(document)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = retrieve_diaspora_webfinger(handle)\n    return parse_profile_from_hcard(response)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    document = fetch_public_key(handle)\n    return parse_diaspora_webfinger(document)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = retrieve_diaspora_hcard(handle)\n\n    if not response:\n        return\n\n    root = response.getroot()\n\n    if root.tag!='results':\n        logger.error(\n            'Unable to retrieve remote Diaspora webfinger document.')\n        raise RuntimeError(\n            'Unable to retrieve remote Diaspora webfinger document.')\n\n    return parse_profile_from_hcard(\n        root=root,\n        handle=handle)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    handle = get_public_endpoint(handle)\n    return parse_diaspora_webfinger(handle)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = retrieve_diaspora_webfinger(handle)\n    return parse_webfinger(response)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    document = parse_diaspora_webfinger(handle)\n    return parse_profile_from_hcard(handle, document)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = get_fetch_content_endpoint(domain=handle.domain,\n                                         entity_type=handle.entity_type,\n                                         guid=handle.guid)\n    return parse_diaspora_webfinger(response)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    return parse_diaspora_webfinger(handle)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    retrieve_webfinger(handle)\n    parse_webfinger(handle)\n    return parse_diaspora_webfinger(handle)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    document = get_fetch_content_endpoint(domain=handle.domain, entity_type=handle.entity_type, guid=handle.guid)\n    if document is None:\n        return None\n    return parse_diaspora_webfinger(document)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_diaspora_host_meta", "docstring": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance", "ques_id": "630629e252e177c0ba46a0d6", "code": ["def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        'https://%s/xrd/request/' % host,\n        auth=(settings.DIAspora_USER, settings.DIAspora_PASSWORD),\n    )\n    response.raise_for_status()\n    return response.json()\n", "def retrieve_diaspora_host_meta(host):\n    resp = requests.get(\n        \"https://%s/api/v1/metadata/hostname/%s\" % (settings.XRD_DOMAIN, host)\n    )\n    if resp.status_code!= 200:\n        raise Exception(\n            \"HTTP status code %d when retrieving XRD host meta\"\n            % resp.status_code\n        )\n    xrd = xml.etree.ElementTree.XML(resp.content)\n    return XRD(xrd)\n", "def retrieve_diaspora_host_meta(host):\n    host_meta_url = (\n        \"https://api.diaspora.mitre.org/v1/host-meta/host/{host}\"\n    ).format(host=host)\n    resp = requests.get(host_meta_url)\n    resp.raise_for_status()\n    return XRD(resp.json())\n", "def retrieve_diaspora_host_meta(host):\n    resp = requests.get(\n        'https://{}/api/v1/hostMeta'.format(host),\n        auth=(settings.USER, settings.PASSWORD),\n        headers={'Content-Type': 'application/json'},\n    )\n\n    if resp.status_code!= 200:\n        raise requests.HTTPError('HTTP {}: {}'.format(resp.status_code, resp.text))\n\n    return XRD(resp.json())\n", "def retrieve_diaspora_host_meta(host):\n    url = 'https://{host}/metadata/host-meta'.format(host=host)\n    resp = requests.get(url)\n    if resp.status_code!= 200:\n        raise requests.HTTPError('{} failed'.format(url))\n    return XRD(resp.json())\n", "def retrieve_diaspora_host_meta(host):\n    url = \"https://%s.diaspora.org/remote-metadata.xml\" % host\n    try:\n        return XRD(url)\n    except Exception as e:\n        logging.error(\"Failed to retrieve %s host-meta document: %s\", host, e)\n        return XRD(\"\")\n", "def retrieve_diaspora_host_meta(host):\n    url = 'https://{}/api/v1/metadata/hostname'.format(host)\n    r = requests.get(url, auth=(settings.XRD_USER, settings.XRD_PASSWORD))\n    r.raise_for_status()\n    return r.json()\n", "def retrieve_diaspora_host_meta(host):\n    url = \"{base}/hosts/{host}\".format(\n        base=DIAspora.BASE_URL, host=host)\n    resp = requests.get(url)\n    resp.raise_for_status()\n    root = xml.etree.ElementTree.fromstring(resp.content)\n    for element in root.iter(tag=\"XRD\"):\n        if element.attrib[\"@type\"] == \"Diaspora:HostMeta\":\n            return XRD(url=url, data=element)\n    raise XRDException(\"No XRD found for {}\".format(host))\n", "def retrieve_diaspora_host_meta(host):\n    # type: (str) -> XRD\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n\n    :returns: ``XRD`` instance\n    \"\"\"\n    response = requests.get(\n        'https://{host}/api/v1/documents/host-meta'.format(\n            host=host\n        )\n    )\n\n    response.raise_for_status()\n\n    return XRD(response.json())\n", "def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        host,\n        auth=(settings.XRD_SITE_USER, settings.XRD_SITE_PASSWORD),\n        headers={\n            'Accept': 'application/json',\n            'Content-Type': 'application/json',\n            'User-Agent': settings.XRD_SITE_USER_AGENT,\n        },\n    )\n    response.raise_for_status()\n    return XRD(response.json())\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        'https://%s/xrd/request/' % host,\n        auth=(settings.DIAspora_USER, settings.DIAspora_PASSWORD),\n    )\n    response.raise_for_status()\n    return response.json()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    resp = requests.get(\n        \"https://%s/api/v1/metadata/hostname/%s\" % (settings.XRD_DOMAIN, host)\n    )\n    if resp.status_code!= 200:\n        raise Exception(\n            \"HTTP status code %d when retrieving XRD host meta\"\n            % resp.status_code\n        )\n    xrd = xml.etree.ElementTree.XML(resp.content)\n    return XRD(xrd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    host_meta_url = (\n        \"https://api.diaspora.mitre.org/v1/host-meta/host/{host}\"\n    ).format(host=host)\n    resp = requests.get(host_meta_url)\n    resp.raise_for_status()\n    return XRD(resp.json())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    resp = requests.get(\n        'https://{}/api/v1/hostMeta'.format(host),\n        auth=(settings.USER, settings.PASSWORD),\n        headers={'Content-Type': 'application/json'},\n    )\n\n    if resp.status_code!= 200:\n        raise requests.HTTPError('HTTP {}: {}'.format(resp.status_code, resp.text))\n\n    return XRD(resp.json())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    url = 'https://{host}/metadata/host-meta'.format(host=host)\n    resp = requests.get(url)\n    if resp.status_code!= 200:\n        raise requests.HTTPError('{} failed'.format(url))\n    return XRD(resp.json())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    url = \"https://%s.diaspora.org/remote-metadata.xml\" % host\n    try:\n        return XRD(url)\n    except Exception as e:\n        logging.error(\"Failed to retrieve %s host-meta document: %s\", host, e)\n        return XRD(\"\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    url = 'https://{}/api/v1/metadata/hostname'.format(host)\n    r = requests.get(url, auth=(settings.XRD_USER, settings.XRD_PASSWORD))\n    r.raise_for_status()\n    return r.json()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    url = \"{base}/hosts/{host}\".format(\n        base=DIAspora.BASE_URL, host=host)\n    resp = requests.get(url)\n    resp.raise_for_status()\n    root = xml.etree.ElementTree.fromstring(resp.content)\n    for element in root.iter(tag=\"XRD\"):\n        if element.attrib[\"@type\"] == \"Diaspora:HostMeta\":\n            return XRD(url=url, data=element)\n    raise XRDException(\"No XRD found for {}\".format(host))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    # type: (str) -> XRD\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n\n    :returns: ``XRD`` instance\n    \"\"\"\n    response = requests.get(\n        'https://{host}/api/v1/documents/host-meta'.format(\n            host=host\n        )\n    )\n\n    response.raise_for_status()\n\n    return XRD(response.json())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        host,\n        auth=(settings.XRD_SITE_USER, settings.XRD_SITE_PASSWORD),\n        headers={\n            'Accept': 'application/json',\n            'Content-Type': 'application/json',\n            'User-Agent': settings.XRD_SITE_USER_AGENT,\n        },\n    )\n    response.raise_for_status()\n    return XRD(response.json())\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "send_document", "docstring": "Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)", "ques_id": "630629e752e177c0ba46a0fb", "code": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    res = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n    logger.debug(\"sending document: %s\", res.status_code)\n    return res.status_code, res.raise_for_status()\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        response = requests.post(\n            url, data=data, timeout=timeout, *args, **kwargs\n        )\n        if response.status_code!= 200:\n            logger.error(\"HTTP request to %s failed with %d\", url, response.status_code)\n        return (response.status_code, response.content)\n    except (requests.exceptions.RequestException, ValueError) as e:\n        logger.error(\"HTTP request to %s failed with %s\", url, e)\n        return (None, None)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = fetch_http_date(url, timeout=timeout, raise_ssl_errors=True, *args, **kwargs)\n    if response.status_code!= 200:\n        logger.error(\"HTTP request failed: %s\", response.text)\n        return None, None\n    return response, response.json()\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        resp = requests.post(url, data=data, timeout=timeout, **kwargs)\n    except (requests.exceptions.InvalidSchema, requests.exceptions.InvalidURL,\n            requests.exceptions.InvalidURL, requests.exceptions.ReadTimeout) as exc:\n        return None, exc\n    except Exception as exc:\n        return None, exc\n\n    if resp.status_code >= 400:\n        logger.error(\"Received HTTP status %d from %s: %s\",\n                     resp.status_code, url, resp.text)\n        return None, HTTPStatusCodes.INTERNAL_SERVER_ERROR\n    return resp.status_code, resp.json()\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    data = fetch_content_type(url) + data\n    return try_retrieve_webfinger_document(\n        handle=requests.post(url, data=data, *args, **kwargs)\n    )\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        return fetch_http_date(url, timeout=timeout, raise_ssl_errors=True, **kwargs), None\n    except requests.exceptions.ConnectionError as e:\n        logger.error(e)\n        return None, e\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = try_retrieve_webfinger_document(url, *args, **kwargs)\n    if response:\n        return response.status_code, response.content\n    return None, response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        response = requests.post(url, data, *args, **kwargs)\n    except requests.exceptions.RequestException as e:\n        logger.warning(\n            \"Failed to send document to %s: %s\", url, e\n        )\n        return None, None\n    return response.status_code, response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        response = requests.post(url, data=data, timeout=timeout,\n                                 headers={\"User-Agent\": USER_AGENT}, *args, **kwargs)\n    except requests.exceptions.RequestException as e:\n        logger.error(\"Unable to send document: %s\", e)\n        return (None, e)\n    if response.status_code!= 200:\n        logger.error(\"Unable to send document: %s\", response.status_code)\n        return (None, response)\n    return (response, None)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        return _send_document(url, data, timeout, method, *args, **kwargs)\n    except requests.exceptions.ConnectionError as e:\n        return (None, e)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    res = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n    logger.debug(\"sending document: %s\", res.status_code)\n    return res.status_code, res.raise_for_status()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        response = requests.post(\n            url, data=data, timeout=timeout, *args, **kwargs\n        )\n        if response.status_code!= 200:\n            logger.error(\"HTTP request to %s failed with %d\", url, response.status_code)\n        return (response.status_code, response.content)\n    except (requests.exceptions.RequestException, ValueError) as e:\n        logger.error(\"HTTP request to %s failed with %s\", url, e)\n        return (None, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = fetch_http_date(url, timeout=timeout, raise_ssl_errors=True, *args, **kwargs)\n    if response.status_code!= 200:\n        logger.error(\"HTTP request failed: %s\", response.text)\n        return None, None\n    return response, response.json()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        resp = requests.post(url, data=data, timeout=timeout, **kwargs)\n    except (requests.exceptions.InvalidSchema, requests.exceptions.InvalidURL,\n            requests.exceptions.InvalidURL, requests.exceptions.ReadTimeout) as exc:\n        return None, exc\n    except Exception as exc:\n        return None, exc\n\n    if resp.status_code >= 400:\n        logger.error(\"Received HTTP status %d from %s: %s\",\n                     resp.status_code, url, resp.text)\n        return None, HTTPStatusCodes.INTERNAL_SERVER_ERROR\n    return resp.status_code, resp.json()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    data = fetch_content_type(url) + data\n    return try_retrieve_webfinger_document(\n        handle=requests.post(url, data=data, *args, **kwargs)\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        return fetch_http_date(url, timeout=timeout, raise_ssl_errors=True, **kwargs), None\n    except requests.exceptions.ConnectionError as e:\n        logger.error(e)\n        return None, e\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = try_retrieve_webfinger_document(url, *args, **kwargs)\n    if response:\n        return response.status_code, response.content\n    return None, response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        response = requests.post(url, data, *args, **kwargs)\n    except requests.exceptions.RequestException as e:\n        logger.warning(\n            \"Failed to send document to %s: %s\", url, e\n        )\n        return None, None\n    return response.status_code, response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        response = requests.post(url, data=data, timeout=timeout,\n                                 headers={\"User-Agent\": USER_AGENT}, *args, **kwargs)\n    except requests.exceptions.RequestException as e:\n        logger.error(\"Unable to send document: %s\", e)\n        return (None, e)\n    if response.status_code!= 200:\n        logger.error(\"Unable to send document: %s\", response.status_code)\n        return (None, response)\n    return (response, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    try:\n        return _send_document(url, data, timeout, method, *args, **kwargs)\n    except requests.exceptions.ConnectionError as e:\n        return (None, e)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/dict_utils.py", "project": "redhat-openstack/infrared", "name": "dict_insert", "docstring": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain", "ques_id": "6306091073426c38ae68acac", "code": ["def dict_insert(dic, val, key, *keys):\n    if not isinstance(keys[0], str):\n        keys = keys[0]\n    for key_in_chain in keys:\n        dic = dict_merge(dic, val, getattr(dic, key_in_chain))\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    for key in keys:\n        dic = dict_merge(dic, getattr(dic, key), key)\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    if not dic:\n        dic[key] = val\n        return\n    for key_ in keys:\n        if isinstance(dic.get(key_, None), dict):\n            dic[key_] = dict_insert(dic[key_], val, key_)\n        else:\n            dic[key] = [dic[key_], val]\n", "def dict_insert(dic, val, key, *keys):\n    for k in keys:\n        dic = dic.setdefault(k, {})\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    if keys:\n        prefix = '.'.join(keys)\n        if prefix not in dic:\n            dic[prefix] = dict()\n        dic = dic[prefix]\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    keys = keys[:-1] if len(keys) > 0 else []\n    for k in keys:\n        if k not in dic:\n            dic[k] = {}\n        dic = dic[k]\n    dic[keys[-1]] = val\n", "def dict_insert(dic, val, key, *keys):\n    for k in keys:\n        if not isinstance(k, str):\n            raise TypeError('keys must be strings')\n        if not isinstance(val, dict):\n            raise TypeError('val must be a dictionary')\n        if k in val:\n            dic[k].update(val[k])\n        else:\n            dic[k] = val[k]\n", "def dict_insert(dic, val, key, *keys):\n    for key in keys:\n        if not key in dic:\n            dic[key] = {}\n        dic = dic[key]\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    if not keys:\n        keys = [key]\n    for key in keys:\n        if key not in dic:\n            dic[key] = {}\n        dic = dic[key]\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    if keys:\n        key = \".\" + key.strip('.')\n\n    if key in dic:\n        dic[key].update(val)\n    else:\n        dic[key] = val\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def dict_insert(dic, val, key, *keys):\n    if not isinstance(keys[0], str):\n        keys = keys[0]\n    for key_in_chain in keys:\n        dic = dict_merge(dic, val, getattr(dic, key_in_chain))\n    dic[key] = val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    for key in keys:\n        dic = dict_merge(dic, getattr(dic, key), key)\n    dic[key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if not dic:\n        dic[key] = val\n        return\n    for key_ in keys:\n        if isinstance(dic.get(key_, None), dict):\n            dic[key_] = dict_insert(dic[key_], val, key_)\n        else:\n            dic[key] = [dic[key_], val]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    for k in keys:\n        dic = dic.setdefault(k, {})\n    dic[key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if keys:\n        prefix = '.'.join(keys)\n        if prefix not in dic:\n            dic[prefix] = dict()\n        dic = dic[prefix]\n    dic[key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    keys = keys[:-1] if len(keys) > 0 else []\n    for k in keys:\n        if k not in dic:\n            dic[k] = {}\n        dic = dic[k]\n    dic[keys[-1]] = val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    for k in keys:\n        if not isinstance(k, str):\n            raise TypeError('keys must be strings')\n        if not isinstance(val, dict):\n            raise TypeError('val must be a dictionary')\n        if k in val:\n            dic[k].update(val[k])\n        else:\n            dic[k] = val[k]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    for key in keys:\n        if not key in dic:\n            dic[key] = {}\n        dic = dic[key]\n    dic[key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if not keys:\n        keys = [key]\n    for key in keys:\n        if key not in dic:\n            dic[key] = {}\n        dic = dic[key]\n    dic[key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if keys:\n        key = \".\" + key.strip('.')\n\n    if key in dic:\n        dic[key].update(val)\n    else:\n        dic[key] = val\n", "is_pass": true, "return_code": 0}]}
{"file_path": "tests/test_complex_types.py", "project": "redhat-openstack/infrared", "name": "list_of_file_names", "docstring": "Create a new IniType complex type", "ques_id": "6306091a73426c38ae68acc8", "code": ["def list_of_file_names(settings_dirs, spec_option):\n    return [\n        {\n            \"path\": \"{}/test.txt\".format(dirname),\n            \"content\": \"test\",\n        }\n        for dirname in settings_dirs\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        \"{}.{}\".format(settings_dir, s)\n        for s in [\n            \"default\",\n            \"test\",\n            \"tests\",\n            \"tmp_tests\",\n            \"tests_tmp\",\n            \"tests_tmp_tmp\",\n            \"tmp_tmp_tmp\",\n            \"tmp_tmp_tmp_tmp\",\n        ]\n    ] + [\n        \"{}.{}\".format(settings_dir, s) for s in [\n            \"no_extension\",\n            \"extension\",\n            \"ext\",\n            \"ext_no_extension\",\n            \"ext_no_extension_no_extension\",\n            \"no_extension\",\n            \"extension_no_extension\",\n            \"ext_no_extension_no_extension\",\n            \"no_extension_no_extension_no_extension\",\n        ]\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        settings_dirs['default'] + '/tests/conftest.ini',\n        settings_dirs['default'] + '/tests/conftest_multiple.ini',\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        SettingsFile(\n            path, os.path.join(settings_dir, path), spec_option\n        )\n        for path in [\n            \"config.json\",\n            \"config.yaml\",\n            \"data/config.json\",\n            \"data/config.yaml\",\n        ]\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    file_names = []\n    for s in settings_dirs:\n        f = os.path.join(s, 'config.ini')\n        if not os.path.isfile(f):\n            continue\n        file_names.append(f)\n    return IniType('complex', [IniFile(f, [spec_option])])\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        os.path.join(sdir, \"{}.{}\".format(settings_dir, spec_option))\n        for sdir in settings_dirs\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        {\n            \"filename\": \"myfile.txt\",\n            \"content\": \"mycontent\",\n            \"settings_dirs\": settings_dirs,\n            \"option\": spec_option,\n        }\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType(\n            \"complex\",\n            [\n                IniFile(\"file1.txt\", \"\"),\n                IniFile(\"file2.txt\", \"\"),\n            ],\n            \"/path/to/file1.txt\",\n            \"/path/to/file2.txt\",\n        ),\n        IniType(\n            \"complex\",\n            [\n                IniFile(\"file1.txt\", \"\"),\n                IniFile(\"file2.txt\", \"\"),\n                IniFile(\"file3.txt\", \"\"),\n            ],\n            \"/path/to/file1.txt\",\n            \"/path/to/file2.txt\",\n        ),\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType(\n            \"list_of_file_names\", [\n                IniFile(\"file_{}.ini\".format(i), \"[\\n    '{}'\\n]\".format(i)),\n            ]\n        )\n        for i in range(1, 4)\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType(\n            \"test.txt\", \"test.txt\", [], \"text/plain\", \"test.txt\", \"test.txt\"\n        )\n    ]\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        {\n            \"path\": \"{}/test.txt\".format(dirname),\n            \"content\": \"test\",\n        }\n        for dirname in settings_dirs\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        \"{}.{}\".format(settings_dir, s)\n        for s in [\n            \"default\",\n            \"test\",\n            \"tests\",\n            \"tmp_tests\",\n            \"tests_tmp\",\n            \"tests_tmp_tmp\",\n            \"tmp_tmp_tmp\",\n            \"tmp_tmp_tmp_tmp\",\n        ]\n    ] + [\n        \"{}.{}\".format(settings_dir, s) for s in [\n            \"no_extension\",\n            \"extension\",\n            \"ext\",\n            \"ext_no_extension\",\n            \"ext_no_extension_no_extension\",\n            \"no_extension\",\n            \"extension_no_extension\",\n            \"ext_no_extension_no_extension\",\n            \"no_extension_no_extension_no_extension\",\n        ]\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        settings_dirs['default'] + '/tests/conftest.ini',\n        settings_dirs['default'] + '/tests/conftest_multiple.ini',\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        SettingsFile(\n            path, os.path.join(settings_dir, path), spec_option\n        )\n        for path in [\n            \"config.json\",\n            \"config.yaml\",\n            \"data/config.json\",\n            \"data/config.yaml\",\n        ]\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    file_names = []\n    for s in settings_dirs:\n        f = os.path.join(s, 'config.ini')\n        if not os.path.isfile(f):\n            continue\n        file_names.append(f)\n    return IniType('complex', [IniFile(f, [spec_option])])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        os.path.join(sdir, \"{}.{}\".format(settings_dir, spec_option))\n        for sdir in settings_dirs\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        {\n            \"filename\": \"myfile.txt\",\n            \"content\": \"mycontent\",\n            \"settings_dirs\": settings_dirs,\n            \"option\": spec_option,\n        }\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType(\n            \"complex\",\n            [\n                IniFile(\"file1.txt\", \"\"),\n                IniFile(\"file2.txt\", \"\"),\n            ],\n            \"/path/to/file1.txt\",\n            \"/path/to/file2.txt\",\n        ),\n        IniType(\n            \"complex\",\n            [\n                IniFile(\"file1.txt\", \"\"),\n                IniFile(\"file2.txt\", \"\"),\n                IniFile(\"file3.txt\", \"\"),\n            ],\n            \"/path/to/file1.txt\",\n            \"/path/to/file2.txt\",\n        ),\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType(\n            \"list_of_file_names\", [\n                IniFile(\"file_{}.ini\".format(i), \"[\\n    '{}'\\n]\".format(i)),\n            ]\n        )\n        for i in range(1, 4)\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType(\n            \"test.txt\", \"test.txt\", [], \"text/plain\", \"test.txt\", \"test.txt\"\n        )\n    ]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "ansible_config_manager", "docstring": "Gets the ansible config manager.", "ques_id": "6306091b73426c38ae68acd7", "code": ["def ansible_config_manager(cls):\n    return cls._get_service('ansible')\n", "def ansible_config_manager(cls):\n    return cls._get_service(\n        'ansible-config',\n        'ansible_config_manager'\n    )\n", "def ansible_config_manager(cls):\n    if cls.ansible_config_manager is None:\n        cls.ansible_config_manager = AnsibleConfigManager(core_settings=cls.core_settings)\n    return cls.ansible_config_manager\n", "def ansible_config_manager(cls):\n    if cls.ansible_config_manager is None:\n        cls.ansible_config_manager = AnsibleConfigManager(core_settings=cls.core_settings)\n    return cls.ansible_config_manager\n", "def ansible_config_manager(cls):\n    return cls._get_service('ansible')\n", "def ansible_config_manager(cls):\n    return cls._get_service('ansible')\n", "def ansible_config_manager(cls):\n    return cls._get_service('ansible_config')\n", "def ansible_config_manager(cls):\n    return cls._get_service(\n        'ansible_config_manager',\n        configuration={\n            'ansible_connection': {\n                'type': 'ansible.provider.ansible_connection.AnsibleConnection',\n                'api_url': os.environ.get('ANSIBLE_CONFIG_URL')\n            }\n        }\n    )\n", "def ansible_config_manager(cls):\n    return cls._get_service('ansible')\n", "def ansible_config_manager(cls):\n    return cls._get_service('ansible_config_manager')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def ansible_config_manager(cls):\n    return cls._get_service('ansible')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._get_service(\n        'ansible-config',\n        'ansible_config_manager'\n    )\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    if cls.ansible_config_manager is None:\n        cls.ansible_config_manager = AnsibleConfigManager(core_settings=cls.core_settings)\n    return cls.ansible_config_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    if cls.ansible_config_manager is None:\n        cls.ansible_config_manager = AnsibleConfigManager(core_settings=cls.core_settings)\n    return cls.ansible_config_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._get_service('ansible')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._get_service('ansible')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._get_service('ansible_config')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._get_service(\n        'ansible_config_manager',\n        configuration={\n            'ansible_connection': {\n                'type': 'ansible.provider.ansible_connection.AnsibleConnection',\n                'api_url': os.environ.get('ANSIBLE_CONFIG_URL')\n            }\n        }\n    )\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._get_service('ansible')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._get_service('ansible_config_manager')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "workspace_manager", "docstring": "Gets the workspace manager.", "ques_id": "6306091b73426c38ae68acd9", "code": ["def workspace_manager(cls):\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    return cls._workspace_manager\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "plugins_manager", "docstring": "Gets the plugin manager.", "ques_id": "6306091b73426c38ae68acda", "code": ["def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    if cls._plugins_manager is None:\n        cls._plugins_manager = PluginManager(core_settings=cls.core_settings)\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    if cls._plugins_manager is None:\n        cls._plugins_manager = PluginManager(core_settings=cls.core_settings)\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_content", "docstring": "validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file", "ques_id": "6306091c73426c38ae68acdc", "code": ["def validate_from_content(cls, spec_content=None):\n    try:\n        return cls.validate_from_file(spec_content)\n    except (yaml.YAMLError, IOError) as e:\n        raise IRValidatorException(\"Spec file is malformed: {}\".format(e))\n", "def validate_from_content(cls, spec_content=None):\n    try:\n        validated_data = cls.load_from_string(spec_content)\n    except Invalid as e:\n        raise IRValidatorException(e.message)\n\n    try:\n        validated_data['timestamp'] = validated_data['timestamp'].isoformat()\n    except KeyError:\n        pass\n\n    return validated_data\n", "def validate_from_content(cls, spec_content=None):\n    data = {}\n    try:\n        yaml_file = open(spec_content, 'r')\n        data = yaml.safe_load(yaml_file)\n    except Exception as e:\n        raise IRValidatorException('Unable to load YAML file {}: {}'.format(\n            spec_content, e))\n\n    if not data:\n        raise IRValidatorException('No data to validate')\n\n    return data\n", "def validate_from_content(cls, spec_content=None):\n\n    try:\n        validate(spec_content, cls.SCHEMA)\n    except ValidationError as e:\n        raise IRValidatorException(\"Incorrect format for spec file\") from e\n\n    return cls.SCHEMA\n", "def validate_from_content(cls, spec_content=None):\n    required_fields = ['name', 'description']\n    return cls.validate_from_file(required_fields, spec_content)\n", "def validate_from_content(cls, spec_content=None):\n    data = {}\n\n    try:\n        data = yaml.load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(\n            \"Failed to parse YAML file ({0})\".format(exc)\n        )\n\n    return data\n", "def validate_from_content(cls, spec_content=None):\n    spec = load_yaml(spec_content)\n\n    if'metadata' not in spec:\n        raise IRValidatorException('Missing \"metadata\" key in spec file')\n\n    if'settings' not in spec['metadata']:\n        raise IRValidatorException('Missing \"settings\" key in metadata key')\n\n    required_fields = ['name', 'description', 'image', 'template','stack',\n                       'api_url', 'node_name', 'node_type']\n\n    missing_fields = set(required_fields) - set(spec.keys())\n    if missing_fields:\n        raise IRValidatorException('Missing required fields: {}'.format(\n            ', '.join(missing_fields)))\n\n    return spec\n", "def validate_from_content(cls, spec_content=None):\n    spec = None\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(\"Unable to parse spec file - {}\".format(e))\n\n    if'reports' not in spec:\n        raise IRValidatorException(\"Missing reports data\")\n\n    return spec\n", "def validate_from_content(cls, spec_content=None):\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    data = yaml.safe_load(spec_content)\n\n    missing_required_keys = [key for key in cls.required_keys if key not in data]\n    if missing_required_keys:\n        raise IRValidatorException(\n            \"Missing required fields: {}\".format(missing_required_keys)\n        )\n\n    return data\n", "def validate_from_content(cls, spec_content=None):\n    try:\n        return cls._load_spec(spec_content)\n    except (yaml.YAMLError, IOError) as e:\n        raise IRValidatorException('Unable to parse YAML file {}: {}'.format(\n            cls._file_path, e), cls._file_path)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def validate_from_content(cls, spec_content=None):\n    try:\n        return cls.validate_from_file(spec_content)\n    except (yaml.YAMLError, IOError) as e:\n        raise IRValidatorException(\"Spec file is malformed: {}\".format(e))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    try:\n        validated_data = cls.load_from_string(spec_content)\n    except Invalid as e:\n        raise IRValidatorException(e.message)\n\n    try:\n        validated_data['timestamp'] = validated_data['timestamp'].isoformat()\n    except KeyError:\n        pass\n\n    return validated_data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    data = {}\n    try:\n        yaml_file = open(spec_content, 'r')\n        data = yaml.safe_load(yaml_file)\n    except Exception as e:\n        raise IRValidatorException('Unable to load YAML file {}: {}'.format(\n            spec_content, e))\n\n    if not data:\n        raise IRValidatorException('No data to validate')\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\n    try:\n        validate(spec_content, cls.SCHEMA)\n    except ValidationError as e:\n        raise IRValidatorException(\"Incorrect format for spec file\") from e\n\n    return cls.SCHEMA\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    required_fields = ['name', 'description']\n    return cls.validate_from_file(required_fields, spec_content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    data = {}\n\n    try:\n        data = yaml.load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(\n            \"Failed to parse YAML file ({0})\".format(exc)\n        )\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    spec = load_yaml(spec_content)\n\n    if'metadata' not in spec:\n        raise IRValidatorException('Missing \"metadata\" key in spec file')\n\n    if'settings' not in spec['metadata']:\n        raise IRValidatorException('Missing \"settings\" key in metadata key')\n\n    required_fields = ['name', 'description', 'image', 'template','stack',\n                       'api_url', 'node_name', 'node_type']\n\n    missing_fields = set(required_fields) - set(spec.keys())\n    if missing_fields:\n        raise IRValidatorException('Missing required fields: {}'.format(\n            ', '.join(missing_fields)))\n\n    return spec\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    spec = None\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(\"Unable to parse spec file - {}\".format(e))\n\n    if'reports' not in spec:\n        raise IRValidatorException(\"Missing reports data\")\n\n    return spec\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    data = yaml.safe_load(spec_content)\n\n    missing_required_keys = [key for key in cls.required_keys if key not in data]\n    if missing_required_keys:\n        raise IRValidatorException(\n            \"Missing required fields: {}\".format(missing_required_keys)\n        )\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    try:\n        return cls._load_spec(spec_content)\n    except (yaml.YAMLError, IOError) as e:\n        raise IRValidatorException('Unable to parse YAML file {}: {}'.format(\n            cls._file_path, e), cls._file_path)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_file", "docstring": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file", "ques_id": "6306091c73426c38ae68acdd", "code": ["def validate_from_file(cls, yaml_file=None):\n    data = load_yaml(yaml_file)\n    cls.validate(data)\n    return data\n", "def validate_from_file(cls, yaml_file=None):\n\n    yaml_file = yaml_file or cls._yaml_file_path\n\n    with open(yaml_file) as yaml_file:\n        data = yaml.safe_load(yaml_file)\n\n    try:\n        cls.validate_from_content(data)\n    except IRValidatorException as error:\n        raise IRValidatorException(error)\n\n    return data\n", "def validate_from_file(cls, yaml_file=None):\n\n    if yaml_file is None:\n        raise IRValidatorException(\"Missing required field 'yaml_file'\")\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.safe_load(f)\n\n    try:\n        validate(data, cls.SCHEMA)\n    except ValidationError as e:\n        raise IRValidatorException(\"Invalid YAML file: {}\".format(e))\n\n    return data\n", "def validate_from_file(cls, yaml_file=None):\n\n    if not yaml_file:\n        raise IRValidatorException('Missing file path')\n\n    with open(yaml_file, 'r') as f:\n        try:\n            data = yaml.load(f, Loader=yaml.SafeLoader)\n        except yaml.YAMLError as exc:\n            raise IRValidatorException('Invalid YAML file {}'.format(exc))\n\n        if data is None or not isinstance(data, dict):\n            raise IRValidatorException('Invalid YAML file {}'.format(yaml_file))\n\n    return data\n", "def validate_from_file(cls, yaml_file=None):\n\n    # load and validate the YAML file\n    with open(yaml_file, 'r') as f:\n        yaml_content = yaml.safe_load(f)\n\n    # validate all required fields\n    return cls.validate_from_content(yaml_content)\n", "def validate_from_file(cls, yaml_file=None):\n\n    with open(yaml_file, 'r') as f:\n        file_content = yaml.safe_load(f)\n\n    try:\n        cls.validate_from_content(file_content)\n    except IRValidatorException as e:\n        raise IRValidatorException(e)\n\n    return file_content\n", "def validate_from_file(cls, yaml_file=None):\n\n    if yaml_file is None:\n        raise IRValidatorException('No file path provided')\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.safe_load(f)\n\n    if not data:\n        raise IRValidatorException('File does not contain required data')\n\n    return data\n", "def validate_from_file(cls, yaml_file=None):\n    try:\n        with open(yaml_file, 'r') as f:\n            file_content = f.read()\n            return cls.validate_from_content(file_content)\n    except IOError as io_err:\n        raise IRValidatorException('Unable to open YAML file {}: {}'.format(yaml_file, io_err))\n", "def validate_from_file(cls, yaml_file=None):\n\n    with open(yaml_file) as f:\n        try:\n            return cls.validate_from_content(yaml.load(f))\n        except yaml.YAMLError as e:\n            raise IRValidatorException('Invalid YAML file ({})'.format(e))\n", "def validate_from_file(cls, yaml_file=None):\n    if yaml_file is None:\n        raise IRValidatorException('Missing YAML file')\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.load(f.read(), Loader=yaml.SafeLoader)\n\n    if not data:\n        raise IRValidatorException('Missing required fields')\n\n    return data\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_file(cls, yaml_file=None):\n    data = load_yaml(yaml_file)\n    cls.validate(data)\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\n    yaml_file = yaml_file or cls._yaml_file_path\n\n    with open(yaml_file) as yaml_file:\n        data = yaml.safe_load(yaml_file)\n\n    try:\n        cls.validate_from_content(data)\n    except IRValidatorException as error:\n        raise IRValidatorException(error)\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\n    if yaml_file is None:\n        raise IRValidatorException(\"Missing required field 'yaml_file'\")\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.safe_load(f)\n\n    try:\n        validate(data, cls.SCHEMA)\n    except ValidationError as e:\n        raise IRValidatorException(\"Invalid YAML file: {}\".format(e))\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\n    if not yaml_file:\n        raise IRValidatorException('Missing file path')\n\n    with open(yaml_file, 'r') as f:\n        try:\n            data = yaml.load(f, Loader=yaml.SafeLoader)\n        except yaml.YAMLError as exc:\n            raise IRValidatorException('Invalid YAML file {}'.format(exc))\n\n        if data is None or not isinstance(data, dict):\n            raise IRValidatorException('Invalid YAML file {}'.format(yaml_file))\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\n    # load and validate the YAML file\n    with open(yaml_file, 'r') as f:\n        yaml_content = yaml.safe_load(f)\n\n    # validate all required fields\n    return cls.validate_from_content(yaml_content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\n    with open(yaml_file, 'r') as f:\n        file_content = yaml.safe_load(f)\n\n    try:\n        cls.validate_from_content(file_content)\n    except IRValidatorException as e:\n        raise IRValidatorException(e)\n\n    return file_content\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\n    if yaml_file is None:\n        raise IRValidatorException('No file path provided')\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.safe_load(f)\n\n    if not data:\n        raise IRValidatorException('File does not contain required data')\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    try:\n        with open(yaml_file, 'r') as f:\n            file_content = f.read()\n            return cls.validate_from_content(file_content)\n    except IOError as io_err:\n        raise IRValidatorException('Unable to open YAML file {}: {}'.format(yaml_file, io_err))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\n    with open(yaml_file) as f:\n        try:\n            return cls.validate_from_content(yaml.load(f))\n        except yaml.YAMLError as e:\n            raise IRValidatorException('Invalid YAML file ({})'.format(e))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    if yaml_file is None:\n        raise IRValidatorException('Missing YAML file')\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.load(f.read(), Loader=yaml.SafeLoader)\n\n    if not data:\n        raise IRValidatorException('Missing required fields')\n\n    return data\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "_include_groups", "docstring": "Resolves the include dict directive in the spec files.", "ques_id": "6306091d73426c38ae68ace5", "code": ["def _include_groups(self, parser_dict):\n    include_groups = copy.deepcopy(parser_dict['__include_groups__'])\n\n    for include_group in include_groups:\n        include_group['commands'] = self._include_groups(include_group['commands'])\n\n    parser_dict['__include_groups__'] = include_groups\n", "def _include_groups(self, parser_dict):\n    include_groups = copy.copy(parser_dict['include'])\n\n    for include_group in include_groups:\n        option_name = include_group.pop('option_name')\n        if option_name in self.spec_dict:\n            self.spec_dict[option_name].extend(include_group)\n", "def _include_groups(self, parser_dict):\n    if \"include_groups\" in parser_dict:\n        parser_dict[\"include_groups\"] = parser_dict[\"include_groups\"].split(\",\")\n        for group in parser_dict[\"include_groups\"]:\n            if \"group_name\" in group:\n                group_name = group[\"group_name\"].lower()\n                if \"include_group\" in parser_dict:\n                    parser_dict[\"include_group\"] = parser_dict[\"include_group\"].split(\",\") + [group_name]\n                else:\n                    parser_dict[\"include_group\"] = [group_name]\n", "def _include_groups(self, parser_dict):\n    # get all options spec\n    all_options_spec = self._get_all_options_spec(parser_dict)\n    # remove the include groups\n    parser_dict.pop('include_groups', None)\n    # add the include groups\n    for option_spec in all_options_spec:\n        if option_spec.group is not None:\n            parser_dict[option_spec.key] = option_spec.group\n", "def _include_groups(self, parser_dict):\n    includes = {}\n\n    for key, value in parser_dict.items():\n        if isinstance(value, dict):\n            includes[key] = self._include_groups(value)\n        else:\n            includes[key] = value\n\n    return includes\n", "def _include_groups(self, parser_dict):\n    includes = {}\n    for group, options_dict in parser_dict.items():\n        for option in options_dict.keys():\n            if option == 'include':\n                includes[group] = options_dict[option]\n    return includes\n", "def _include_groups(self, parser_dict):\n    if 'include_groups' in parser_dict:\n        include_groups = parser_dict['include_groups']\n        if isinstance(include_groups, list):\n            parser_dict['include_groups'] = [\n                self.spec_dict.get_option_spec('group', i)\n                for i in include_groups\n            ]\n    return parser_dict\n", "def _include_groups(self, parser_dict):\n    if parser_dict['include_groups']:\n        include_groups = parser_dict['include_groups']\n        for include_spec in include_groups.split(','):\n            spec = copy.deepcopy(self.spec_dict)\n            spec['include_groups'] = include_spec.split(':')\n            yield spec\n", "def _include_groups(self, parser_dict):\n    include_group_specs = []\n\n    for parser_name, parser_spec in parser_dict.items():\n        # If the parser spec is a group, recurse into it\n        if isinstance(parser_spec, dict):\n            for include_group_name, include_group_spec in parser_spec.items():\n                include_group_specs.append(\n                    self.spec_dict[include_group_name]\n                )\n                include_group_specs.extend(\n                    self._include_groups(include_group_spec)\n                )\n        else:\n            include_group_specs.append(parser_spec)\n\n    return include_group_specs\n", "def _include_groups(self, parser_dict):\n    for command_name, command_spec in parser_dict.items():\n        command_name = command_name.replace(\"-\", \"_\")\n\n        if command_name not in self._include_groups:\n            continue\n\n        for include_spec in command_spec.get(\"include\", []):\n            include_spec = include_spec.replace(\"/\", \"\\\\/\")\n            parser_dict[command_name][\"include\"].append(include_spec)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _include_groups(self, parser_dict):\n    include_groups = copy.deepcopy(parser_dict['__include_groups__'])\n\n    for include_group in include_groups:\n        include_group['commands'] = self._include_groups(include_group['commands'])\n\n    parser_dict['__include_groups__'] = include_groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    include_groups = copy.copy(parser_dict['include'])\n\n    for include_group in include_groups:\n        option_name = include_group.pop('option_name')\n        if option_name in self.spec_dict:\n            self.spec_dict[option_name].extend(include_group)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    if \"include_groups\" in parser_dict:\n        parser_dict[\"include_groups\"] = parser_dict[\"include_groups\"].split(\",\")\n        for group in parser_dict[\"include_groups\"]:\n            if \"group_name\" in group:\n                group_name = group[\"group_name\"].lower()\n                if \"include_group\" in parser_dict:\n                    parser_dict[\"include_group\"] = parser_dict[\"include_group\"].split(\",\") + [group_name]\n                else:\n                    parser_dict[\"include_group\"] = [group_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    # get all options spec\n    all_options_spec = self._get_all_options_spec(parser_dict)\n    # remove the include groups\n    parser_dict.pop('include_groups', None)\n    # add the include groups\n    for option_spec in all_options_spec:\n        if option_spec.group is not None:\n            parser_dict[option_spec.key] = option_spec.group\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    includes = {}\n\n    for key, value in parser_dict.items():\n        if isinstance(value, dict):\n            includes[key] = self._include_groups(value)\n        else:\n            includes[key] = value\n\n    return includes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    includes = {}\n    for group, options_dict in parser_dict.items():\n        for option in options_dict.keys():\n            if option == 'include':\n                includes[group] = options_dict[option]\n    return includes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    if 'include_groups' in parser_dict:\n        include_groups = parser_dict['include_groups']\n        if isinstance(include_groups, list):\n            parser_dict['include_groups'] = [\n                self.spec_dict.get_option_spec('group', i)\n                for i in include_groups\n            ]\n    return parser_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    if parser_dict['include_groups']:\n        include_groups = parser_dict['include_groups']\n        for include_spec in include_groups.split(','):\n            spec = copy.deepcopy(self.spec_dict)\n            spec['include_groups'] = include_spec.split(':')\n            yield spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    include_group_specs = []\n\n    for parser_name, parser_spec in parser_dict.items():\n        # If the parser spec is a group, recurse into it\n        if isinstance(parser_spec, dict):\n            for include_group_name, include_group_spec in parser_spec.items():\n                include_group_specs.append(\n                    self.spec_dict[include_group_name]\n                )\n                include_group_specs.extend(\n                    self._include_groups(include_group_spec)\n                )\n        else:\n            include_group_specs.append(parser_spec)\n\n    return include_group_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    for command_name, command_spec in parser_dict.items():\n        command_name = command_name.replace(\"-\", \"_\")\n\n        if command_name not in self._include_groups:\n            continue\n\n        for include_spec in command_spec.get(\"include\", []):\n            include_spec = include_spec.replace(\"/\", \"\\\\/\")\n            parser_dict[command_name][\"include\"].append(include_spec)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_spec_defaults", "docstring": "Resolve arguments' values from spec and other sources.", "ques_id": "6306092373426c38ae68acfa", "code": ["def get_spec_defaults(self):\n    return self.defaults\n", "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "def get_spec_defaults(self):\n    return self.vars[\"__spec_defaults__\"]\n", "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "def get_spec_defaults(self):\n    return self.vars.get('_spec_defaults', {})\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_spec_defaults(self):\n    return self.defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return self.vars[\"__spec_defaults__\"]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return self.vars.get('__spec_defaults__', {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return self.vars.get('_spec_defaults', {})\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_deprecated_args", "docstring": "Returning dict with options which deprecate others.", "ques_id": "6306092973426c38ae68ad01", "code": ["def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.deprecated_args)\n", "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.get_deprecated_args())\n", "def get_deprecated_args(self):\n    return self._get_conditionally_required_args(\n        'cli', ['options', 'args', 'arguments']\n    )\n", "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.get_deprecated_args())\n", "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.vars.get('deprecated_args', []))\n", "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.parser._actions)\n", "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.parser.deprecated_args)\n", "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.parser.deprecated_args)\n", "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.parser._actions)\n", "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.get_deprecated_args())\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.deprecated_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.get_deprecated_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._get_conditionally_required_args(\n        'cli', ['options', 'args', 'arguments']\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.get_deprecated_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.vars.get('deprecated_args', []))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.parser._actions)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.parser.deprecated_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.parser.deprecated_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.parser._actions)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._merge_duplicated_cli_args(self.get_deprecated_args())\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_arg_deprecation", "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files", "ques_id": "6306092c73426c38ae68ad02", "code": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecation_message = \"The '--{}' option is deprecated and will be removed in an upcoming release.\".format(\n        self.option_name)\n    if cli_args.type == \"\":\n        deprecation_message += \" Use '{} {}' instead\".format(\n            self.get_spec_defaults()['type'], self.get_spec_defaults()['name'])\n    if deprecation_message not in cli_args.deprecation_messages:\n        LOG.warning(deprecation_message)\n        cli_args.deprecation_messages.append(deprecation_message)\n    if cli_args.deprecation_messages and cli_args.deprecation_messages[-1] is not None:\n        cli_args.deprecation_messages[-1] +='and '\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    LOG.debug('Validating arguments deprecation')\n    self._validate_arg_sources('cli_args', cli_args)\n    self._validate_arg_sources('answer_file_args', answer_file_args)\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    for arg_name, arg_spec in cli_args.items():\n        arg_type, arg_default = self._get_conditionally_required_args(\n            arg_name, arg_spec, args=cli_args)\n        arg_default = self.validate_arg_default(arg_name, arg_default, arg_spec)\n        arg_spec['default'] = arg_default\n\n        arg_spec['required'] = self.validate_requires_args(args=args,\n                                                          arg_spec=arg_spec)\n        arg_spec['type'] = arg_type\n\n        arg_spec['plugin_path'] = self.plugin_path\n\n        self._merge_duplicated_cli_args(args)\n\n        if arg_spec['deprecated']:\n            LOG.warning(\"'%s' argument is deprecated, use '%s'\",\n                        arg_name, arg_name)\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    LOG.warn(\"Arguments were ignored due to deprecation of args for \"\n             \"command '%s' and are therefore not supported.\" % (\n                 cli_args.command_name, ))\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    LOG.debug(\"validate_arg_deprecation: %s\", cli_args)\n    LOG.debug(\"validate_arg_deprecation: %s\", answer_file_args)\n    if not isinstance(cli_args, list):\n        cli_args = [cli_args]\n    if not isinstance(answer_file_args, list):\n        answer_file_args = [answer_file_args]\n    return self.validate_arg_sources(cli_args, answer_file_args)\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if cli_args.deprecation_type == 'range':\n        try:\n            min_arg, max_arg = cli_args.deprecation_range\n            if min_arg is not None and max_arg is not None:\n                LOG.warn('You cannot set both --deprecation-type=range '\n                         'and --deprecation-range=range(min,max) at the same '\n                         'time. This will break the argument parser and '\n                         'will result in errors.')\n        except AttributeError:\n            LOG.warn('Please provide a deprecation type that is a list of '\n                     'deprecation types and will be ignored.')\n            min_arg = None\n            max_arg = None\n    else:\n        min_arg = answer_file_args.get('--min', None)\n        ma", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    required_args = self._get_conditionally_required_args('args', cli_args, answer_file_args)\n    if required_args:\n        LOG.warn(\"Argument '%s' is deprecated, it will be removed in the next release.\", required_args.pop())\n    return required_args\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if '--fail-if-missing' in cli_args:\n        raise ArgumentDeprecationError(''.join([\n            'Use `--fail-if-missing <arg_name>` instead of `--help`.',\n            'For more information, see the specification of argparse.',\n        ]))\n\n    if '--fail-on-no-required' in cli_args:\n        raise ArgumentDeprecationError(''.join([\n            'Use `--fail-on-no-required <arg_name>` instead of `--help`.',\n            'For more information, see the specification of argparse.',\n        ]))\n\n    if '--fail-on-no-deprecated' in cli_args:\n        LOG.info(''.join([\n            'Provide an argument with default value \"None\" in the '\n            '`--help` message.',\n        ]))\n\n    return cli_args\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if cli_args['deprecated']:\n        LOG.warning('The \"--%s\" option is deprecated, please use '\n                    '\"--%s=%s\" instead.',\n                    cli_args['command'], cli_args['option'],\n                    cli_args['value'])\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if cli_args is None:\n        return\n\n    if len(cli_args)!= len(answer_file_args):\n        LOG.warning('Invalid number of arguments for command %s. '\n                    'Expected %d, found %d.',\n                    self.command_name, len(cli_args), len(answer_file_args))\n        return\n\n    for a, b in zip(cli_args, answer_file_args):\n        self._merge_duplicated_cli_args(a)\n        self._merge_duplicated_cli_args(b)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecation_message = \"The '--{}' option is deprecated and will be removed in an upcoming release.\".format(\n        self.option_name)\n    if cli_args.type == \"\":\n        deprecation_message += \" Use '{} {}' instead\".format(\n            self.get_spec_defaults()['type'], self.get_spec_defaults()['name'])\n    if deprecation_message not in cli_args.deprecation_messages:\n        LOG.warning(deprecation_message)\n        cli_args.deprecation_messages.append(deprecation_message)\n    if cli_args.deprecation_messages and cli_args.deprecation_messages[-1] is not None:\n        cli_args.deprecation_messages[-1] +='and '\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    LOG.debug('Validating arguments deprecation')\n    self._validate_arg_sources('cli_args', cli_args)\n    self._validate_arg_sources('answer_file_args', answer_file_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    for arg_name, arg_spec in cli_args.items():\n        arg_type, arg_default = self._get_conditionally_required_args(\n            arg_name, arg_spec, args=cli_args)\n        arg_default = self.validate_arg_default(arg_name, arg_default, arg_spec)\n        arg_spec['default'] = arg_default\n\n        arg_spec['required'] = self.validate_requires_args(args=args,\n                                                          arg_spec=arg_spec)\n        arg_spec['type'] = arg_type\n\n        arg_spec['plugin_path'] = self.plugin_path\n\n        self._merge_duplicated_cli_args(args)\n\n        if arg_spec['deprecated']:\n            LOG.warning(\"'%s' argument is deprecated, use '%s'\",\n                        arg_name, arg_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    LOG.warn(\"Arguments were ignored due to deprecation of args for \"\n             \"command '%s' and are therefore not supported.\" % (\n                 cli_args.command_name, ))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    LOG.debug(\"validate_arg_deprecation: %s\", cli_args)\n    LOG.debug(\"validate_arg_deprecation: %s\", answer_file_args)\n    if not isinstance(cli_args, list):\n        cli_args = [cli_args]\n    if not isinstance(answer_file_args, list):\n        answer_file_args = [answer_file_args]\n    return self.validate_arg_sources(cli_args, answer_file_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if cli_args.deprecation_type == 'range':\n        try:\n            min_arg, max_arg = cli_args.deprecation_range\n            if min_arg is not None and max_arg is not None:\n                LOG.warn('You cannot set both --deprecation-type=range '\n                         'and --deprecation-range=range(min,max) at the same '\n                         'time. This will break the argument parser and '\n                         'will result in errors.')\n        except AttributeError:\n            LOG.warn('Please provide a deprecation type that is a list of '\n                     'deprecation types and will be ignored.')\n            min_arg = None\n            max_arg = None\n    else:\n        min_arg = answer_file_args.get('--min', None)\n        ma", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    required_args = self._get_conditionally_required_args('args', cli_args, answer_file_args)\n    if required_args:\n        LOG.warn(\"Argument '%s' is deprecated, it will be removed in the next release.\", required_args.pop())\n    return required_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if '--fail-if-missing' in cli_args:\n        raise ArgumentDeprecationError(''.join([\n            'Use `--fail-if-missing <arg_name>` instead of `--help`.',\n            'For more information, see the specification of argparse.',\n        ]))\n\n    if '--fail-on-no-required' in cli_args:\n        raise ArgumentDeprecationError(''.join([\n            'Use `--fail-on-no-required <arg_name>` instead of `--help`.',\n            'For more information, see the specification of argparse.',\n        ]))\n\n    if '--fail-on-no-deprecated' in cli_args:\n        LOG.info(''.join([\n            'Provide an argument with default value \"None\" in the '\n            '`--help` message.',\n        ]))\n\n    return cli_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if cli_args['deprecated']:\n        LOG.warning('The \"--%s\" option is deprecated, please use '\n                    '\"--%s=%s\" instead.',\n                    cli_args['command'], cli_args['option'],\n                    cli_args['value'])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if cli_args is None:\n        return\n\n    if len(cli_args)!= len(answer_file_args):\n        LOG.warning('Invalid number of arguments for command %s. '\n                    'Expected %d, found %d.',\n                    self.command_name, len(cli_args), len(answer_file_args))\n        return\n\n    for a, b in zip(cli_args, answer_file_args):\n        self._merge_duplicated_cli_args(a)\n        self._merge_duplicated_cli_args(b)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_parser_option_specs", "docstring": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options", "ques_id": "6306092d73426c38ae68ad04", "code": ["def get_parser_option_specs(self, command_name):\n    if command_name in self.spec_dict:\n        return self.spec_dict[command_name][\"options\"]\n    return []\n", "def get_parser_option_specs(self, command_name):\n    # type: (str) -> list[dict[str, Any]]\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    option_specs = []\n    for parser_dict in self.iterate_parsers():\n        option_specs.append(self.get_option_spec(command_name, parser_dict[\"option_name\"]))\n    return option_specs\n", "def get_parser_option_specs(self, command_name):\n    option_spec = self.spec_dict.get(command_name, None)\n    if not option_spec:\n        return []\n    return option_spec.options\n", "def get_parser_option_specs(self, command_name):\n    return self._get_all_options_spec(command_name)\n", "def get_parser_option_specs(self, command_name):\n    if command_name not in self.spec_dict:\n        raise AttributeError(\"Command name '%s' is not defined\" % command_name)\n    return self.spec_dict[command_name].keys()\n", "def get_parser_option_specs(self, command_name):\n    return self.spec_dict[command_name].get('options', [])\n", "def get_parser_option_specs(self, command_name):\n    return self.spec_dict.get(command_name, {}).get(\"options\", [])\n", "def get_parser_option_specs(self, command_name):\n    return self._get_all_options_spec(command_name)\n", "def get_parser_option_specs(self, command_name):\n    return self.spec_dict[command_name]['parser_option_specs']\n", "def get_parser_option_specs(self, command_name):\n    if command_name in self.spec_dict:\n        return self.spec_dict[command_name].keys()\n    else:\n        return []\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_parser_option_specs(self, command_name):\n    if command_name in self.spec_dict:\n        return self.spec_dict[command_name][\"options\"]\n    return []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    # type: (str) -> list[dict[str, Any]]\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    option_specs = []\n    for parser_dict in self.iterate_parsers():\n        option_specs.append(self.get_option_spec(command_name, parser_dict[\"option_name\"]))\n    return option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    option_spec = self.spec_dict.get(command_name, None)\n    if not option_spec:\n        return []\n    return option_spec.options\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    return self._get_all_options_spec(command_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    if command_name not in self.spec_dict:\n        raise AttributeError(\"Command name '%s' is not defined\" % command_name)\n    return self.spec_dict[command_name].keys()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    return self.spec_dict[command_name].get('options', [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    return self.spec_dict.get(command_name, {}).get(\"options\", [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    return self._get_all_options_spec(command_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    return self.spec_dict[command_name]['parser_option_specs']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    if command_name in self.spec_dict:\n        return self.spec_dict[command_name].keys()\n    else:\n        return []\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_option_spec", "docstring": "Gets the specification for the specified option name.", "ques_id": "6306092d73426c38ae68ad05", "code": ["def get_option_spec(self, command_name, argument_name):\n    if command_name not in self.spec_dict:\n        raise KeyError('Command name \"%s\" has no option specified.' %\n                       command_name)\n    if argument_name not in self.spec_dict[command_name]:\n        raise KeyError('Option named \"%s\" has no specified option specified.' %\n                       argument_name)\n    return self.spec_dict[command_name][argument_name]\n", "def get_option_spec(self, command_name, argument_name):\n    option_spec = self.spec_dict['options'].get(command_name, {})\n    return option_spec.get(argument_name, {})\n", "def get_option_spec(self, command_name, argument_name):\n    return self.spec_dict[command_name].get(argument_name)\n", "def get_option_spec(self, command_name, argument_name):\n    parser_dict = self.spec_dict.get(command_name, {})\n    return parser_dict.get(argument_name)\n", "def get_option_spec(self, command_name, argument_name):\n    if command_name not in self.spec_dict:\n        raise InvalidSpecification(\n            'Unknown command specification: \"%s\"' % command_name)\n    option_spec = self.spec_dict[command_name].get(argument_name)\n    if not option_spec:\n        raise InvalidSpecification(\n            'Unknown option spec: \"%s\"' % argument_name)\n    return option_spec\n", "def get_option_spec(self, command_name, argument_name):\n    parser_dict = self.spec_dict.get(command_name, {})\n    return parser_dict.get(argument_name, None)\n", "def get_option_spec(self, command_name, argument_name):\n    parser_spec = self.spec_dict.get(command_name + '_' + argument_name)\n    if not parser_spec:\n        parser_spec = self.spec_dict.get(command_name + '_*')\n    return parser_spec\n", "def get_option_spec(self, command_name, argument_name):\n    parser_specs = self.get_parser_option_specs(command_name)\n    return parser_specs.get(argument_name, None)\n", "def get_option_spec(self, command_name, argument_name):\n    if command_name == 'command' and argument_name == '--':\n        return self._get_all_options_spec(self.spec_dict)\n\n    return super(InfraredCommand, self).get_option_spec(command_name, argument_name)\n", "def get_option_spec(self, command_name, argument_name):\n    if command_name not in self.spec_dict:\n        raise KeyError(\n            'Command name \\'{}\\' is not a valid command name.'.format(\n                command_name))\n    if argument_name not in self.spec_dict[command_name]:\n        raise KeyError(\n            'The spec key \\'{}\\' does not exist.'.format(argument_name))\n    return self.spec_dict[command_name][argument_name]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_option_spec(self, command_name, argument_name):\n    if command_name not in self.spec_dict:\n        raise KeyError('Command name \"%s\" has no option specified.' %\n                       command_name)\n    if argument_name not in self.spec_dict[command_name]:\n        raise KeyError('Option named \"%s\" has no specified option specified.' %\n                       argument_name)\n    return self.spec_dict[command_name][argument_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    option_spec = self.spec_dict['options'].get(command_name, {})\n    return option_spec.get(argument_name, {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    return self.spec_dict[command_name].get(argument_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    parser_dict = self.spec_dict.get(command_name, {})\n    return parser_dict.get(argument_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    if command_name not in self.spec_dict:\n        raise InvalidSpecification(\n            'Unknown command specification: \"%s\"' % command_name)\n    option_spec = self.spec_dict[command_name].get(argument_name)\n    if not option_spec:\n        raise InvalidSpecification(\n            'Unknown option spec: \"%s\"' % argument_name)\n    return option_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    parser_dict = self.spec_dict.get(command_name, {})\n    return parser_dict.get(argument_name, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    parser_spec = self.spec_dict.get(command_name + '_' + argument_name)\n    if not parser_spec:\n        parser_spec = self.spec_dict.get(command_name + '_*')\n    return parser_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    parser_specs = self.get_parser_option_specs(command_name)\n    return parser_specs.get(argument_name, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    if command_name == 'command' and argument_name == '--':\n        return self._get_all_options_spec(self.spec_dict)\n\n    return super(InfraredCommand, self).get_option_spec(command_name, argument_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    if command_name not in self.spec_dict:\n        raise KeyError(\n            'Command name \\'{}\\' is not a valid command name.'.format(\n                command_name))\n    if argument_name not in self.spec_dict[command_name]:\n        raise KeyError(\n            'The spec key \\'{}\\' does not exist.'.format(argument_name))\n    return self.spec_dict[command_name][argument_name]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_silent_args", "docstring": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names", "ques_id": "6306092d73426c38ae68ad06", "code": ["def get_silent_args(self, args):\n    return self._merge_duplicated_cli_args(args)\n", "def get_silent_args(self, args):\n    return super(Command, self).get_silent_args(args) + [\n        '--insecure',\n        '--no-color',\n        '--quiet',\n        '--no-required'\n    ]\n", "def get_silent_args(self, args):\n    return self._merge_duplicated_cli_args(args)\n", "def get_silent_args(self, args):\n    return [arg for arg in args if arg is not None]\n", "def get_silent_args(self, args):\n    return [arg for arg in args if arg is not None]\n", "def get_silent_args(self, args):\n    return super(InfraredCLI, self).get_silent_args(args) + [\n        '--infrared-only',\n    ]\n", "def get_silent_args(self, args):\n    return super(CLI, self).get_silent_args(args) + ['--no-color']\n", "def get_silent_args(self, args):\n    return []\n", "def get_silent_args(self, args):\n    return args\n", "def get_silent_args(self, args):\n    return self._merge_duplicated_cli_args(args)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_silent_args(self, args):\n    return self._merge_duplicated_cli_args(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return super(Command, self).get_silent_args(args) + [\n        '--insecure',\n        '--no-color',\n        '--quiet',\n        '--no-required'\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return self._merge_duplicated_cli_args(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return [arg for arg in args if arg is not None]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return [arg for arg in args if arg is not None]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return super(InfraredCLI, self).get_silent_args(args) + [\n        '--infrared-only',\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return super(CLI, self).get_silent_args(args) + ['--no-color']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return []\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_silent_args(self, args):\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return self._merge_duplicated_cli_args(args)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_requires_args", "docstring": "Check if all the required arguments have been provided.", "ques_id": "6306092d73426c38ae68ad07", "code": ["def validate_requires_args(self, args):\n    arg_spec = dict(self.parser._required_args)\n    for arg_name, arg_type in args.items():\n        if arg_name not in arg_spec:\n            raise CLIError('Required argument {} not found'.format(arg_name))\n        arg_spec[arg_name] = arg_type\n", "def validate_requires_args(self, args):\n    return self._validate_requires_args(args)\n", "def validate_requires_args(self, args):\n    if args.requires_args is None:\n        return\n\n    spec = vars(args)\n    required_args = spec.get('requires', [])\n    for required_arg in required_args:\n        if required_arg not in self.vars:\n            raise MissingArgument(\n                \"Missing required argument '{}'\".format(required_arg)\n            )\n", "def validate_requires_args(self, args):\n    return self._validate_requires_args(args)\n", "def validate_requires_args(self, args):\n    spec_args = vars(self.vars)\n    required_args = spec_args.pop('required')\n    for arg in args:\n        if arg not in required_args:\n            print('{} is not a required argument for {}'.format(arg, self.command_name))\n            raise InvalidArgumentError\n", "def validate_requires_args(self, args):\n    spec_args = self.vars.get('__spec_args__')\n    if spec_args is not None:\n        required_args = spec_args.get(args.command, [])\n        if args.command in required_args:\n            raise exceptions.RequireArgument(args.command, required_args)\n", "def validate_requires_args(self, args):\n    required_args = self._get_conditionally_required_args(\n       'requires',\n        args)\n    if not required_args:\n        return\n\n    err_msg = 'Argument(s) \"{}\" must have at least one'\\\n        'conditional argument.'\n    raise MissingArgsError(err_msg.format(', '.join(required_args)))\n", "def validate_requires_args(self, args):\n    arg_names = [arg.name for arg in args]\n    if '--' in arg_names:\n        raise InvalidArgumentException('-- arguments cannot be specified at the same time')\n    return args\n", "def validate_requires_args(self, args):\n    arg_spec = inspect.getargspec(self.parser.parse_args)\n    required_args = arg_spec.args[-len(args):]\n    deprecated_args = [arg for arg in args if arg not in required_args]\n    if deprecated_args:\n        deprecation_warning(\"Received arguments {} that are deprecated \"\n                            \"in {}. Please consult the documentation \"\n                            \"for further information.\"\n                           .format(deprecated_args, self.plugin_path))\n", "def validate_requires_args(self, args):\n    arg_spec = inspect.getfullargspec(args)\n    arg_spec.defaults = args.__defaults__ if arg_spec.defaults is not None else []\n    arg_spec.args = [arg_spec.args[i] if i < len(arg_spec.args) else None for i in range(len(arg_spec.args))]\n    return arg_spec\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_requires_args(self, args):\n    arg_spec = dict(self.parser._required_args)\n    for arg_name, arg_type in args.items():\n        if arg_name not in arg_spec:\n            raise CLIError('Required argument {} not found'.format(arg_name))\n        arg_spec[arg_name] = arg_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    return self._validate_requires_args(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    if args.requires_args is None:\n        return\n\n    spec = vars(args)\n    required_args = spec.get('requires', [])\n    for required_arg in required_args:\n        if required_arg not in self.vars:\n            raise MissingArgument(\n                \"Missing required argument '{}'\".format(required_arg)\n            )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    return self._validate_requires_args(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    spec_args = vars(self.vars)\n    required_args = spec_args.pop('required')\n    for arg in args:\n        if arg not in required_args:\n            print('{} is not a required argument for {}'.format(arg, self.command_name))\n            raise InvalidArgumentError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    spec_args = self.vars.get('__spec_args__')\n    if spec_args is not None:\n        required_args = spec_args.get(args.command, [])\n        if args.command in required_args:\n            raise exceptions.RequireArgument(args.command, required_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    required_args = self._get_conditionally_required_args(\n       'requires',\n        args)\n    if not required_args:\n        return\n\n    err_msg = 'Argument(s) \"{}\" must have at least one'\\\n        'conditional argument.'\n    raise MissingArgsError(err_msg.format(', '.join(required_args)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    arg_names = [arg.name for arg in args]\n    if '--' in arg_names:\n        raise InvalidArgumentException('-- arguments cannot be specified at the same time')\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    arg_spec = inspect.getargspec(self.parser.parse_args)\n    required_args = arg_spec.args[-len(args):]\n    deprecated_args = [arg for arg in args if arg not in required_args]\n    if deprecated_args:\n        deprecation_warning(\"Received arguments {} that are deprecated \"\n                            \"in {}. Please consult the documentation \"\n                            \"for further information.\"\n                           .format(deprecated_args, self.plugin_path))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    arg_spec = inspect.getfullargspec(args)\n    arg_spec.defaults = args.__defaults__ if arg_spec.defaults is not None else []\n    arg_spec.args = [arg_spec.args[i] if i < len(arg_spec.args) else None for i in range(len(arg_spec.args))]\n    return arg_spec\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_get_conditionally_required_args", "docstring": "List arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition", "ques_id": "6306092d73426c38ae68ad08", "code": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Returns a list of all the arguments that must be required when the\n    specified command is run. The returned list should not include arguments\n    that are specified in the ``--`` or ``----opt`` arguments.\n    \"\"\"\n    spec_args = self.vars[command_name]['args']\n    required_args = [arg for arg in args if arg not in spec_args]\n    return required_args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Get required arguments from args and validate them with the spec.\n    \"\"\"\n    arg_spec = vars(args)\n    arg_spec.update(options_spec)\n    args = vars(args)\n    args['required'] = self._validate_required_args(args)\n    args = self.validate_arg_deprecation(args, command_name)\n    return args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Get conditionally required arguments from command arguments.\n    \"\"\"\n    # parse arguments\n    args = self.parse_args(args)\n\n    # resolve arguments\n    cli_args = self._resolve_cli_args(args)\n\n    # validate arguments\n    self.validate_arg_sources(cli_args, args)\n    self.validate_choices_args(cli_args, args)\n\n    # return conditionally required arguments\n    return self._get_conditionally_required_args(command_name, options_spec,\n                                                args)\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    This method is used to validate the requirements of a command.\n    \"\"\"\n    args_without_defaults = args[:-len(options_spec[command_name]['defaults'])]\n    args_without_defaults += [arg for arg in args_without_defaults\n                              if arg is not None]\n    return args_without_defaults\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Generate required arguments for command.\n\n    \"\"\"\n    args = super(CLI, self)._get_conditionally_required_args(command_name,\n                                                             options_spec,\n                                                             args)\n    if 'choices' in args:\n        args['choices'] = self._merge_duplicated_cli_args(args['choices'])\n    return args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\" get conditionally required args \"\"\"\n    arg_spec = ArgumentSpec.from_command_name_and_options(\n        command_name, options_spec, args)\n    args = arg_spec.args\n    required_args = set()\n\n    for arg_name in args:\n        if arg_name in self.vars and arg_name not in self.defaults:\n            required_args.update(self.vars[arg_name])\n\n    return required_args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    The conditionally required args are taken from the spec arguments.\n    \"\"\"\n    required_args = []\n    for arg in args:\n        if arg['name'] in options_spec and arg['required']:\n            required_args.append(arg)\n\n    return required_args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    :type command_name: string\n    :type options_spec: dict\n    :type args: list[string]\n    :rtype: list[string]\n    \"\"\"\n    arg_spec = dict(args)\n\n    for arg_name, arg_type in arg_spec.items():\n        if not arg_type.is_type_of(command_name):\n            continue\n\n        for arg_name_, arg_type_ in arg_spec.items():\n            if arg_name_ == arg_name and arg_type_ == arg_type:\n                continue\n\n            if arg_name_ not in options_spec or arg_type_!= arg_type:\n                raise InvalidArgument('Unknown argument %s' % arg_name)\n\n            args.remove(arg_name)\n\n    return args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    This method is used to get the required arguments for the ``generate``\n    command when the command is run as a single command with the ``generate``\n    command and it will return the list of arguments that are not\n    required by the plugin.\n    \"\"\"\n    required_args = []\n\n    if command_name in self.vars and self.vars[command_name].required:\n        required_args = self.vars[command_name].required\n\n    return self._merge_duplicated_cli_args(required_args, args)\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Get the required arguments for the conditionally required command\n    \"\"\"\n    args = super(ConditionalArgumentParser, self)._get_conditionally_required_args(\n        command_name, options_spec, args)\n\n    args.update(self._get_conditionally_required_args(\n        command_name, options_spec, args))\n\n    return args\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Returns a list of all the arguments that must be required when the\n    specified command is run. The returned list should not include arguments\n    that are specified in the ``--`` or ``----opt`` arguments.\n    \"\"\"\n    spec_args = self.vars[command_name]['args']\n    required_args = [arg for arg in args if arg not in spec_args]\n    return required_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Get required arguments from args and validate them with the spec.\n    \"\"\"\n    arg_spec = vars(args)\n    arg_spec.update(options_spec)\n    args = vars(args)\n    args['required'] = self._validate_required_args(args)\n    args = self.validate_arg_deprecation(args, command_name)\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Get conditionally required arguments from command arguments.\n    \"\"\"\n    # parse arguments\n    args = self.parse_args(args)\n\n    # resolve arguments\n    cli_args = self._resolve_cli_args(args)\n\n    # validate arguments\n    self.validate_arg_sources(cli_args, args)\n    self.validate_choices_args(cli_args, args)\n\n    # return conditionally required arguments\n    return self._get_conditionally_required_args(command_name, options_spec,\n                                                args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    This method is used to validate the requirements of a command.\n    \"\"\"\n    args_without_defaults = args[:-len(options_spec[command_name]['defaults'])]\n    args_without_defaults += [arg for arg in args_without_defaults\n                              if arg is not None]\n    return args_without_defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Generate required arguments for command.\n\n    \"\"\"\n    args = super(CLI, self)._get_conditionally_required_args(command_name,\n                                                             options_spec,\n                                                             args)\n    if 'choices' in args:\n        args['choices'] = self._merge_duplicated_cli_args(args['choices'])\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\" get conditionally required args \"\"\"\n    arg_spec = ArgumentSpec.from_command_name_and_options(\n        command_name, options_spec, args)\n    args = arg_spec.args\n    required_args = set()\n\n    for arg_name in args:\n        if arg_name in self.vars and arg_name not in self.defaults:\n            required_args.update(self.vars[arg_name])\n\n    return required_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    The conditionally required args are taken from the spec arguments.\n    \"\"\"\n    required_args = []\n    for arg in args:\n        if arg['name'] in options_spec and arg['required']:\n            required_args.append(arg)\n\n    return required_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    :type command_name: string\n    :type options_spec: dict\n    :type args: list[string]\n    :rtype: list[string]\n    \"\"\"\n    arg_spec = dict(args)\n\n    for arg_name, arg_type in arg_spec.items():\n        if not arg_type.is_type_of(command_name):\n            continue\n\n        for arg_name_, arg_type_ in arg_spec.items():\n            if arg_name_ == arg_name and arg_type_ == arg_type:\n                continue\n\n            if arg_name_ not in options_spec or arg_type_!= arg_type:\n                raise InvalidArgument('Unknown argument %s' % arg_name)\n\n            args.remove(arg_name)\n\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    This method is used to get the required arguments for the ``generate``\n    command when the command is run as a single command with the ``generate``\n    command and it will return the list of arguments that are not\n    required by the plugin.\n    \"\"\"\n    required_args = []\n\n    if command_name in self.vars and self.vars[command_name].required:\n        required_args = self.vars[command_name].required\n\n    return self._merge_duplicated_cli_args(required_args, args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    Get the required arguments for the conditionally required command\n    \"\"\"\n    args = super(ConditionalArgumentParser, self)._get_conditionally_required_args(\n        command_name, options_spec, args)\n\n    args.update(self._get_conditionally_required_args(\n        command_name, options_spec, args))\n\n    return args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_length_args", "docstring": "Check if value of arguments is not longer than length specified.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad09", "code": ["def validate_length_args(self, args):\n    if len(args) < 2:\n        raise CommandArgumentException(\"This command takes at least two arguments.\")\n", "def validate_length_args(self, args):\n    if len(args)!= len(self.vars):\n        raise AnsibleFilterError(\"Incorrect number of variables provided \"\n                                \"for command %s\" % args)\n    return args\n", "def validate_length_args(self, args):\n    return self._validate_length_args(args)\n", "def validate_length_args(self, args):\n    for arg in args:\n        if len(args[arg]) > self.vars[\"max_args\"]:\n            raise ArgumentTooLongException(\n                \"Argument '{}' cannot be more than {} \"\n                \"arguments\".format(arg, self.vars[\"max_args\"])\n            )\n", "def validate_length_args(self, args):\n    if len(args) > len(self.vars):\n        raise ValueError('Too many arguments for \"%s\". Expected %d '\n                         '(got %d)' % (args[0], len(self.vars), len(args) - len(self.vars)))\n", "def validate_length_args(self, args):\n    required_args = [arg for arg in args if arg['required']]\n    if len(required_args) == 0:\n        self._print_help('')\n        return None\n    elif len(required_args) > 1:\n        self._print_help('')\n        return None\n    return required_args[0]\n", "def validate_length_args(self, args):\n    if len(args) > 1:\n        raise InvalidArgumentException(args, 'Too many arguments.')\n\n    return args[0]\n", "def validate_length_args(self, args):\n    arg_spec = inspect.getfullargspec(args)\n    if arg_spec.varargs is None:\n        if arg_spec.keywords is None:\n            if arg_spec.defaults is None:\n                raise CliArgumentError('Either a single arg or a var arg must be provided.')\n            else:\n                return args\n        else:\n            raise CliArgumentError('Var args not provided. Please provide at least one arg.')\n    else:\n        return args\n", "def validate_length_args(self, args):\n    if len(args)!= len(self.vars):\n        raise AnsibleFilterError(\"{} requires {} arguments\".format(\n            args, len(self.vars)\n        ))\n\n    return args\n", "def validate_length_args(self, args):\n    if len(args)!= self.vars['num_vars']:\n        raise InvalidArguments('Invalid number of arguments!')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_length_args(self, args):\n    if len(args) < 2:\n        raise CommandArgumentException(\"This command takes at least two arguments.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args)!= len(self.vars):\n        raise AnsibleFilterError(\"Incorrect number of variables provided \"\n                                \"for command %s\" % args)\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    return self._validate_length_args(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    for arg in args:\n        if len(args[arg]) > self.vars[\"max_args\"]:\n            raise ArgumentTooLongException(\n                \"Argument '{}' cannot be more than {} \"\n                \"arguments\".format(arg, self.vars[\"max_args\"])\n            )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args) > len(self.vars):\n        raise ValueError('Too many arguments for \"%s\". Expected %d '\n                         '(got %d)' % (args[0], len(self.vars), len(args) - len(self.vars)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    required_args = [arg for arg in args if arg['required']]\n    if len(required_args) == 0:\n        self._print_help('')\n        return None\n    elif len(required_args) > 1:\n        self._print_help('')\n        return None\n    return required_args[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args) > 1:\n        raise InvalidArgumentException(args, 'Too many arguments.')\n\n    return args[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    arg_spec = inspect.getfullargspec(args)\n    if arg_spec.varargs is None:\n        if arg_spec.keywords is None:\n            if arg_spec.defaults is None:\n                raise CliArgumentError('Either a single arg or a var arg must be provided.')\n            else:\n                return args\n        else:\n            raise CliArgumentError('Var args not provided. Please provide at least one arg.')\n    else:\n        return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args)!= len(self.vars):\n        raise AnsibleFilterError(\"{} requires {} arguments\".format(\n            args, len(self.vars)\n        ))\n\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args)!= self.vars['num_vars']:\n        raise InvalidArguments('Invalid number of arguments!')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_choices_args", "docstring": "Check if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad0a", "code": ["def validate_choices_args(self, args):\n    if 'choices' in args:\n        if not isinstance(args['choices'], list):\n            raise InvalidArgumentTypeError('choices', 'list')\n        args['choices'] = ','.join(args['choices'])\n    return args\n", "def validate_choices_args(self, args):\n    for choice, arg_name in args.items():\n        arg_spec = self.vars.get(arg_name)\n        if arg_spec is None:\n            continue\n        if arg_spec.choices and choice not in arg_spec.choices:\n            raise exceptions.CLIAbort(\n                'Must be one of {0}.choices'.format(arg_spec.choices))\n", "def validate_choices_args(self, args):\n    if args['--choices']:\n        if args['--choices'] not in self.vars['CHOICES']:\n            raise CLIError('{0} is not a valid choice for --choices'.format(args['--choices']))\n", "def validate_choices_args(self, args):\n    args = self._merge_duplicated_cli_args(args)\n    return self.validate_arg_deprecation(args)\n", "def validate_choices_args(self, args):\n    if not args:\n        raise InvalidArgumentException('You must specify at least one choice argument!')\n    if len(args) > 1:\n        raise InvalidArgumentException('You can only specify one choice argument!')\n    if args[0] not in self.vars:\n        raise InvalidArgumentException('Unknown choice argument: {}'.format(args[0]))\n", "def validate_choices_args(self, args):\n    choices_args = self.get_spec_defaults().get('choices')\n    if choices_args:\n        for arg in args:\n            if arg not in choices_args:\n                raise ArgumentTypeError('Argument {0} is not a valid choice'.format(arg))\n    return args\n", "def validate_choices_args(self, args):\n    arg_spec = vars(args)\n    arg_spec.update(self.vars)\n    arg_spec.pop('self', None)\n    return arg_spec\n", "def validate_choices_args(self, args):\n    if not isinstance(args, dict):\n        return False\n\n    for arg_name, arg_spec in args.items():\n        if arg_name in self.vars and arg_spec.choices is None:\n            return False\n        if arg_name not in self.vars and arg_spec.choices is not None:\n            return False\n\n    return True\n", "def validate_choices_args(self, args):\n    if args['--choices']:\n        args['--choices'] = args['--choices'].split(',')\n\n    return args\n", "def validate_choices_args(self, args):\n    self._merge_duplicated_cli_args(args)\n    return args\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_choices_args(self, args):\n    if 'choices' in args:\n        if not isinstance(args['choices'], list):\n            raise InvalidArgumentTypeError('choices', 'list')\n        args['choices'] = ','.join(args['choices'])\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    for choice, arg_name in args.items():\n        arg_spec = self.vars.get(arg_name)\n        if arg_spec is None:\n            continue\n        if arg_spec.choices and choice not in arg_spec.choices:\n            raise exceptions.CLIAbort(\n                'Must be one of {0}.choices'.format(arg_spec.choices))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    if args['--choices']:\n        if args['--choices'] not in self.vars['CHOICES']:\n            raise CLIError('{0} is not a valid choice for --choices'.format(args['--choices']))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    args = self._merge_duplicated_cli_args(args)\n    return self.validate_arg_deprecation(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    if not args:\n        raise InvalidArgumentException('You must specify at least one choice argument!')\n    if len(args) > 1:\n        raise InvalidArgumentException('You can only specify one choice argument!')\n    if args[0] not in self.vars:\n        raise InvalidArgumentException('Unknown choice argument: {}'.format(args[0]))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    choices_args = self.get_spec_defaults().get('choices')\n    if choices_args:\n        for arg in args:\n            if arg not in choices_args:\n                raise ArgumentTypeError('Argument {0} is not a valid choice'.format(arg))\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    arg_spec = vars(args)\n    arg_spec.update(self.vars)\n    arg_spec.pop('self', None)\n    return arg_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    if not isinstance(args, dict):\n        return False\n\n    for arg_name, arg_spec in args.items():\n        if arg_name in self.vars and arg_spec.choices is None:\n            return False\n        if arg_name not in self.vars and arg_spec.choices is not None:\n            return False\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    if args['--choices']:\n        args['--choices'] = args['--choices'].split(',')\n\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    self._merge_duplicated_cli_args(args)\n    return args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_min_max_args", "docstring": "Check if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad0b", "code": ["def validate_min_max_args(self, args):\n    args = self._merge_duplicated_cli_args(args)\n    return self.validate_arg_sources(args)\n", "def validate_min_max_args(self, args):\n    min_args = []\n    max_args = []\n    for name, arg in args.items():\n        arg_type = self.vars[name].type\n        if arg_type not in [int, float]:\n            raise InvalidArgumentError(\n                \"Argument {} must be integer or float values\".format(name))\n        if not isinstance(arg, arg_type):\n            raise InvalidArgumentError(\n                \"Argument {} must be a valid type {}\".format(name, arg_type))\n        min_args.append(name)\n        max_args.append(name)\n\n    if len(min_args) > 1:\n        raise InvalidArgumentError(\n            \"Arguments must have at most one named argument\")\n\n    if len(max_args) > 1:\n        raise InvalidArgumentError(\n            \"Arguments must have at most one named argument\")\n", "def validate_min_max_args(self, args):\n    if len(args) < 1:\n        raise InvalidArgumentException('Arguments provided to \"min\" or \"max\" command must be at least one argument')\n    if args[0] not in ['min','max']:\n        raise InvalidArgumentException('Argument \"{}\" is not a valid argument for \"min\" or \"max\" command'.format(args[0]))\n", "def validate_min_max_args(self, args):\n    arg_spec = vars(args)\n    if arg_spec['type'] == 'int':\n        if arg_spec['min'] is not None and arg_spec['max'] is not None:\n            raise ArgumentError(\n                'Both --type int and --min and --max cannot be specified together')\n    elif arg_spec['type'] =='str':\n        if arg_spec['min'] is not None and arg_spec['max'] is not None:\n            raise ArgumentError(\n                'Both --type str and --min and --max cannot be specified together')\n    return args\n", "def validate_min_max_args(self, args):\n    validate_min_max_args(args, self.vars, self.vars_dir)\n", "def validate_min_max_args(self, args):\n    if not args:\n        raise NoArgumentsError('No arguments were provided.')\n    min_args = self.parser._get_arg_count(args[0])\n    max_args = min_args + len(args) - 1\n    if min_args > max_args:\n        raise ArgumentCountError('Arguments count %s is greater than the min '\n                                 'arguments count %s' %\n                                 (min_args, max_args))\n", "def validate_min_max_args(self, args):\n    if len(args) < 2:\n        raise InvalidArgumentException(\n            'Not enough arguments provided to function \"{}\"'.format(\n                self.__name__))\n    elif args[0] not in self.vars:\n        raise InvalidArgumentException(\n            'Unknown variable \"{}\". Choose from {}.'.format(\n                args[0], ', '.join(self.vars)))\n", "def validate_min_max_args(self, args):\n    return self._validate_min_max_args(args,'min','max')\n", "def validate_min_max_args(self, args):\n    args = super(CLIArgumentParser, self).validate_min_max_args(args)\n    if args['--min'] > args['--max']:\n        msg = 'Minimum argument must be greater or equal than maximum argument'\n        raise ArgumentTypeError(msg)\n    return args\n", "def validate_min_max_args(self, args):\n    min_args, max_args = self._get_conditionally_required_args('min', args)\n    if min_args > max_args:\n        msg = 'Argument {args} must be between {min_args} and {max_args}'\n        raise CLIError(msg.format(args=args, min_args=min_args, max_args=max_args))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_min_max_args(self, args):\n    args = self._merge_duplicated_cli_args(args)\n    return self.validate_arg_sources(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    min_args = []\n    max_args = []\n    for name, arg in args.items():\n        arg_type = self.vars[name].type\n        if arg_type not in [int, float]:\n            raise InvalidArgumentError(\n                \"Argument {} must be integer or float values\".format(name))\n        if not isinstance(arg, arg_type):\n            raise InvalidArgumentError(\n                \"Argument {} must be a valid type {}\".format(name, arg_type))\n        min_args.append(name)\n        max_args.append(name)\n\n    if len(min_args) > 1:\n        raise InvalidArgumentError(\n            \"Arguments must have at most one named argument\")\n\n    if len(max_args) > 1:\n        raise InvalidArgumentError(\n            \"Arguments must have at most one named argument\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    if len(args) < 1:\n        raise InvalidArgumentException('Arguments provided to \"min\" or \"max\" command must be at least one argument')\n    if args[0] not in ['min','max']:\n        raise InvalidArgumentException('Argument \"{}\" is not a valid argument for \"min\" or \"max\" command'.format(args[0]))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    arg_spec = vars(args)\n    if arg_spec['type'] == 'int':\n        if arg_spec['min'] is not None and arg_spec['max'] is not None:\n            raise ArgumentError(\n                'Both --type int and --min and --max cannot be specified together')\n    elif arg_spec['type'] =='str':\n        if arg_spec['min'] is not None and arg_spec['max'] is not None:\n            raise ArgumentError(\n                'Both --type str and --min and --max cannot be specified together')\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    validate_min_max_args(args, self.vars, self.vars_dir)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    if not args:\n        raise NoArgumentsError('No arguments were provided.')\n    min_args = self.parser._get_arg_count(args[0])\n    max_args = min_args + len(args) - 1\n    if min_args > max_args:\n        raise ArgumentCountError('Arguments count %s is greater than the min '\n                                 'arguments count %s' %\n                                 (min_args, max_args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    if len(args) < 2:\n        raise InvalidArgumentException(\n            'Not enough arguments provided to function \"{}\"'.format(\n                self.__name__))\n    elif args[0] not in self.vars:\n        raise InvalidArgumentException(\n            'Unknown variable \"{}\". Choose from {}.'.format(\n                args[0], ', '.join(self.vars)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    return self._validate_min_max_args(args,'min','max')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    args = super(CLIArgumentParser, self).validate_min_max_args(args)\n    if args['--min'] > args['--max']:\n        msg = 'Minimum argument must be greater or equal than maximum argument'\n        raise ArgumentTypeError(msg)\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    min_args, max_args = self._get_conditionally_required_args('min', args)\n    if min_args > max_args:\n        msg = 'Argument {args} must be between {min_args} and {max_args}'\n        raise CLIError(msg.format(args=args, min_args=min_args, max_args=max_args))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "create_complex_argumet_type", "docstring": "Build the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance", "ques_id": "6306092e73426c38ae68ad0d", "code": ["def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    :param subcommand: command name\n    :param type_name: type name\n    :param option_name: option name\n    :param spec_option: spec option value\n    \"\"\"\n    if subcommand not in self.vars:\n        self.vars[subcommand] = dict()\n    if type_name not in self.vars[subcommand]:\n        self.vars[subcommand][type_name] = dict()\n    if option_name not in self.vars[subcommand][type_name]:\n        self.vars[subcommand][type_name][option_name] = dict()\n    self.vars[subcommand][type_name][option_name][spec_option] = True\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type.\n    \"\"\"\n    if subcommand == \"init\":\n        return InitArgument(option_name, spec_option)\n\n    return Argument(type_name, option_name, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type for a command\n    \"\"\"\n    spec_option = spec_option or '--{0}'.format(option_name)\n\n    def decorator(func):\n        argument_type = ArgumentType(\n            func.__name__, spec_option, argparse.SUPPRESS,\n            func, option_name, spec_option)\n        self._merge_duplicated_cli_args([argument_type])\n        self._merge_duplicated_cli_args([args for args in\n                                         argument_type.args])\n        return func\n\n    return decorator\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type\n    \"\"\"\n    def _parse_value(value):\n        value = value.strip()\n        if not value:\n            return None\n        return value\n\n    if isinstance(spec_option, list):\n        spec_option = [_parse_value(item) for item in spec_option]\n    elif isinstance(spec_option, tuple):\n        spec_option = tuple(_parse_value(item) for item in spec_option)\n    elif isinstance(spec_option, dict):\n        spec_option = {_parse_value(key): _parse_value(value)\n                       for key, value in spec_option.items()}\n    else:\n        spec_option = _parse_value(spec_option)\n\n    return ComplexArgumentType(subcommand, type_name, option_name, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    This method creates a complex argument type that is used to specify the\n    type of the variable that is required by the command. It is only valid for\n    commands that require arguments.\n    \"\"\"\n    spec_option = spec_option.lower()\n    if spec_option not in self.defaults:\n        raise CliArgumentTypeError('Unknown variable \"%s\" specified in '\n                                   '\"%s\" subcommand. Options are [%s]' %\n                                   (option_name, subcommand,\n                                    ', '.join(self.defaults.keys())))\n\n    def required_type(arg_parser, args):\n        return self.defaults[spec_option]\n\n    return required_type\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type.\n    \"\"\"\n    description = \"%s argument type\" % type_name\n    parser = argparse.ArgumentParser(\n        description=description,\n        add_help=False,\n        )\n    parser.add_argument(\n        '--%s' % option_name,\n        metavar='PATH',\n        type=argparse.FileType('r'),\n        default='',\n        help='path to the configuration file',\n        )\n    parser.add_argument(\n        '--%s-type' % option_name,\n        metavar='TYPE',\n        choices=self.vars[type_name].choices,\n        type=argparse.FileType('r'),\n        default='',\n        help='type of the argument (one of %s)' % (\n            ', '.join(self.vars[type_name].choices),\n        ),\n        )\n    return parser.parse_args(arg_parser=args)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    This method is used to create a complex argument type\n    \"\"\"\n    if isinstance(option_name, list):\n        option_name = ','.join(option_name)\n\n    if isinstance(spec_option, list):\n        spec_option = ','.join(spec_option)\n\n    if isinstance(subcommand, list):\n        subcommand = ','.join(subcommand)\n\n    return \"{subcommand} {option} {spec_option}\".format(\n        subcommand=subcommand,\n        option=option_name,\n        spec_option=spec_option\n    )\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type.\n    \"\"\"\n    deprecation_warning_message = (\n        \"The 'create_complex_argumet_type' command is deprecated. \"\n        \"Use 'create_argument_type' instead.\")\n    deprecation_warning_message += (\n        \"This command has been renamed to 'create_argument_type'.\")\n    deprecation_warning_message += (\n        \"For more information please see %s\" % deprecation_warning_message)\n\n    def decorator(func):\n        self.create_argument_type(subcommand, type_name, option_name, spec_option, func)\n        return func\n    return decorator\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type\n    \"\"\"\n    if subcommand in self.vars:\n        raise Exception('duplicate command %s' % subcommand)\n    self.vars[subcommand] = {}\n    self.vars[subcommand][type_name] = {}\n    self.vars[subcommand][type_name][option_name] = spec_option\n    return self.vars[subcommand][type_name][option_name]\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a new argument type for a command\n    \"\"\"\n    def argument_type(self, *args, **kwargs):\n        if args and args[0]:\n            raise InvalidCommandError('Unexpected argument count '\n                                       'for %s command' % subcommand)\n        return self._get_conditionally_required_args(subcommand, option_name,\n                                                            args)\n    return argument_type\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    :param subcommand: command name\n    :param type_name: type name\n    :param option_name: option name\n    :param spec_option: spec option value\n    \"\"\"\n    if subcommand not in self.vars:\n        self.vars[subcommand] = dict()\n    if type_name not in self.vars[subcommand]:\n        self.vars[subcommand][type_name] = dict()\n    if option_name not in self.vars[subcommand][type_name]:\n        self.vars[subcommand][type_name][option_name] = dict()\n    self.vars[subcommand][type_name][option_name][spec_option] = True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type.\n    \"\"\"\n    if subcommand == \"init\":\n        return InitArgument(option_name, spec_option)\n\n    return Argument(type_name, option_name, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type for a command\n    \"\"\"\n    spec_option = spec_option or '--{0}'.format(option_name)\n\n    def decorator(func):\n        argument_type = ArgumentType(\n            func.__name__, spec_option, argparse.SUPPRESS,\n            func, option_name, spec_option)\n        self._merge_duplicated_cli_args([argument_type])\n        self._merge_duplicated_cli_args([args for args in\n                                         argument_type.args])\n        return func\n\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type\n    \"\"\"\n    def _parse_value(value):\n        value = value.strip()\n        if not value:\n            return None\n        return value\n\n    if isinstance(spec_option, list):\n        spec_option = [_parse_value(item) for item in spec_option]\n    elif isinstance(spec_option, tuple):\n        spec_option = tuple(_parse_value(item) for item in spec_option)\n    elif isinstance(spec_option, dict):\n        spec_option = {_parse_value(key): _parse_value(value)\n                       for key, value in spec_option.items()}\n    else:\n        spec_option = _parse_value(spec_option)\n\n    return ComplexArgumentType(subcommand, type_name, option_name, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    This method creates a complex argument type that is used to specify the\n    type of the variable that is required by the command. It is only valid for\n    commands that require arguments.\n    \"\"\"\n    spec_option = spec_option.lower()\n    if spec_option not in self.defaults:\n        raise CliArgumentTypeError('Unknown variable \"%s\" specified in '\n                                   '\"%s\" subcommand. Options are [%s]' %\n                                   (option_name, subcommand,\n                                    ', '.join(self.defaults.keys())))\n\n    def required_type(arg_parser, args):\n        return self.defaults[spec_option]\n\n    return required_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type.\n    \"\"\"\n    description = \"%s argument type\" % type_name\n    parser = argparse.ArgumentParser(\n        description=description,\n        add_help=False,\n        )\n    parser.add_argument(\n        '--%s' % option_name,\n        metavar='PATH',\n        type=argparse.FileType('r'),\n        default='',\n        help='path to the configuration file',\n        )\n    parser.add_argument(\n        '--%s-type' % option_name,\n        metavar='TYPE',\n        choices=self.vars[type_name].choices,\n        type=argparse.FileType('r'),\n        default='',\n        help='type of the argument (one of %s)' % (\n            ', '.join(self.vars[type_name].choices),\n        ),\n        )\n    return parser.parse_args(arg_parser=args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    This method is used to create a complex argument type\n    \"\"\"\n    if isinstance(option_name, list):\n        option_name = ','.join(option_name)\n\n    if isinstance(spec_option, list):\n        spec_option = ','.join(spec_option)\n\n    if isinstance(subcommand, list):\n        subcommand = ','.join(subcommand)\n\n    return \"{subcommand} {option} {spec_option}\".format(\n        subcommand=subcommand,\n        option=option_name,\n        spec_option=spec_option\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type.\n    \"\"\"\n    deprecation_warning_message = (\n        \"The 'create_complex_argumet_type' command is deprecated. \"\n        \"Use 'create_argument_type' instead.\")\n    deprecation_warning_message += (\n        \"This command has been renamed to 'create_argument_type'.\")\n    deprecation_warning_message += (\n        \"For more information please see %s\" % deprecation_warning_message)\n\n    def decorator(func):\n        self.create_argument_type(subcommand, type_name, option_name, spec_option, func)\n        return func\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a complex argument type\n    \"\"\"\n    if subcommand in self.vars:\n        raise Exception('duplicate command %s' % subcommand)\n    self.vars[subcommand] = {}\n    self.vars[subcommand][type_name] = {}\n    self.vars[subcommand][type_name][option_name] = spec_option\n    return self.vars[subcommand][type_name][option_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Create a new argument type for a command\n    \"\"\"\n    def argument_type(self, *args, **kwargs):\n        if args and args[0]:\n            raise InvalidCommandError('Unexpected argument count '\n                                       'for %s command' % subcommand)\n        return self._get_conditionally_required_args(subcommand, option_name,\n                                                            args)\n    return argument_type\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_nested_custom_and_control_args", "docstring": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)", "ques_id": "6306092e73426c38ae68ad0f", "code": ["def get_nested_custom_and_control_args(self, args):\n    if args.nested_args is not None:\n        self.nested_args = args.nested_args\n        LOG.debug(\"nested args: %s\", self.nested_args)\n        return True\n\n    return False\n", "def get_nested_custom_and_control_args(self, args):\n    return [args.pop(0)] + self._merge_duplicated_cli_args(args)\n", "def get_nested_custom_and_control_args(self, args):\n\n    nested_args = collections.defaultdict(list)\n    for arg in args:\n        if arg in self.vars:\n            nested_args[arg].append(arg)\n        elif arg in self.defaults:\n            nested_args[arg].append(arg)\n    return nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    return self._iterate_received_arguments(args)\n", "def get_nested_custom_and_control_args(self, args):\n    return self._iterate_received_arguments(args)\n", "def get_nested_custom_and_control_args(self, args):\n    nested_args = dict()\n    for arg in args:\n        if arg.startswith('--') and arg.endswith('='):\n            nested_args[arg[2:-1]] = args[arg]\n        else:\n            nested_args[arg] = args[arg]\n\n    return nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    args = super(InfraredCLI, self).get_nested_custom_and_control_args(args)\n    args['cli_args'] = self.parser.parse_args(args['cli_args'])\n    args['args'] = self._merge_duplicated_cli_args(args['args'])\n    return args\n", "def get_nested_custom_and_control_args(self, args):\n    arg_list = []\n    for arg in args:\n        if arg.startswith('--'):\n            arg_list.append(arg)\n        elif arg in self.vars:\n            arg_list.extend(['--', arg])\n        else:\n            LOG.debug('Unrecognized argument %s', arg)\n    return arg_list\n", "def get_nested_custom_and_control_args(self, args):\n\n    def get_nested_custom_args(args):\n        return self._get_conditionally_required_args(args)\n\n    nested_args = self.parser.parse_known_args(args)\n    nested_args = get_nested_custom_args(nested_args)\n\n    return nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    return self._get_conditionally_required_args(args, 'c')\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nested_custom_and_control_args(self, args):\n    if args.nested_args is not None:\n        self.nested_args = args.nested_args\n        LOG.debug(\"nested args: %s\", self.nested_args)\n        return True\n\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    return [args.pop(0)] + self._merge_duplicated_cli_args(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\n    nested_args = collections.defaultdict(list)\n    for arg in args:\n        if arg in self.vars:\n            nested_args[arg].append(arg)\n        elif arg in self.defaults:\n            nested_args[arg].append(arg)\n    return nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    return self._iterate_received_arguments(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    return self._iterate_received_arguments(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    nested_args = dict()\n    for arg in args:\n        if arg.startswith('--') and arg.endswith('='):\n            nested_args[arg[2:-1]] = args[arg]\n        else:\n            nested_args[arg] = args[arg]\n\n    return nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    args = super(InfraredCLI, self).get_nested_custom_and_control_args(args)\n    args['cli_args'] = self.parser.parse_args(args['cli_args'])\n    args['args'] = self._merge_duplicated_cli_args(args['args'])\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    arg_list = []\n    for arg in args:\n        if arg.startswith('--'):\n            arg_list.append(arg)\n        elif arg in self.vars:\n            arg_list.extend(['--', arg])\n        else:\n            LOG.debug('Unrecognized argument %s', arg)\n    return arg_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\n    def get_nested_custom_args(args):\n        return self._get_conditionally_required_args(args)\n\n    nested_args = self.parser.parse_known_args(args)\n    nested_args = get_nested_custom_args(nested_args)\n\n    return nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    return self._get_conditionally_required_args(args, 'c')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/settings.py", "project": "redhat-openstack/infrared", "name": "merge_extra_vars", "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars", "ques_id": "6306092e73426c38ae68ad11", "code": ["def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if not vars_dict:\n        return vars_dict\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = None\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    vars_dict.update(extra_vars)\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    vars_dict.update(extra_vars)\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(vars_dict, dict):\n        for key, value in vars_dict.items():\n            if key not in extra_vars:\n                vars_dict[key] = value\n            else:\n                extra_vars.append(key)\n\n    return dict(vars_dict, **extra_vars)\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    vars_dict.update(extra_vars)\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if '_extra' not in vars_dict:\n        vars_dict['_extra'] = extra_vars\n    else:\n        vars_dict['_extra'].extend(extra_vars)\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(vars_dict, dict):\n        for key, value in vars_dict.items():\n            if key not in extra_vars:\n                vars_dict[key] = value\n            else:\n                extra_vars.extend(value)\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, str):\n        extra_vars = [extra_vars]\n\n    vars_dict.update(dict(extra_vars))\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        return vars_dict\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict.update(extra_var)\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    vars_dict.update(extra_vars)\n    return vars_dict\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if not vars_dict:\n        return vars_dict\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = None\n\n    return vars_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    vars_dict.update(extra_vars)\n    return vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    vars_dict.update(extra_vars)\n    return vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(vars_dict, dict):\n        for key, value in vars_dict.items():\n            if key not in extra_vars:\n                vars_dict[key] = value\n            else:\n                extra_vars.append(key)\n\n    return dict(vars_dict, **extra_vars)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    vars_dict.update(extra_vars)\n    return vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if '_extra' not in vars_dict:\n        vars_dict['_extra'] = extra_vars\n    else:\n        vars_dict['_extra'].extend(extra_vars)\n\n    return vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(vars_dict, dict):\n        for key, value in vars_dict.items():\n            if key not in extra_vars:\n                vars_dict[key] = value\n            else:\n                extra_vars.extend(value)\n\n    return vars_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, str):\n        extra_vars = [extra_vars]\n\n    vars_dict.update(dict(extra_vars))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        return vars_dict\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict.update(extra_var)\n\n    return vars_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    vars_dict.update(extra_vars)\n    return vars_dict\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "ansible_playbook", "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.", "ques_id": "6306092f73426c38ae68ad13", "code": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n\n    # Init variables\n    ir_workspace = ir_workspace\n    ir_plugin = ir_plugin\n    playbook_path = playbook_path\n    verbose = verbose or ir_plugin.get_verbosity()\n    extra_vars = extra_vars or ir_plugin.get_ansible_extra_vars()\n    ansible_args = ansible_args or ir_plugin.get_ansible_args()\n\n    # Run playbook\n    LOG.debug('Executing playbook %s in %s with args %s',\n              playbook_path, ir_workspace.name, ansible_args)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # FIXME: pass extra_vars to the infrared plugin\n    ir_workspace.ansible_logger.log(verbose, ir_workspace.ansible_vars)\n    return _run_playbook(\n        ir_workspace, ir_plugin, playbook_path, verbose, extra_vars, ansible_args)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible\n    \"\"\"\n    try:\n        return _run_playbook(\n            ir_workspace, ir_plugin, playbook_path, verbose,\n            extra_vars=extra_vars, ansible_args=ansible_args)\n    except Exception as e:\n        LOG.error('An error occurred while running the ansible-playbook: %s', e)\n        if ansible_args:\n            print('ANSIBLE_ARGS=%s' % ansible_args)\n        raise\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # set the verbosity and extra_vars to be passed to Ansible\n    _run_playbook(cli_args=extra_vars, vars_dict=extra_vars,\n                  ir_workspace=ir_workspace, ir_plugin=ir_plugin,\n                  verbose=verbose, playbook_path=playbook_path,\n                  ansible_args=ansible_args)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    ir_playbook_path = os.path.join(ir_workspace.ir_path, playbook_path)\n    ansible_args = ansible_args or {}\n\n    LOG.info(\"Running ansible-playbook %s on %s\", playbook_path, ir_workspace)\n    return _run_playbook(ir_workspace, ir_plugin, playbook_path, verbose,\n                         extra_vars, ansible_args)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible\n    \"\"\"\n    # Get the plugin's current workspace\n    ir_workspace = workspace.get_workspace(ir_workspace)\n\n    # Run the playbook\n    LOG.debug('Running ansible-playbook %s', playbook_path)\n    return _run_playbook(\n        ir_workspace,\n        ir_plugin,\n        playbook_path,\n        verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible\n    \"\"\"\n    playbook = ir_workspace.get_playbook(playbook_path)\n    if playbook:\n        return _run_playbook(ir_workspace, ir_plugin, playbook, verbose,\n                             extra_vars, ansible_args)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                        directly to Ansible.\n    \"\"\"\n    _run_playbook(cli_args, vars_dict=extra_vars, ir_workspace=ir_workspace,\n                 ir_plugin=ir_plugin, playbook_path=playbook_path,\n                 verbose=verbose, extra_vars=extra_vars, ansible_args=ansible_args)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    ansible_args = ansible_args or {}\n    args = dict(vars(ir_args) for ir_args in extra_vars.items())\n    args.update(vars(ir_args))\n\n    ansible_args.update({\n        'ir_workspace': ir_workspace,\n        'ir_plugin': ir_plugin,\n        'playbook_path': playbook_path,\n    })\n    LOG.debug(\"Ansible args: %s\", args)\n\n    LOG.debug(\"Calling 'ansible-playbook' with args: %s\", args)\n\n    try:\n   ", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    playbook_name = os.path.basename(playbook_path)\n    ansible_args = ansible_args or {}\n    ansible_args.update({'ANSIBLE_RUN_playbook': playbook_name})\n    if verbose is not None:\n        ansible_args.update({'verbosity': verbose})\n    if extra_vars is not None:\n        ansible_args.update(extra_vars)\n    LOG.debug('Invoking %s', ansible_args)\n    _run_playbook(ir_workspace, ir_plugin, playbook_path, ir_plugin, **ansible_args)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n\n    # Init variables\n    ir_workspace = ir_workspace\n    ir_plugin = ir_plugin\n    playbook_path = playbook_path\n    verbose = verbose or ir_plugin.get_verbosity()\n    extra_vars = extra_vars or ir_plugin.get_ansible_extra_vars()\n    ansible_args = ansible_args or ir_plugin.get_ansible_args()\n\n    # Run playbook\n    LOG.debug('Executing playbook %s in %s with args %s',\n              playbook_path, ir_workspace.name, ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # FIXME: pass extra_vars to the infrared plugin\n    ir_workspace.ansible_logger.log(verbose, ir_workspace.ansible_vars)\n    return _run_playbook(\n        ir_workspace, ir_plugin, playbook_path, verbose, extra_vars, ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible\n    \"\"\"\n    try:\n        return _run_playbook(\n            ir_workspace, ir_plugin, playbook_path, verbose,\n            extra_vars=extra_vars, ansible_args=ansible_args)\n    except Exception as e:\n        LOG.error('An error occurred while running the ansible-playbook: %s', e)\n        if ansible_args:\n            print('ANSIBLE_ARGS=%s' % ansible_args)\n        raise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # set the verbosity and extra_vars to be passed to Ansible\n    _run_playbook(cli_args=extra_vars, vars_dict=extra_vars,\n                  ir_workspace=ir_workspace, ir_plugin=ir_plugin,\n                  verbose=verbose, playbook_path=playbook_path,\n                  ansible_args=ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    ir_playbook_path = os.path.join(ir_workspace.ir_path, playbook_path)\n    ansible_args = ansible_args or {}\n\n    LOG.info(\"Running ansible-playbook %s on %s\", playbook_path, ir_workspace)\n    return _run_playbook(ir_workspace, ir_plugin, playbook_path, verbose,\n                         extra_vars, ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible\n    \"\"\"\n    # Get the plugin's current workspace\n    ir_workspace = workspace.get_workspace(ir_workspace)\n\n    # Run the playbook\n    LOG.debug('Running ansible-playbook %s', playbook_path)\n    return _run_playbook(\n        ir_workspace,\n        ir_plugin,\n        playbook_path,\n        verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible\n    \"\"\"\n    playbook = ir_workspace.get_playbook(playbook_path)\n    if playbook:\n        return _run_playbook(ir_workspace, ir_plugin, playbook, verbose,\n                             extra_vars, ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                        directly to Ansible.\n    \"\"\"\n    _run_playbook(cli_args, vars_dict=extra_vars, ir_workspace=ir_workspace,\n                 ir_plugin=ir_plugin, playbook_path=playbook_path,\n                 verbose=verbose, extra_vars=extra_vars, ansible_args=ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    ansible_args = ansible_args or {}\n    args = dict(vars(ir_args) for ir_args in extra_vars.items())\n    args.update(vars(ir_args))\n\n    ansible_args.update({\n        'ir_workspace': ir_workspace,\n        'ir_plugin': ir_plugin,\n        'playbook_path': playbook_path,\n    })\n    LOG.debug(\"Ansible args: %s\", args)\n\n    LOG.debug(\"Calling 'ansible-playbook' with args: %s\", args)\n\n    try:\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    playbook_name = os.path.basename(playbook_path)\n    ansible_args = ansible_args or {}\n    ansible_args.update({'ANSIBLE_RUN_playbook': playbook_name})\n    if verbose is not None:\n        ansible_args.update({'verbosity': verbose})\n    if extra_vars is not None:\n        ansible_args.update(extra_vars)\n    LOG.debug('Invoking %s', ansible_args)\n    _run_playbook(ir_workspace, ir_plugin, playbook_path, ir_plugin, **ansible_args)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "_run_playbook", "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results", "ques_id": "6306093273426c38ae68ad15", "code": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object representing the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: an ansible result\n    \"\"\"\n    ansible_runner = Runner(ir_workspace, ir_plugin, cli_args)\n    return ansible_runner.run_playbook(cli_args, vars_dict)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # pylint: disable=unused-argument\n    return ir_plugin.main(cli_args)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object representing the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: an ansible result\n    \"\"\"\n    try:\n        ansible_runner = Runner(cli_args=cli_args, workspace=ir_workspace, plugin=ir_plugin)\n        result = ansible_runner.run(vars_dict=vars_dict)\n        return result\n    except Exception as e:\n        print('Error when running the ansible cli: {}'.format(str(e)))\n        return {'failed': True}\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_plugin.get_vars(cli_args))\n\n    return run_ansible_playbook(cli_args, ansible_vars, ir_workspace, ir_plugin)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object represnting the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_workspace.vars)\n    ansible_vars.update(ir_plugin.vars)\n    ansible_result = AnsibleRunner(cli_args, ir_workspace, ir_plugin, vars_dict).run()\n    return ansible_result\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_workspace.ansible_vars)\n    ansible_vars.update(ir_plugin.ansible_vars)\n    ansible_args = ['-i', cli_args, '--extra-vars={}'.format(json.dumps(ansible_vars))]\n    result = runner.invoke(cli_plugins.ansible, args=ansible_args)\n    return result.exit_code == 0\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    if ir_workspace is None:\n        raise exceptions.MissingWorkspaceException()\n    if ir_plugin is None:\n        raise exceptions.MissingPluginException()\n\n    results = {}\n    ir_plugin.run(cli_args, vars_dict, ir_workspace)\n    results['ansible_facts'] = ir_workspace.ansible_facts\n    results['ir_facts'] = ir_facts.ir_facts\n    results['ir_vars'] = vars_dict\n    results['ir_plugin'] = ir_plugin\n\n    return results\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # pylint: disable=unused-argument\n    return ir_workspace.infrared_plugin.main(cli_args=cli_args, ir_plugin=ir_plugin)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_plugin.get_vars())\n    return cli_args, run_inventory(cli_args, ir_workspace, ansible_vars)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object representing the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # TODO handle vars_dict and ir_workspace in infrared plugin config\n    return ir_workspace.ansible.run_playbook(cli_args=cli_args, vars_dict=vars_dict, ir_plugin=ir_plugin)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object representing the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: an ansible result\n    \"\"\"\n    ansible_runner = Runner(ir_workspace, ir_plugin, cli_args)\n    return ansible_runner.run_playbook(cli_args, vars_dict)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # pylint: disable=unused-argument\n    return ir_plugin.main(cli_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object representing the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: an ansible result\n    \"\"\"\n    try:\n        ansible_runner = Runner(cli_args=cli_args, workspace=ir_workspace, plugin=ir_plugin)\n        result = ansible_runner.run(vars_dict=vars_dict)\n        return result\n    except Exception as e:\n        print('Error when running the ansible cli: {}'.format(str(e)))\n        return {'failed': True}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_plugin.get_vars(cli_args))\n\n    return run_ansible_playbook(cli_args, ansible_vars, ir_workspace, ir_plugin)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object represnting the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_workspace.vars)\n    ansible_vars.update(ir_plugin.vars)\n    ansible_result = AnsibleRunner(cli_args, ir_workspace, ir_plugin, vars_dict).run()\n    return ansible_result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_workspace.ansible_vars)\n    ansible_vars.update(ir_plugin.ansible_vars)\n    ansible_args = ['-i', cli_args, '--extra-vars={}'.format(json.dumps(ansible_vars))]\n    result = runner.invoke(cli_plugins.ansible, args=ansible_args)\n    return result.exit_code == 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    if ir_workspace is None:\n        raise exceptions.MissingWorkspaceException()\n    if ir_plugin is None:\n        raise exceptions.MissingPluginException()\n\n    results = {}\n    ir_plugin.run(cli_args, vars_dict, ir_workspace)\n    results['ansible_facts'] = ir_workspace.ansible_facts\n    results['ir_facts'] = ir_facts.ir_facts\n    results['ir_vars'] = vars_dict\n    results['ir_plugin'] = ir_plugin\n\n    return results\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # pylint: disable=unused-argument\n    return ir_workspace.infrared_plugin.main(cli_args=cli_args, ir_plugin=ir_plugin)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_plugin.get_vars())\n    return cli_args, run_inventory(cli_args, ir_workspace, ansible_vars)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object representing the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # TODO handle vars_dict and ir_workspace in infrared plugin config\n    return ir_workspace.ansible.run_playbook(cli_args=cli_args, vars_dict=vars_dict, ir_plugin=ir_plugin)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_convert_non_cli_args", "docstring": "Casts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments", "ques_id": "63060ada73426c38ae68ad31", "code": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if isinstance(value, list):\n            values_dict[key] = ','.join(value)\n        elif isinstance(value, dict):\n            values_dict[key] = ','.join(\n                ['{0}={1}'.format(key, self._convert_non_cli_args(parser_name, value)) for value in value.values()]\n            )\n    return values_dict\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_value in six.iteritems(values_dict):\n        if isinstance(arg_value, list):\n            values_dict[arg_name] = ','.join(arg_value)\n        elif isinstance(arg_value, dict):\n            values_dict[arg_name] = ','.join([\n                '{0}={1}'.format(arg_name, arg_value[arg_name])\n                for arg_name in self._merge_duplicated_cli_args(args=arg_value)\n            ])\n    return values_dict\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    new_values_dict = {}\n    for arg_name, arg_value in values_dict.items():\n        if arg_name in self.parser._get_required_args():\n            arg_value = self.parser._get_required_args()[arg_name]\n        if arg_name in self.parser._get_deprecated_args():\n            arg_value = self.parser._get_deprecated_args()[arg_name]\n        new_values_dict[arg_name] = arg_value\n    return new_values_dict\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_value in values_dict.items():\n        if not arg_name.startswith('--'):\n            arg_name = parser_name + '_' + arg_name\n        if isinstance(arg_value, list):\n            values_dict[arg_name] = arg_value[0]\n        elif isinstance(arg_value, dict):\n            values_dict[arg_name] = self._convert_non_cli_args(parser_name, arg_value)\n    return values_dict\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg in parser_name._short_opts:\n        if arg.dest in values_dict:\n            parser_name._short_opts[arg.dest] = values_dict[arg.dest]\n    for arg in parser_name._long_opts:\n        if arg.dest in values_dict:\n            parser_name._long_opts[arg.dest] = values_dict[arg.dest]\n\n    for arg in parser_name._optional_opts:\n        if arg.dest in values_dict:\n            parser_name._optional_opts[arg.dest] = values_dict[arg.dest]\n\n    for arg in parser_name._positional_opts:\n        if arg.dest in values_dict:\n            parser_name._positional_opts[arg.dest] = values_dict[arg.dest]\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name in values_dict:\n        if not arg_name.startswith(parser_name + '_'):\n            continue\n        return arg_name.replace(parser_name + '_', '', 1)\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_type in values_dict.items():\n        if arg_type is None:\n            continue\n\n        value = args.get(arg_name)\n        if value is None:\n            continue\n\n        if arg_type == \"int\":\n            value = int(value)\n        elif arg_type == \"float\":\n            value = float(value)\n        elif arg_type == \"boolean\":\n            value = value.lower() in [\"true\", \"1\"]\n\n        values_dict[arg_name] = value\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    if parser_name in self.vars:\n        for arg in values_dict:\n            if arg in self.vars[parser_name]:\n                values_dict[arg] = self.vars[parser_name][arg]\n\n    return values_dict\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_type in values_dict.items():\n        if arg_name not in self.vars:\n            continue\n        if arg_type.__name__ == 'IntArg':\n            values_dict[arg_name] = int(arg_type.convert(arg_type.default))\n        elif arg_type.__name__ == 'FloatArg':\n            values_dict[arg_name] = float(arg_type.convert(arg_type.default))\n        elif arg_type.__name__ == 'CommaSeparatedListArg':\n            values_dict[arg_name] = [item.strip() for item in arg_type.convert(arg_type.default).split(',')]\n    return values_dict\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_spec in iteritems(values_dict):\n        if arg_spec.converter:\n            converter_name = arg_spec.converter\n            if converter_name not in self.vars.keys():\n                raise exceptions.ArgumentParserError(\n                    '{}: {} argument not found. Converter name must be one of {}'.format(\n                        parser_name, arg_name, self.vars.keys()\n                    )\n                )\n            values_dict[arg_name] = self.vars[converter_name](\n                arg_spec.format_value(arg_spec.parser, arg_spec.default)\n            )\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if isinstance(value, list):\n            values_dict[key] = ','.join(value)\n        elif isinstance(value, dict):\n            values_dict[key] = ','.join(\n                ['{0}={1}'.format(key, self._convert_non_cli_args(parser_name, value)) for value in value.values()]\n            )\n    return values_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_value in six.iteritems(values_dict):\n        if isinstance(arg_value, list):\n            values_dict[arg_name] = ','.join(arg_value)\n        elif isinstance(arg_value, dict):\n            values_dict[arg_name] = ','.join([\n                '{0}={1}'.format(arg_name, arg_value[arg_name])\n                for arg_name in self._merge_duplicated_cli_args(args=arg_value)\n            ])\n    return values_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    new_values_dict = {}\n    for arg_name, arg_value in values_dict.items():\n        if arg_name in self.parser._get_required_args():\n            arg_value = self.parser._get_required_args()[arg_name]\n        if arg_name in self.parser._get_deprecated_args():\n            arg_value = self.parser._get_deprecated_args()[arg_name]\n        new_values_dict[arg_name] = arg_value\n    return new_values_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_value in values_dict.items():\n        if not arg_name.startswith('--'):\n            arg_name = parser_name + '_' + arg_name\n        if isinstance(arg_value, list):\n            values_dict[arg_name] = arg_value[0]\n        elif isinstance(arg_value, dict):\n            values_dict[arg_name] = self._convert_non_cli_args(parser_name, arg_value)\n    return values_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg in parser_name._short_opts:\n        if arg.dest in values_dict:\n            parser_name._short_opts[arg.dest] = values_dict[arg.dest]\n    for arg in parser_name._long_opts:\n        if arg.dest in values_dict:\n            parser_name._long_opts[arg.dest] = values_dict[arg.dest]\n\n    for arg in parser_name._optional_opts:\n        if arg.dest in values_dict:\n            parser_name._optional_opts[arg.dest] = values_dict[arg.dest]\n\n    for arg in parser_name._positional_opts:\n        if arg.dest in values_dict:\n            parser_name._positional_opts[arg.dest] = values_dict[arg.dest]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name in values_dict:\n        if not arg_name.startswith(parser_name + '_'):\n            continue\n        return arg_name.replace(parser_name + '_', '', 1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_type in values_dict.items():\n        if arg_type is None:\n            continue\n\n        value = args.get(arg_name)\n        if value is None:\n            continue\n\n        if arg_type == \"int\":\n            value = int(value)\n        elif arg_type == \"float\":\n            value = float(value)\n        elif arg_type == \"boolean\":\n            value = value.lower() in [\"true\", \"1\"]\n\n        values_dict[arg_name] = value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    if parser_name in self.vars:\n        for arg in values_dict:\n            if arg in self.vars[parser_name]:\n                values_dict[arg] = self.vars[parser_name][arg]\n\n    return values_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_type in values_dict.items():\n        if arg_name not in self.vars:\n            continue\n        if arg_type.__name__ == 'IntArg':\n            values_dict[arg_name] = int(arg_type.convert(arg_type.default))\n        elif arg_type.__name__ == 'FloatArg':\n            values_dict[arg_name] = float(arg_type.convert(arg_type.default))\n        elif arg_type.__name__ == 'CommaSeparatedListArg':\n            values_dict[arg_name] = [item.strip() for item in arg_type.convert(arg_type.default).split(',')]\n    return values_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for arg_name, arg_spec in iteritems(values_dict):\n        if arg_spec.converter:\n            converter_name = arg_spec.converter\n            if converter_name not in self.vars.keys():\n                raise exceptions.ArgumentParserError(\n                    '{}: {} argument not found. Converter name must be one of {}'.format(\n                        parser_name, arg_name, self.vars.keys()\n                    )\n                )\n            values_dict[arg_name] = self.vars[converter_name](\n                arg_spec.format_value(arg_spec.parser, arg_spec.default)\n            )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_plugins.py", "project": "redhat-openstack/infrared", "name": "get_plugin_spec_flatten_dict", "docstring": "Creates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties", "ques_id": "63060b1a73426c38ae68ad3e", "code": ["def get_plugin_spec_flatten_dict(plugin_dir):\n    flat_dict = {}\n    for root, dirs, files in os.walk(plugin_dir):\n        for filename in files:\n            if not filename.endswith(\".py\"):\n                continue\n\n            filepath = os.path.join(root, filename)\n            with open(filepath) as f:\n                content = f.read()\n                content = content.replace(\"# from container_plugins.py import *\", \"from container_plugins.py import *\")\n                flat_dict.update(load_yaml(content))\n\n    return flat_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    plugin_spec_flatten_dict = {}\n    for root, dirs, files in os.walk(plugin_dir):\n        for file_name in files:\n            if file_name.endswith('.py') and '__init__.py' not in file_name:\n                file_path = os.path.join(root, file_name)\n                with open(file_path, 'r') as f:\n                    plugin_spec_flatten_dict.update({\n                        file_name[:-3]: f.read()\n                    })\n    return plugin_spec_flatten_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return dict((key, value) for key, value in get_plugin_spec_flatten_items(plugin_dir))\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return flatten_dict(load_plugin_spec(plugin_dir))\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        'name': os.path.basename(plugin_dir),\n       'version': get_version(plugin_dir),\n        'description': \"\",\n        'groups': [],\n        'packages': [],\n        'files': [],\n        'plugin_type': \"\",\n        'options': [],\n        'path': plugin_dir\n    }\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        \"name\": None,\n        \"version\": None,\n        \"description\": None,\n        \"author\": None,\n        \"author_email\": None,\n        \"dependencies\": [],\n        \"install_requires\": [],\n        \"install_requires_eggs\": [],\n        \"install_requires_dists\": [],\n        \"install_requires_extras\": [],\n        \"extras_require\": [],\n        \"extras_require_eggs\": [],\n        \"extras_require_dists\": [],\n        \"install_requires_sources\": [],\n        \"tests\": [],\n        \"tests_require\": [],\n        \"tests_require_eggs\": []\n    }\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        'name': os.path.basename(plugin_dir),\n        'title': os.path.basename(plugin_dir),\n        'description': '',\n       'version': '1.0',\n        'dependencies': {},\n        'inputs': {},\n        'outputs': {},\n        'groups': {},\n        'options': {},\n        'config': {},\n    }\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    plugin_spec_flatten_dict = {}\n    for root, _, files in os.walk(plugin_dir):\n        for file_name in files:\n            if file_name.endswith(\".py\"):\n                full_file_path = os.path.join(root, file_name)\n                try:\n                    spec_dict = import_module(full_file_path).SPEC\n                    plugin_spec_flatten_dict.update(spec_dict)\n                except ImportError as e:\n                    # Skip unknown files\n                    logging.debug(\"Unable to import plugin spec from {0} due to {1}\".format(full_file_path, e))\n    return plugin_spec_flatten_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {key: value for sub_dict in [get_plugin_spec_dict(os.path.join(plugin_dir, key)) for key in os.listdir(plugin_dir)] for value in sub_dict.values()}\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return flatten_dict({\n        key: value for sublist in get_plugin_spec_flatten_list(plugin_dir)\n        for key, value in sublist\n    })\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    flat_dict = {}\n    for root, dirs, files in os.walk(plugin_dir):\n        for filename in files:\n            if not filename.endswith(\".py\"):\n                continue\n\n            filepath = os.path.join(root, filename)\n            with open(filepath) as f:\n                content = f.read()\n                content = content.replace(\"# from container_plugins.py import *\", \"from container_plugins.py import *\")\n                flat_dict.update(load_yaml(content))\n\n    return flat_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    plugin_spec_flatten_dict = {}\n    for root, dirs, files in os.walk(plugin_dir):\n        for file_name in files:\n            if file_name.endswith('.py') and '__init__.py' not in file_name:\n                file_path = os.path.join(root, file_name)\n                with open(file_path, 'r') as f:\n                    plugin_spec_flatten_dict.update({\n                        file_name[:-3]: f.read()\n                    })\n    return plugin_spec_flatten_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return dict((key, value) for key, value in get_plugin_spec_flatten_items(plugin_dir))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return flatten_dict(load_plugin_spec(plugin_dir))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        'name': os.path.basename(plugin_dir),\n       'version': get_version(plugin_dir),\n        'description': \"\",\n        'groups': [],\n        'packages': [],\n        'files': [],\n        'plugin_type': \"\",\n        'options': [],\n        'path': plugin_dir\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        \"name\": None,\n        \"version\": None,\n        \"description\": None,\n        \"author\": None,\n        \"author_email\": None,\n        \"dependencies\": [],\n        \"install_requires\": [],\n        \"install_requires_eggs\": [],\n        \"install_requires_dists\": [],\n        \"install_requires_extras\": [],\n        \"extras_require\": [],\n        \"extras_require_eggs\": [],\n        \"extras_require_dists\": [],\n        \"install_requires_sources\": [],\n        \"tests\": [],\n        \"tests_require\": [],\n        \"tests_require_eggs\": []\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        'name': os.path.basename(plugin_dir),\n        'title': os.path.basename(plugin_dir),\n        'description': '',\n       'version': '1.0',\n        'dependencies': {},\n        'inputs': {},\n        'outputs': {},\n        'groups': {},\n        'options': {},\n        'config': {},\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    plugin_spec_flatten_dict = {}\n    for root, _, files in os.walk(plugin_dir):\n        for file_name in files:\n            if file_name.endswith(\".py\"):\n                full_file_path = os.path.join(root, file_name)\n                try:\n                    spec_dict = import_module(full_file_path).SPEC\n                    plugin_spec_flatten_dict.update(spec_dict)\n                except ImportError as e:\n                    # Skip unknown files\n                    logging.debug(\"Unable to import plugin spec from {0} due to {1}\".format(full_file_path, e))\n    return plugin_spec_flatten_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {key: value for sub_dict in [get_plugin_spec_dict(os.path.join(plugin_dir, key)) for key in os.listdir(plugin_dir)] for value in sub_dict.values()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return flatten_dict({\n        key: value for sublist in get_plugin_spec_flatten_list(plugin_dir)\n        for key, value in sublist\n    })\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/ansible_config.py", "project": "redhat-openstack/infrared", "name": "inject_config", "docstring": "Set the environment variable for config path, if it is undefined.", "ques_id": "63060b1b73426c38ae68ad42", "code": ["def inject_config(self):\n    if self.ansible_config_path is None:\n        self._create_ansible_config(self.infrared_home)\n", "def inject_config(self):\n    if not os.environ.get('INFRREL_CONFIG'):\n        os.environ['INFRREL_CONFIG'] = self.ansible_config_path\n", "def inject_config(self):\n    if 'ansible_config_path' not in os.environ:\n        self._create_ansible_config(self.infrared_home)\n", "def inject_config(self):\n    if 'INFRENTALED_HOME' not in os.environ:\n        self.ansible_config_path = self._get_ansible_conf_path(\n            infrared_home=os.environ.get('INFRENTALED_HOME')\n        )\n", "def inject_config(self):\n    if not os.environ.get('INFRACELD_CONFIG'):\n        self.ansible_config_path = self._get_ansible_conf_path(\n            infrared_home=os.environ.get('INFRACELD_HOME')\n        )\n", "def inject_config(self):\n    if self.ansible_config_path:\n        return\n\n    self._create_ansible_config(infrared_home=os.environ.get(\n        'INFRARED_HOME', None))\n", "def inject_config(self):\n    if 'infrared_config' not in os.environ:\n        self._create_ansible_config(\n            self.infrared_home,\n        )\n", "def inject_config(self):\n    if self.ansible_config_path:\n        os.environ['INFRANRAD_CONFIG'] = self.ansible_config_path\n", "def inject_config(self):\n    if self.ansible_config_path is None:\n        self.ansible_config_path = self._get_ansible_conf_path(\n            os.environ.get('INFRRELIGHT_HOME', '~/.infrared')\n        )\n", "def inject_config(self):\n    if 'INFRARED_HOME' not in os.environ:\n        self._create_ansible_config(self.infrared_home)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def inject_config(self):\n    if self.ansible_config_path is None:\n        self._create_ansible_config(self.infrared_home)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n    if not os.environ.get('INFRREL_CONFIG'):\n        os.environ['INFRREL_CONFIG'] = self.ansible_config_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n    if 'ansible_config_path' not in os.environ:\n        self._create_ansible_config(self.infrared_home)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    if 'INFRENTALED_HOME' not in os.environ:\n        self.ansible_config_path = self._get_ansible_conf_path(\n            infrared_home=os.environ.get('INFRENTALED_HOME')\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    if not os.environ.get('INFRACELD_CONFIG'):\n        self.ansible_config_path = self._get_ansible_conf_path(\n            infrared_home=os.environ.get('INFRACELD_HOME')\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    if self.ansible_config_path:\n        return\n\n    self._create_ansible_config(infrared_home=os.environ.get(\n        'INFRARED_HOME', None))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n    if 'infrared_config' not in os.environ:\n        self._create_ansible_config(\n            self.infrared_home,\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    if self.ansible_config_path:\n        os.environ['INFRANRAD_CONFIG'] = self.ansible_config_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n    if self.ansible_config_path is None:\n        self.ansible_config_path = self._get_ansible_conf_path(\n            os.environ.get('INFRRELIGHT_HOME', '~/.infrared')\n        )\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n    if 'INFRARED_HOME' not in os.environ:\n        self._create_ansible_config(self.infrared_home)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/main.py", "project": "redhat-openstack/infrared", "name": "extend_cli", "docstring": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.", "ques_id": "63060b1b73426c38ae68ad43", "code": ["def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('infrared', help='Show infrared information')\n    parser.add_argument('--inventory', action='store_true',\n                        help='Print infrared inventory attributes')\n    parser.add_argument('--workspace', action='store',\n                        dest='workspace_manager',\n                        default='infrared',\n                        help='Workspace manager to use. Defaults to \"infrared\".')\n    parser.add_argument('--selected-workspace', action='store',\n                        dest='workspace',\n                        help='Selected workspace to use.')\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('spec', help='Displays the current '\n                                      'spec and its dependencies.')\n\n    parser.add_argument('--short', action='store_true',\n                        help='Show only the spec details.')\n    parser.add_argument('--version', action='store_true',\n                        help='Show the version number.')\n\n    parser.add_argument('--inventory', metavar='PKG_INFO',\n                        help='Load the inventory from the spec.')\n\n    parser.add_argument('--upgrade', metavar='PKG_INFO',\n                        help='Update the package information from the '\n                            'spec.')\n\n    parser.add_argument('--remove-orphans', action='store_true',\n                        help=", "def extend_cli(self, root_subparsers):\n    infrared_parser = root_subparsers.add_parser(\n        'infrared',\n        formatter_class=ArgumentDefaultsHelpFormatter,\n        description=textwrap.dedent(\n            \"\"\"\n            Infrared stack deployment.\n\n            Usage: infrared [--checkout-workspace] [--force] [--debug]\n            \"\"\"))\n\n    infrared_parser.add_argument(\n        '--checkout-workspace',\n        type=str,\n        required=False,\n        help='Path to checkout workspace.')\n\n    infrared_parser.add_argument(\n        '--force',\n        action='store_true',\n        help='Force the infrared deployment to use the root workspace.')\n\n    infrared_parser.add_argument(\n        '--debug',\n        action='store_true',\n        help='Debug output.')\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(self.name, help=self.description)\n    self._fetch_inventory(self.name)\n    self.spec_handler(parser, args=parser.add_subparsers())\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(\n        self.name,\n        help='manage infrared packages and export inventory to output'\n    )\n    parser.add_argument(\n        '--workspace',\n        dest='workspace',\n        action='store',\n        metavar='PACKAGE',\n        required=True,\n        help='workspace to export to',\n        default=self._checkout_workspace('infrared'),\n    )\n    parser.add_argument(\n        '--no-cleanup',\n        dest='cleanup',\n        action='store_true',\n        help='do not cleanup infrared packages',\n    )\n    parser.add_argument(\n        '--output-file',\n        dest='output_file',\n        action='store',\n        metavar='FILE',\n        required=False,\n        help='output file name',\n    )\n    parser.add_argument(\n        '--no-update'", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('infrared', help='Infrared CLI options')\n\n    parser.add_argument('--scan-only', action='store_true',\n                        help='Scan only the infrared devices')\n\n    parser.add_argument('--no-scan', action='store_true',\n                        help='Disable scanning of devices')\n\n    parser.add_argument('--clear-scan', action='store_true',\n                        help='Clear the infrared device list and re-scan')\n\n    parser.add_argument('--update-spec-file', action='store_true',\n                        help='Use spec file for updating the infrared device list')\n\n    parser.add_argument('--remove-spec-file', action='store_true',\n                        help='Use spec file for removing the infrared devices')\n\n    parser.add_argument('--overwrite-spec-fil", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('infrared', help='Infrared CLI arguments')\n    parser.add_argument('--inventory', type=str, required=False, default=self._checkout_workspace('inventory'), help='Infrared inventory directory')\n    parser.add_argument('--workspace', type=str, required=False, default=self._checkout_workspace('workspace'), help='Workspace name')\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(\n        self.name, help='manage infrared packages and update them from the '\n                 'infrared repository')\n    self.spec_handler(parser, args=parser.add_argument_group(\n        title='arguments for infrared package management'+ self.name))\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('spec', help='Manage infrared specs')\n    self._fetch_inventory('all')\n    self._checkout_workspace('spec', create=True)\n    parser.add_argument('--force', action='store_true',\n                        help='Force overwrite of existing infrared '\n                            'specs')\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('infrared', description='Infrared CLI entry point.')\n    parser.add_argument('--inventory', '-i', default=None, required=True,\n                        help='path to the inventory file.')\n    parser.add_argument('--workspace', '-w', default=None, required=True,\n                        help='path to the workspace to fetch from.')\n    parser.add_argument('--port', '-p', default=None, required=True,\n                        help='port to use (default: all).')\n    parser.add_argument('--address', '-a', default=None, required=True,\n                        help='ip address of the infrared device.')\n    parser.add_argument('--username', '-u', default=None, required=True,\n                        help='username to use (default: root).')\n    parser.add_argument('"], "level": "project_runnable", "generate_results": [{"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('infrared', help='Show infrared information')\n    parser.add_argument('--inventory', action='store_true',\n                        help='Print infrared inventory attributes')\n    parser.add_argument('--workspace', action='store',\n                        dest='workspace_manager',\n                        default='infrared',\n                        help='Workspace manager to use. Defaults to \"infrared\".')\n    parser.add_argument('--selected-workspace', action='store',\n                        dest='workspace',\n                        help='Selected workspace to use.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('spec', help='Displays the current '\n                                      'spec and its dependencies.')\n\n    parser.add_argument('--short', action='store_true',\n                        help='Show only the spec details.')\n    parser.add_argument('--version', action='store_true',\n                        help='Show the version number.')\n\n    parser.add_argument('--inventory', metavar='PKG_INFO',\n                        help='Load the inventory from the spec.')\n\n    parser.add_argument('--upgrade', metavar='PKG_INFO',\n                        help='Update the package information from the '\n                            'spec.')\n\n    parser.add_argument('--remove-orphans', action='store_true',\n                        help=", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    infrared_parser = root_subparsers.add_parser(\n        'infrared',\n        formatter_class=ArgumentDefaultsHelpFormatter,\n        description=textwrap.dedent(\n            \"\"\"\n            Infrared stack deployment.\n\n            Usage: infrared [--checkout-workspace] [--force] [--debug]\n            \"\"\"))\n\n    infrared_parser.add_argument(\n        '--checkout-workspace',\n        type=str,\n        required=False,\n        help='Path to checkout workspace.')\n\n    infrared_parser.add_argument(\n        '--force',\n        action='store_true',\n        help='Force the infrared deployment to use the root workspace.')\n\n    infrared_parser.add_argument(\n        '--debug',\n        action='store_true',\n        help='Debug output.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(self.name, help=self.description)\n    self._fetch_inventory(self.name)\n    self.spec_handler(parser, args=parser.add_subparsers())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(\n        self.name,\n        help='manage infrared packages and export inventory to output'\n    )\n    parser.add_argument(\n        '--workspace',\n        dest='workspace',\n        action='store',\n        metavar='PACKAGE',\n        required=True,\n        help='workspace to export to',\n        default=self._checkout_workspace('infrared'),\n    )\n    parser.add_argument(\n        '--no-cleanup',\n        dest='cleanup',\n        action='store_true',\n        help='do not cleanup infrared packages',\n    )\n    parser.add_argument(\n        '--output-file',\n        dest='output_file',\n        action='store',\n        metavar='FILE',\n        required=False,\n        help='output file name',\n    )\n    parser.add_argument(\n        '--no-update'", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('infrared', help='Infrared CLI options')\n\n    parser.add_argument('--scan-only', action='store_true',\n                        help='Scan only the infrared devices')\n\n    parser.add_argument('--no-scan', action='store_true',\n                        help='Disable scanning of devices')\n\n    parser.add_argument('--clear-scan', action='store_true',\n                        help='Clear the infrared device list and re-scan')\n\n    parser.add_argument('--update-spec-file', action='store_true',\n                        help='Use spec file for updating the infrared device list')\n\n    parser.add_argument('--remove-spec-file', action='store_true',\n                        help='Use spec file for removing the infrared devices')\n\n    parser.add_argument('--overwrite-spec-fil", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('infrared', help='Infrared CLI arguments')\n    parser.add_argument('--inventory', type=str, required=False, default=self._checkout_workspace('inventory'), help='Infrared inventory directory')\n    parser.add_argument('--workspace', type=str, required=False, default=self._checkout_workspace('workspace'), help='Workspace name')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(\n        self.name, help='manage infrared packages and update them from the '\n                 'infrared repository')\n    self.spec_handler(parser, args=parser.add_argument_group(\n        title='arguments for infrared package management'+ self.name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('spec', help='Manage infrared specs')\n    self._fetch_inventory('all')\n    self._checkout_workspace('spec', create=True)\n    parser.add_argument('--force', action='store_true',\n                        help='Force overwrite of existing infrared '\n                            'specs')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('infrared', description='Infrared CLI entry point.')\n    parser.add_argument('--inventory', '-i', default=None, required=True,\n                        help='path to the inventory file.')\n    parser.add_argument('--workspace', '-w', default=None, required=True,\n                        help='path to the workspace to fetch from.')\n    parser.add_argument('--port', '-p', default=None, required=True,\n                        help='port to use (default: all).')\n    parser.add_argument('--address', '-a', default=None, required=True,\n                        help='ip address of the infrared device.')\n    parser.add_argument('--username', '-u', default=None, required=True,\n                        help='username to use (default: root).')\n    parser.add_argument('", "is_pass": false, "return_code": 1}]}
