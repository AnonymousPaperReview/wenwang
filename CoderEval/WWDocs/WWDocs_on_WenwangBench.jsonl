{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "hydrate_time", "docstring": "Hydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time", "ques_id": "62e60f43d76274f8a4026e28", "code": ["def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds < 0:\n        return LocalTime(nanoseconds)\n    else:\n        return Time(nanoseconds, tz)\n", "def hydrate_time(nanoseconds, tz=None):\n    if not tz:\n        tz = timezone.get_default_timezone()\n\n    if nanoseconds == 0:\n        return datetime.time(0, 0)\n\n    return datetime.time(\n        nanoseconds / 1000000000,\n        nanoseconds % 1000000000,\n        tzinfo=tz\n    )\n", "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = get_localzone()\n    if not isinstance(nanoseconds, (int, float)):\n        raise ValueError('Invalid nanoseconds %r' % nanoseconds)\n    nanos = nanoseconds * 1e9\n    return Time(nanos, tz=tz)\n", "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = timezone.get_current_timezone()\n\n    if nanoseconds >= 1000000000:\n        nanos = nanoseconds // 1000000000\n        ns = nanoseconds % 1000000000\n\n        return Time(nanos, ns, tz)\n    elif nanoseconds >= 1000:\n        nanos = nanoseconds // 1000\n        ns = nanoseconds % 1000\n\n        return LocalTime(nanos, ns, tz)\n    elif nanoseconds >= 100:\n        nanos = nanoseconds // 100\n        ns = nanoseconds % 100\n\n        return Time(nanos, ns, tz)\n    elif nanoseconds >= 10:\n        nanos = nanoseconds // 10\n        ns = nanoseconds % 10\n\n        return LocalTime(nanos, ns, tz)\n    elif nanoseconds >= 5:\n        nanos = nanoseconds // 5\n        ns = nanoseconds % 5\n\n        return Time(n", "def hydrate_time(nanoseconds, tz=None):\n    if not tz:\n        tz = timezone.utc\n    return Time(nanoseconds=nanoseconds, tzinfo=tz)\n", "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = get_localzone()\n\n    if nanoseconds == 0:\n        return LocalTime(0, tz)\n    else:\n        dt = datetime.datetime.utcfromtimestamp(nanoseconds)\n        return Time(dt.hour, dt.minute, dt.second, dt.microsecond, tz)\n", "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = timezone.utc\n\n    if nanoseconds < EPOCH:\n        return LocalTime(nanoseconds=nanoseconds, tz=tz)\n    else:\n        return Time(nanosecond=nanoseconds, tz=tz)\n", "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = timezone.utc\n\n    if isinstance(nanoseconds, int):\n        nanoseconds = timedelta(microseconds=nanoseconds)\n\n    return Time(nanoseconds=nanoseconds, tzinfo=tz)\n", "def hydrate_time(nanoseconds, tz=None):\n    if not isinstance(nanoseconds, int):\n        raise ValueError('nanoseconds must be an integer')\n    if nanoseconds < 0:\n        raise ValueError('nanoseconds must be greater than 0')\n\n    if tz is None:\n        tz = timezone.utc\n\n    nanoseconds += _NANO_TO_NSEC * (nanoseconds // _NANO_TO_NSEC)\n    if nanoseconds == 0:\n        return None\n    elif nanoseconds == 1:\n        return Time(0, tz=tz)\n    elif nanoseconds % _NANO_TO_NSEC == 0:\n        return LocalTime(nanoseconds / _NANO_TO_NSEC, tz=tz)\n    else:\n        fractional = (nanoseconds - nanoseconds % _NANO_TO_NSEC) * _NANO_TO_NSEC // _NANO_TO_NSEC\n        return Time(fractional, tz=tz)\n", "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = timezone.utc\n    if nanoseconds >= 1000000:\n        return hydrate_time_from_nanoseconds(nanoseconds // 1000000, tz)\n    else:\n        return hydrate_time_from_nanoseconds(nanoseconds, tz)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if nanoseconds < 0:\n        return LocalTime(nanoseconds)\n    else:\n        return Time(nanoseconds, tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if not tz:\n        tz = timezone.get_default_timezone()\n\n    if nanoseconds == 0:\n        return datetime.time(0, 0)\n\n    return datetime.time(\n        nanoseconds / 1000000000,\n        nanoseconds % 1000000000,\n        tzinfo=tz\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = get_localzone()\n    if not isinstance(nanoseconds, (int, float)):\n        raise ValueError('Invalid nanoseconds %r' % nanoseconds)\n    nanos = nanoseconds * 1e9\n    return Time(nanos, tz=tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = timezone.get_current_timezone()\n\n    if nanoseconds >= 1000000000:\n        nanos = nanoseconds // 1000000000\n        ns = nanoseconds % 1000000000\n\n        return Time(nanos, ns, tz)\n    elif nanoseconds >= 1000:\n        nanos = nanoseconds // 1000\n        ns = nanoseconds % 1000\n\n        return LocalTime(nanos, ns, tz)\n    elif nanoseconds >= 100:\n        nanos = nanoseconds // 100\n        ns = nanoseconds % 100\n\n        return Time(nanos, ns, tz)\n    elif nanoseconds >= 10:\n        nanos = nanoseconds // 10\n        ns = nanoseconds % 10\n\n        return LocalTime(nanos, ns, tz)\n    elif nanoseconds >= 5:\n        nanos = nanoseconds // 5\n        ns = nanoseconds % 5\n\n        return Time(n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if not tz:\n        tz = timezone.utc\n    return Time(nanoseconds=nanoseconds, tzinfo=tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = get_localzone()\n\n    if nanoseconds == 0:\n        return LocalTime(0, tz)\n    else:\n        dt = datetime.datetime.utcfromtimestamp(nanoseconds)\n        return Time(dt.hour, dt.minute, dt.second, dt.microsecond, tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = timezone.utc\n\n    if nanoseconds < EPOCH:\n        return LocalTime(nanoseconds=nanoseconds, tz=tz)\n    else:\n        return Time(nanosecond=nanoseconds, tz=tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = timezone.utc\n\n    if isinstance(nanoseconds, int):\n        nanoseconds = timedelta(microseconds=nanoseconds)\n\n    return Time(nanoseconds=nanoseconds, tzinfo=tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if not isinstance(nanoseconds, int):\n        raise ValueError('nanoseconds must be an integer')\n    if nanoseconds < 0:\n        raise ValueError('nanoseconds must be greater than 0')\n\n    if tz is None:\n        tz = timezone.utc\n\n    nanoseconds += _NANO_TO_NSEC * (nanoseconds // _NANO_TO_NSEC)\n    if nanoseconds == 0:\n        return None\n    elif nanoseconds == 1:\n        return Time(0, tz=tz)\n    elif nanoseconds % _NANO_TO_NSEC == 0:\n        return LocalTime(nanoseconds / _NANO_TO_NSEC, tz=tz)\n    else:\n        fractional = (nanoseconds - nanoseconds % _NANO_TO_NSEC) * _NANO_TO_NSEC // _NANO_TO_NSEC\n        return Time(fractional, tz=tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n    if tz is None:\n        tz = timezone.utc\n    if nanoseconds >= 1000000:\n        return hydrate_time_from_nanoseconds(nanoseconds // 1000000, tz)\n    else:\n        return hydrate_time_from_nanoseconds(nanoseconds, tz)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_timedelta", "docstring": "Dehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:", "ques_id": "62e60f3bd76274f8a4026e10", "code": ["def dehydrate_timedelta(value):\n    return datetime.timedelta(seconds=value.days * 86400 + value.seconds)\n", "def dehydrate_timedelta(value):\n    return timedelta(seconds=value.days * 86400 + value.seconds)\n", "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n\n    return datetime.timedelta(\n        seconds=value.seconds + value.days * 24 * 3600\n    )\n", "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n\n    return datetime.timedelta(seconds=value.days * 86400 + value.seconds)\n", "def dehydrate_timedelta(value):\n    if value.days == 0 and value.seconds == 0:\n        return datetime.timedelta()\n    return timedelta(days=value.days, seconds=value.seconds)\n", "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n    return datetime.timedelta(days=value.days)\n", "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n\n    return datetime.timedelta(\n        value.days,\n        value.seconds,\n        value.microseconds\n    )\n", "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n    return timedelta(days=value.days)\n", "def dehydrate_timedelta(value):\n    if isinstance(value, datetime.timedelta):\n        return value\n\n    # Convert milliseconds to seconds\n    value /= 1000\n\n    # Convert seconds to milliseconds\n    value -= 604800\n\n    return datetime.timedelta(seconds=value)\n", "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n\n    return datetime.timedelta(\n        value.days,\n        value.seconds + value.microseconds // 1000,\n    )\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_timedelta(value):\n    return datetime.timedelta(seconds=value.days * 86400 + value.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    return timedelta(seconds=value.days * 86400 + value.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n\n    return datetime.timedelta(\n        seconds=value.seconds + value.days * 24 * 3600\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n\n    return datetime.timedelta(seconds=value.days * 86400 + value.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days == 0 and value.seconds == 0:\n        return datetime.timedelta()\n    return timedelta(days=value.days, seconds=value.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n    return datetime.timedelta(days=value.days)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n\n    return datetime.timedelta(\n        value.days,\n        value.seconds,\n        value.microseconds\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n    return timedelta(days=value.days)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if isinstance(value, datetime.timedelta):\n        return value\n\n    # Convert milliseconds to seconds\n    value /= 1000\n\n    # Convert seconds to milliseconds\n    value -= 604800\n\n    return datetime.timedelta(seconds=value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n    if value.days == 0:\n        return datetime.timedelta(0)\n\n    return datetime.timedelta(\n        value.days,\n        value.seconds + value.microseconds // 1000,\n    )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_time", "docstring": "Dehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:", "ques_id": "62e60f37d76274f8a4026dfd", "code": ["def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "def dehydrate_time(value):\n    return Time(value.hour, value.minute)\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "def dehydrate_time(value):\n    return Time(value.hour, value.minute, value.second)\n", "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return Time(value.hour, value.minute)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return Time(value.hour, value.minute, value.second)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n    return value.strftime('%H:%M')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/spatial.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_point", "docstring": "Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:", "ques_id": "62e60f33d76274f8a4026de9", "code": ["def dehydrate_point(value):\n    return [value.latitude, value.longitude]\n", "def dehydrate_point(value):\n    return {\n        'type': 'Point',\n        'coordinates': list(value.coordinates)\n    }\n", "def dehydrate_point(value):\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n", "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadData(\"Not a valid point\")\n\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n", "def dehydrate_point(value):\n    return {\n        'type': 'Point',\n        'coordinates': list(value.coordinates)\n    }\n", "def dehydrate_point(value):\n    return [value.latitude, value.longitude]\n", "def dehydrate_point(value):\n    return {\n        'type': 'Point',\n        'coordinates': value.coords\n    }\n", "def dehydrate_point(value):\n    return {\n        'type': 'Point',\n        'coordinates': value.coordinates\n    }\n", "def dehydrate_point(value):\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n", "def dehydrate_point(value):\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_point(value):\n    return [value.latitude, value.longitude]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'type': 'Point',\n        'coordinates': list(value.coordinates)\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    if not isinstance(value, Point):\n        raise BadData(\"Not a valid point\")\n\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'type': 'Point',\n        'coordinates': list(value.coordinates)\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return [value.latitude, value.longitude]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'type': 'Point',\n        'coordinates': value.coords\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'type': 'Point',\n        'coordinates': value.coordinates\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n    return {\n        'x': value.x,\n        'y': value.y\n    }\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "keys", "docstring": "Return the keys of the record.\n\n:return: list of key names", "ques_id": "62e60ed4d76274f8a4026da0", "code": ["def keys(self):\n    return self._keys\n", "def keys(self):\n    return [key for key in self._fields]\n", "def keys(self):\n    return [k for k in self._store.keys()]\n", "def keys(self):\n    return self._keys\n", "def keys(self):\n    return self._fields\n", "def keys(self):\n    return list(self._keys)\n", "def keys(self):\n    return list(self._fields)\n", "def keys(self):\n    return self._fields\n", "def keys(self):\n    return list(self._fields)\n", "def keys(self):\n    return [key for key in self._keys]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def keys(self):\n    return self._keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return [key for key in self._fields]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return [k for k in self._store.keys()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self._keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self._fields\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return list(self._keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return list(self._fields)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return self._fields\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return list(self._fields)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n    return [key for key in self._keys]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_sync/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "ques_id": "62e60ecfd76274f8a4026d6a", "code": ["def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    else:\n        return {\n            version: getattr(cls, '_' + version)\n            for version in protocol_version\n        }\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    elif isinstance(protocol_version, tuple):\n        return {\n            version: protocol_handler\n            for version, protocol_handler in cls._protocol_handlers.items()\n            if version in protocol_version\n        }\n    else:\n        raise TypeError('protocol_version must be a tuple or None')\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        version_list = list(ProtocolFactory.registered_protocols.keys())\n        version_tuple_list = [(v, ProtocolFactory.registered_protocols[v]) for v in version_list]\n        protocol_version = max(version_tuple_list, key=lambda v_tup: v_tup[1].priority)\n\n    return {\n        v_tup[0]: v_tup[1]\n        for v_tup in ProtocolFactory.registered_protocols.get(protocol_version, [])\n    }\n", "def protocol_handlers(cls, protocol_version=None):\n    versions = {}\n    for protocol_version_tuple in protocol_handler_classes():\n        if protocol_version_tuple[1] is not None and protocol_version_tuple[1] >= cls.MIN_PROTOCOL_VERSION:\n            if protocol_version is None or protocol_version_tuple[0] <= protocol_version:\n                versions[protocol_version_tuple[0]] = protocol_version_tuple[1]\n    if protocol_version is not None:\n        if protocol_version in versions:\n            return versions[protocol_version]\n    return versions\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        version_tuples = cls._protocol_version_tuples\n    else:\n        version_tuples = [protocol_version] + cls._protocol_version_tuples\n    return {version: get_protocol_handler(version) for version in version_tuples}\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    else:\n        if len(protocol_version) == 2:\n            # Assume that it's a sequence of two integers\n            version_tuple = protocol_version\n        else:\n            raise TypeError(\n                \"Sequence protocol versions must be passed in \"\n                \"as an tuple (major, minor) or None\"\n            )\n        # Convert version tuple to lower case\n        version_tuple = tuple(\n            (int(x), int(y)) for x, y in version_tuple\n        )\n        # Return the handlers for that version\n        return cls._protocol_handlers.get(version_tuple, {})\n", "def protocol_handlers(cls, protocol_version=None):\n    versions = _protocol_versions(protocol_version)\n    return {version: handler for version, handler in versions.items()\n            if handler is not None}\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return _available_protocol_handlers\n    else:\n        try:\n            return _available_protocol_handlers[protocol_version]\n        except KeyError:\n            raise TypeError('protocol_version must be a tuple containing 0 or '\n                            'one items, not {0}'.format(protocol_version))\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        version_list = cls._protocol_version_classes.keys()\n    else:\n        version_list = [protocol_version]\n    return {version: cls._protocol_version_classes[version] for version in version_list\n            if cls._protocol_version_classes[version] is not None}\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    else:\n        return {\n            version: _get_protocol_class(version, protocol_version)\n            for version in cls._protocol_handlers\n        }\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    else:\n        return {\n            version: getattr(cls, '_' + version)\n            for version in protocol_version\n        }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    elif isinstance(protocol_version, tuple):\n        return {\n            version: protocol_handler\n            for version, protocol_handler in cls._protocol_handlers.items()\n            if version in protocol_version\n        }\n    else:\n        raise TypeError('protocol_version must be a tuple or None')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        version_list = list(ProtocolFactory.registered_protocols.keys())\n        version_tuple_list = [(v, ProtocolFactory.registered_protocols[v]) for v in version_list]\n        protocol_version = max(version_tuple_list, key=lambda v_tup: v_tup[1].priority)\n\n    return {\n        v_tup[0]: v_tup[1]\n        for v_tup in ProtocolFactory.registered_protocols.get(protocol_version, [])\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    versions = {}\n    for protocol_version_tuple in protocol_handler_classes():\n        if protocol_version_tuple[1] is not None and protocol_version_tuple[1] >= cls.MIN_PROTOCOL_VERSION:\n            if protocol_version is None or protocol_version_tuple[0] <= protocol_version:\n                versions[protocol_version_tuple[0]] = protocol_version_tuple[1]\n    if protocol_version is not None:\n        if protocol_version in versions:\n            return versions[protocol_version]\n    return versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        version_tuples = cls._protocol_version_tuples\n    else:\n        version_tuples = [protocol_version] + cls._protocol_version_tuples\n    return {version: get_protocol_handler(version) for version in version_tuples}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    else:\n        if len(protocol_version) == 2:\n            # Assume that it's a sequence of two integers\n            version_tuple = protocol_version\n        else:\n            raise TypeError(\n                \"Sequence protocol versions must be passed in \"\n                \"as an tuple (major, minor) or None\"\n            )\n        # Convert version tuple to lower case\n        version_tuple = tuple(\n            (int(x), int(y)) for x, y in version_tuple\n        )\n        # Return the handlers for that version\n        return cls._protocol_handlers.get(version_tuple, {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    versions = _protocol_versions(protocol_version)\n    return {version: handler for version, handler in versions.items()\n            if handler is not None}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return _available_protocol_handlers\n    else:\n        try:\n            return _available_protocol_handlers[protocol_version]\n        except KeyError:\n            raise TypeError('protocol_version must be a tuple containing 0 or '\n                            'one items, not {0}'.format(protocol_version))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        version_list = cls._protocol_version_classes.keys()\n    else:\n        version_list = [protocol_version]\n    return {version: cls._protocol_version_classes[version] for version in version_list\n            if cls._protocol_version_classes[version] is not None}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    else:\n        return {\n            version: _get_protocol_class(version, protocol_version)\n            for version in cls._protocol_handlers\n        }\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/work/query.py", "project": "neo4j/neo4j-python-driver", "name": "unit_of_work", "docstring": "This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`", "ques_id": "62e60e49d76274f8a4026d25", "code": ["def unit_of_work(metadata=None, timeout=None):\n    def _wrapper(f):\n        f.__unit_of_work__ = metadata or {}\n        f.__unit_of_work__[\"timeout\"] = timeout\n        return f\n    return _wrapper\n", "def unit_of_work(metadata=None, timeout=None):\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            with Transaction() as tx:\n                result = func(*args, **kwargs)\n                return _query_helper(tx, metadata, timeout, result)\n\n        # Add transaction function\n        wrapper.__name__ = func.__name__\n        wrapper.__wrapped__ = func\n        TRANSACTIONS.append(wrapper)\n        return wrapper\n\n    return decorator\n", "def unit_of_work(metadata=None, timeout=None):\n    def decorator(f):\n        @wraps(f)\n        def wrapper(*args, **kw):\n            with Transaction(metadata, timeout):\n                return f(*args, **kw)\n        return wrapper\n    return decorator\n", "def unit_of_work(metadata=None, timeout=None):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kw):\n            tx = func(*args, **kw)\n            return _process_unit_of_work(tx, metadata, timeout)\n        return wrapper\n    return decorator\n", "def unit_of_work(metadata=None, timeout=None):\n    def wrapper(f):\n        if not callable(f):\n            raise TypeError(\"Unit of work must be callable.\")\n        if metadata is not None and not isinstance(metadata, dict):\n            raise TypeError(\"Unit of work metadata must be a dictionary.\")\n        if timeout is not None and not isinstance(timeout, (float, int)):\n            raise TypeError(\"Unit of work timeout must be a float or int.\")\n        f.__unit_of_work__ = metadata or {}\n        f.__unit_of_work__[\"timeout\"] = timeout\n        return f\n    return wrapper\n", "def unit_of_work(metadata=None, timeout=None):\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            with transaction(metadata=metadata, timeout=timeout):\n                return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\n    def wrapper(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            with Transaction() as tx:\n                return func(tx, *args, **kwargs)\n\n        if metadata:\n            wrapped.metadata = metadata\n        if timeout:\n            wrapped.timeout = timeout\n\n        @wraps(func)\n        def callback(*args, **kwargs):\n            with Transaction() as tx:\n                return func(tx, *args, **kwargs)\n\n            return callback()\n\n        return wrapped\n\n    return wrapper\n", "def unit_of_work(metadata=None, timeout=None):\n\n    def wrapper(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            with transaction.manager:\n                return func(*args, **kwargs)\n\n        if callable(metadata):\n            wrapped.__transaction__ = metadata\n        else:\n            wrapped.__transaction__ = metadata or {}\n\n        if timeout is not None:\n            wrapped.__timeout__ = timeout\n\n        return wrapped\n\n    return wrapper\n", "def unit_of_work(metadata=None, timeout=None):\n    def wrapper(f):\n        @functools.wraps(f)\n        def wrapped(*args, **kwargs):\n            with unit_of_work_ctx() as tx:\n                result = f(tx, *args, **kwargs)\n            return result\n\n        if metadata is None:\n            metadata = {}\n        if timeout is None:\n            timeout = dbms.transaction.timeout\n        metadata[\"timeout\"] = timeout\n\n        @wrapped\n        def wrapped_inner(*args, **kwargs):\n            with unit_of_work_ctx() as tx:\n                result = f(tx, *args, **kwargs)\n            return result\n\n        return wrapped_inner\n\n    if callable(metadata):\n        return wrapper(metadata)\n    else:\n        raise TypeError(\"metadata must be callable or None\"", "def unit_of_work(metadata=None, timeout=None):\n    def wrapper(f):\n        @wraps(f)\n        def wrapped(*args, **kwargs):\n            with UnitOfWork(timeout=timeout, metadata=metadata):\n                return f(*args, **kwargs)\n        return wrapped\n    return wrapper\n"], "level": "self_contained", "generate_results": [{"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def _wrapper(f):\n        f.__unit_of_work__ = metadata or {}\n        f.__unit_of_work__[\"timeout\"] = timeout\n        return f\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            with Transaction() as tx:\n                result = func(*args, **kwargs)\n                return _query_helper(tx, metadata, timeout, result)\n\n        # Add transaction function\n        wrapper.__name__ = func.__name__\n        wrapper.__wrapped__ = func\n        TRANSACTIONS.append(wrapper)\n        return wrapper\n\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def decorator(f):\n        @wraps(f)\n        def wrapper(*args, **kw):\n            with Transaction(metadata, timeout):\n                return f(*args, **kw)\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kw):\n            tx = func(*args, **kw)\n            return _process_unit_of_work(tx, metadata, timeout)\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def wrapper(f):\n        if not callable(f):\n            raise TypeError(\"Unit of work must be callable.\")\n        if metadata is not None and not isinstance(metadata, dict):\n            raise TypeError(\"Unit of work metadata must be a dictionary.\")\n        if timeout is not None and not isinstance(timeout, (float, int)):\n            raise TypeError(\"Unit of work timeout must be a float or int.\")\n        f.__unit_of_work__ = metadata or {}\n        f.__unit_of_work__[\"timeout\"] = timeout\n        return f\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            with transaction(metadata=metadata, timeout=timeout):\n                return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\n    def wrapper(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            with Transaction() as tx:\n                return func(tx, *args, **kwargs)\n\n        if metadata:\n            wrapped.metadata = metadata\n        if timeout:\n            wrapped.timeout = timeout\n\n        @wraps(func)\n        def callback(*args, **kwargs):\n            with Transaction() as tx:\n                return func(tx, *args, **kwargs)\n\n            return callback()\n\n        return wrapped\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\n    def wrapper(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            with transaction.manager:\n                return func(*args, **kwargs)\n\n        if callable(metadata):\n            wrapped.__transaction__ = metadata\n        else:\n            wrapped.__transaction__ = metadata or {}\n\n        if timeout is not None:\n            wrapped.__timeout__ = timeout\n\n        return wrapped\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def wrapper(f):\n        @functools.wraps(f)\n        def wrapped(*args, **kwargs):\n            with unit_of_work_ctx() as tx:\n                result = f(tx, *args, **kwargs)\n            return result\n\n        if metadata is None:\n            metadata = {}\n        if timeout is None:\n            timeout = dbms.transaction.timeout\n        metadata[\"timeout\"] = timeout\n\n        @wrapped\n        def wrapped_inner(*args, **kwargs):\n            with unit_of_work_ctx() as tx:\n                result = f(tx, *args, **kwargs)\n            return result\n\n        return wrapped_inner\n\n    if callable(metadata):\n        return wrapper(metadata)\n    else:\n        raise TypeError(\"metadata must be callable or None\"", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n    def wrapper(f):\n        @wraps(f)\n        def wrapped(*args, **kwargs):\n            with UnitOfWork(timeout=timeout, metadata=metadata):\n                return f(*args, **kwargs)\n        return wrapped\n    return wrapper\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "index", "docstring": "Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int", "ques_id": "62e60e05d76274f8a4026cfd", "code": ["def index(self, key):\n    k = self._hash(key)\n    return self._items[k].index(key)\n", "def index(self, key):\n    k = self._hash(key)\n    idx = k % self._length\n    for (k2, value) in self._table[idx]:\n        if value == key:\n            return k2\n    raise ValueError(\"Key not in hash table\")\n", "def index(self, key):\n    bucket_index = self._hash(key) % self._buckets\n    for bucket in self._buckets:\n        if bucket.contains(key):\n            for i, entry in enumerate(bucket):\n                if entry.key == key:\n                    return i\n    return -1\n", "def index(self, key):\n    return self._keys.index(key)\n", "def index(self, key):\n    return self._keys.index(key)\n", "def index(self, key):\n    return self._dict[key][0]\n", "def index(self, key):\n    return self._dict.index(key)\n", "def index(self, key):\n    return self._dict[key][1]\n", "def index(self, key):\n    return self._keys.index(key)\n", "def index(self, key):\n    k = self._hash(key)\n    for i, k_value in enumerate(self._table[k]):\n        if k_value == key:\n            return i\n\n    raise KeyError('Key not found: {}'.format(key))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def index(self, key):\n    k = self._hash(key)\n    return self._items[k].index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    k = self._hash(key)\n    idx = k % self._length\n    for (k2, value) in self._table[idx]:\n        if value == key:\n            return k2\n    raise ValueError(\"Key not in hash table\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    bucket_index = self._hash(key) % self._buckets\n    for bucket in self._buckets:\n        if bucket.contains(key):\n            for i, entry in enumerate(bucket):\n                if entry.key == key:\n                    return i\n    return -1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self._keys.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self._keys.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self._dict[key][0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self._dict.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self._dict[key][1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    return self._keys.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n    k = self._hash(key)\n    for i, k_value in enumerate(self._table[k]):\n        if k_value == key:\n            return i\n\n    raise KeyError('Key not found: {}'.format(key))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "values", "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list", "ques_id": "62e60da4d76274f8a4026cf1", "code": ["def values(self, *keys):\n\n    return list(self.itervalues(*keys))\n", "def values(self, *keys):\n\n    items = self.items\n    if not keys:\n        items = self.iteritems()\n\n    return [item[1] for item in items if item[0] in keys]\n", "def values(self, *keys):\n    return list(self.itervalues(*keys))\n", "def values(self, *keys):\n\n    return [record[self._index] for record in self._values(*keys)]\n", "def values(self, *keys):\n\n    return [self.record[key] for key in keys if key in self.record]\n", "def values(self, *keys):\n    if not keys:\n        return list(self._values)\n    result = []\n    for k in keys:\n        if k >= len(self._values):\n            raise IndexError(\"Index %d is out of bounds %d >= %d\" % (k, len(self._values), k))\n        if k < 0:\n            k += len(self._values)\n        result.append(self._values[k])\n    return result\n", "def values(self, *keys):\n\n    if len(keys) == 0:\n        values = [record[self._key_field] for record in self._record]\n    else:\n        values = [record[self._key_field] for record in self._record if self._key_field in record]\n\n    if len(keys) == 0:\n        return values\n    else:\n        return [values[key_index] for key_index in keys]\n", "def values(self, *keys):\n    return [record[self._index_key] for record in self._values(*keys)]\n", "def values(self, *keys):\n    if keys:\n        return [self.data[k] for k in keys]\n    return self.data.values()\n", "def values(self, *keys):\n\n    return [self._values[k] for k in keys] if keys else self._values\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def values(self, *keys):\n\n    return list(self.itervalues(*keys))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\n    items = self.items\n    if not keys:\n        items = self.iteritems()\n\n    return [item[1] for item in items if item[0] in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n    return list(self.itervalues(*keys))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\n    return [record[self._index] for record in self._values(*keys)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\n    return [self.record[key] for key in keys if key in self.record]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n    if not keys:\n        return list(self._values)\n    result = []\n    for k in keys:\n        if k >= len(self._values):\n            raise IndexError(\"Index %d is out of bounds %d >= %d\" % (k, len(self._values), k))\n        if k < 0:\n            k += len(self._values)\n        result.append(self._values[k])\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\n    if len(keys) == 0:\n        values = [record[self._key_field] for record in self._record]\n    else:\n        values = [record[self._key_field] for record in self._record if self._key_field in record]\n\n    if len(keys) == 0:\n        return values\n    else:\n        return [values[key_index] for key_index in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n    return [record[self._index_key] for record in self._values(*keys)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n    if keys:\n        return [self.data[k] for k in keys]\n    return self.data.values()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\n    return [self._values[k] for k in keys] if keys else self._values\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "data", "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified", "ques_id": "62e60b10d76274f8a4026ccd", "code": ["def data(self, *keys):\n    keys = _make_index_keys(keys, len(self))\n    return dict(zip(keys, self._values[keys]))\n", "def data(self, *keys):\n    if len(keys) == 0:\n        keys = self._fields\n    return dict(zip(keys, self.getall(*keys)))\n", "def data(self, *keys):\n    return dict(zip(keys, self._iterdata(*keys)))\n", "def data(self, *keys):\n    d = {}\n    for key in keys or self.keys():\n        try:\n            value = self[key]\n        except IndexError:\n            raise IndexError('Record contains no items')\n        if isinstance(key, int):\n            d[key] = value\n        else:\n            d[key[1:]] = value\n    return d\n", "def data(self, *keys):\n    if len(keys) == 0:\n        return dict(self)\n\n    if len(keys) == 1:\n        key, = keys\n        if key < 0:\n            key += len(self)\n        if key < 0 or key >= len(self):\n            raise IndexError(\"Index %d is out of bounds for field %s\" % (key, self._fields))\n        return dict(self)[key]\n\n    result = {}\n    for key in keys:\n        if key < 0:\n            key += len(self)\n        if key < 0 or key >= len(self):\n            raise IndexError(\"Index %d is out of bounds for field %s\" % (key, self._fields))\n        result[key] = self[key]\n    return result\n", "def data(self, *keys):\n    return dict(zip(keys, self._iterdata(*keys)))\n", "def data(self, *keys):\n    if not keys:\n        return dict(self)\n    return dict((k, self[k]) for k in keys if k in self)\n", "def data(self, *keys):\n    rv = {}\n    for key in keys:\n        try:\n            rv[key] = self[key]\n        except KeyError:\n            pass\n    return rv\n", "def data(self, *keys):\n    if len(keys) == 0:\n        keys = self._fields\n    return dict(zip(keys, self.getall(*keys)))\n", "def data(self, *keys):\n    r = dict(self._row)\n    for k in keys or self._fields:\n        if k not in r:\n            raise IndexError('Index {} out of bounds'.format(k))\n        r[k] = r[k][0] if isinstance(r[k], list) else None\n    return r\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def data(self, *keys):\n    keys = _make_index_keys(keys, len(self))\n    return dict(zip(keys, self._values[keys]))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    if len(keys) == 0:\n        keys = self._fields\n    return dict(zip(keys, self.getall(*keys)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    return dict(zip(keys, self._iterdata(*keys)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    d = {}\n    for key in keys or self.keys():\n        try:\n            value = self[key]\n        except IndexError:\n            raise IndexError('Record contains no items')\n        if isinstance(key, int):\n            d[key] = value\n        else:\n            d[key[1:]] = value\n    return d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    if len(keys) == 0:\n        return dict(self)\n\n    if len(keys) == 1:\n        key, = keys\n        if key < 0:\n            key += len(self)\n        if key < 0 or key >= len(self):\n            raise IndexError(\"Index %d is out of bounds for field %s\" % (key, self._fields))\n        return dict(self)[key]\n\n    result = {}\n    for key in keys:\n        if key < 0:\n            key += len(self)\n        if key < 0 or key >= len(self):\n            raise IndexError(\"Index %d is out of bounds for field %s\" % (key, self._fields))\n        result[key] = self[key]\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    return dict(zip(keys, self._iterdata(*keys)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    if not keys:\n        return dict(self)\n    return dict((k, self[k]) for k in keys if k in self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    rv = {}\n    for key in keys:\n        try:\n            rv[key] = self[key]\n        except KeyError:\n            pass\n    return rv\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    if len(keys) == 0:\n        keys = self._fields\n    return dict(zip(keys, self.getall(*keys)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n    r = dict(self._row)\n    for k in keys or self._fields:\n        if k not in r:\n            raise IndexError('Index {} out of bounds'.format(k))\n        r[k] = r[k][0] if isinstance(r[k], list) else None\n    return r\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/packstream/v1/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "pop_u16", "docstring": "Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.", "ques_id": "62e6087bd76274f8a4026bfa", "code": ["def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "def pop_u16(self):\n    return self.pop_bits(2, 2)\n", "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_bits(2, 2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n    return self.pop_u8() << 8 | self.pop_u8()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "discard", "docstring": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object", "ques_id": "62e6087ad76274f8a4026bf2", "code": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from\n        type (class) to dehydration function). Dehydration functions\n        receives the value and returns an object of type desired by\n        packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from\n        type (class) to dehydration function). Dehydration functions\n        receives the value of type desired by packstream and are free\n        to return anything.\n    :param handlers: handler functions passed in the returned Response\n        object when received.\n    \"\"\"\n    return self._append(Request(discard=n, qid=qid,\n                                discard_method='discard',\n                       ", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function). Dehydration functions\n        receives the value and returns an object of type Undehyded (not dehydrated).\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receives the value and returns an object of type Undehyded (not dehydrated).\n    :param handlers: handler functions passed in the returned Response object (unused).\n    \"\"\"\n    return self._append(ResponseType.DISCARD, n, qid, dehydration_hooks,\n                        hydration_hooks, handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hopers to dehydrate types (dict from type\n                                (class) to dehydration function). Dehydration\n                                functions receives the value and returns\n                                an object of type desired by packstream.\n    :param hydration_hooks: Hopers to dehydrate types (mapping from\n                                type (class) to dehydration function). Dehydration\n                                functions receives the value of type\n                                desired by packstream and are free to\n                       ", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class)\n                                                            to dehydration function).\n                                Dehydration functions receive the value and returns an\n                                object of type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from\n                                                                type (class) to dehydration\n                                                              ", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function). Dehydration functions\n        receives the value and returns an object of type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function). Dehydration functions\n        receives the value of type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed in the returned Response object is used to return the value of\n        understood by packstream when a response is received.\n    \"\"\"\n    self._append(discard.Discard(n, qid, dehydration_hooks, hydration_hooks, **handlers))\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type\n                                    (class) to dehydration function). Dehydration\n                                    functions receives the value and returns\n                                    an object of type desired by packstream.\n    :param hydration_hooks: Hooks to dehydrate types (mapping from type\n                                                                to dehydration\n                                                                function).\n    :param handlers: handler function", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n        Dehydration functions receive the value and returns an object of type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receive the value of type understood by packstream and are free to return\n        anything.\n    :param handlers: handler functions passed in the returned Response object which will be used\n        to resolve the value of type understood by packstream and are free to return anything.\n    \"\"\"\n    self._enqueue('discard', n, qid, dehydration_hooks, hydration_hooks, handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function). Dehydration functions\n        receives the value and returns an object of type underdetected by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receives the value of type underdetected by packstream and are free to return\n        anything.\n    :param handlers: handler functions passed in the returned Response object (unused).\n    \"\"\"\n    if not isinstance(handlers, dict):\n        raise TypeError(\"'handlers' must be of type dict.\")\n\n    self._add_message(DISCARD, n, qid, dehydration_hooks, hydration_hooks, handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n        Dehydration functions receive the value and returns an object of type understood by\n        packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receive the value of type understood by packstream and are free to\n        return things.\n    :param handlers: handler functions passed in the returned Response object instead of the\n        Response object.\n    \"\"\"\n    self._send('DISCARD', {'n': n, 'qid': qid, 'dehydration_hooks': dehydration_hooks,\n                            'hydration_hooks': hydration_hooks, 'handler", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n        Dehydration functions receive the value and returns an object of type understood by\n        packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receive the value of type understood by packstream and are free to\n        return anything.\n    :param handlers: handler functions passed in the returned Response object instead of the returned\n        Response object from the returned method\n    \"\"\"\n    self._enqueue('discard', n=n, qid=qid, dehydration_hooks=dehydration_hooks,\n                                             "], "level": "project_runnable", "generate_results": [{"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from\n        type (class) to dehydration function). Dehydration functions\n        receives the value and returns an object of type desired by\n        packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from\n        type (class) to dehydration function). Dehydration functions\n        receives the value of type desired by packstream and are free\n        to return anything.\n    :param handlers: handler functions passed in the returned Response\n        object when received.\n    \"\"\"\n    return self._append(Request(discard=n, qid=qid,\n                                discard_method='discard',\n                       ", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function). Dehydration functions\n        receives the value and returns an object of type Undehyded (not dehydrated).\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receives the value and returns an object of type Undehyded (not dehydrated).\n    :param handlers: handler functions passed in the returned Response object (unused).\n    \"\"\"\n    return self._append(ResponseType.DISCARD, n, qid, dehydration_hooks,\n                        hydration_hooks, handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hopers to dehydrate types (dict from type\n                                (class) to dehydration function). Dehydration\n                                functions receives the value and returns\n                                an object of type desired by packstream.\n    :param hydration_hooks: Hopers to dehydrate types (mapping from\n                                type (class) to dehydration function). Dehydration\n                                functions receives the value of type\n                                desired by packstream and are free to\n                       ", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class)\n                                                            to dehydration function).\n                                Dehydration functions receive the value and returns an\n                                object of type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from\n                                                                type (class) to dehydration\n                                                              ", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function). Dehydration functions\n        receives the value and returns an object of type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function). Dehydration functions\n        receives the value of type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed in the returned Response object is used to return the value of\n        understood by packstream when a response is received.\n    \"\"\"\n    self._append(discard.Discard(n, qid, dehydration_hooks, hydration_hooks, **handlers))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type\n                                    (class) to dehydration function). Dehydration\n                                    functions receives the value and returns\n                                    an object of type desired by packstream.\n    :param hydration_hooks: Hooks to dehydrate types (mapping from type\n                                                                to dehydration\n                                                                function).\n    :param handlers: handler function", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n        Dehydration functions receive the value and returns an object of type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receive the value of type understood by packstream and are free to return\n        anything.\n    :param handlers: handler functions passed in the returned Response object which will be used\n        to resolve the value of type understood by packstream and are free to return anything.\n    \"\"\"\n    self._enqueue('discard', n, qid, dehydration_hooks, hydration_hooks, handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function). Dehydration functions\n        receives the value and returns an object of type underdetected by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receives the value of type underdetected by packstream and are free to return\n        anything.\n    :param handlers: handler functions passed in the returned Response object (unused).\n    \"\"\"\n    if not isinstance(handlers, dict):\n        raise TypeError(\"'handlers' must be of type dict.\")\n\n    self._add_message(DISCARD, n, qid, dehydration_hooks, hydration_hooks, handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n        Dehydration functions receive the value and returns an object of type understood by\n        packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receive the value of type understood by packstream and are free to\n        return things.\n    :param handlers: handler functions passed in the returned Response object instead of the\n        Response object.\n    \"\"\"\n    self._send('DISCARD', {'n': n, 'qid': qid, 'dehydration_hooks': dehydration_hooks,\n                            'hydration_hooks': hydration_hooks, 'handler", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n        Dehydration functions receive the value and returns an object of type understood by\n        packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n        Dehydration functions receive the value of type understood by packstream and are free to\n        return anything.\n    :param handlers: handler functions passed in the returned Response object instead of the returned\n        Response object from the returned method\n    \"\"\"\n    self._enqueue('discard', n=n, qid=qid, dehydration_hooks=dehydration_hooks,\n                                             ", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "begin", "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object", "ques_id": "62e60879d76274f8a4026bec", "code": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this\n                      transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the\n              transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class)\n                             to dehydration function). Dehydration functions\n                             receives the value and returns an object of\n                             type understood by packstream.\n    :param hyperlinks: hyperlinks to be processed\n    :param handlers: handler functions passe", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                        should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                            Dehydration functions receive the value and returns an object of type\n                            understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration\n                           function). Dehydration functions receive the value", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n                  (default)\n    :param bookmarks: iterable of bookmark values after which this\n                       transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from\n                           type (class) to dehydration function). Dehydration\n                           functions receive the value and returns an object\n                           of type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from\n          ", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n                 (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                       should be begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydration types (dict from type (class)\n                              to dehydration function). Dehydration functions\n                              receives the value and returns an object of\n                              type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration types (mapping from", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                            Dehydration functions receive the value and returns an object of type understood by\n                            packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n                            Dehydration functions receive the value of type understood by packstream and are\n    ", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                      should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class)\n                             to dehydration function). Dehydration functions\n                             receives the value and returns an object of\n                             type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from\n                           ", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                      should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                             Dehydration functions receive the value and returns an object of type\n                             understood by packstream.\n    :param handlers: handlers passed in to the returned Response object\n    :return: Response object\n    \"\"\"\n    return self._append('begin', mode=mode, bookmarks=boo", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                       should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                            Dehydration functions receive the value and returns an object of type\n                            understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n                            Dehydration functions receive the value", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                      should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                           Dehydration functions receive the value and returns an object of type\n                           understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n                           Dehydration functions receive the value of type", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                      should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n               (Requires Bolt 4.0+)\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class)\n                           to dehydration function). Dehydration functions\n                           receive the value and returns an object of type\n                           understood by packstream.\n    :param handlers: handler functions passed into the returned Response object\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this\n                      transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the\n              transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class)\n                             to dehydration function). Dehydration functions\n                             receives the value and returns an object of\n                             type understood by packstream.\n    :param hyperlinks: hyperlinks to be processed\n    :param handlers: handler functions passe", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                        should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                            Dehydration functions receive the value and returns an object of type\n                            understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration\n                           function). Dehydration functions receive the value", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n                  (default)\n    :param bookmarks: iterable of bookmark values after which this\n                       transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from\n                           type (class) to dehydration function). Dehydration\n                           functions receive the value and returns an object\n                           of type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from\n          ", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n                 (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                       should be begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydration types (dict from type (class)\n                              to dehydration function). Dehydration functions\n                              receives the value and returns an object of\n                              type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration types (mapping from", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                            Dehydration functions receive the value and returns an object of type understood by\n                            packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n                            Dehydration functions receive the value of type understood by packstream and are\n    ", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                      should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class)\n                             to dehydration function). Dehydration functions\n                             receives the value and returns an object of\n                             type understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from\n                           ", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                      should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                             Dehydration functions receive the value and returns an object of type\n                             understood by packstream.\n    :param handlers: handlers passed in to the returned Response object\n    :return: Response object\n    \"\"\"\n    return self._append('begin', mode=mode, bookmarks=boo", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                       should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                            Dehydration functions receive the value and returns an object of type\n                            understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n                            Dehydration functions receive the value", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                      should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class) to dehydration function).\n                           Dehydration functions receive the value and returns an object of type\n                           understood by packstream.\n    :param hydration_hooks: Hooks to dehydration functions (mapping from type (class) to dehydration function).\n                           Dehydration functions receive the value of type", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction\n                      should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n               (Requires Bolt 4.0+)\n    :param imp_user: the user to impersonate\n    :param dehydration_hooks: Hooks to dehydrate types (dict from type (class)\n                           to dehydration function). Dehydration functions\n                           receive the value and returns an object of type\n                           understood by packstream.\n    :param handlers: handler functions passed into the returned Response object\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/_arithmetic.py", "project": "neo4j/neo4j-python-driver", "name": "round_half_to_even", "docstring": ">>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:", "ques_id": "62e60723d76274f8a4026b75", "code": ["def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round_half_to_even(int(ten_n)))\n    else:\n        return int(round_half_to_even(ten_n))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(ten_n))\n    else:\n        return int(round(ten_n + 0.5))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round_half_to_even(int(ten_n)))\n    else:\n        return int(round_half_to_even(ten_n))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        ten_n = int(round(ten_n))\n        if ten_n == int(round(ten_n)):\n            ten_n = int(round(n))\n    return ten_n\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(ten_n))\n    else:\n        return int(round(ten_n)) + 1\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        return int(round_half_to_even(int(n) / 2))\n    else:\n        return int(round_half_to_even(int(n)) + 0.5)\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        return int(round_half_to_even(int(n / 10))) * 10\n    else:\n        return int(round_half_to_even(n))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(ten_n))\n    else:\n        return int(round_half(ten_n))\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        return int(round_half_to_even(int(n) / 2))\n    else:\n        return int(round_half_to_even(int(n)) + 0.5)\n", "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        return int(round(ten_n / 10))\n    else:\n        return int(round_half_to_odd(ten_n))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round_half_to_even(int(ten_n)))\n    else:\n        return int(round_half_to_even(ten_n))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(ten_n))\n    else:\n        return int(round(ten_n + 0.5))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round_half_to_even(int(ten_n)))\n    else:\n        return int(round_half_to_even(ten_n))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        ten_n = int(round(ten_n))\n        if ten_n == int(round(ten_n)):\n            ten_n = int(round(n))\n    return ten_n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(ten_n))\n    else:\n        return int(round(ten_n)) + 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        return int(round_half_to_even(int(n) / 2))\n    else:\n        return int(round_half_to_even(int(n)) + 0.5)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        return int(round_half_to_even(int(n / 10))) * 10\n    else:\n        return int(round_half_to_even(n))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n):\n        return int(round(ten_n))\n    else:\n        return int(round_half(ten_n))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        return int(round_half_to_even(int(n) / 2))\n    else:\n        return int(round_half_to_even(int(n)) + 0.5)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n    ten_n = 10 * n\n    if ten_n == int(ten_n) and ten_n % 10 == 5:\n        return int(round(ten_n / 10))\n    else:\n        return int(round_half_to_odd(ten_n))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_spatial/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "point_type", "docstring": "Dynamically create a Point subclass.", "ques_id": "62e60707d76274f8a4026b69", "code": ["def point_type(name, fields, srid_map):\n    return type(\n        name,\n        (Point,),\n        {\n            '_fields': tuple(fields),\n            '__module__': 'geomapy.geo.types',\n            '__doc__': Point.__doc__,\n           'srid': srid_map[name],\n        },\n    )\n", "def point_type(name, fields, srid_map):\n    return type(\n        name,\n        (Point,),\n        {\n            '_fields': tuple(fields),\n            '__module__': 'django.contrib.gis',\n           'srid': srid_map[name],\n            **dict(list(fields) + [('srid', srid_map[name])])\n        }\n    )\n", "def point_type(name, fields, srid_map):\n    class Point(Geometry):\n        \"\"\"A simple point geometry.\"\"\"\n        def __init__(self, x, y, z):\n            self.x = x\n            self.y = y\n            self.z = z\n            super(Point, self).__init__(srid=srid_map[fields[0]])\n\n    return Point\n", "def point_type(name, fields, srid_map):\n    class Point(models.Point):\n        \"\"\"A simple point model.\"\"\"\n        def __init__(self, coords, srid=None):\n            super(Point, self).__init__(coords, srid)\n            for key, value in fields.items():\n                setattr(self, key, value)\n\n        def __str__(self):\n            return 'Point with {0.x}, {0.y} {0.z}'.format(self)\n\n    if srid_map:\n        field_names = ', '.join('{0}={1}'.format(k, srid_map[k]) for k in fields)\n        return 'Point({0})'.format(field_names)\n    else:\n        return Point\n", "def point_type(name, fields, srid_map):\n\n    class Point(PointBase):\n        \"\"\" Dynamically create a Point subclass. \"\"\"\n\n        def __init__(self, x, y):\n            self.x = x\n            self.y = y\n\n        def distance(self, other):\n            return math.hypot(self.x - other.x, self.y - other.y)\n\n        def length(self):\n            return math.hypot(self.x - self.y, self.x - self.y)\n\n        def contains(self, other):\n            return self.distance(other) <= srid_map['max_distance']\n\n        def intersects(self, other):\n            return self.distance(other) <= srid_map['max_distance']\n\n        def contains_or_intersects(self, other):\n            return self.distance(other) <= srid_map['max_distance']\n\n        def area(self):\n            return math.hypot(self.x, self.y)\n\n   ", "def point_type(name, fields, srid_map):\n    return type(name, (Point,), {\n        '__module__': 'pyresample.point',\n        '__fields__': tuple(fields),\n        '__srid__': srid_map[name],\n    })\n", "def point_type(name, fields, srid_map):\n    class Point(coconut.schema.Point):\n        def __init__(self, x, y):\n            super().__init__(x, y)\n            self.name = name\n            self.srid = srid_map[self.name]\n\n    return Point\n", "def point_type(name, fields, srid_map):\n\n    return type(name, (Point,), {\n        '_fields': fields,\n        '_srid': srid_map[name],\n    })\n", "def point_type(name, fields, srid_map):\n    return type(name, fields, {'_srid': srid_map.get(srid, 4326)})\n", "def point_type(name, fields, srid_map):\n    class Point(BaseGeometry):\n        \"\"\"Dynamically create a Point subclass.\"\"\"\n        def __init__(self, x, y):\n            super().__init__(srid=srid_map[x][y])\n            self.x = x\n            self.y = y\n\n        def transform(self, mat):\n            return (mat[0] * self.x, mat[1] * self.y, mat[2])\n\n        def length(self):\n            return math.sqrt((self.x - self.y) ** 2 + (self.x - self.y) ** 2)\n\n    name = name.lower()\n    fields = list(fields)\n    fields.sort()\n\n    point = Point(fields[0], fields[1])\n    for i in fields[2:]:\n        point = point.transform(srid_map[i])\n\n    return point\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def point_type(name, fields, srid_map):\n    return type(\n        name,\n        (Point,),\n        {\n            '_fields': tuple(fields),\n            '__module__': 'geomapy.geo.types',\n            '__doc__': Point.__doc__,\n           'srid': srid_map[name],\n        },\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    return type(\n        name,\n        (Point,),\n        {\n            '_fields': tuple(fields),\n            '__module__': 'django.contrib.gis',\n           'srid': srid_map[name],\n            **dict(list(fields) + [('srid', srid_map[name])])\n        }\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    class Point(Geometry):\n        \"\"\"A simple point geometry.\"\"\"\n        def __init__(self, x, y, z):\n            self.x = x\n            self.y = y\n            self.z = z\n            super(Point, self).__init__(srid=srid_map[fields[0]])\n\n    return Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    class Point(models.Point):\n        \"\"\"A simple point model.\"\"\"\n        def __init__(self, coords, srid=None):\n            super(Point, self).__init__(coords, srid)\n            for key, value in fields.items():\n                setattr(self, key, value)\n\n        def __str__(self):\n            return 'Point with {0.x}, {0.y} {0.z}'.format(self)\n\n    if srid_map:\n        field_names = ', '.join('{0}={1}'.format(k, srid_map[k]) for k in fields)\n        return 'Point({0})'.format(field_names)\n    else:\n        return Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\n    class Point(PointBase):\n        \"\"\" Dynamically create a Point subclass. \"\"\"\n\n        def __init__(self, x, y):\n            self.x = x\n            self.y = y\n\n        def distance(self, other):\n            return math.hypot(self.x - other.x, self.y - other.y)\n\n        def length(self):\n            return math.hypot(self.x - self.y, self.x - self.y)\n\n        def contains(self, other):\n            return self.distance(other) <= srid_map['max_distance']\n\n        def intersects(self, other):\n            return self.distance(other) <= srid_map['max_distance']\n\n        def contains_or_intersects(self, other):\n            return self.distance(other) <= srid_map['max_distance']\n\n        def area(self):\n            return math.hypot(self.x, self.y)\n\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    return type(name, (Point,), {\n        '__module__': 'pyresample.point',\n        '__fields__': tuple(fields),\n        '__srid__': srid_map[name],\n    })\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    class Point(coconut.schema.Point):\n        def __init__(self, x, y):\n            super().__init__(x, y)\n            self.name = name\n            self.srid = srid_map[self.name]\n\n    return Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\n    return type(name, (Point,), {\n        '_fields': fields,\n        '_srid': srid_map[name],\n    })\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    return type(name, fields, {'_srid': srid_map.get(srid, 4326)})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n    class Point(BaseGeometry):\n        \"\"\"Dynamically create a Point subclass.\"\"\"\n        def __init__(self, x, y):\n            super().__init__(srid=srid_map[x][y])\n            self.x = x\n            self.y = y\n\n        def transform(self, mat):\n            return (mat[0] * self.x, mat[1] * self.y, mat[2])\n\n        def length(self):\n            return math.sqrt((self.x - self.y) ** 2 + (self.x - self.y) ** 2)\n\n    name = name.lower()\n    fields = list(fields)\n    fields.sort()\n\n    point = Point(fields[0], fields[1])\n    for i in fields[2:]:\n        point = point.transform(srid_map[i])\n\n    return point\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_meta.py", "project": "neo4j/neo4j-python-driver", "name": "deprecated", "docstring": "Decorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass", "ques_id": "62e5dc9ed76274f8a4026b5b", "code": ["def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n    def f__(f):\n        @wraps(f)\n        def f_(*args, **kwargs):\n            deprecation_warn(message)\n            return f(*args, **kwargs)\n        return f_\n    return f__\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/r.py", "project": "pre-commit/pre-commit", "name": "_inline_r_setup", "docstring": "Some behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.", "ques_id": "62e4fc3c85ea98643089041e", "code": ["def _inline_r_setup(code: str) -> str:\n    env = os.environ\n    if 'R_CONFIG' in env:\n        config_file = env['R_CONFIG']\n        if config_file and os.path.isfile(config_file):\n            with open(config_file, 'r') as f:\n                code = f.read()\n    return code\n", "def _inline_r_setup(code: str) -> str:\n    env = os.environ\n    r_inline = env.get('R', None)\n    if r_inline is None:\n        r_inline = env.get('R_INLINE', None)\n    if r_inline is None:\n        r_inline = ''\n    return f'R = {r_inline}'\n", "def _inline_r_setup(code: str) -> str:\n    env = os.environ.copy()\n    env['R_VERSION'] = 'unknown'\n\n    proc = subprocess.Popen(\n        ['env', 'R'],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env)\n    stdout, stderr = proc.communicate(code.encode('utf-8'))\n\n    if proc.returncode!= 0:\n        raise Exception('env failed: {}'.format(stderr.decode('utf-8')))\n\n    return stdout.decode('utf-8')\n", "def _inline_r_setup(code: str) -> str:\n    if 'R_RUN_ENV' not in os.environ:\n        os.environ['R_RUN_ENV'] = code\n        os.environ['R_RUN_ON_STARTUP'] = '1'\n    return code\n", "def _inline_r_setup(code: str) -> str:\n    inline = (\n        \"from _rpython import r, get_ipython\\n\"\n        \"from _rpython.config import inline_r\\n\"\n        \"from _rpython.util import unquote_plus\\n\"\n    )\n    if code:\n        inline += \"from _rpython.util import quote_plus\\n\"\n        inline += \"from _rpython.util import unquote_plus\\n\"\n        inline += \"from _rpython.util import quote_plus\\n\"\n        inline += \"from _rpython.util import unquote_plus\\n\"\n        inline += \"from _rpython.util import quote_plus\\n\"\n        inline += \"from _rpython import unquote_plus\\n\"\n        inline += \"from _rpython import quote_plus\\n\"\n        inline += \"from _rpython import unquote_plus\\n\"\n        inline += \"from _rpython import quote_plus\\n\"\n        inline += \"from _rpython import unquote_plus\\n\"\n        inline += \"from _rpython i", "def _inline_r_setup(code: str) -> str:\n    if \"R\" in os.environ:\n        raise Exception(\n            \"Cannot set environment variable R in test runs. Please set a new one in the tests/ directory\"\n        )\n\n    return _inline_r_setup_impl(code)\n", "def _inline_r_setup(code: str) -> str:\n    env = os.environ.copy()\n    env[\"R_ENVIRONMENT\"] = code\n    return _inline_r(env)\n", "def _inline_r_setup(code: str) -> str:\n    env = os.environ.copy()\n    env.update(_get_r_env())\n    cmd = f\"{_get_r_exec()} -I -e'set -e; set PYTHONPATH={env['PYTHONPATH']}'\"\n    return subprocess.check_output(cmd, shell=True, env=env).decode()\n", "def _inline_r_setup(code: str) -> str:\n    r_command = os.getenv('R_COMMAND')\n    if not r_command:\n        r_command = ''\n    env_command = os.getenv('R_COMMAND_INLINE', '')\n    if not env_command:\n        env_command = ''\n    command = f'{r_command} --r-enable-external-tools {env_command} {code}'\n    return command\n", "def _inline_r_setup(code: str) -> str:\n    import os\n    import re\n\n    env_file = os.environ.get(\"JUJU_R_CONFIG\", None)\n    if not env_file:\n        return code\n\n    r_prefix = \"JUJU_R_\"\n\n    r_config = re.compile(r'{}(?P<file>[^{}]*){}'.format(r_prefix, os.linesep, os.linesep))\n\n    for line in env_file.splitlines():\n        match = r_config.match(line)\n        if not match:\n            continue\n\n        option_name = match.group(\"file\")\n        option_value = match.group(\"file\")\n\n        # Ignore options that are set via the env variable\n        if option_name in os.environ:\n            continue\n\n        # Ignore options that are set via the env option -- we can't override it\n        if re.match(r'^{}'.format(r_prefix), option_value):\n            continue\n\n        return code.replace(\"JUJU_R_{}\".format(option_name), option_value)\n\n    # If we"], "level": "self_contained", "generate_results": [{"generate_code": "def _inline_r_setup(code: str) -> str:\n    env = os.environ\n    if 'R_CONFIG' in env:\n        config_file = env['R_CONFIG']\n        if config_file and os.path.isfile(config_file):\n            with open(config_file, 'r') as f:\n                code = f.read()\n    return code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    env = os.environ\n    r_inline = env.get('R', None)\n    if r_inline is None:\n        r_inline = env.get('R_INLINE', None)\n    if r_inline is None:\n        r_inline = ''\n    return f'R = {r_inline}'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    env = os.environ.copy()\n    env['R_VERSION'] = 'unknown'\n\n    proc = subprocess.Popen(\n        ['env', 'R'],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env)\n    stdout, stderr = proc.communicate(code.encode('utf-8'))\n\n    if proc.returncode!= 0:\n        raise Exception('env failed: {}'.format(stderr.decode('utf-8')))\n\n    return stdout.decode('utf-8')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    if 'R_RUN_ENV' not in os.environ:\n        os.environ['R_RUN_ENV'] = code\n        os.environ['R_RUN_ON_STARTUP'] = '1'\n    return code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    inline = (\n        \"from _rpython import r, get_ipython\\n\"\n        \"from _rpython.config import inline_r\\n\"\n        \"from _rpython.util import unquote_plus\\n\"\n    )\n    if code:\n        inline += \"from _rpython.util import quote_plus\\n\"\n        inline += \"from _rpython.util import unquote_plus\\n\"\n        inline += \"from _rpython.util import quote_plus\\n\"\n        inline += \"from _rpython.util import unquote_plus\\n\"\n        inline += \"from _rpython.util import quote_plus\\n\"\n        inline += \"from _rpython import unquote_plus\\n\"\n        inline += \"from _rpython import quote_plus\\n\"\n        inline += \"from _rpython import unquote_plus\\n\"\n        inline += \"from _rpython import quote_plus\\n\"\n        inline += \"from _rpython import unquote_plus\\n\"\n        inline += \"from _rpython i", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    if \"R\" in os.environ:\n        raise Exception(\n            \"Cannot set environment variable R in test runs. Please set a new one in the tests/ directory\"\n        )\n\n    return _inline_r_setup_impl(code)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    env = os.environ.copy()\n    env[\"R_ENVIRONMENT\"] = code\n    return _inline_r(env)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    env = os.environ.copy()\n    env.update(_get_r_env())\n    cmd = f\"{_get_r_exec()} -I -e'set -e; set PYTHONPATH={env['PYTHONPATH']}'\"\n    return subprocess.check_output(cmd, shell=True, env=env).decode()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    r_command = os.getenv('R_COMMAND')\n    if not r_command:\n        r_command = ''\n    env_command = os.getenv('R_COMMAND_INLINE', '')\n    if not env_command:\n        env_command = ''\n    command = f'{r_command} --r-enable-external-tools {env_command} {code}'\n    return command\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n    import os\n    import re\n\n    env_file = os.environ.get(\"JUJU_R_CONFIG\", None)\n    if not env_file:\n        return code\n\n    r_prefix = \"JUJU_R_\"\n\n    r_config = re.compile(r'{}(?P<file>[^{}]*){}'.format(r_prefix, os.linesep, os.linesep))\n\n    for line in env_file.splitlines():\n        match = r_config.match(line)\n        if not match:\n            continue\n\n        option_name = match.group(\"file\")\n        option_value = match.group(\"file\")\n\n        # Ignore options that are set via the env variable\n        if option_name in os.environ:\n            continue\n\n        # Ignore options that are set via the env option -- we can't override it\n        if re.match(r'^{}'.format(r_prefix), option_value):\n            continue\n\n        return code.replace(\"JUJU_R_{}\".format(option_name), option_value)\n\n    # If we", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/xargs.py", "project": "pre-commit/pre-commit", "name": "xargs", "docstring": "A simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently", "ques_id": "62e4fbda85ea986430890405", "code": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    logger.info(\"Running xargs %s\", \" \".join(cmd))\n    if not IS_WINDOWS:\n        logger.info(\"Using pty on non-windows, falling back to non-blocking\")\n        return (\n            \"pty\",\n            \"-c\",\n            str(target_concurrency),\n        )\n    return _xargs(\n        cmd,\n        varargs,\n        _max_length,\n        color,\n        target_concurrency,\n        **kwargs,\n    )\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> list[str]:\n    \"\"\"\n    A simplified implementation of xargs.\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently\n    \"\"\"\n    if not is_supported_platform():\n        return []\n\n    env = os.environ.copy()\n    env['PYTHONPATH'] = os.pathsep.join(sys.path)\n\n    kwargs = _ensure_vars(\n        env,\n        varargs,\n        target_concurrency,\n        color,\n        _max_length,\n        **kwargs\n    )\n\n    process = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        **kwargs\n    )\n\n    stdout, stderr = process.communicate()\n\n    if process.returncode!= 0:\n        raise XargsError(process.returncode, stderr.decode())\n\n    return ", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n\n    \"\"\"\n    p = subprocess.Popen(\n        cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n        close_fds=True, universal_newlines=True,\n        bufsize=0, executable='/bin/bash', shell=False, cwd=HERE,\n        env={**os.environ, 'COLOR': 'none' if color else ''},\n        **kwargs)\n    p.stdin.close()\n    stdout, stderr = p.communicate(input=varargs)\n    stdout = stdout.decode('utf-8')\n    stderr = stderr.decode('utf-8')\n    return p.returncode, stdout, stderr\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # xargs requires Python 2 on Windows, and supports Python 3 on Unix\n    if not is_windows:\n        return _xargs_fallback(cmd, varargs, color=color, target_concurrency=target_concurrency)\n\n    # Do not use `with` here because the code in this function will return\n    # `None` if `cmd` is not a tuple or a list.\n    with _xargs_lock:\n        # If the `color` argument is `True`, make a pty if on a platform\n        # that supports it\n        if color:\n            return _xargs_concurrency(\n                cmd, varargs, target_concurrency=target_concurrency,\n                _max_length=_max_length)\n     ", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently (default to 1).\n    \"\"\"\n    pty = None\n    if sys.platform == 'win32':\n        pty = _windows_pty()\n        if not pty:\n            _windows_pty()\n            if _windows_pty():\n                pty = _windows_pty()\n    if not pty:\n        return _popen_or_die(cmd, varargs, **kwargs)\n    elif pty.is_alive():\n        return _popen_or_die(cmd, varargs, **kwargs)\n    else:\n        return pty\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any) -> str:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if sys.platform.startswith('win'):\n        raise NotImplementedError(\"Windows only\")\n    elif sys.platform.startswith('darwin'):\n        raise NotImplementedError(\"Mac OSX\")\n\n    p = subprocess.Popen(\n        cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE, close_fds=True,\n        universal_newlines=True)\n\n    def _handle_output(p: subprocess.Popen, output: str) -> None:\n        sys.stdout.write(output.strip())\n        if color:\n            sys.stdout.flush()\n\n    try:\n        output = p.communicate(input=varargs)[0]\n        _handle_output(p, output)\n    except subprocess.CalledProcessErro", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently configured at this time\n    \"\"\"\n    if sys.platform.startswith('win'):\n        return _xargs_windows(\n            cmd, varargs, color, target_concurrency, _max_length, **kwargs\n        )\n    return _xargs_posix(\n        cmd, varargs, color, target_concurrency, _max_length, **kwargs\n    )\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently configured for processes\n    \"\"\"\n    if is_linux:\n        return _xargs_linux(cmd, varargs, color, target_concurrency, _max_length, **kwargs)\n    else:\n        return _xargs_windows(cmd, varargs, color, target_concurrency, _max_length, **kwargs)\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n    color: Make a pty if on a platform that supports it target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if sys.platform.startswith('win'):\n        kwargs['stdout'] = DEVNULL\n        kwargs['stderr'] = DEVNULL\n        kwargs['close_fds'] = True\n        # On Windows we can't use subprocess.Popen in the case of a command line argument\n        if isinstance(cmd, tuple):\n            subprocess.Popen(cmd)\n        else:\n            subprocess.Popen(['\"{}\"'.format(cmd)] + list(varargs))\n        return ('', '')\n    else:\n        return run(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False,\n            target_concurrency=target_concurrency, _max_length=_max_length, **kwargs)\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> str:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it target_concurrency: Target number of partitions to\n    run concurrently\n    \"\"\"\n    if is_windows:\n        raise NotImplementedError(\"This function is not available on Windows\")\n\n    if not isinstance(varargs, list):\n        varargs = [varargs]\n\n    p = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True,\n        shell=False,\n        bufsize=0,\n        close_fds=True,\n        preexec_fn=os.setsid,\n        **kwargs\n    )\n\n    if color:\n        output = p.stdout\n        output = output.encode(sys.getfilesystemencoding())\n        p.stdout = output\n\n    for var in varargs:\n     "], "level": "project_runnable", "generate_results": [{"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    logger.info(\"Running xargs %s\", \" \".join(cmd))\n    if not IS_WINDOWS:\n        logger.info(\"Using pty on non-windows, falling back to non-blocking\")\n        return (\n            \"pty\",\n            \"-c\",\n            str(target_concurrency),\n        )\n    return _xargs(\n        cmd,\n        varargs,\n        _max_length,\n        color,\n        target_concurrency,\n        **kwargs,\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> list[str]:\n    \"\"\"\n    A simplified implementation of xargs.\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently\n    \"\"\"\n    if not is_supported_platform():\n        return []\n\n    env = os.environ.copy()\n    env['PYTHONPATH'] = os.pathsep.join(sys.path)\n\n    kwargs = _ensure_vars(\n        env,\n        varargs,\n        target_concurrency,\n        color,\n        _max_length,\n        **kwargs\n    )\n\n    process = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        **kwargs\n    )\n\n    stdout, stderr = process.communicate()\n\n    if process.returncode!= 0:\n        raise XargsError(process.returncode, stderr.decode())\n\n    return ", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n\n    \"\"\"\n    p = subprocess.Popen(\n        cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n        close_fds=True, universal_newlines=True,\n        bufsize=0, executable='/bin/bash', shell=False, cwd=HERE,\n        env={**os.environ, 'COLOR': 'none' if color else ''},\n        **kwargs)\n    p.stdin.close()\n    stdout, stderr = p.communicate(input=varargs)\n    stdout = stdout.decode('utf-8')\n    stderr = stderr.decode('utf-8')\n    return p.returncode, stdout, stderr\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # xargs requires Python 2 on Windows, and supports Python 3 on Unix\n    if not is_windows:\n        return _xargs_fallback(cmd, varargs, color=color, target_concurrency=target_concurrency)\n\n    # Do not use `with` here because the code in this function will return\n    # `None` if `cmd` is not a tuple or a list.\n    with _xargs_lock:\n        # If the `color` argument is `True`, make a pty if on a platform\n        # that supports it\n        if color:\n            return _xargs_concurrency(\n                cmd, varargs, target_concurrency=target_concurrency,\n                _max_length=_max_length)\n     ", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently (default to 1).\n    \"\"\"\n    pty = None\n    if sys.platform == 'win32':\n        pty = _windows_pty()\n        if not pty:\n            _windows_pty()\n            if _windows_pty():\n                pty = _windows_pty()\n    if not pty:\n        return _popen_or_die(cmd, varargs, **kwargs)\n    elif pty.is_alive():\n        return _popen_or_die(cmd, varargs, **kwargs)\n    else:\n        return pty\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any) -> str:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it target_concurrency:\n\n        target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if sys.platform.startswith('win'):\n        raise NotImplementedError(\"Windows only\")\n    elif sys.platform.startswith('darwin'):\n        raise NotImplementedError(\"Mac OSX\")\n\n    p = subprocess.Popen(\n        cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE, close_fds=True,\n        universal_newlines=True)\n\n    def _handle_output(p: subprocess.Popen, output: str) -> None:\n        sys.stdout.write(output.strip())\n        if color:\n            sys.stdout.flush()\n\n    try:\n        output = p.communicate(input=varargs)[0]\n        _handle_output(p, output)\n    except subprocess.CalledProcessErro", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently configured at this time\n    \"\"\"\n    if sys.platform.startswith('win'):\n        return _xargs_windows(\n            cmd, varargs, color, target_concurrency, _max_length, **kwargs\n        )\n    return _xargs_posix(\n        cmd, varargs, color, target_concurrency, _max_length, **kwargs\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color:\n    Make a pty if on a platform that supports it target_concurrency:\n    Target number of partitions to run concurrently configured for processes\n    \"\"\"\n    if is_linux:\n        return _xargs_linux(cmd, varargs, color, target_concurrency, _max_length, **kwargs)\n    else:\n        return _xargs_windows(cmd, varargs, color, target_concurrency, _max_length, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[str,...]:\n    \"\"\"\n    A simplified implementation of xargs.\n    color: Make a pty if on a platform that supports it target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if sys.platform.startswith('win'):\n        kwargs['stdout'] = DEVNULL\n        kwargs['stderr'] = DEVNULL\n        kwargs['close_fds'] = True\n        # On Windows we can't use subprocess.Popen in the case of a command line argument\n        if isinstance(cmd, tuple):\n            subprocess.Popen(cmd)\n        else:\n            subprocess.Popen(['\"{}\"'.format(cmd)] + list(varargs))\n        return ('', '')\n    else:\n        return run(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False,\n            target_concurrency=target_concurrency, _max_length=_max_length, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any\n) -> str:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it target_concurrency: Target number of partitions to\n    run concurrently\n    \"\"\"\n    if is_windows:\n        raise NotImplementedError(\"This function is not available on Windows\")\n\n    if not isinstance(varargs, list):\n        varargs = [varargs]\n\n    p = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True,\n        shell=False,\n        bufsize=0,\n        close_fds=True,\n        preexec_fn=os.setsid,\n        **kwargs\n    )\n\n    if color:\n        output = p.stdout\n        output = output.encode(sys.getfilesystemencoding())\n        p.stdout = output\n\n    for var in varargs:\n     ", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/helpers.py", "project": "pre-commit/pre-commit", "name": "_shuffled", "docstring": "Deterministically shuffle", "ques_id": "62e4fbda85ea986430890403", "code": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    return random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    random.shuffle(seq)\n    return seq\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    random.shuffle(seq)\n    return seq\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    import random\n    return list(\n        map(\n            lambda x: ''.join(\n                random.choice('qwertyuiop') if c == '1' else c for c in x\n            ),\n            seq\n        )\n    )\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    return _random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    import random\n    return [i for i in seq if random.random() < 0.1] + [i for i in seq if not random.random() < 0.1]\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    import random\n    return list(map(_single_char, random.sample(seq, len(seq))))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    return random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    return random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    return random.sample(seq, len(seq))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    return random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    random.shuffle(seq)\n    return seq\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    random.shuffle(seq)\n    return seq\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    import random\n    return list(\n        map(\n            lambda x: ''.join(\n                random.choice('qwertyuiop') if c == '1' else c for c in x\n            ),\n            seq\n        )\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    return _random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    import random\n    return [i for i in seq if random.random() < 0.1] + [i for i in seq if not random.random() < 0.1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    import random\n    return list(map(_single_char, random.sample(seq, len(seq))))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    return random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    return random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n    return random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/util.py", "project": "pre-commit/pre-commit", "name": "parse_version", "docstring": "poor man's version comparison", "ques_id": "62e4fb6585ea98643089032b", "code": ["def parse_version(s: str) -> tuple[int, ...]:\n    # split on \".\" and strip the [v]\n    return tuple(int(x) for x in s.split(\".\")[:-1])\n", "def parse_version(s: str) -> tuple[int, ...]:\n    try:\n        return tuple(int(x) for x in s.split(\".\"))\n    except ValueError:\n        return (0,)\n", "def parse_version(s: str) -> tuple[int, ...]:\n    m = re.match(r'(\\d+)\\.(\\d+)(?:-(\\d+))?', s)\n    if not m:\n        raise ValueError('invalid version string {!r}'.format(s))\n    return tuple(m.groups())\n", "def parse_version(s: str) -> tuple[int, ...]:\n    try:\n        return tuple(int(x) for x in s.split(\".\"))\n    except ValueError:\n        return ()\n", "def parse_version(s: str) -> tuple[int, ...]:\n    return tuple(int(x) for x in s.split(\".\"))\n", "def parse_version(s: str) -> tuple[int, ...]:\n    s = s.split(\".\")\n    if len(s) == 3:\n        s = tuple(int(i) for i in s)\n    else:\n        s = tuple(int(i) for i in s[:-1]) + (s[-1],)\n    return s\n", "def parse_version(s: str) -> tuple[int, ...]:\n    try:\n        return tuple(int(x) for x in s.split(\".\"))\n    except ValueError:\n        raise ValueError(\"not a valid version string: %r\" % s)\n", "def parse_version(s: str) -> tuple[int, ...]:\n    try:\n        return tuple(int(x) for x in s.split(\".\"))\n    except ValueError:\n        raise ParseError(\"version must be integer\")\n", "def parse_version(s: str) -> tuple[int, ...]:\n    m = re.match(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\", s)\n    assert m, f\"bad version string: {s!r}\"\n    return tuple(map(int, m.groups()))\n", "def parse_version(s: str) -> tuple[int, ...]:\n    return tuple(int(x) for x in s.split(\".\"))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    # split on \".\" and strip the [v]\n    return tuple(int(x) for x in s.split(\".\")[:-1])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    try:\n        return tuple(int(x) for x in s.split(\".\"))\n    except ValueError:\n        return (0,)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    m = re.match(r'(\\d+)\\.(\\d+)(?:-(\\d+))?', s)\n    if not m:\n        raise ValueError('invalid version string {!r}'.format(s))\n    return tuple(m.groups())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    try:\n        return tuple(int(x) for x in s.split(\".\"))\n    except ValueError:\n        return ()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    return tuple(int(x) for x in s.split(\".\"))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    s = s.split(\".\")\n    if len(s) == 3:\n        s = tuple(int(i) for i in s)\n    else:\n        s = tuple(int(i) for i in s[:-1]) + (s[-1],)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    try:\n        return tuple(int(x) for x in s.split(\".\"))\n    except ValueError:\n        raise ValueError(\"not a valid version string: %r\" % s)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    try:\n        return tuple(int(x) for x in s.split(\".\"))\n    except ValueError:\n        raise ParseError(\"version must be integer\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    m = re.match(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\", s)\n    assert m, f\"bad version string: {s!r}\"\n    return tuple(map(int, m.groups()))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n    return tuple(int(x) for x in s.split(\".\"))\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/parse_shebang.py", "project": "pre-commit/pre-commit", "name": "normalize_cmd", "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine", "ques_id": "62e4fb4d85ea9864308902e7", "code": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    if sys.platform == \"win32\" and \"/usr/bin/noshell\" in cmd[0]:\n        cmd[0] = cmd[0].replace(\"/usr/bin/noshell\", \"/usr/bin\")\n    return cmd\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # cmd is a tuple of (args, env) pairs\n    # The normalize_cmd calls normalize_args on the args list and returns the\n    # normalized versions of the args list so we don't need to do anything further\n    # than this because the normalization is done on the args list in normalize_cmd\n    # and the return values here should be the same as the original cmd\n    args, env = cmd\n\n    # If the environment variable is not set, we are running in a new\n    # process, so we must use /usr/local/bin/bash. This is for backwards\n    # compatibility with python < 2.6\n    if env is not None:\n        args = [\"-c\", \"set -e\", env[0]] + args\n\n    return args\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    if os.name == \"nt\":\n        # On Windows, the command is \"/usr/bin/nt-config\".\n        # We need to replace the first two components with \"/usr\" so that the\n        # resulting command starts with \"/usr/bin\"\n        cmd = cmd[2:]\n        cmd[0] = \"/usr\"\n    return cmd\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    if sys.platform == \"win32\" and \"/usr/bin/env\" not in cmd[0]:\n        # /usr/bin/env is the same as /usr/bin, but /usr/local/bin/env is the same as /usr/local\n        # So we need to remove the /usr/bin to avoid creating shebangs like this:\n        #     cp -r /usr/bin/env /usr/local/bin/env\n        cmd[0] = cmd[0].replace(\"/usr/bin/env\", \"/usr/local/bin/env\")\n    return cmd\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # Taken from https://stackoverflow.com/a/45091961\n    cmd = (\n        cmd[0],\n        os.path.normpath(cmd[1]),\n        os.path.normpath(cmd[2]),\n        os.path.normpath(cmd[3]),\n    )\n    return cmd\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # Python 3 does not have a shebang field, so we need to look at the\n    # extension in the resulting command\n    ext = os.path.splitext(cmd[1])[1]\n    if ext == \".py\":\n        ext = \"\"\n\n    # Use PATH to find the executable\n    exe = shlex.split(cmd[0])[0]\n\n    # Filter out anything not in the standard (non-executable) shebangs\n    # and keep the common case\n    cmd = [f for f in cmd if f[0]!= ext]\n\n    # Filter out any paths in the PATH shebangs\n    if os.path.isfile(exe):\n        exe = os.path.realpath(exe)\n\n    return exe, ext\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # TODO: support multiple inheritance (shebangs and executables)\n    # (other are unknown in this case)\n    cmd = list(cmd)\n    if sys.platform == \"win32\":\n        executable = cmd.pop(0)\n        if executable and executable!= \"exec\":\n            # fall back to executable if it wasn't executable\n            executable = \"{}.exe\".format(executable)\n        executable = os.path.normpath(executable)\n        cmd[0] = executable\n    return tuple(cmd)\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # normalize cmd on windows, this is safer than os.path.normcase\n    cmd = (\n        cmd[0].replace(\"/\", os.sep),\n        os.path.normcase(cmd[1]).replace(\"/\", os.sep),\n    )\n    return cmd\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    cmd = list(cmd)\n    # Under Windows, the paths in cmd are absolute, so we need to reverse\n    # the tuple here for the case where cmd[0] is the path\n    if sys.platform == 'win32':\n        cmd[0] = sys.executable, cmd[0][1:]\n    return cmd\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    if sys.platform == 'win32':\n        cmd = tuple([cmd[0], cmd[1]])\n    return cmd\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    if sys.platform == \"win32\" and \"/usr/bin/noshell\" in cmd[0]:\n        cmd[0] = cmd[0].replace(\"/usr/bin/noshell\", \"/usr/bin\")\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # cmd is a tuple of (args, env) pairs\n    # The normalize_cmd calls normalize_args on the args list and returns the\n    # normalized versions of the args list so we don't need to do anything further\n    # than this because the normalization is done on the args list in normalize_cmd\n    # and the return values here should be the same as the original cmd\n    args, env = cmd\n\n    # If the environment variable is not set, we are running in a new\n    # process, so we must use /usr/local/bin/bash. This is for backwards\n    # compatibility with python < 2.6\n    if env is not None:\n        args = [\"-c\", \"set -e\", env[0]] + args\n\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    if os.name == \"nt\":\n        # On Windows, the command is \"/usr/bin/nt-config\".\n        # We need to replace the first two components with \"/usr\" so that the\n        # resulting command starts with \"/usr/bin\"\n        cmd = cmd[2:]\n        cmd[0] = \"/usr\"\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    if sys.platform == \"win32\" and \"/usr/bin/env\" not in cmd[0]:\n        # /usr/bin/env is the same as /usr/bin, but /usr/local/bin/env is the same as /usr/local\n        # So we need to remove the /usr/bin to avoid creating shebangs like this:\n        #     cp -r /usr/bin/env /usr/local/bin/env\n        cmd[0] = cmd[0].replace(\"/usr/bin/env\", \"/usr/local/bin/env\")\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # Taken from https://stackoverflow.com/a/45091961\n    cmd = (\n        cmd[0],\n        os.path.normpath(cmd[1]),\n        os.path.normpath(cmd[2]),\n        os.path.normpath(cmd[3]),\n    )\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # Python 3 does not have a shebang field, so we need to look at the\n    # extension in the resulting command\n    ext = os.path.splitext(cmd[1])[1]\n    if ext == \".py\":\n        ext = \"\"\n\n    # Use PATH to find the executable\n    exe = shlex.split(cmd[0])[0]\n\n    # Filter out anything not in the standard (non-executable) shebangs\n    # and keep the common case\n    cmd = [f for f in cmd if f[0]!= ext]\n\n    # Filter out any paths in the PATH shebangs\n    if os.path.isfile(exe):\n        exe = os.path.realpath(exe)\n\n    return exe, ext\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # TODO: support multiple inheritance (shebangs and executables)\n    # (other are unknown in this case)\n    cmd = list(cmd)\n    if sys.platform == \"win32\":\n        executable = cmd.pop(0)\n        if executable and executable!= \"exec\":\n            # fall back to executable if it wasn't executable\n            executable = \"{}.exe\".format(executable)\n        executable = os.path.normpath(executable)\n        cmd[0] = executable\n    return tuple(cmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    # normalize cmd on windows, this is safer than os.path.normcase\n    cmd = (\n        cmd[0].replace(\"/\", os.sep),\n        os.path.normcase(cmd[1]).replace(\"/\", os.sep),\n    )\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    cmd = list(cmd)\n    # Under Windows, the paths in cmd are absolute, so we need to reverse\n    # the tuple here for the case where cmd[0] is the path\n    if sys.platform == 'win32':\n        cmd[0] = sys.executable, cmd[0][1:]\n    return cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    if sys.platform == 'win32':\n        cmd = tuple([cmd[0], cmd[1]])\n    return cmd\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cached", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nresults in a cache.", "ques_id": "62b8d27a48ba5a41d1c3f4c6", "code": ["def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(f):\n        cached_f = _Cached(f, cache, key, lock)\n        return lambda *a, **kw: cached_f(*a, **kw)\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(f):\n        cached_func = _Cached(f, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(f):\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            key = keyfunc(*args, **kwargs)\n            result = cache.get(key)\n            if result is None:\n                result = cache.set(key, f(*args, **kwargs))\n                lock.acquire()\n                try:\n                    return result\n                finally:\n                    lock.release()\n            return result\n        return wrapper\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(fn):\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            key = keyfunc(*args, **kwargs)\n            rv = cache.get(key)\n            if not rv:\n                rv = fn(*args, **kwargs)\n                cache.set(key, rv, cache_expiry)\n            return rv\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(fn):\n        cached_fn = _Cached(fn, key, lock)\n        return lambda *a, **kw: cached_fn(*a, **kw)\n\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(f):\n        cached_f = _Cached(f, cache, key, lock)\n        return lambda *a, **kw: cached_f(*a, **kw)\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(f):\n        cached_func = _Cached(f, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(f):\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            key = keyfunc(*args, **kwargs)\n            result = cache.get(key)\n            if result is None:\n                result = cache.set(key, f(*args, **kwargs))\n                lock.acquire()\n                try:\n                    return result\n                finally:\n                    lock.release()\n            return result\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(fn):\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            key = keyfunc(*args, **kwargs)\n            rv = cache.get(key)\n            if not rv:\n                rv = fn(*args, **kwargs)\n                cache.set(key, rv, cache_expiry)\n            return rv\n        return wrapper\n    if lock is None:\n        lock = threading.Lock()\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(fn):\n        cached_fn = _Cached(fn, key, lock)\n        return lambda *a, **kw: cached_fn(*a, **kw)\n\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorator(func):\n        cached_func = _Cached(func, cache, key, lock)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "ttl_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.", "ques_id": "62b8d24048ba5a41d1c3f49f", "code": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cached('ttl', maxsize, ttl, timer, False, typed)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cached('ttl', maxsize, ttl, timer, False, typed)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cache('ttl', maxsize, ttl, timer, False, typed)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cache('ttl', maxsize, ttl, timer, typed)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if not isinstance(ttl, int):\n        raise TypeError('ttl must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than zero')\n    if ttl < 0:\n        raise ValueError('ttl must be greater than zero')\n    if timer is None:\n        return functools.partial(ttl_cache, maxsize=maxsize, ttl=ttl,\n                                typed=typed)\n\n    def wrapper(fn):\n        cached = {}\n\n        @functools.wraps(fn)\n        def wrapped(*args, **kwargs):\n            key = str((args, kwargs))\n            if key not in cached:\n                cached[key] = fn(*args, **kwargs)\n\n            value = c", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cached('ttl', maxsize, ttl, timer, False, typed)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return cached(TTLCache(maxsize, ttl, timer, typed))\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cached('ttl', maxsize, ttl, timer, False, typed)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cache('ttl', maxsize, ttl, timer, False, typed)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cache('ttl', maxsize, ttl, timer, False, typed)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cached('ttl', maxsize, ttl, timer, False, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cached('ttl', maxsize, ttl, timer, False, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cache('ttl', maxsize, ttl, timer, False, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cache('ttl', maxsize, ttl, timer, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if not isinstance(ttl, int):\n        raise TypeError('ttl must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than zero')\n    if ttl < 0:\n        raise ValueError('ttl must be greater than zero')\n    if timer is None:\n        return functools.partial(ttl_cache, maxsize=maxsize, ttl=ttl,\n                                typed=typed)\n\n    def wrapper(fn):\n        cached = {}\n\n        @functools.wraps(fn)\n        def wrapped(*args, **kwargs):\n            key = str((args, kwargs))\n            if key not in cached:\n                cached[key] = fn(*args, **kwargs)\n\n            value = c", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cached('ttl', maxsize, ttl, timer, False, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return cached(TTLCache(maxsize, ttl, timer, typed))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cached('ttl', maxsize, ttl, timer, False, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cache('ttl', maxsize, ttl, timer, False, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    return _cache('ttl', maxsize, ttl, timer, False, typed)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "mru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.", "ques_id": "62b8d23b48ba5a41d1c3f49a", "code": ["def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize=128)\n    else:\n        return _MRUCache(maxsize, typed)\n", "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable (and not a keyword)\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def decorate(f):\n        cache = MruCache(f, maxsize, typed)\n        return cache\n    return decorate\n", "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable (and not a keyword)\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def decorator(func):\n        # Must be a function\n        if not isinstance(func, _CacheMeta):\n            raise TypeError('@mru_cache decorator must be applied to '\n                            'functions that return cached results.')\n\n        # Store the result of the decorated function\n        cache = MemoizeCache(maxsize, typed)\n        # Start the process of caching the result\n        result = func(cache)\n        # Put the result in the cache, and return the function\n        cache.store(func, result)\n        return result\n    return decorator\n", "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable (i.e. decorator)\n    if not callable(maxsize):\n        raise TypeError('maxsize must be a callable function.')\n\n    # Must return the cached result if it exists\n    if typed:\n        @functools.wraps(maxsize)\n        def wrapper(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return CACHE[key]\n            except KeyError:\n                ret = CACHE[key] = typed_maxsize(*args, **kwargs)\n                return ret\n    else:\n        @functools.wraps(maxsize)\n        def wrapper(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return CACHE[key]\n            except KeyError:\n          ", "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable and not a keyword.\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def decorator(func):\n        cached = MruCache(maxsize, typed)\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cached._cache[key]\n            except KeyError:\n                value = cached._cache[key] = func(*args, **kwargs)\n                return value\n        cached._cache = {}\n        return wrapper\n    return decorator\n", "def mru_cache(maxsize=128, typed=False):\n    # Must be a function\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def decorator(fn):\n        cached = MruCache(fn, maxsize, typed)\n        return lambda *a, **kw: cached.wrap(fn(*a, **kw))\n    return decorator\n", "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable and not a keyword.\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than zero')\n\n    def decorator(func):\n        cached = MRU(maxsize=maxsize, typed=typed)(func)\n        cached.cache_clear()\n        return cached\n    return decorator\n", "def mru_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> def mru_cache(maxsize, typed=False):\n    #  ...     return MruCache(maxsize, typed)\n    def decorator(fn):\n        cached_fn = _CachedFunctionWrapper(fn, fn)\n        if maxsize is None or maxsize <= 0:\n            return cached_fn\n\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = cached_fn(*args, **kwargs)\n                return ret\n\n        if typed:\n            wrapper.__wrapped__ = lambda *a, **kw: type(args[0])(wrapper(*a, **kw))\n        else:\n         ", "def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize=128)\n    else:\n        return _MRUCache(maxsize, typed)\n", "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable (i.e. decorator)\n    if not callable(maxsize):\n        raise TypeError('maxsize must be a callable function')\n\n    def wrapper(f):\n        cache = MruCache(f, maxsize, typed)\n        return cache\n    return wrapper\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize=128)\n    else:\n        return _MRUCache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable (and not a keyword)\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def decorate(f):\n        cache = MruCache(f, maxsize, typed)\n        return cache\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable (and not a keyword)\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def decorator(func):\n        # Must be a function\n        if not isinstance(func, _CacheMeta):\n            raise TypeError('@mru_cache decorator must be applied to '\n                            'functions that return cached results.')\n\n        # Store the result of the decorated function\n        cache = MemoizeCache(maxsize, typed)\n        # Start the process of caching the result\n        result = func(cache)\n        # Put the result in the cache, and return the function\n        cache.store(func, result)\n        return result\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable (i.e. decorator)\n    if not callable(maxsize):\n        raise TypeError('maxsize must be a callable function.')\n\n    # Must return the cached result if it exists\n    if typed:\n        @functools.wraps(maxsize)\n        def wrapper(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return CACHE[key]\n            except KeyError:\n                ret = CACHE[key] = typed_maxsize(*args, **kwargs)\n                return ret\n    else:\n        @functools.wraps(maxsize)\n        def wrapper(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return CACHE[key]\n            except KeyError:\n          ", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable and not a keyword.\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def decorator(func):\n        cached = MruCache(maxsize, typed)\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cached._cache[key]\n            except KeyError:\n                value = cached._cache[key] = func(*args, **kwargs)\n                return value\n        cached._cache = {}\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # Must be a function\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 0:\n        raise ValueError('maxsize must be greater than 0')\n\n    def decorator(fn):\n        cached = MruCache(fn, maxsize, typed)\n        return lambda *a, **kw: cached.wrap(fn(*a, **kw))\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable and not a keyword.\n    if not isinstance(maxsize, int):\n        raise TypeError('maxsize must be an integer')\n    if maxsize < 1:\n        raise ValueError('maxsize must be greater than zero')\n\n    def decorator(func):\n        cached = MRU(maxsize=maxsize, typed=typed)(func)\n        cached.cache_clear()\n        return cached\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> def mru_cache(maxsize, typed=False):\n    #  ...     return MruCache(maxsize, typed)\n    def decorator(fn):\n        cached_fn = _CachedFunctionWrapper(fn, fn)\n        if maxsize is None or maxsize <= 0:\n            return cached_fn\n\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = cached_fn(*args, **kwargs)\n                return ret\n\n        if typed:\n            wrapper.__wrapped__ = lambda *a, **kw: type(args[0])(wrapper(*a, **kw))\n        else:\n         ", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(mru_cache, maxsize=128)\n    else:\n        return _MRUCache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n    # Must be a callable (i.e. decorator)\n    if not callable(maxsize):\n        raise TypeError('maxsize must be a callable function')\n\n    def wrapper(f):\n        cache = MruCache(f, maxsize, typed)\n        return cache\n    return wrapper\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.", "ques_id": "62b8d23948ba5a41d1c3f498", "code": ["def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return _decorator\n    else:\n        return _decorator(_cached, maxsize, typed)\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError(\"maxsize must be greater than zero, not %r\" % maxsize)\n\n    def wrapper(f):\n        cached = LRU(maxsize=maxsize)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cached[key]\n            except KeyError:\n                if typed:\n                    value = f(*args, **kwargs)\n                    cached[key] = value\n                    return value\n                else:\n                    return f(*args, **kwargs)\n        return wrapped\n    return wrapper\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(lru_cache, typed=typed)\n    else:\n        return _LRUCache(maxsize, typed)\n", "def lru_cache(maxsize=128, typed=False):\n    # This function must be wrapped with @lru_cache and not directly called\n    # by @wraps.wraps(func)\n    if maxsize is None or maxsize < 0:\n        return partial(lru_cache, maxsize=0)\n    if typed:\n        cache = typed_cache(maxsize)\n    else:\n        cache = lua_cache(maxsize)\n    def wrapper(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        return wrapped\n    return wrapper\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return _cached\n    else:\n        return _cached(maxsize, typed)\n", "def lru_cache(maxsize=128, typed=False):\n    # Memoize is disabled in Python 3, so do not use it for simple caching.\n    return _Cache(maxsize, typed)\n", "def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return _wrapper(typed)\n    else:\n        return _wrapper(_LRUCache(maxsize))\n", "def lru_cache(maxsize=128, typed=False):\n    # Memoize is disabled in the Python 3 version, and can be disabled in the\n    # 2.x version (also in 3.x)\n    if sys.version_info[0] >= 3:\n        return partial(_lru_cache, maxsize=maxsize, typed=typed)\n    else:\n        return _lru_cache(maxsize, typed)\n", "def lru_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> def lru_cache(maxsize=128, typed=False):\n    #  ...     return LRUCache(maxsize, typed)\n    #\n    # To get around this, we create a wrapper function that saves up to\n    # `maxsize` results and returns the wrapped function that saves them\n    # to the passed argument list.\n\n    def wrapper(fn):\n        cached_fn = _CachedFunction(fn, fn.__name__, maxsize, typed)\n        return lambda *a, **kw: cached_fn(*a, **kw)\n\n    return wrapper\n", "def lru_cache(maxsize=128, typed=False):\n    # Memoize is disabled in the Python 3 decorator, and will cause errors when\n    # calling `get`.\n    if not _cache_enabled:\n        return functools.lru_cache(maxsize)\n\n    def decorate(fn):\n        return _CacheStatus(fn, maxsize, typed)\n    return decorate\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return _decorator\n    else:\n        return _decorator(_cached, maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize < 1:\n        raise ValueError(\"maxsize must be greater than zero, not %r\" % maxsize)\n\n    def wrapper(f):\n        cached = LRU(maxsize=maxsize)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cached[key]\n            except KeyError:\n                if typed:\n                    value = f(*args, **kwargs)\n                    cached[key] = value\n                    return value\n                else:\n                    return f(*args, **kwargs)\n        return wrapped\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return partial(lru_cache, typed=typed)\n    else:\n        return _LRUCache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    # This function must be wrapped with @lru_cache and not directly called\n    # by @wraps.wraps(func)\n    if maxsize is None or maxsize < 0:\n        return partial(lru_cache, maxsize=0)\n    if typed:\n        cache = typed_cache(maxsize)\n    else:\n        cache = lua_cache(maxsize)\n    def wrapper(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n        return wrapped\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return _cached\n    else:\n        return _cached(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    # Memoize is disabled in Python 3, so do not use it for simple caching.\n    return _Cache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return _wrapper(typed)\n    else:\n        return _wrapper(_LRUCache(maxsize))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    # Memoize is disabled in the Python 3 version, and can be disabled in the\n    # 2.x version (also in 3.x)\n    if sys.version_info[0] >= 3:\n        return partial(_lru_cache, maxsize=maxsize, typed=typed)\n    else:\n        return _lru_cache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> def lru_cache(maxsize=128, typed=False):\n    #  ...     return LRUCache(maxsize, typed)\n    #\n    # To get around this, we create a wrapper function that saves up to\n    # `maxsize` results and returns the wrapped function that saves them\n    # to the passed argument list.\n\n    def wrapper(fn):\n        cached_fn = _CachedFunction(fn, fn.__name__, maxsize, typed)\n        return lambda *a, **kw: cached_fn(*a, **kw)\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n    # Memoize is disabled in the Python 3 decorator, and will cause errors when\n    # calling `get`.\n    if not _cache_enabled:\n        return functools.lru_cache(maxsize)\n\n    def decorate(fn):\n        return _CacheStatus(fn, maxsize, typed)\n    return decorate\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lfu_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.", "ques_id": "62b8d23748ba5a41d1c3f496", "code": ["def lfu_cache(maxsize=128, typed=False):\n    # You can access the __doc__ attribute of this class in your own code,\n    # or cache your implementation if you need it.\n    def _wrapper(func):\n        cache = LFUCache(maxsize, typed)\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = pickle.dumps((args, kwargs))\n            if key not in cache:\n                cache[key] = result = func(*args, **kwargs)\n            else:\n                result = cache[key]\n            return result\n        return wrapper\n    return _wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> def lfu_cache(maxsize=128, typed=False):\n    #  ...     return LfuCache(maxsize, typed)\n    def wrapper(fn):\n        return LfuCache(maxsize, typed, fn)\n    return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    # You can cache the result by passing the function as a keyword argument.\n    # This is for backwards compatibility, please remove later.\n    return _cached(_lfu, maxsize, typed)\n", "def lfu_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> @lfu_cache(maxsize=128)\n    #  ... def f(x):\n    #       return x + 1\n    # <br>\n    # This call wraps the decorator with a memoized function that saves\n    # up to `maxsize` results based on a Least frequently Used (LFU)\n    # algorithm.\n    def wrapper(func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n\n        def wrapped_with_cache(*args, **kwargs):\n            cache_key = (args, frozenset(sorted(kwargs.items())))\n            try:\n", "def lfu_cache(maxsize=128, typed=False):\n    # You can access the __doc__ attribute of this class in your own code,\n    # or cache it in an attribute named __doc__ if you need it.\n    def _wrapper(func):\n        cached = _Cached(func, maxsize, typed)\n        return lambda *a, **kw: cached._call(*a, **kw)\n    return _wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    # Yes, this is ugly, but we don't want to interfere with mypy\n    return _cache('lfu', maxsize, typed)\n", "def lfu_cache(maxsize=128, typed=False):\n    # We create a cache object so that it can be used in conjunction with\n    # @lfu_cache.cached\n    cache = {}\n\n    def wrapper(fun):\n        @functools.wraps(fun)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fun(*args, **kwargs)\n                return ret\n\n        if typed:\n            @wrapped.typed\n            def wrapped_typed(*args, **kwargs):\n                return wrapped(*args, **kwargs)\n        else:\n            @wrapped.cached\n            def wrapped_cached(*args, **kwargs):\n                return wrapped(*args, **kwargs)\n", "def lfu_cache(maxsize=128, typed=False):\n    # We create a new cache object so that it doesn't get garbage collected\n    # (due to the nature of the callable parameters).  This will also clear\n    # the cache on exit (close all filehandles and disk usage).\n    cache = LRUCache(maxsize)\n\n    def wrapper(fn):\n        @functools.wraps(fn)\n        def wrapped(*args, **kwargs):\n            key = pickle.dumps((args, kwargs))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fn(*args, **kwargs)\n                return ret\n\n        if typed:\n            wrapped.__wrapped__ = fn\n            wrapped.__cache__ = {}\n            return wrapped\n        else:\n            return wrapped\n\n    return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> def lfu_cache(maxsize=128, typed=False):\n    #  ...     return LfuCache(maxsize, typed)\n    #\n    # To get around this, we create a function that saves up to `maxsize`\n    # calls and returns the first of them. The cache is not shared by\n    # LfuCache and it is not required to be accessed by the application.\n    def wrapper(fun):\n        cache = LfuCache(maxsize, typed)\n\n        @wraps(fun)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fun(*args, **kwargs)\n                return ret\n\n        return wrapped\n\n    return wrapper\n", "def lfu_cache(maxsize=128, typed=False):\n    # Yes, this is ugly, but we need to preserve Python class constants\n    # TODO: Remove this once we drop Python 3+\n    if maxsize is None:\n        return _decorator\n    else:\n        return _decorator(_LFU, maxsize, typed)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # You can access the __doc__ attribute of this class in your own code,\n    # or cache your implementation if you need it.\n    def _wrapper(func):\n        cache = LFUCache(maxsize, typed)\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = pickle.dumps((args, kwargs))\n            if key not in cache:\n                cache[key] = result = func(*args, **kwargs)\n            else:\n                result = cache[key]\n            return result\n        return wrapper\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> def lfu_cache(maxsize=128, typed=False):\n    #  ...     return LfuCache(maxsize, typed)\n    def wrapper(fn):\n        return LfuCache(maxsize, typed, fn)\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # You can cache the result by passing the function as a keyword argument.\n    # This is for backwards compatibility, please remove later.\n    return _cached(_lfu, maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> @lfu_cache(maxsize=128)\n    #  ... def f(x):\n    #       return x + 1\n    # <br>\n    # This call wraps the decorator with a memoized function that saves\n    # up to `maxsize` results based on a Least frequently Used (LFU)\n    # algorithm.\n    def wrapper(func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = func(*args, **kwargs)\n                return ret\n\n        def wrapped_with_cache(*args, **kwargs):\n            cache_key = (args, frozenset(sorted(kwargs.items())))\n            try:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # You can access the __doc__ attribute of this class in your own code,\n    # or cache it in an attribute named __doc__ if you need it.\n    def _wrapper(func):\n        cached = _Cached(func, maxsize, typed)\n        return lambda *a, **kw: cached._call(*a, **kw)\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # Yes, this is ugly, but we don't want to interfere with mypy\n    return _cache('lfu', maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # We create a cache object so that it can be used in conjunction with\n    # @lfu_cache.cached\n    cache = {}\n\n    def wrapper(fun):\n        @functools.wraps(fun)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fun(*args, **kwargs)\n                return ret\n\n        if typed:\n            @wrapped.typed\n            def wrapped_typed(*args, **kwargs):\n                return wrapped(*args, **kwargs)\n        else:\n            @wrapped.cached\n            def wrapped_cached(*args, **kwargs):\n                return wrapped(*args, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # We create a new cache object so that it doesn't get garbage collected\n    # (due to the nature of the callable parameters).  This will also clear\n    # the cache on exit (close all filehandles and disk usage).\n    cache = LRUCache(maxsize)\n\n    def wrapper(fn):\n        @functools.wraps(fn)\n        def wrapped(*args, **kwargs):\n            key = pickle.dumps((args, kwargs))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fn(*args, **kwargs)\n                return ret\n\n        if typed:\n            wrapped.__wrapped__ = fn\n            wrapped.__cache__ = {}\n            return wrapped\n        else:\n            return wrapped\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # You can use a docstring like:\n    #   >>> def lfu_cache(maxsize=128, typed=False):\n    #  ...     return LfuCache(maxsize, typed)\n    #\n    # To get around this, we create a function that saves up to `maxsize`\n    # calls and returns the first of them. The cache is not shared by\n    # LfuCache and it is not required to be accessed by the application.\n    def wrapper(fun):\n        cache = LfuCache(maxsize, typed)\n\n        @wraps(fun)\n        def wrapped(*args, **kwargs):\n            key = (args, frozenset(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                ret = cache[key] = fun(*args, **kwargs)\n                return ret\n\n        return wrapped\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n    # Yes, this is ugly, but we need to preserve Python class constants\n    # TODO: Remove this once we drop Python 3+\n    if maxsize is None:\n        return _decorator\n    else:\n        return _decorator(_LFU, maxsize, typed)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/fifo.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair first inserted.", "ques_id": "62b8d22f48ba5a41d1c3f488", "code": ["def popitem(self):\n    try:\n        k, v = dict.popitem(self)\n        self._list.remove((k, v))\n    except KeyError:\n        raise KeyError('%s not in the dictionary' % id(self))\n    return k, v\n", "def popitem(self):\n    try:\n        key, value = dict.popitem(self)\n    except KeyError:\n        raise exceptions.NotFound('no item found to remove')\n    else:\n        self.__delitem__(key)\n        return value\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        self._list_add(value)\n        return key, value\n    except KeyError as err:\n        raise exceptions.NotFound(err)\n", "def popitem(self):\n    try:\n        k, v = dict.popitem(self)\n        self._list.remove((k, v))\n        return k, v\n    except KeyError:\n        raise exceptions.BadRequestKeyError(\"Key '%s' not found\" % self._key_name)\n", "def popitem(self):\n    try:\n        k = list(self._sequence)[0]\n        v = dict.pop(self, k)\n        self._sequence.remove(k)\n        return (k, v)\n    except IndexError:\n        raise KeyError('%s is empty' % self._key_sep)\n", "def popitem(self):\n    try:\n        key, value = dict.popitem(self)\n        self.__delitem__(key)\n        return (key, value)\n    except KeyError as e:\n        raise exceptions.BadRequestKeyError(str(e))\n", "def popitem(self):\n    try:\n        k, v = dict.popitem(self)\n        self._list_add((k, v))\n    except KeyError:\n        raise exceptions.NotFound('No item to remove.')\n    return k, v\n", "def popitem(self):\n    if not self:\n        raise KeyError('%s is already empty' % self.__class__.__name__)\n    key = self._keys[0]\n    value = dict.pop(self, key)\n    dict.pop(self, key)\n    return (key, value)\n", "def popitem(self):\n    try:\n        key, value = dict.popitem(self)\n        self._list_remove(key)\n        return key, value\n    except KeyError:\n        raise exceptions.NotFound('Key \"%s\" not found' % self._key_sep)\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        return (key, value)\n    except KeyError as err:\n        raise exceptions.NotFound(str(err))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    try:\n        k, v = dict.popitem(self)\n        self._list.remove((k, v))\n    except KeyError:\n        raise KeyError('%s not in the dictionary' % id(self))\n    return k, v\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key, value = dict.popitem(self)\n    except KeyError:\n        raise exceptions.NotFound('no item found to remove')\n    else:\n        self.__delitem__(key)\n        return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        self._list_add(value)\n        return key, value\n    except KeyError as err:\n        raise exceptions.NotFound(err)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        k, v = dict.popitem(self)\n        self._list.remove((k, v))\n        return k, v\n    except KeyError:\n        raise exceptions.BadRequestKeyError(\"Key '%s' not found\" % self._key_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        k = list(self._sequence)[0]\n        v = dict.pop(self, k)\n        self._sequence.remove(k)\n        return (k, v)\n    except IndexError:\n        raise KeyError('%s is empty' % self._key_sep)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key, value = dict.popitem(self)\n        self.__delitem__(key)\n        return (key, value)\n    except KeyError as e:\n        raise exceptions.BadRequestKeyError(str(e))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        k, v = dict.popitem(self)\n        self._list_add((k, v))\n    except KeyError:\n        raise exceptions.NotFound('No item to remove.')\n    return k, v\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    if not self:\n        raise KeyError('%s is already empty' % self.__class__.__name__)\n    key = self._keys[0]\n    value = dict.pop(self, key)\n    dict.pop(self, key)\n    return (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key, value = dict.popitem(self)\n        self._list_remove(key)\n        return key, value\n    except KeyError:\n        raise exceptions.NotFound('Key \"%s\" not found' % self._key_sep)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        return (key, value)\n    except KeyError as err:\n        raise exceptions.NotFound(str(err))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "setdefault", "docstring": "D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D", "ques_id": "62b8d22a48ba5a41d1c3f47e", "code": ["def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.__setitem__(self, key, default)\n    return default\n", "def setdefault(self, key, default=None):\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.setdefault(self, key.upper(), default)\n    return default\n", "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    super(MultiDict, self).setdefault(key, default)\n    return default\n", "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.__setitem__(self, key, default)\n    return default\n", "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.__setitem__(self, key, default)\n    return default\n", "def setdefault(self, key, default=None):\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.setdefault(self, key.lower(), default)\n    return default\n", "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.setdefault(self, key.lower(), default)\n    return default\n", "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.__setitem__(self, key, default)\n    return default\n"], "level": "self_contained", "generate_results": [{"generate_code": "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.__setitem__(self, key, default)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.setdefault(self, key.upper(), default)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    super(MultiDict, self).setdefault(key, default)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.__setitem__(self, key, default)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.__setitem__(self, key, default)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.setdefault(self, key.lower(), default)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.setdefault(self, key.lower(), default)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    dict.__setitem__(self, key, default)\n    return default\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "get", "docstring": "D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.", "ques_id": "62b8d22948ba5a41d1c3f47c", "code": ["def get(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "def get(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "def get(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "def get(self, key, default=None):\n    return super(DotDict, self).setdefault(key, default)\n", "def get(self, key, default=None):\n    return self.__dict__.get(key, default)\n", "def get(self, key, default=None):\n    return self._dict.get(key, default)\n", "def get(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "def get(self, key, default=None):\n    return self.__dict__.get(key, default)\n", "def get(self, key, default=None):\n    return self.__dict__.get(key, default)\n", "def get(self, key, default=None):\n    return self.__dict__.get(key, default)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return super(DotDict, self).setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return self.__dict__.get(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return self._dict.get(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    if default is None:\n        default = self.default_factory()\n    return super(DefaultGetDict, self).setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return self.__dict__.get(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return self.__dict__.get(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n    return self.__dict__.get(key, default)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cachedmethod", "docstring": "Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.", "ques_id": "62b8d22548ba5a41d1c3f472", "code": ["def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kw):\n            key = keyfunc(self, *args, **kw)\n            result = cache.get(key)\n            if result is None:\n                result = func(self, *args, **kw)\n                cache.set(key, result, cache.timeout)\n            return result\n\n        return wrapper\n\n    if callable(cache):\n        cache = cache()\n\n    return decorator(cache)\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        memo = cache(key, func)\n\n        def wrapper(self, *args, **kw):\n            if not hasattr(self, '_cache'):\n                self._cache = {}\n            if memo not in self._cache:\n                self._cache[memo] = func(self, *args, **kw)\n            return self._cache[memo]\n        return wrapper\n    return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        cached = cache(key, lock)(func)\n        def wrapper(self, *args, **kwargs):\n            # We must use `as of 0.4.0` for cache keys that are too large\n            # for the underlying python implementation.\n            expires = (datetime.datetime.now() +\n                      datetime.timedelta(seconds=self.cache_expire_seconds))\n            if self.expired(expires):\n                return cached(*args, **kwargs)\n            return self.get(*args, **kwargs)\n        return wrapper\n    return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorate(fn):\n        @functools.wraps(fn)\n        def wrapper(self, *args, **kwargs):\n            key = keyfunc(self, *args, **kwargs)\n            result = cache.get(key)\n            if result is None:\n                result = fn(self, *args, **kwargs)\n                cache.set(key, result, cache.timeout)\n            return result\n\n        return wrapper\n\n    if callable(cache):\n        return decorate(cache)\n    else:\n        raise TypeError('@cachedmethod must be callable or composite function.')\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        memo = cache(key, lock)\n\n        def wrapper(self, *args, **kw):\n            if not memo:\n                self._cache = func(self, *args, **kw)\n            else:\n                memo[0] = self._cache\n            return memo[0]\n        return wrapper\n    return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorate(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            key = keyfunc(self, *args, **kwargs)\n            result = cache.get(key)\n            if result is None:\n                result = func(self, *args, **kwargs)\n                cache.set(key, result, cache.timeout)\n            return result\n\n        return wrapper\n\n    if isinstance(cache, Cache):\n        key = keyfunc(cache, *args, **kwargs)\n        return decorate(cache.get(key))\n    else:\n        decorate = partial(decorate, cache)\n        return decorate\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(func):\n        cached = cache(key, func)\n        def newfunc(*args, **kwds):\n            # We must use the old cache if the function is a method or\n            # function with a __call__ method. Otherwise, we must use\n            # the new cache wrapper.\n            if hasattr(func, '__call__') or not inspect.ismethod(func):\n                return cached(*args, **kwds)\n            key = (args, frozenset(sorted(kwds.items())))\n            try:\n                return cached[key]\n            except KeyError:\n                ret = cached[key] = func(*args, **kwds)\n                return ret\n        return newfunc\n    return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(func):\n        memo = cache(key)\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if memo is None:\n                ret = func(*args, **kwargs)\n                lock.acquire()\n                try:\n                    memo = memoize(func, key, args, kwargs)\n                finally:\n                    lock.release()\n                return memo\n\n            return memo\n\n        return wrapper\n\n    return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        memo = cache(key, lock)\n\n        def wrapper(self, *args, **kwds):\n            if not memo:\n                result = func(self, *args, **kwds)\n                memo[0] = result\n            else:\n                memo[1] = func(self, *args, **kwds)\n            return memo[0]\n        return wrapper\n    return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        memo = _CachedMethod(func, cache, key, lock)\n        return lambda *a, **kw: memo.get()\n    return decorator\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kw):\n            key = keyfunc(self, *args, **kw)\n            result = cache.get(key)\n            if result is None:\n                result = func(self, *args, **kw)\n                cache.set(key, result, cache.timeout)\n            return result\n\n        return wrapper\n\n    if callable(cache):\n        cache = cache()\n\n    return decorator(cache)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        memo = cache(key, func)\n\n        def wrapper(self, *args, **kw):\n            if not hasattr(self, '_cache'):\n                self._cache = {}\n            if memo not in self._cache:\n                self._cache[memo] = func(self, *args, **kw)\n            return self._cache[memo]\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        cached = cache(key, lock)(func)\n        def wrapper(self, *args, **kwargs):\n            # We must use `as of 0.4.0` for cache keys that are too large\n            # for the underlying python implementation.\n            expires = (datetime.datetime.now() +\n                      datetime.timedelta(seconds=self.cache_expire_seconds))\n            if self.expired(expires):\n                return cached(*args, **kwargs)\n            return self.get(*args, **kwargs)\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorate(fn):\n        @functools.wraps(fn)\n        def wrapper(self, *args, **kwargs):\n            key = keyfunc(self, *args, **kwargs)\n            result = cache.get(key)\n            if result is None:\n                result = fn(self, *args, **kwargs)\n                cache.set(key, result, cache.timeout)\n            return result\n\n        return wrapper\n\n    if callable(cache):\n        return decorate(cache)\n    else:\n        raise TypeError('@cachedmethod must be callable or composite function.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        memo = cache(key, lock)\n\n        def wrapper(self, *args, **kw):\n            if not memo:\n                self._cache = func(self, *args, **kw)\n            else:\n                memo[0] = self._cache\n            return memo[0]\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = threading.Lock()\n\n    def decorate(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            key = keyfunc(self, *args, **kwargs)\n            result = cache.get(key)\n            if result is None:\n                result = func(self, *args, **kwargs)\n                cache.set(key, result, cache.timeout)\n            return result\n\n        return wrapper\n\n    if isinstance(cache, Cache):\n        key = keyfunc(cache, *args, **kwargs)\n        return decorate(cache.get(key))\n    else:\n        decorate = partial(decorate, cache)\n        return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(func):\n        cached = cache(key, func)\n        def newfunc(*args, **kwds):\n            # We must use the old cache if the function is a method or\n            # function with a __call__ method. Otherwise, we must use\n            # the new cache wrapper.\n            if hasattr(func, '__call__') or not inspect.ismethod(func):\n                return cached(*args, **kwds)\n            key = (args, frozenset(sorted(kwds.items())))\n            try:\n                return cached[key]\n            except KeyError:\n                ret = cached[key] = func(*args, **kwds)\n                return ret\n        return newfunc\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    if lock is None:\n        lock = Lock()\n\n    def decorator(func):\n        memo = cache(key)\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if memo is None:\n                ret = func(*args, **kwargs)\n                lock.acquire()\n                try:\n                    memo = memoize(func, key, args, kwargs)\n                finally:\n                    lock.release()\n                return memo\n\n            return memo\n\n        return wrapper\n\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        memo = cache(key, lock)\n\n        def wrapper(self, *args, **kwds):\n            if not memo:\n                result = func(self, *args, **kwds)\n                memo[0] = result\n            else:\n                memo[1] = func(self, *args, **kwds)\n            return memo[0]\n        return wrapper\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n    def decorator(func):\n        memo = _CachedMethod(func, cache, key, lock)\n        return lambda *a, **kw: memo.get()\n    return decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "extostr", "docstring": "Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str", "ques_id": "62b8c517e0d34b282c18122e", "code": ["def extostr(cls, e, max_level=30, max_path_level=5):\n    if sys.version_info[0] > max_path_level:\n        return '{0}: {1}'.format(cls.__name__, e)\n    else:\n        return str(e)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    return cls._extostr(e, max_level, max_path_level)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    return \"{0}: {1} at {2}\".format(cls.__name__, e, hex(id(e)))\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, Exception):\n        e_str = e.__class__.__name__\n    elif isinstance(e, bytes):\n        e_str = e.decode('utf-8')\n    else:\n        e_str = str(e)\n\n    if len(e_str) > max_path_level:\n        e_str = e_str[:max_path_level - 3] + \"...\"\n\n    if isinstance(e, (HTTPException, ConnectionError)):\n        e_str = \"{} ({})\".format(e_str, type(e).__name__)\n\n    if max_level <= 0:\n        return e_str\n\n    return \"[{}] {}\".format(max_level, e_str)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, HostNotFound):\n        return \"{0} {1}\".format(cls.__name__, e.args[1])\n\n    if isinstance(e, RequestException):\n        return \"{0} {1}\".format(cls.__name__, e.args[1])\n\n    return \"{0} {1}\".format(cls.__name__, e)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, exceptions.HTTPException):\n        code = e.code if hasattr(e, \"code\") else 500\n        return \"%s (HTTP %d)\" % (e.description, code)\n    if hasattr(e, \"args\"):\n        args = e.args if hasattr(e, \"args\") else []\n        return \"%s: %s\" % (cls.__name__, args)\n    else:\n        return cls.__name__\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, str):\n        return e\n\n    if not isinstance(e, Exception):\n        e = Exception('')\n\n    res = ''\n    for t in traceback.extract_tb(e.__traceback__):\n        res += '\\n' +'' * (max_path_level - len(t[2])) + t[2]\n\n    return '{}: {}'.format(cls.__name__, res)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    return cls.__getExceptionClass(e) + \": \" + cls.__getExceptionDetails(e)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    if not e.args:\n        return ''\n    return '{}: {}'.format(cls.__name__,''.join(e.args))\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n    return _format_with_path(cls, e, '', max_path_level=max_path_level,\n                              max_level=max_level)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if sys.version_info[0] > max_path_level:\n        return '{0}: {1}'.format(cls.__name__, e)\n    else:\n        return str(e)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    return cls._extostr(e, max_level, max_path_level)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    return \"{0}: {1} at {2}\".format(cls.__name__, e, hex(id(e)))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, Exception):\n        e_str = e.__class__.__name__\n    elif isinstance(e, bytes):\n        e_str = e.decode('utf-8')\n    else:\n        e_str = str(e)\n\n    if len(e_str) > max_path_level:\n        e_str = e_str[:max_path_level - 3] + \"...\"\n\n    if isinstance(e, (HTTPException, ConnectionError)):\n        e_str = \"{} ({})\".format(e_str, type(e).__name__)\n\n    if max_level <= 0:\n        return e_str\n\n    return \"[{}] {}\".format(max_level, e_str)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, HostNotFound):\n        return \"{0} {1}\".format(cls.__name__, e.args[1])\n\n    if isinstance(e, RequestException):\n        return \"{0} {1}\".format(cls.__name__, e.args[1])\n\n    return \"{0} {1}\".format(cls.__name__, e)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, exceptions.HTTPException):\n        code = e.code if hasattr(e, \"code\") else 500\n        return \"%s (HTTP %d)\" % (e.description, code)\n    if hasattr(e, \"args\"):\n        args = e.args if hasattr(e, \"args\") else []\n        return \"%s: %s\" % (cls.__name__, args)\n    else:\n        return cls.__name__\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if isinstance(e, str):\n        return e\n\n    if not isinstance(e, Exception):\n        e = Exception('')\n\n    res = ''\n    for t in traceback.extract_tb(e.__traceback__):\n        res += '\\n' +'' * (max_path_level - len(t[2])) + t[2]\n\n    return '{}: {}'.format(cls.__name__, res)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    return cls.__getExceptionClass(e) + \": \" + cls.__getExceptionDetails(e)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    if not e.args:\n        return ''\n    return '{}: {}'.format(cls.__name__,''.join(e.args))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n    return _format_with_path(cls, e, '', max_path_level=max_path_level,\n                              max_level=max_level)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "append_text_to_file", "docstring": "Write to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int", "ques_id": "62b8bbbfe0d34b282c181210", "code": ["def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    try:\n        with io.open(file_name, mode='ab', encoding=encoding) as file_handle:\n            file_handle.write(text_buffer)\n        return len(text_buffer)\n    except (IOError, OSError) as error:\n        if overwrite:\n            return 0\n        else:\n            raise error\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    with open(file_name, 'ab' if overwrite else 'wb') as file:\n        return file.write(text_buffer.encode(encoding))\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not overwrite and os.path.exists(file_name):\n        # If the file already exists write to it.\n        try:\n            with open(file_name, 'r', encoding=encoding) as f:\n                return f.write(text_buffer)\n        except UnicodeError:\n            # The file is not UTF-8 encoded and cannot be written.\n            return 0\n    else:\n        # Write to the file.\n        try:\n            with open(file_name, 'wb', encoding=encoding) as f:\n                return f.write(text_buffer)\n        except UnicodeError:\n            # The file is not UTF-8 encoded and cannot be written.\n            return 0\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    try:\n        with open(file_name, \"ab\") as file_pointer:\n            file_pointer.write(text_buffer)\n            return len(text_buffer)\n    except IOError as error:\n        if overwrite:\n            with open(file_name, \"wb\") as file_pointer:\n                file_pointer.write(text_buffer)\n            return 0\n        else:\n            raise error\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    try:\n        with open(file_name, 'ab') as f:\n            return f.write(text_buffer.encode(encoding))\n    except EnvironmentError:\n        if overwrite:\n            with open(file_name, 'wb') as f:\n                return f.write(text_buffer.encode(encoding))\n        return 0\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if overwrite or not os.path.exists(file_name):\n        with open(file_name, 'wb') as file:\n            file.write(text_buffer)\n        return len(text_buffer)\n    else:\n        return 0\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not overwrite and os.path.isfile(file_name):\n        return 0\n\n    with open(file_name, 'ab') as file_handle:\n        file_handle.write(text_buffer.encode(encoding))\n    return len(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not overwrite and os.path.exists(file_name):\n        return 0\n\n    with open(file_name, 'ab') as f:\n        f.write(text_buffer.encode(encoding))\n\n    return len(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if os.path.isfile(file_name) and not overwrite:\n        return 0\n    with open(file_name, 'ab') as f:\n        f.write(text_buffer.encode(encoding))\n    return len(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not overwrite and os.path.isfile(file_name):\n        return 0\n\n    try:\n        with open(file_name, \"ab\") as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except EnvironmentError as e:\n        return e.errno\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    try:\n        with io.open(file_name, mode='ab', encoding=encoding) as file_handle:\n            file_handle.write(text_buffer)\n        return len(text_buffer)\n    except (IOError, OSError) as error:\n        if overwrite:\n            return 0\n        else:\n            raise error\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    with open(file_name, 'ab' if overwrite else 'wb') as file:\n        return file.write(text_buffer.encode(encoding))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not overwrite and os.path.exists(file_name):\n        # If the file already exists write to it.\n        try:\n            with open(file_name, 'r', encoding=encoding) as f:\n                return f.write(text_buffer)\n        except UnicodeError:\n            # The file is not UTF-8 encoded and cannot be written.\n            return 0\n    else:\n        # Write to the file.\n        try:\n            with open(file_name, 'wb', encoding=encoding) as f:\n                return f.write(text_buffer)\n        except UnicodeError:\n            # The file is not UTF-8 encoded and cannot be written.\n            return 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    try:\n        with open(file_name, \"ab\") as file_pointer:\n            file_pointer.write(text_buffer)\n            return len(text_buffer)\n    except IOError as error:\n        if overwrite:\n            with open(file_name, \"wb\") as file_pointer:\n                file_pointer.write(text_buffer)\n            return 0\n        else:\n            raise error\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    try:\n        with open(file_name, 'ab') as f:\n            return f.write(text_buffer.encode(encoding))\n    except EnvironmentError:\n        if overwrite:\n            with open(file_name, 'wb') as f:\n                return f.write(text_buffer.encode(encoding))\n        return 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if overwrite or not os.path.exists(file_name):\n        with open(file_name, 'wb') as file:\n            file.write(text_buffer)\n        return len(text_buffer)\n    else:\n        return 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not overwrite and os.path.isfile(file_name):\n        return 0\n\n    with open(file_name, 'ab') as file_handle:\n        file_handle.write(text_buffer.encode(encoding))\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not overwrite and os.path.exists(file_name):\n        return 0\n\n    with open(file_name, 'ab') as f:\n        f.write(text_buffer.encode(encoding))\n\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if os.path.isfile(file_name) and not overwrite:\n        return 0\n    with open(file_name, 'ab') as f:\n        f.write(text_buffer.encode(encoding))\n    return len(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    if not overwrite and os.path.isfile(file_name):\n        return 0\n\n    try:\n        with open(file_name, \"ab\") as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except EnvironmentError as e:\n        return e.errno\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "file_to_textbuffer", "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str", "ques_id": "62b8bbbfe0d34b282c18120f", "code": ["def file_to_textbuffer(file_name, encoding):\n    try:\n        buffer = gtk.TextBuffer()\n        if sys.version_info >= (3, 0):\n            with open(file_name, \"r\", encoding=encoding) as file:\n                buffer.set_text(file.read())\n        else:\n            buffer.set_text(file_name)\n        return buffer\n    except (IOError, OSError):\n        return None\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        text_buffer = None\n        with io.open(file_name, mode='rt', encoding=encoding) as f:\n            text_buffer = f.read()\n    except IOError:\n        logger.exception('Unable to read file %s', file_name)\n    return text_buffer\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        file = codecs.open(file_name, 'r', encoding)\n    except IOError:\n        print(\"Cannot open file {}\".format(file_name))\n        return None\n    else:\n        text = file.read()\n        file.close()\n        return text\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        buf = read_file(file_name, encoding=encoding)\n    except Exception:\n        logging.exception('Unable to read file \"%s\".', file_name)\n        return None\n    return buf\n", "def file_to_textbuffer(file_name, encoding):\n    text_buffer = None\n    try:\n        text_buffer = codecs.open(file_name, 'r', encoding)\n    except IOError:\n        pass\n    return text_buffer\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        buffer = unicode(codecs.open(file_name, 'r', encoding), 'utf-8')\n    except IOError:\n        buffer = None\n    return buffer\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        return codecs.open(file_name, encoding=encoding).read()\n    except IOError:\n        return None\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        if sys.version_info >= (3, 0):\n            with codecs.open(file_name, encoding=encoding) as f:\n                return f.read()\n        else:\n            with io.open(file_name, encoding=encoding) as f:\n                return f.read()\n    except IOError:\n        return None\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        with io.open(file_name, encoding=encoding, errors='ignore') as f:\n            text = f.read()\n        return text\n    except IOError:\n        return None\n", "def file_to_textbuffer(file_name, encoding):\n    try:\n        text = read_file(file_name, encoding)\n    except UnicodeDecodeError:\n        text = read_file(file_name, \"utf-8\")\n    return text\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        buffer = gtk.TextBuffer()\n        if sys.version_info >= (3, 0):\n            with open(file_name, \"r\", encoding=encoding) as file:\n                buffer.set_text(file.read())\n        else:\n            buffer.set_text(file_name)\n        return buffer\n    except (IOError, OSError):\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        text_buffer = None\n        with io.open(file_name, mode='rt', encoding=encoding) as f:\n            text_buffer = f.read()\n    except IOError:\n        logger.exception('Unable to read file %s', file_name)\n    return text_buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        file = codecs.open(file_name, 'r', encoding)\n    except IOError:\n        print(\"Cannot open file {}\".format(file_name))\n        return None\n    else:\n        text = file.read()\n        file.close()\n        return text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        buf = read_file(file_name, encoding=encoding)\n    except Exception:\n        logging.exception('Unable to read file \"%s\".', file_name)\n        return None\n    return buf\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    text_buffer = None\n    try:\n        text_buffer = codecs.open(file_name, 'r', encoding)\n    except IOError:\n        pass\n    return text_buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        buffer = unicode(codecs.open(file_name, 'r', encoding), 'utf-8')\n    except IOError:\n        buffer = None\n    return buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        return codecs.open(file_name, encoding=encoding).read()\n    except IOError:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        if sys.version_info >= (3, 0):\n            with codecs.open(file_name, encoding=encoding) as f:\n                return f.read()\n        else:\n            with io.open(file_name, encoding=encoding) as f:\n                return f.read()\n    except IOError:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        with io.open(file_name, encoding=encoding, errors='ignore') as f:\n            text = f.read()\n        return text\n    except IOError:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n    try:\n        text = read_file(file_name, encoding)\n    except UnicodeDecodeError:\n        text = read_file(file_name, \"utf-8\")\n    return text\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "is_file_exist", "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool", "ques_id": "62b8bbbce0d34b282c18120d", "code": ["def is_file_exist(file_name):\n    if os.path.isfile(file_name):\n        return os.path.isfile(file_name)\n    else:\n        return False\n", "def is_file_exist(file_name):\n    if not os.path.exists(file_name):\n        return False\n\n    if os.path.isdir(file_name):\n        raise argparse.ArgumentTypeError('{0} is a directory'.format(file_name))\n\n    return True\n", "def is_file_exist(file_name):\n    if not is_string(file_name):\n        return False\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_file_exist(file_name):\n    if os.path.isfile(file_name):\n        return os.path.isfile(file_name)\n    else:\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    if not os.path.exists(file_name):\n        return False\n\n    if os.path.isdir(file_name):\n        raise argparse.ArgumentTypeError('{0} is a directory'.format(file_name))\n\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    if not is_string(file_name):\n        return False\n    return os.path.isfile(file_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n    return os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "_reset_logging", "docstring": "Reset", "ques_id": "62b8b99de0d34b282c1811f8", "code": ["def _reset_logging(cls):\n    \"Reset \"\n    cls.logger.removeHandler(cls._handler)\n    cls.logger.handlers = []\n    cls.logger.setLevel(logging.DEBUG)\n", "def _reset_logging(cls):\n    cls._log = []\n", "def _reset_logging(cls):\n    cls._logger.handlers = []\n", "def _reset_logging(cls):\n    logging.disable(logging.NOTSET)\n", "def _reset_logging(cls):\n    cls.logger = None\n", "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n", "def _reset_logging(cls):\n    for handler in logging.getLogger().handlers:\n        logging.getLogger().removeHandler(handler)\n", "def _reset_logging(cls):\n    logging.disable(logging.NOTSET)\n", "def _reset_logging(cls):\n    if cls._logger:\n        cls._logger.close()\n        cls._logger = None\n", "def _reset_logging(cls):\n    global _logger\n    _logger = logging.getLogger(cls.__name__)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _reset_logging(cls):\n    \"Reset \"\n    cls.logger.removeHandler(cls._handler)\n    cls.logger.handlers = []\n    cls.logger.setLevel(logging.DEBUG)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    cls._log = []\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    cls._logger.handlers = []\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.disable(logging.NOTSET)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    cls.logger = None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.getLogger().handlers = []\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    for handler in logging.getLogger().handlers:\n        logging.getLogger().removeHandler(handler)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    logging.disable(logging.NOTSET)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    if cls._logger:\n        cls._logger.close()\n        cls._logger = None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n    global _logger\n    _logger = logging.getLogger(cls.__name__)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/tests/test_declarations.py", "project": "pexip/os-zope", "name": "_getTargetClass", "docstring": "Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.", "ques_id": "62b8b59feb7e40a82d2d1291", "code": ["def _getTargetClass(self):\n    return AsyncHTTPClient\n", "def _getTargetClass(self):\n    return JavaScriptScriptTextTarget\n", "def _getTargetClass(self):\n    return AsyncHTTPClient\n", "def _getTargetClass(self):\n    return html.Target\n", "def _getTargetClass(self):\n    return AttributeInjector\n", "def _getTargetClass(self):\n    return TextTestResult\n", "def _getTargetClass(self):\n    return Template\n", "def _getTargetClass(self):\n    return DataTarget\n", "def _getTargetClass(self):\n    return Server\n", "def _getTargetClass(self):\n    return AsyncHTTPClient\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _getTargetClass(self):\n    return AsyncHTTPClient\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return JavaScriptScriptTextTarget\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return AsyncHTTPClient\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return html.Target\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return AttributeInjector\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return TextTestResult\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return Template\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return DataTarget\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return Server\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n    return AsyncHTTPClient\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/ro.py", "project": "pexip/os-zope", "name": "_legacy_mergeOrderings", "docstring": "Merge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']", "ques_id": "62b8b590eb7e40a82d2d1275", "code": ["def _legacy_mergeOrderings(orderings):\n    merged = []\n    for ordering in orderings:\n        for o in ordering:\n            if o not in merged:\n                merged.append(o)\n    return merged\n", "def _legacy_mergeOrderings(orderings):\n    result = []\n    for o in orderings:\n        for i in range(0, len(o), 2):\n            result.append(o[i])\n    return result\n", "def _legacy_mergeOrderings(orderings):\n    merged = []\n\n    for o in orderings:\n        for el in o:\n            if el not in merged:\n                merged.append(el)\n\n    return merged\n", "def _legacy_mergeOrderings(orderings):\n    mergedOrderings = []\n    for ordering in reversed(orderings):\n        for o in ordering:\n            if o not in mergedOrderings:\n                mergedOrderings.insert(0, o)\n    return mergedOrderings\n", "def _legacy_mergeOrderings(orderings):\n    mergedOrderings = []\n    for ordering in orderings:\n        for o in ordering:\n            if o not in mergedOrderings:\n                mergedOrderings.append(o)\n    return mergedOrderings\n", "def _legacy_mergeOrderings(orderings):\n\n    merged_orderings = []\n    for ordering in orderings:\n        for obj in ordering:\n            if obj not in merged_orderings:\n                merged_orderings.append(obj)\n    return merged_orderings\n", "def _legacy_mergeOrderings(orderings):\n    orderings = sorted(orderings, key=_legacyOrderingKey)\n    merged_orderings = []\n    for o in orderings:\n        for e in o:\n            if e not in merged_orderings:\n                merged_orderings.append(e)\n    return merged_orderings\n", "def _legacy_mergeOrderings(orderings):\n    orderings = sorted(orderings, key=_legacy_key)\n    merged = []\n    for o in orderings:\n        for e in o:\n            if e not in merged:\n                merged.append(e)\n    return merged\n", "def _legacy_mergeOrderings(orderings):\n    if len(orderings) == 0:\n        return orderings\n\n    # find the longest suffix\n    max_suffix = 0\n    for i in range(len(orderings[0])):\n        for ordering in reversed(orderings):\n            if len(ordering[i]) > max_suffix:\n                max_suffix = len(ordering[i])\n\n    # find the longest prefix\n    for i in range(len(orderings)):\n        for ordering in reversed(orderings):\n            if len(ordering[i]) == max_suffix:\n                break\n        else:\n            raise ValueError(\"Unable to merge orderings with longest prefix\")\n\n    # merge the two orderings\n    return [orderings[0][j] for j in range(max_suffix + 1)] + \\\n        [orderings[i][j] for j in range(max_suffix + 1) for i in range(len(orderings[i]))]\n", "def _legacy_mergeOrderings(orderings):\n    merged = []\n    current = merged\n    for ordering in orderings:\n        if len(current) == 0:\n            continue\n        elif len(current) == 1:\n            if current[0][0] == ordering[0][0]:\n                current.pop(0)\n        else:\n            current.insert(0, ordering)\n    return current\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _legacy_mergeOrderings(orderings):\n    merged = []\n    for ordering in orderings:\n        for o in ordering:\n            if o not in merged:\n                merged.append(o)\n    return merged\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    result = []\n    for o in orderings:\n        for i in range(0, len(o), 2):\n            result.append(o[i])\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    merged = []\n\n    for o in orderings:\n        for el in o:\n            if el not in merged:\n                merged.append(el)\n\n    return merged\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    mergedOrderings = []\n    for ordering in reversed(orderings):\n        for o in ordering:\n            if o not in mergedOrderings:\n                mergedOrderings.insert(0, o)\n    return mergedOrderings\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    mergedOrderings = []\n    for ordering in orderings:\n        for o in ordering:\n            if o not in mergedOrderings:\n                mergedOrderings.append(o)\n    return mergedOrderings\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\n    merged_orderings = []\n    for ordering in orderings:\n        for obj in ordering:\n            if obj not in merged_orderings:\n                merged_orderings.append(obj)\n    return merged_orderings\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    orderings = sorted(orderings, key=_legacyOrderingKey)\n    merged_orderings = []\n    for o in orderings:\n        for e in o:\n            if e not in merged_orderings:\n                merged_orderings.append(e)\n    return merged_orderings\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    orderings = sorted(orderings, key=_legacy_key)\n    merged = []\n    for o in orderings:\n        for e in o:\n            if e not in merged:\n                merged.append(e)\n    return merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    if len(orderings) == 0:\n        return orderings\n\n    # find the longest suffix\n    max_suffix = 0\n    for i in range(len(orderings[0])):\n        for ordering in reversed(orderings):\n            if len(ordering[i]) > max_suffix:\n                max_suffix = len(ordering[i])\n\n    # find the longest prefix\n    for i in range(len(orderings)):\n        for ordering in reversed(orderings):\n            if len(ordering[i]) == max_suffix:\n                break\n        else:\n            raise ValueError(\"Unable to merge orderings with longest prefix\")\n\n    # merge the two orderings\n    return [orderings[0][j] for j in range(max_suffix + 1)] + \\\n        [orderings[i][j] for j in range(max_suffix + 1) for i in range(len(orderings[i]))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n    merged = []\n    current = merged\n    for ordering in orderings:\n        if len(current) == 0:\n            continue\n        elif len(current) == 1:\n            if current[0][0] == ordering[0][0]:\n                current.pop(0)\n        else:\n            current.insert(0, ordering)\n    return current\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "directlyProvidedBy", "docstring": "Return the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.", "ques_id": "62b8b58deb7e40a82d2d1269", "code": ["def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is None:\n            return None\n    if providedBy is object:\n        return interfaces.IDeclaration(object)\n    return providedBy\n", "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is not None:\n        return providedBy\n    provides = providedBy = getattr(object, '__provides__', None)\n    if provides is not None:\n        return provides\n    raise TypeError(\n        \"%s doesn't provide any interfaces by itself.\" % object)\n", "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is None:\n        return None\n    return providedBy()\n", "def directlyProvidedBy(object):\n    providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is not None:\n        provided = providedBy()\n    else:\n        provided = None\n    return provided\n", "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is None:\n            return None\n        object.__providedBy__ = providedBy\n    return providedBy\n", "def directlyProvidedBy(object):\n    if IContentType.providedBy(object):\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is not None:\n            return providedBy\n    if IContent.providedBy(object):\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is not None:\n            return providedBy\n    if IObject.providedBy(object):\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is not None:\n            return providedBy\n    if IDirectlyProvided.providedBy(object):\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is not None:\n            return providedBy\n    raise TypeError(\"%s does not provide directly providing interfaces\" % (object,))\n", "def directlyProvidedBy(object):\n    providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is None:\n        providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        return None\n    return providedBy()\n", "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        providedBy = getattr(object, '__parent__', None)\n        if providedBy is None:\n            return None\n    if providedBy is object:\n        raise ValueError(\"Can't resolve directlyProvidedBy to %r\" % object)\n    return providedBy\n", "def directlyProvidedBy(object):\n    return providedBy(object)[1]\n", "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is not None:\n        providedBy = providedBy()\n    if providedBy is None:\n        raise ValueError(\"Object %s doesn't provide an implementation for \"\n                         \"'directlyProvidedBy'\" % (object,))\n    return providedBy\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is None:\n            return None\n    if providedBy is object:\n        return interfaces.IDeclaration(object)\n    return providedBy\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is not None:\n        return providedBy\n    provides = providedBy = getattr(object, '__provides__', None)\n    if provides is not None:\n        return provides\n    raise TypeError(\n        \"%s doesn't provide any interfaces by itself.\" % object)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is None:\n        return None\n    return providedBy()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is not None:\n        provided = providedBy()\n    else:\n        provided = None\n    return provided\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is None:\n            return None\n        object.__providedBy__ = providedBy\n    return providedBy\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    if IContentType.providedBy(object):\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is not None:\n            return providedBy\n    if IContent.providedBy(object):\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is not None:\n            return providedBy\n    if IObject.providedBy(object):\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is not None:\n            return providedBy\n    if IDirectlyProvided.providedBy(object):\n        providedBy = getattr(object, '__providedBy__', None)\n        if providedBy is not None:\n            return providedBy\n    raise TypeError(\"%s does not provide directly providing interfaces\" % (object,))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, '__providedBy__', None)\n    if providedBy is None:\n        providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        return None\n    return providedBy()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is None:\n        providedBy = getattr(object, '__parent__', None)\n        if providedBy is None:\n            return None\n    if providedBy is object:\n        raise ValueError(\"Can't resolve directlyProvidedBy to %r\" % object)\n    return providedBy\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    return providedBy(object)[1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n    providedBy = getattr(object, 'providedBy', None)\n    if providedBy is not None:\n        providedBy = providedBy()\n    if providedBy is None:\n        raise ValueError(\"Object %s doesn't provide an implementation for \"\n                         \"'directlyProvidedBy'\" % (object,))\n    return providedBy\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "minimalBases", "docstring": "Reduce a list of base classes to its ordered minimum equivalent", "ques_id": "62b8b559eb7e40a82d2d11f8", "code": ["def minimalBases(classes):\n    minBases = {}\n    for c in classes:\n        for m in c.__mro__:\n            if 'Base' in m.__dict__ and issubclass(m, c):\n                if m not in minBases:\n                    minBases[m] = m\n                elif m not in minBases or minBases[m]!= m:\n                    return None\n    return sorted(minBases.keys())\n", "def minimalBases(classes):\n    classes = list(classes)\n    classes.sort()\n    return [c for c in classes if issubclass(c, BASE)]\n", "def minimalBases(classes):\n    classes = list(classes)\n    classes.sort(key=len, reverse=True)\n    return classes\n", "def minimalBases(classes):\n    result = []\n    for c in classes:\n        result.extend(c.__mro__[1:])\n    result.sort()\n    return result\n", "def minimalBases(classes):\n    return reduce(_minimalBases, classes, [])\n", "def minimalBases(classes):\n    min_bases = []\n    for base in classes:\n        bases = [b for b in classes if issubclass(b, base)]\n        if bases:\n            min_bases.append(base)\n        else:\n            min_bases.append(base)\n    return min_bases\n", "def minimalBases(classes):\n    return sorted(classes, key=lambda x: len(x))\n", "def minimalBases(classes):\n    return sorted(classes, key=lambda x: len(x))\n", "def minimalBases(classes):\n    minimal = []\n    for base in classes:\n        minimal.extend([base])\n        minimal.extend([c for c in base.__bases__ if c not in minimal])\n    return minimal\n", "def minimalBases(classes):\n    classes = list(classes)\n    classes.sort(key=lambda x: x.__name__)\n    return classes\n"], "level": "self_contained", "generate_results": [{"generate_code": "def minimalBases(classes):\n    minBases = {}\n    for c in classes:\n        for m in c.__mro__:\n            if 'Base' in m.__dict__ and issubclass(m, c):\n                if m not in minBases:\n                    minBases[m] = m\n                elif m not in minBases or minBases[m]!= m:\n                    return None\n    return sorted(minBases.keys())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    classes = list(classes)\n    classes.sort()\n    return [c for c in classes if issubclass(c, BASE)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    classes = list(classes)\n    classes.sort(key=len, reverse=True)\n    return classes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    result = []\n    for c in classes:\n        result.extend(c.__mro__[1:])\n    result.sort()\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    return reduce(_minimalBases, classes, [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    min_bases = []\n    for base in classes:\n        bases = [b for b in classes if issubclass(b, base)]\n        if bases:\n            min_bases.append(base)\n        else:\n            min_bases.append(base)\n    return min_bases\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    return sorted(classes, key=lambda x: len(x))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    return sorted(classes, key=lambda x: len(x))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    minimal = []\n    for base in classes:\n        minimal.extend([base])\n        minimal.extend([c for c in base.__bases__ if c not in minimal])\n    return minimal\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n    classes = list(classes)\n    classes.sort(key=lambda x: x.__name__)\n    return classes\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "namesAndDescriptions", "docstring": "Return attribute names and descriptions defined by interface.", "ques_id": "62b8b4b9eb7e40a82d2d1134", "code": ["def namesAndDescriptions(self, all=False):\n    names = []\n    descs = []\n    for name, iface in self.items():\n        if not name.startswith('_'):\n            names.append(name)\n            descs.append(iface.description())\n    if all:\n        return names, descs\n    return names, dict((n, d.split(':', 1)[1].strip()) for n, d in descs)\n", "def namesAndDescriptions(self, all=False):\n    names = []\n    descs = []\n    for name, iface in self.items():\n        if not isinstance(iface, IField):\n            continue\n        desc = iface.Description()\n        if desc is None:\n            continue\n        descs.append(desc)\n        names.append(name)\n    if not all:\n        return names, descs\n    return names, descs, names\n", "def namesAndDescriptions(self, all=False):\n    names = []\n    descriptions = []\n    for name, value in self.items():\n        if (not all or not value) and (name!= '__interfaces__'):\n            names.append(name)\n            descriptions.append(value.__doc__)\n    return names, descriptions\n", "def namesAndDescriptions(self, all=False):\n    names = []\n    for name, interface in self.items():\n        if not IInterface.providedBy(interface):\n            continue\n        desc = interface.__doc__ or ''\n        if not all or name not in self.__used_names:\n            names.append(name)\n            if desc:\n                names.append(desc)\n    return names\n", "def namesAndDescriptions(self, all=False):\n    names = []\n    for iface in self.__iro__:\n        names.extend(iface.names(all))\n    return names\n", "def namesAndDescriptions(self, all=False):\n    names = []\n    for name, obj in self.items():\n        if (not all or not getattr(obj, '__providedBy__', None)) \\\n                and not name.startswith('_'):\n            names.append(name)\n            if IContentish.providedBy(obj):\n                names.append('')\n    return tuple(names)\n", "def namesAndDescriptions(self, all=False):\n    names = []\n    descriptions = []\n    for name, iface in self.items():\n        if (not all or iface.providedBy(self.context)) \\\n           and name not in self.exclude_from_list:\n            names.append(name)\n            descriptions.append(\n                self.descriptionFor(iface, name=name))\n    return names, descriptions\n", "def namesAndDescriptions(self, all=False):\n    names = []\n    descs = []\n    for name, iface in sorted(self.items()):\n        desc = iface.description\n        if desc is not None:\n            descs.append(desc)\n            if not all:\n                names.append(name)\n    return names, descs\n", "def namesAndDescriptions(self, all=False):\n    names = []\n    for name, iface in self.items():\n        if providedBy(iface):\n            names.append(name)\n            if not all:\n                yield name\n    if not all:\n        yield 'Name', names\n", "def namesAndDescriptions(self, all=False):\n    names = []\n    for name, interface in self.items():\n        if not interface:\n            continue\n        desc = interface.description\n        if not desc:\n            continue\n        names.append((name, desc))\n\n    names.sort()\n    if all:\n        return names\n    return [name for name, desc in names]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    descs = []\n    for name, iface in self.items():\n        if not name.startswith('_'):\n            names.append(name)\n            descs.append(iface.description())\n    if all:\n        return names, descs\n    return names, dict((n, d.split(':', 1)[1].strip()) for n, d in descs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    descs = []\n    for name, iface in self.items():\n        if not isinstance(iface, IField):\n            continue\n        desc = iface.Description()\n        if desc is None:\n            continue\n        descs.append(desc)\n        names.append(name)\n    if not all:\n        return names, descs\n    return names, descs, names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    descriptions = []\n    for name, value in self.items():\n        if (not all or not value) and (name!= '__interfaces__'):\n            names.append(name)\n            descriptions.append(value.__doc__)\n    return names, descriptions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    for name, interface in self.items():\n        if not IInterface.providedBy(interface):\n            continue\n        desc = interface.__doc__ or ''\n        if not all or name not in self.__used_names:\n            names.append(name)\n            if desc:\n                names.append(desc)\n    return names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    for iface in self.__iro__:\n        names.extend(iface.names(all))\n    return names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    for name, obj in self.items():\n        if (not all or not getattr(obj, '__providedBy__', None)) \\\n                and not name.startswith('_'):\n            names.append(name)\n            if IContentish.providedBy(obj):\n                names.append('')\n    return tuple(names)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    descriptions = []\n    for name, iface in self.items():\n        if (not all or iface.providedBy(self.context)) \\\n           and name not in self.exclude_from_list:\n            names.append(name)\n            descriptions.append(\n                self.descriptionFor(iface, name=name))\n    return names, descriptions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    descs = []\n    for name, iface in sorted(self.items()):\n        desc = iface.description\n        if desc is not None:\n            descs.append(desc)\n            if not all:\n                names.append(name)\n    return names, descs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    for name, iface in self.items():\n        if providedBy(iface):\n            names.append(name)\n            if not all:\n                yield name\n    if not all:\n        yield 'Name', names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n    names = []\n    for name, interface in self.items():\n        if not interface:\n            continue\n        desc = interface.description\n        if not desc:\n            continue\n        names.append((name, desc))\n\n    names.sort()\n    if all:\n        return names\n    return [name for name, desc in names]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "names", "docstring": "Return the attribute names defined by the interface.", "ques_id": "62b8b416eb7e40a82d2d1129", "code": ["def names(self, all=False):\n    attrs = []\n    for name, obj in self._attributes.items():\n        if not all or not obj.defined:\n            attrs.append(name)\n    return attrs\n", "def names(self, all=False):\n    if all:\n        return self._attrs.keys()\n    return self._attrs\n", "def names(self, all=False):\n    names = []\n    for key in self.keys():\n        if key.startswith('_'):\n            continue\n        name = getattr(self, key)\n        if name is not None:\n            if not all and not key.startswith('_'):\n                names.append(name)\n            else:\n                names.append(key)\n    return names\n", "def names(self, all=False):\n    names = []\n    for attr in self.__attributes:\n        if attr[0]!= '_':\n            names.append(attr)\n    if all:\n        return names\n    return names[:]\n", "def names(self, all=False):\n    return [k for k in self.keys() if not k.startswith('_')]\n", "def names(self, all=False):\n    return self._attrs\n", "def names(self, all=False):\n    names = set()\n    for iface in self._interfaces:\n        names.update(iface.names(all))\n    return names\n", "def names(self, all=False):\n    if not all:\n        return self._names\n    else:\n        return set(self._names)\n", "def names(self, all=False):\n    if all:\n        return dir(self.__class__)\n    return self.__dict__.keys()\n", "def names(self, all=False):\n    return attrs(self.__interface, all=all)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def names(self, all=False):\n    attrs = []\n    for name, obj in self._attributes.items():\n        if not all or not obj.defined:\n            attrs.append(name)\n    return attrs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    if all:\n        return self._attrs.keys()\n    return self._attrs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    names = []\n    for key in self.keys():\n        if key.startswith('_'):\n            continue\n        name = getattr(self, key)\n        if name is not None:\n            if not all and not key.startswith('_'):\n                names.append(name)\n            else:\n                names.append(key)\n    return names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    names = []\n    for attr in self.__attributes:\n        if attr[0]!= '_':\n            names.append(attr)\n    if all:\n        return names\n    return names[:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    return [k for k in self.keys() if not k.startswith('_')]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    return self._attrs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    names = set()\n    for iface in self._interfaces:\n        names.update(iface.names(all))\n    return names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    if not all:\n        return self._names\n    else:\n        return set(self._names)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    if all:\n        return dir(self.__class__)\n    return self.__dict__.keys()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n    return attrs(self.__interface, all=all)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "_normalizeargs", "docstring": "Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.", "ques_id": "62b8b3d6eb7e40a82d2d111c", "code": ["def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for item in sequence:\n        if isinstance(item, InterfaceClass):\n            output.append(item)\n            continue\n        if isinstance(item, Implements):\n            output.extend(_normalizeargs(item.args, output=output))\n            output.append(item)\n            continue\n        output.append(item)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for arg in sequence:\n        if isinstance(arg, InterfaceClass):\n            arg = arg._implied\n        elif isinstance(arg, tuple):\n            arg = _normalizeargs(arg, output)\n        elif isinstance(arg, type) and issubclass(arg, Declaration):\n            pass\n        else:\n            output.append(arg)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for item in sequence:\n        if isinstance(item, InterfaceClass):\n            item = _normalizeargs(item, output)\n        elif isinstance(item, Implements):\n            item = _normalizeimplements(item, output)\n        elif isinstance(item, Declarations):\n            item = _normalizedeclarations(item, output)\n        elif isinstance(item, tuple) and len(item) == 2:\n            item = _normalizeargs(item[0], output)\n        output.append(item)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for item in sequence:\n        if isinstance(item, InterfaceClass) or isinstance(item, Implements):\n            output.append(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.append(item)\n        else:\n            output.extend(_normalizeargs(item, output))\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    if isinstance(sequence, InterfaceClass):\n        output.append(sequence)\n        return\n    for x in sequence:\n        _normalizeargs(x, output)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    output = output or []\n    for arg in sequence:\n        if isinstance(arg, (InterfaceClass, Implements)):\n            arg = arg._with_bases(output)\n        elif not isinstance(arg, (list, tuple)):\n            output.append(arg)\n            continue\n        _normalizeargs(arg, output)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for arg in sequence:\n        if isinstance(arg, Declaration):\n            output.append(arg._normalizeargs())\n        elif isinstance(arg, InterfaceClass) or isinstance(arg, Implements):\n            output.append(arg._normalizeargs())\n        else:\n            output.append(arg)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    if isinstance(sequence, InterfaceClass):\n        sequence = [sequence]\n    for obj in sequence:\n        if not isinstance(obj, InterfaceClass):\n            obj = getInterface(obj)\n        output.append(obj.normalize())\n    return output\n", "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for arg in sequence:\n        if isinstance(arg, Decl):\n            output.append(arg._normalizeargs())\n        elif isinstance(arg, InterfaceClass):\n            output.append(arg)\n        elif isinstance(arg, Implementation):\n            output.extend(arg._normalizeargs())\n        else:\n            output.append(arg)\n    return output\n", "def _normalizeargs(sequence, output=None):\n    output = output or []\n\n    def normalize(arg, output):\n        if isinstance(arg, InterfaceClass):\n            output.append(arg._impl)\n        elif isinstance(arg, InterfaceSpec):\n            output.append(arg)\n        elif isinstance(arg, Implements):\n            output.extend(arg._impls)\n        elif isinstance(arg, tuple):\n            output.append(arg)\n        elif isinstance(arg, str):\n            output.append(normalize(arg, output))\n        else:\n            for i in range(len(arg)):\n                arg[i] = normalize(arg[i], output)\n        return output\n\n    if isinstance(sequence, Implements):\n        output = normalize(sequence._impls, output)\n    elif isinstance(sequence, tuple):\n        output = normalize(tuple(sequence), output)\n    else:\n        for i in range(le"], "level": "file_runnable", "generate_results": [{"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for item in sequence:\n        if isinstance(item, InterfaceClass):\n            output.append(item)\n            continue\n        if isinstance(item, Implements):\n            output.extend(_normalizeargs(item.args, output=output))\n            output.append(item)\n            continue\n        output.append(item)\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for arg in sequence:\n        if isinstance(arg, InterfaceClass):\n            arg = arg._implied\n        elif isinstance(arg, tuple):\n            arg = _normalizeargs(arg, output)\n        elif isinstance(arg, type) and issubclass(arg, Declaration):\n            pass\n        else:\n            output.append(arg)\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for item in sequence:\n        if isinstance(item, InterfaceClass):\n            item = _normalizeargs(item, output)\n        elif isinstance(item, Implements):\n            item = _normalizeimplements(item, output)\n        elif isinstance(item, Declarations):\n            item = _normalizedeclarations(item, output)\n        elif isinstance(item, tuple) and len(item) == 2:\n            item = _normalizeargs(item[0], output)\n        output.append(item)\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for item in sequence:\n        if isinstance(item, InterfaceClass) or isinstance(item, Implements):\n            output.append(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.append(item)\n        else:\n            output.extend(_normalizeargs(item, output))\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    if isinstance(sequence, InterfaceClass):\n        output.append(sequence)\n        return\n    for x in sequence:\n        _normalizeargs(x, output)\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    output = output or []\n    for arg in sequence:\n        if isinstance(arg, (InterfaceClass, Implements)):\n            arg = arg._with_bases(output)\n        elif not isinstance(arg, (list, tuple)):\n            output.append(arg)\n            continue\n        _normalizeargs(arg, output)\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for arg in sequence:\n        if isinstance(arg, Declaration):\n            output.append(arg._normalizeargs())\n        elif isinstance(arg, InterfaceClass) or isinstance(arg, Implements):\n            output.append(arg._normalizeargs())\n        else:\n            output.append(arg)\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    if isinstance(sequence, InterfaceClass):\n        sequence = [sequence]\n    for obj in sequence:\n        if not isinstance(obj, InterfaceClass):\n            obj = getInterface(obj)\n        output.append(obj.normalize())\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    if output is None:\n        output = []\n    for arg in sequence:\n        if isinstance(arg, Decl):\n            output.append(arg._normalizeargs())\n        elif isinstance(arg, InterfaceClass):\n            output.append(arg)\n        elif isinstance(arg, Implementation):\n            output.extend(arg._normalizeargs())\n        else:\n            output.append(arg)\n    return output\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n    output = output or []\n\n    def normalize(arg, output):\n        if isinstance(arg, InterfaceClass):\n            output.append(arg._impl)\n        elif isinstance(arg, InterfaceSpec):\n            output.append(arg)\n        elif isinstance(arg, Implements):\n            output.extend(arg._impls)\n        elif isinstance(arg, tuple):\n            output.append(arg)\n        elif isinstance(arg, str):\n            output.append(normalize(arg, output))\n        else:\n            for i in range(len(arg)):\n                arg[i] = normalize(arg[i], output)\n        return output\n\n    if isinstance(sequence, Implements):\n        output = normalize(sequence._impls, output)\n    elif isinstance(sequence, tuple):\n        output = normalize(tuple(sequence), output)\n    else:\n        for i in range(le", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_available", "docstring": "Return the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.", "ques_id": "62b8b3d5eb7e40a82d2d1110", "code": ["def _c_optimizations_available():\n    try:\n        from pytachy.c.optimizations import Optimizations\n        return Optimizations\n    except ImportError:\n        return False\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations\n        return c_optimizations\n    except ImportError:\n        return False\n", "def _c_optimizations_available():\n    from.c_optimizations import C_Optimizations\n    return C_Optimizations\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n    except ImportError:\n        return False\n    else:\n        return c_optimizations\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations\n        return c_optimizations\n    except ImportError:\n        return False\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations\n    except ImportError:\n        return False\n    else:\n        return c_optimizations\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n        return c_optimizations\n    except ImportError:\n        return False\n", "def _c_optimizations_available():\n    try:\n        import Cython.Build.cythonize\n        return Cython.Build.cythonize\n    except ImportError:\n        return False\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations\n        return c_optimizations\n    except ImportError:\n        return False\n", "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n    except ImportError:\n        return False\n    else:\n        return c_optimizations\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _c_optimizations_available():\n    try:\n        from pytachy.c.optimizations import Optimizations\n        return Optimizations\n    except ImportError:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations\n        return c_optimizations\n    except ImportError:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    from.c_optimizations import C_Optimizations\n    return C_Optimizations\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n    except ImportError:\n        return False\n    else:\n        return c_optimizations\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations\n        return c_optimizations\n    except ImportError:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations\n    except ImportError:\n        return False\n    else:\n        return c_optimizations\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n        return c_optimizations\n    except ImportError:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import Cython.Build.cythonize\n        return Cython.Build.cythonize\n    except ImportError:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations\n        return c_optimizations\n    except ImportError:\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n    try:\n        import c_optimizations  # noqa\n    except ImportError:\n        return False\n    else:\n        return c_optimizations\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_should_attempt_c_optimizations", "docstring": "Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.", "ques_id": "62b8b3d4eb7e40a82d2d110f", "code": ["def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    return pure_pypy is not None and pure_pypy == '1'\n", "def _should_attempt_c_optimizations():\n    pure_env_var = os.environ.get('PURE_PYTHON')\n    if pure_env_var:\n        return pure_env_var == '1'\n\n    return not _use_c_impl()\n", "def _should_attempt_c_optimizations():\n    pure_env_name = os.environ.get('PURE_PYTHON')\n    if pure_env_name is not None:\n        return pure_env_name.startswith('C')\n\n    return False\n", "def _should_attempt_c_optimizations():\n    pure_env_name = os.environ.get('PURE_PYTHON')\n    return pure_env_name is not None and pure_env_name!= '1'\n", "def _should_attempt_c_optimizations():\n    pure_env_var = os.environ.get('PURE_PYTHON')\n    return pure_env_var is not None and pure_env_var!= '0'\n", "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy:\n        return True\n\n    return _use_c_impl()\n", "def _should_attempt_c_optimizations():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env == '1'\n", "def _should_attempt_c_optimizations():\n    pure_pypy_env_var = \"PURE_PYTHON\"\n    return pure_pypy_env_var in os.environ\n", "def _should_attempt_c_optimizations():\n    return os.environ.get('PURE_PYTHON') == '1'\n", "def _should_attempt_c_optimizations():\n    pure_env_var = os.environ.get('PURE_PYTHON')\n\n    if pure_env_var is not None:\n        return pure_env_var == '1'\n\n    return not IS_PYPY\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    return pure_pypy is not None and pure_pypy == '1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_env_var = os.environ.get('PURE_PYTHON')\n    if pure_env_var:\n        return pure_env_var == '1'\n\n    return not _use_c_impl()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_env_name = os.environ.get('PURE_PYTHON')\n    if pure_env_name is not None:\n        return pure_env_name.startswith('C')\n\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_env_name = os.environ.get('PURE_PYTHON')\n    return pure_env_name is not None and pure_env_name!= '1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_env_var = os.environ.get('PURE_PYTHON')\n    return pure_env_var is not None and pure_env_var!= '0'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy = os.environ.get('PURE_PYTHON')\n    if pure_pypy:\n        return True\n\n    return _use_c_impl()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env == '1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_pypy_env_var = \"PURE_PYTHON\"\n    return pure_pypy_env_var in os.environ\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    return os.environ.get('PURE_PYTHON') == '1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n    pure_env_var = os.environ.get('PURE_PYTHON')\n\n    if pure_env_var is not None:\n        return pure_env_var == '1'\n\n    return not IS_PYPY\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_ignored", "docstring": "The opposite of `_c_optimizations_required`.", "ques_id": "62b8b3d4eb7e40a82d2d110e", "code": ["def _c_optimizations_ignored():\n    if IS_PYTHON_3:\n        return False\n    else:\n        return _c_optimizations_required()\n", "def _c_optimizations_ignored():\n    return os.environ.get('CYTHONOPTIMIZATIONS') is not None\n", "def _c_optimizations_ignored():\n    return (\n        # Old, pre-1.4 compatibility\n        ('-fopenmp', '-ffast_compat'),\n        ('-fopenmp', '-fparse_as_sparse'),\n        ('-fopenmp', '-fparse_as_np'),\n        ('-fopenmp', '-funroll_loops'),\n        ('-fopenmp', '-fmax_allocated'),\n        ('-fopenmp', '-ftrack_error'),\n        ('-fopenmp', '-fparse_as_sparse_legacy'),\n        ('-fopenmp', '-fparse_as_np_legacy'),\n        ('-fopenmp', '-ftrack_error_legacy'),\n        ('-fopenmp', '-fparse_as_np_legacy'),\n    )\n", "def _c_optimizations_ignored():\n    return False\n", "def _c_optimizations_ignored():\n    if sys.platform!= 'win32':\n        return False\n    if not _c_optimizations_required():\n        return False\n    if _have_cffi():\n        _c_optimizations_ignored = _c_optimizations_ignored_cffi\n    return _c_optimizations_ignored()\n", "def _c_optimizations_ignored():\n    if _c_optimizations_required():\n        if _c_optimizations_allowed():\n            return True\n        else:\n            return False\n    else:\n        return True\n", "def _c_optimizations_ignored():\n    return lambda opt: not opt.required\n", "def _c_optimizations_ignored():\n    return lambda inst: False\n", "def _c_optimizations_ignored():\n    if sys.platform!= 'win32':\n        return False\n    if not is_cygwin():\n        return False\n    return _c_optimizations_required() or _c_optimizations_allowed()\n", "def _c_optimizations_ignored():\n    return lambda state, *args, **kwargs: None\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_ignored():\n    if IS_PYTHON_3:\n        return False\n    else:\n        return _c_optimizations_required()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return os.environ.get('CYTHONOPTIMIZATIONS') is not None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n    return (\n        # Old, pre-1.4 compatibility\n        ('-fopenmp', '-ffast_compat'),\n        ('-fopenmp', '-fparse_as_sparse'),\n        ('-fopenmp', '-fparse_as_np'),\n        ('-fopenmp', '-funroll_loops'),\n        ('-fopenmp', '-fmax_allocated'),\n        ('-fopenmp', '-ftrack_error'),\n        ('-fopenmp', '-fparse_as_sparse_legacy'),\n        ('-fopenmp', '-fparse_as_np_legacy'),\n        ('-fopenmp', '-ftrack_error_legacy'),\n        ('-fopenmp', '-fparse_as_np_legacy'),\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n    if sys.platform!= 'win32':\n        return False\n    if not _c_optimizations_required():\n        return False\n    if _have_cffi():\n        _c_optimizations_ignored = _c_optimizations_ignored_cffi\n    return _c_optimizations_ignored()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n    if _c_optimizations_required():\n        if _c_optimizations_allowed():\n            return True\n        else:\n            return False\n    else:\n        return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return lambda opt: not opt.required\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    return lambda inst: False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n    if sys.platform!= 'win32':\n        return False\n    if not is_cygwin():\n        return False\n    return _c_optimizations_required() or _c_optimizations_allowed()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n    return lambda state, *args, **kwargs: None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_required", "docstring": "Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.", "ques_id": "62b8b3d4eb7e40a82d2d110d", "code": ["def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    if pure_env == '1':\n        return True\n    else:\n        return False\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n    return False\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON', '')\n    return pure_env!= '0'\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    else:\n        return pure_env == '1'\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return True\n\n    return pure_env is True\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    return pure_env in ('1', '0', 'FALSE', '0FALSE', '0', '0FALSE')\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON', '')\n    return pure_env!= '0'\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= '0'\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= '0'\n", "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n\n    return sys.platform.startswith('pure')\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    if pure_env == '1':\n        return True\n    else:\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n    return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON', '')\n    return pure_env!= '0'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    else:\n        return pure_env == '1'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return True\n\n    return pure_env is True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is None:\n        return False\n    return pure_env in ('1', '0', 'FALSE', '0FALSE', '0', '0FALSE')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON', '')\n    return pure_env!= '0'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= '0'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env!= '0'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n    pure_env = os.environ.get('PURE_PYTHON')\n    if pure_env is not None:\n        return pure_env == '1'\n\n    return sys.platform.startswith('pure')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "reset", "docstring": "Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).", "ques_id": "62b87b989a0c4fa8b80b35ee", "code": ["def reset(self):\n    self._hist.clear()\n    self._hist.update({k: {} for k in self._hist_keys})\n", "def reset(self):\n    self._hist.clear()\n    self._update_bin_counts()\n", "def reset(self):\n    self._hist.clear()\n    self._last_update = 0\n    self._curr_update = 0\n", "def reset(self):\n    self._hist.clear()\n    self._update_bins()\n", "def reset(self):\n    self.hist_list = []\n    self.cur_context = {}\n    self.cur_bin_values = {}\n    self.cur_bin_count = 0\n    self.cur_bin_edges = {}\n", "def reset(self):\n    self._hist.clear()\n    self._bin_edges = self.make_bins()\n", "def reset(self):\n    self._hist.clear()\n    self._n = 0\n    self._sum = 0\n    self._max = 0\n    self._min = 0\n    self._mean = 0\n    self._std = 0\n    self._avg = 0\n    self._var = 0\n    self._sum_sq = 0\n    self._max_sq = 0\n    self._min_sq = 0\n    self._mean_sq = 0\n    self._var_sq = 0\n    self._sum_avg_sq = 0\n    self._max_avg_sq = 0\n    self._min_avg_sq = 0\n    self._mean_avg_sq = 0\n    self._var_avg_sq = 0\n    self._sum_sq_avg_sq = 0\n    self._max_sq_avg_sq = 0\n    self._min_sq_avg_sq = 0\n    self._mean_avg_sq_avg_sq = 0\n    self._var_avg_sq_avg_sq = 0\n    self._sum_avg_sq_avg_sq = 0\n    self._max_sq_avg_sq_avg_sq = 0\n    self._min_sq_", "def reset(self):\n    self._hist.clear()\n    self._initial_value = {}\n    self._sum = {}\n    self._n = 0\n", "def reset(self):\n    self._hist.clear()\n    self._count = 0\n    self._last = self._first = None\n", "def reset(self):\n    self._hist.clear()\n    self._update_bins()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def reset(self):\n    self._hist.clear()\n    self._hist.update({k: {} for k in self._hist_keys})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n    self._hist.clear()\n    self._update_bin_counts()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n    self._hist.clear()\n    self._last_update = 0\n    self._curr_update = 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n    self._hist.clear()\n    self._update_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n    self.hist_list = []\n    self.cur_context = {}\n    self.cur_bin_values = {}\n    self.cur_bin_count = 0\n    self.cur_bin_edges = {}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def reset(self):\n    self._hist.clear()\n    self._bin_edges = self.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n    self._hist.clear()\n    self._n = 0\n    self._sum = 0\n    self._max = 0\n    self._min = 0\n    self._mean = 0\n    self._std = 0\n    self._avg = 0\n    self._var = 0\n    self._sum_sq = 0\n    self._max_sq = 0\n    self._min_sq = 0\n    self._mean_sq = 0\n    self._var_sq = 0\n    self._sum_avg_sq = 0\n    self._max_avg_sq = 0\n    self._min_avg_sq = 0\n    self._mean_avg_sq = 0\n    self._var_avg_sq = 0\n    self._sum_sq_avg_sq = 0\n    self._max_sq_avg_sq = 0\n    self._min_sq_avg_sq = 0\n    self._mean_avg_sq_avg_sq = 0\n    self._var_avg_sq_avg_sq = 0\n    self._sum_avg_sq_avg_sq = 0\n    self._max_sq_avg_sq_avg_sq = 0\n    self._min_sq_", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n    self._hist.clear()\n    self._initial_value = {}\n    self._sum = {}\n    self._n = 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n    self._hist.clear()\n    self._count = 0\n    self._last = self._first = None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n    self._hist.clear()\n    self._update_bins()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "to_csv", "docstring": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.", "ques_id": "62b87b859a0c4fa8b80b35d7", "code": ["def to_csv(self, separator=\",\", header=None):\n    from.output import ToCSV\n    return ToCSV(self, separator=separator, header=header)\n", "def to_csv(self, separator=\",\", header=None):\n    return to_csv(self, separator=separator, header=header)\n", "def to_csv(self, separator=\",\", header=None):\n    from..output import ToCSV\n    return ToCSV(self, separator=separator, header=header)\n", "def to_csv(self, separator=\",\", header=None):\n    from..utils.io import StringIO\n\n    return ToCSV(self, separator=separator, header=header).stream()\n", "def to_csv(self, separator=\",\", header=None):\n    import warnings\n    warnings.warn(\"to_csv is deprecated. Please use graph.to_csv(separator, header) instead\",\n                  DeprecationWarning)\n    return self.to_csv_iterable(separator, header)\n", "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(self, separator=separator, header=header)\n", "def to_csv(self, separator=\",\", header=None):\n    return to_csv(self, separator=separator, header=header)\n", "def to_csv(self, separator=\",\", header=None):\n    return list(self.graph.points_to_csv(separator=separator, header=header))\n", "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(separator=separator, header=header).do(self)\n", "def to_csv(self, separator=\",\", header=None):\n    import warnings\n    from..utils.deprecation import deprecate\n    warnings.warn(\"to_csv is deprecated, and will be removed in a \"\n                  \"future release. Please use graph.to_csv(separator=',', \"\n                  \"header=None) instead.\",\n                  DeprecationWarning)\n    return self.to_csv_from_points(separator=separator, header=header)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def to_csv(self, separator=\",\", header=None):\n    from.output import ToCSV\n    return ToCSV(self, separator=separator, header=header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return to_csv(self, separator=separator, header=header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    from..output import ToCSV\n    return ToCSV(self, separator=separator, header=header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    from..utils.io import StringIO\n\n    return ToCSV(self, separator=separator, header=header).stream()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    import warnings\n    warnings.warn(\"to_csv is deprecated. Please use graph.to_csv(separator, header) instead\",\n                  DeprecationWarning)\n    return self.to_csv_iterable(separator, header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(self, separator=separator, header=header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return to_csv(self, separator=separator, header=header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return list(self.graph.points_to_csv(separator=separator, header=header))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    return ToCSV(separator=separator, header=header).do(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n    import warnings\n    from..utils.deprecation import deprecate\n    warnings.warn(\"to_csv is deprecated, and will be removed in a \"\n                  \"future release. Please use graph.to_csv(separator=',', \"\n                  \"header=None) instead.\",\n                  DeprecationWarning)\n    return self.to_csv_from_points(separator=separator, header=header)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_get_err_indices", "docstring": "Get error indices corresponding to a coordinate.", "ques_id": "62b87b839a0c4fa8b80b35cb", "code": ["def _get_err_indices(self, coord_name):\n    if coord_name in self._coords:\n        return self._coords[coord_name].err_indices\n    else:\n        raise KeyError(\"Coordinate '{}' not found.\".format(coord_name))\n", "def _get_err_indices(self, coord_name):\n    if coord_name in self._coords:\n        return self._coords[coord_name].err_indices\n    return None\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError(\"Coordinate '%s' not in shape dataset\" % coord_name)\n    return self._err_coords[coord_name]\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coords:\n        raise ValueError('coordinate \\'{}\\' is not in the system'.format(coord_name))\n\n    return self._coords[coord_name]['err_indices']\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError(\"Coordinate '{0}' is not a valid coordinate \"\n                         \"register.\".format(coord_name))\n    return self._coord_names.index(coord_name)\n", "def _get_err_indices(self, coord_name):\n    err_name = self._coord_names[coord_name] + '_err'\n    return self._coord_names[coord_name] + '_err', self._coord_names[coord_name] + '_idx'\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coords:\n        raise ValueError(\"Indices not found.\")\n    return self._coords[coord_name][\"err_indices\"]\n", "def _get_err_indices(self, coord_name):\n    if coord_name in self._coord_dict:\n        return self._coord_dict[coord_name]\n    else:\n        raise KeyError(\n            'coordinate {} not found in {} representation'.format(\n                coord_name, self._repr_string))\n", "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coords:\n        raise KeyError('coordinate \"{0}\" is not a recognized coordinate.'\n                      .format(coord_name))\n    return self._coords[coord_name].keys()\n", "def _get_err_indices(self, coord_name):\n    if coord_name in self.coords:\n        return self.coords[coord_name].values\n    else:\n        raise KeyError(f\"Coordinate {coord_name} not found.\")\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name in self._coords:\n        return self._coords[coord_name].err_indices\n    else:\n        raise KeyError(\"Coordinate '{}' not found.\".format(coord_name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name in self._coords:\n        return self._coords[coord_name].err_indices\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError(\"Coordinate '%s' not in shape dataset\" % coord_name)\n    return self._err_coords[coord_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coords:\n        raise ValueError('coordinate \\'{}\\' is not in the system'.format(coord_name))\n\n    return self._coords[coord_name]['err_indices']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coord_names:\n        raise ValueError(\"Coordinate '{0}' is not a valid coordinate \"\n                         \"register.\".format(coord_name))\n    return self._coord_names.index(coord_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    err_name = self._coord_names[coord_name] + '_err'\n    return self._coord_names[coord_name] + '_err', self._coord_names[coord_name] + '_idx'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coords:\n        raise ValueError(\"Indices not found.\")\n    return self._coords[coord_name][\"err_indices\"]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name in self._coord_dict:\n        return self._coord_dict[coord_name]\n    else:\n        raise KeyError(\n            'coordinate {} not found in {} representation'.format(\n                coord_name, self._repr_string))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name not in self._coords:\n        raise KeyError('coordinate \"{0}\" is not a recognized coordinate.'\n                      .format(coord_name))\n    return self._coords[coord_name].keys()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n    if coord_name in self.coords:\n        return self.coords[coord_name].values\n    else:\n        raise KeyError(f\"Coordinate {coord_name} not found.\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_update_context", "docstring": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.", "ques_id": "62b87b7e9a0c4fa8b80b35bc", "code": ["def _update_context(self, context):\n    self._update_value_context(context, self.value)\n    self._update_error_context(context, self.error)\n", "def _update_context(self, context):\n    context.error = []\n    context.value = {}\n    for key, value in self.items():\n        if key not in context.value:\n            context.value[key] = {}\n        if isinstance(value, Edge):\n            context.value[key]['edge_index'] = value.edge_index\n            context.value[key]['node_index'] = value.node_index\n        elif isinstance(value, Node):\n            context.value[key]['node_index'] = value.index\n            context.value[key]['edge_index'] = value.edge_index\n        else:\n            context.value[key] = value\n", "def _update_context(self, context):\n    if self._value is not None:\n        context.error = {\"x\": {\"index\": self._value.index}}\n", "def _update_context(self, context):\n\n    context.error = []\n    for i, (x, y, z) in enumerate(self.value):\n        try:\n            error = context.error.append(i)\n        except AttributeError:\n            error = []\n        context.value[i] = (x, y, z)\n        if isinstance(error, tuple):\n            context.error_E_low[i] = error\n        else:\n            context.error.append(error)\n", "def _update_context(self, context):\n    context.error = []\n    for k, v in self._data.items():\n        if k!= \"error\":\n            context.value[k] = v.value\n        else:\n            context.error.append(k)\n", "def _update_context(self, context):\n    self._update_error_context(context)\n    self._update_value_context(context)\n", "def _update_context(self, context):\n\n    self._update_context_properties(context)\n    self._update_context_errors(context)\n", "def _update_context(self, context):\n    for field in self._fields:\n        if field in context:\n            setattr(context[field], 'index', self._indices[field])\n        else:\n            context[field] = self._default_values[field]\n\n    for key, val in context.items():\n        if key in self._value_attrs:\n            setattr(context[key], key, val)\n        elif key in self._error_attrs:\n            setattr(context[key], key, val)\n        else:\n            raise KeyError(\"Graph key {} not found in context.\".format(key))\n", "def _update_context(self, context):\n    context.error = []\n    context.value = {}\n    context.indices = []\n    context.update(self.properties)\n    context.update(self._properties)\n    context.update(self._style)\n    context.update(self._transitions)\n    context.update(self._nodes)\n", "def _update_context(self, context):\n    # Attributes that are assigned to the context are handled by this\n    # method so it can be overridden by subclasses.\n    self._update_attributes(context, \"error\")\n    # Update context with the graph data\n    context.value = self._graph._graph\n    context.error = context.error = self._graph._graph.error\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _update_context(self, context):\n    self._update_value_context(context, self.value)\n    self._update_error_context(context, self.error)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    context.error = []\n    context.value = {}\n    for key, value in self.items():\n        if key not in context.value:\n            context.value[key] = {}\n        if isinstance(value, Edge):\n            context.value[key]['edge_index'] = value.edge_index\n            context.value[key]['node_index'] = value.node_index\n        elif isinstance(value, Node):\n            context.value[key]['node_index'] = value.index\n            context.value[key]['edge_index'] = value.edge_index\n        else:\n            context.value[key] = value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    if self._value is not None:\n        context.error = {\"x\": {\"index\": self._value.index}}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\n    context.error = []\n    for i, (x, y, z) in enumerate(self.value):\n        try:\n            error = context.error.append(i)\n        except AttributeError:\n            error = []\n        context.value[i] = (x, y, z)\n        if isinstance(error, tuple):\n            context.error_E_low[i] = error\n        else:\n            context.error.append(error)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    context.error = []\n    for k, v in self._data.items():\n        if k!= \"error\":\n            context.value[k] = v.value\n        else:\n            context.error.append(k)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    self._update_error_context(context)\n    self._update_value_context(context)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\n    self._update_context_properties(context)\n    self._update_context_errors(context)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    for field in self._fields:\n        if field in context:\n            setattr(context[field], 'index', self._indices[field])\n        else:\n            context[field] = self._default_values[field]\n\n    for key, val in context.items():\n        if key in self._value_attrs:\n            setattr(context[key], key, val)\n        elif key in self._error_attrs:\n            setattr(context[key], key, val)\n        else:\n            raise KeyError(\"Graph key {} not found in context.\".format(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    context.error = []\n    context.value = {}\n    context.indices = []\n    context.update(self.properties)\n    context.update(self._properties)\n    context.update(self._style)\n    context.update(self._transitions)\n    context.update(self._nodes)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n    # Attributes that are assigned to the context are handled by this\n    # method so it can be overridden by subclasses.\n    self._update_attributes(context, \"error\")\n    # Update context with the graph data\n    context.value = self._graph._graph\n    context.error = context.error = self._graph._graph.error\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "integral", "docstring": "Compute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.", "ques_id": "62b87b4f9a0c4fa8b80b3580", "code": ["def integral(bins, edges):\n    return np.trapz(bins, edges)\n", "def integral(bins, edges):\n    return np.diff(bins) * edges[1:] + np.diff(bins) * edges[:-1]\n", "def integral(bins, edges):\n    if bins.shape[0]!= edges.shape[0]:\n        raise ValueError(\"bins and edges must have the same length\")\n\n    n = edges.shape[0]\n    hist = np.zeros(n)\n    for i in range(n):\n        hist[i] = np.sum(bins[i]*edges[i])\n    return hist\n", "def integral(bins, edges):\n    xedges, yedges = np.histogram2d(edges[:-1], bins, density=True)\n    xmid, ymid = xedges[:-1] + 0.5 * (edges[1:] - edges[:-1])\n\n    return np.trapz(yedges, xmid) * xmid + np.trapz(ymid, xedges) * ymid\n", "def integral(bins, edges):\n    if len(bins)!= len(edges):\n        raise ValueError(\"bins and edges must be the same length\")\n\n    if edges[0] < edges[-1]:\n        raise ValueError(\"edges must be sorted in ascending order\")\n\n    if len(edges) < 2:\n        raise ValueError(\"edges must contain at least two values\")\n\n    # edges[1:] is the center of the bin\n    edges = np.concatenate((edges[:-1], edges[1:]))\n\n    # sum up the values for all bins and convert to integral\n    return np.diff(bins) * edges[1:] / edges[:-1]\n", "def integral(bins, edges):\n    n_bins = len(bins)\n    total = 0\n    for i in range(n_bins):\n        total += bins[i] * edges[i]\n    return total\n", "def integral(bins, edges):\n    return np.diff(bins) * edges[:-1] / bins[1:]\n", "def integral(bins, edges):\n    return np.diff(bins) * edges[1:] / edges[:-1]\n", "def integral(bins, edges):\n    return np.diff(bins) * edges[1:] / edges[:-1]\n", "def integral(bins, edges):\n    nbins = bins.shape[0]\n    nedges = edges.shape[0]\n    values = np.zeros(nbins)\n    weights = np.zeros(nbins)\n    for i in range(nbins):\n        values[i] = bins[i]\n        weights[i] = edges[i]\n    return values, weights\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def integral(bins, edges):\n    return np.trapz(bins, edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return np.diff(bins) * edges[1:] + np.diff(bins) * edges[:-1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    if bins.shape[0]!= edges.shape[0]:\n        raise ValueError(\"bins and edges must have the same length\")\n\n    n = edges.shape[0]\n    hist = np.zeros(n)\n    for i in range(n):\n        hist[i] = np.sum(bins[i]*edges[i])\n    return hist\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    xedges, yedges = np.histogram2d(edges[:-1], bins, density=True)\n    xmid, ymid = xedges[:-1] + 0.5 * (edges[1:] - edges[:-1])\n\n    return np.trapz(yedges, xmid) * xmid + np.trapz(ymid, xedges) * ymid\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    if len(bins)!= len(edges):\n        raise ValueError(\"bins and edges must be the same length\")\n\n    if edges[0] < edges[-1]:\n        raise ValueError(\"edges must be sorted in ascending order\")\n\n    if len(edges) < 2:\n        raise ValueError(\"edges must contain at least two values\")\n\n    # edges[1:] is the center of the bin\n    edges = np.concatenate((edges[:-1], edges[1:]))\n\n    # sum up the values for all bins and convert to integral\n    return np.diff(bins) * edges[1:] / edges[:-1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    n_bins = len(bins)\n    total = 0\n    for i in range(n_bins):\n        total += bins[i] * edges[i]\n    return total\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return np.diff(bins) * edges[:-1] / bins[1:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return np.diff(bins) * edges[1:] / edges[:-1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    return np.diff(bins) * edges[1:] / edges[:-1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n    nbins = bins.shape[0]\n    nedges = edges.shape[0]\n    values = np.zeros(nbins)\n    weights = np.zeros(nbins)\n    for i in range(nbins):\n        values[i] = bins[i]\n        weights[i] = edges[i]\n    return values, weights\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_seq", "docstring": "Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.", "ques_id": "62b87b199a0c4fa8b80b354e", "code": ["def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) and not isinstance(seq, SourceSeq)\n", "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) and not is_source_seq(seq)\n", "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequest) and not is_source_seq(seq)\n", "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) and not isinstance(seq, SourceSeq)\n", "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) and not is_source_seq(seq)\n", "def is_fill_request_seq(seq):\n    return (isinstance(seq, FillRequest)\n            and not isinstance(seq, SourceSequence))\n", "def is_fill_request_seq(seq):\n    return (isinstance(seq, FillRequest) or\n            any(isinstance(x, Source) for x in seq) and\n            not isinstance(seq, SourceSequence))\n", "def is_fill_request_seq(seq):\n    return (isinstance(seq, FillRequest) or\n            any(isinstance(s, Source) and s.is_fill_request_seq()\n                for s in get_flat_sequence(seq)))\n", "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequest) or (\n        isinstance(seq, Source) and any([seq.has_option(opt.name)\n                                       for opt in Source._option_sources]))\n", "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) \\\n        and not isinstance(seq, SourceSequence)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) and not isinstance(seq, SourceSeq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) and not is_source_seq(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequest) and not is_source_seq(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) and not isinstance(seq, SourceSeq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) and not is_source_seq(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return (isinstance(seq, FillRequest)\n            and not isinstance(seq, SourceSequence))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return (isinstance(seq, FillRequest) or\n            any(isinstance(x, Source) for x in seq) and\n            not isinstance(seq, SourceSequence))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return (isinstance(seq, FillRequest) or\n            any(isinstance(s, Source) and s.is_fill_request_seq()\n                for s in get_flat_sequence(seq)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequest) or (\n        isinstance(seq, Source) and any([seq.has_option(opt.name)\n                                       for opt in Source._option_sources]))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n    return isinstance(seq, FillRequestSeq) \\\n        and not isinstance(seq, SourceSequence)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_el", "docstring": "Object contains executable methods 'fill' and 'request'.", "ques_id": "62b87b099a0c4fa8b80b3538", "code": ["def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n", "def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n", "def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n", "def is_fill_request_el(obj):\n    return isinstance(obj, dict) and obj.get('type') =='request' \\\n        and obj.get('fill') is True\n", "def is_fill_request_el(obj):\n    return isinstance(obj, (Fill, Request))\n", "def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n", "def is_fill_request_el(obj):\n    return isinstance(obj, dict) and obj.get('fill') is not None and \\\n        obj.get('request') is not None\n", "def is_fill_request_el(obj):\n    return isinstance(obj, dict) and obj.get('method') == 'fill' and obj.get('request')\n", "def is_fill_request_el(obj):\n    return (isinstance(obj, (commands.Fill, commands.Request)) and\n            obj.fill is not None)\n", "def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, dict) and obj.get('type') =='request' \\\n        and obj.get('fill') is True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, (Fill, Request))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, dict) and obj.get('fill') is not None and \\\n        obj.get('request') is not None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, dict) and obj.get('method') == 'fill' and obj.get('request')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return (isinstance(obj, (commands.Fill, commands.Request)) and\n            obj.fill is not None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n    return isinstance(obj, (FillRequest, Request))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_run_el", "docstring": "Object contains executable method 'run'.", "ques_id": "62b87af99a0c4fa8b80b3524", "code": ["def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and obj.__name__.startswith('run')\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and obj.__name__.startswith('run')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n    return isinstance(obj, types.FunctionType) and hasattr(obj, 'run')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_compute_el", "docstring": "Object contains executable methods 'fill' and 'compute'.", "ques_id": "62b87af69a0c4fa8b80b351a", "code": ["def is_fill_compute_el(obj):\n    return (isinstance(obj, FillElement) and\n            hasattr(obj, 'fill') and\n            hasattr(obj, 'compute'))\n", "def is_fill_compute_el(obj):\n    return isinstance(obj, dict) and 'fill' in obj and 'compute' in obj\n", "def is_fill_compute_el(obj):\n    return isinstance(obj, FillElement) and obj.compute is not None\n", "def is_fill_compute_el(obj):\n    return (isinstance(obj, Fill) or\n            (isinstance(obj, Compute) and obj.fill is not None))\n", "def is_fill_compute_el(obj):\n    return (isinstance(obj, Fill) or\n            isinstance(obj, Compute) or\n            isinstance(obj, NdArray))\n", "def is_fill_compute_el(obj):\n    return isinstance(obj, (Fill, Compute))\n", "def is_fill_compute_el(obj):\n    return isinstance(obj, Fill) and isinstance(obj.fill, Compute)\n", "def is_fill_compute_el(obj):\n    return (isinstance(obj, Fill) or\n            (isinstance(obj, Compute) and obj.fill is not None))\n", "def is_fill_compute_el(obj):\n    return (isinstance(obj, Fill) or\n            isinstance(obj, Compute))\n", "def is_fill_compute_el(obj):\n    return isinstance(obj, (Fill, Compute))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_compute_el(obj):\n    return (isinstance(obj, FillElement) and\n            hasattr(obj, 'fill') and\n            hasattr(obj, 'compute'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return isinstance(obj, dict) and 'fill' in obj and 'compute' in obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return isinstance(obj, FillElement) and obj.compute is not None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return (isinstance(obj, Fill) or\n            (isinstance(obj, Compute) and obj.fill is not None))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return (isinstance(obj, Fill) or\n            isinstance(obj, Compute) or\n            isinstance(obj, NdArray))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return isinstance(obj, (Fill, Compute))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return isinstance(obj, Fill) and isinstance(obj.fill, Compute)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return (isinstance(obj, Fill) or\n            (isinstance(obj, Compute) and obj.fill is not None))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return (isinstance(obj, Fill) or\n            isinstance(obj, Compute))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n    return isinstance(obj, (Fill, Compute))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/context/functions.py", "project": "ynikitenko/lena", "name": "difference", "docstring": "Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.", "ques_id": "62b87af19a0c4fa8b80b34f7", "code": ["def difference(d1, d2, level=-1):\n    d = deepcopy(d1)\n    if level == 0:\n        return d\n    for key in d2:\n        if key not in d1 or d1[key]!= d2[key]:\n            d[level] = key\n            d1[key] = d2[key]\n    return d\n", "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if not hasattr(d1, 'keys'):\n        d1 = dict(d1)\n    if not hasattr(d2, 'keys'):\n        d2 = dict(d2)\n    d1_keys = d1.keys()\n    d2_keys = d2.keys()\n    if level < 0:\n        level = len(d1_keys)\n    if level < 0 or level >= len(d2_keys):\n        return d2\n    if d1_keys!= d2_keys:\n        return d1\n    d = dict(d1)\n    for k in d1_keys:\n        if k not in d2:\n            del d[k]\n    for k in d2_keys:\n        if k not in d1:\n            del d[k]\n    return d\n", "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n\n    if level < 0:\n        return d1\n\n    d2_keys = set(d2.keys())\n    d1_keys = set(d1.keys())\n    intersect = d1_keys.intersection(d2_keys)\n    if intersect:\n        d2_copy = {}\n        for key in intersect:\n            d2_copy[key] = d1[key]\n        return difference(d2_copy, d1, level - 1)\n    else:\n        d1_copy = d1.copy()\n        for key in d1_keys:\n            if d1[key] in d2:\n                del d1[key]\n        return difference(d1_copy, d2, level)\n", "def difference(d1, d2, level=-1):\n    d = deepcopy(d1)\n    _difference(d, d2, level)\n    return d\n", "def difference(d1, d2, level=-1):\n    if level < 0:\n        d1 = d1.copy()\n        d2 = d2.copy()\n        return _difference(d1, d2, level)\n\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n\n    result = {}\n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n    for key in d2:\n        if key not in d1:\n            result[key] = d2[key]\n    return result\n", "def difference(d1, d2, level=-1):\n    d1 = dict(d1)\n    if not d1:\n        return d2\n    if level < 0:\n        return d1\n    result = deepcopy(d1)\n    keys = list(d1) + list(d2)\n    for key in keys:\n        if key in d1 and d1[key]!= d2[key]:\n            result[key] = d1[key]\n    return result\n", "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if isinstance(d1, dict) and isinstance(d2, dict):\n        d1 = deepcopy(d1)\n        d2 = deepcopy(d2)\n        keys = d1.keys() & d2.keys()\n        for key in keys:\n            if d1[key]!= d2[key]:\n                d1[key] = intersection(d1[key], d2[key], level=level - 1)\n    elif isinstance(d1, (list, tuple)) and isinstance(d2, (list, tuple)):\n        d1 = deepcopy(d1)\n        d2 = deepcopy(d2)\n        for i, val1 in enumerate(d1):\n            for j, val2 in enumerate(d2[i]):\n                if val1!= val2:\n                    d1[i][j] = intersection(val1, val2, level=level - 1)\n    else:\n        return d2\n    return d1\n", "def difference(d1, d2, level=-1):\n    d2_copy = d2.copy()\n    if level == 0:\n        return d1\n    for key in d1:\n        if key not in d2:\n            d2_copy[key] = d1[key]\n        else:\n            d2_copy[key] = intersection(d1[key], d2[key], level - 1)\n    return d2_copy\n", "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if len(d1) == 1 and len(d2) == 1:\n        return d1.copy() if level == 0 else d2\n    if level == 0:\n        d1.update(d2)\n        return d1\n    d1.update(d1.keys())\n    d2.update(d2.keys())\n    res = {}\n    for k in d1:\n        if k not in d2:\n            res[k] = d1[k]\n    for k in d2:\n        if k not in d1:\n            res[k] = d2[k]\n    return res\n", "def difference(d1, d2, level=-1):\n    d2_copy = d2.copy()\n    if level == 0:\n        return d1\n    if not isinstance(d1, dict):\n        return d2\n    if not isinstance(d2, dict):\n        return d1\n    res = {}\n    for key in d1:\n        if key not in d2 or d1[key]!= d2[key]:\n            res[key] = d1[key]\n    d2_copy.update(d1)\n    d1 = d2\n    if level == 1:\n        return d1\n    return intersection(d1, d2, level - 1)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def difference(d1, d2, level=-1):\n    d = deepcopy(d1)\n    if level == 0:\n        return d\n    for key in d2:\n        if key not in d1 or d1[key]!= d2[key]:\n            d[level] = key\n            d1[key] = d2[key]\n    return d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if not hasattr(d1, 'keys'):\n        d1 = dict(d1)\n    if not hasattr(d2, 'keys'):\n        d2 = dict(d2)\n    d1_keys = d1.keys()\n    d2_keys = d2.keys()\n    if level < 0:\n        level = len(d1_keys)\n    if level < 0 or level >= len(d2_keys):\n        return d2\n    if d1_keys!= d2_keys:\n        return d1\n    d = dict(d1)\n    for k in d1_keys:\n        if k not in d2:\n            del d[k]\n    for k in d2_keys:\n        if k not in d1:\n            del d[k]\n    return d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n\n    if level < 0:\n        return d1\n\n    d2_keys = set(d2.keys())\n    d1_keys = set(d1.keys())\n    intersect = d1_keys.intersection(d2_keys)\n    if intersect:\n        d2_copy = {}\n        for key in intersect:\n            d2_copy[key] = d1[key]\n        return difference(d2_copy, d1, level - 1)\n    else:\n        d1_copy = d1.copy()\n        for key in d1_keys:\n            if d1[key] in d2:\n                del d1[key]\n        return difference(d1_copy, d2, level)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    d = deepcopy(d1)\n    _difference(d, d2, level)\n    return d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if level < 0:\n        d1 = d1.copy()\n        d2 = d2.copy()\n        return _difference(d1, d2, level)\n\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n\n    result = {}\n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n    for key in d2:\n        if key not in d1:\n            result[key] = d2[key]\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    d1 = dict(d1)\n    if not d1:\n        return d2\n    if level < 0:\n        return d1\n    result = deepcopy(d1)\n    keys = list(d1) + list(d2)\n    for key in keys:\n        if key in d1 and d1[key]!= d2[key]:\n            result[key] = d1[key]\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if isinstance(d1, dict) and isinstance(d2, dict):\n        d1 = deepcopy(d1)\n        d2 = deepcopy(d2)\n        keys = d1.keys() & d2.keys()\n        for key in keys:\n            if d1[key]!= d2[key]:\n                d1[key] = intersection(d1[key], d2[key], level=level - 1)\n    elif isinstance(d1, (list, tuple)) and isinstance(d2, (list, tuple)):\n        d1 = deepcopy(d1)\n        d2 = deepcopy(d2)\n        for i, val1 in enumerate(d1):\n            for j, val2 in enumerate(d2[i]):\n                if val1!= val2:\n                    d1[i][j] = intersection(val1, val2, level=level - 1)\n    else:\n        return d2\n    return d1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    d2_copy = d2.copy()\n    if level == 0:\n        return d1\n    for key in d1:\n        if key not in d2:\n            d2_copy[key] = d1[key]\n        else:\n            d2_copy[key] = intersection(d1[key], d2[key], level - 1)\n    return d2_copy\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    if not d1:\n        return d2\n    if not d2:\n        return d1\n    if len(d1) == 1 and len(d2) == 1:\n        return d1.copy() if level == 0 else d2\n    if level == 0:\n        d1.update(d2)\n        return d1\n    d1.update(d1.keys())\n    d2.update(d2.keys())\n    res = {}\n    for k in d1:\n        if k not in d2:\n            res[k] = d1[k]\n    for k in d2:\n        if k not in d1:\n            res[k] = d2[k]\n    return res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n    d2_copy = d2.copy()\n    if level == 0:\n        return d1\n    if not isinstance(d1, dict):\n        return d2\n    if not isinstance(d2, dict):\n        return d1\n    res = {}\n    for key in d1:\n        if key not in d2 or d1[key]!= d2[key]:\n            res[key] = d1[key]\n    d2_copy.update(d1)\n    d1 = d2\n    if level == 1:\n        return d1\n    return intersection(d1, d2, level - 1)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "fill", "docstring": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.", "ques_id": "62b87af09a0c4fa8b80b34f1", "code": ["def fill(self, coord, weight=1):\n    self._data[coord] = weight\n", "def fill(self, coord, weight=1):\n    self._fill(coord, weight)\n", "def fill(self, coord, weight=1):\n    if self._bins is None:\n        raise ValueError(\"No histogram bin information available.\")\n    if coord >= self._bins or coord < 0:\n        raise ValueError(\"Invalid coordinate.\")\n    if coord < 0:\n        coord += self._bins\n    self._data[coord] += weight\n", "def fill(self, coord, weight=1):\n    if self._bins == 1:\n        return\n\n    if coord >= self._bins or coord < 0:\n        raise IndexError('coord {} outside histogram edges {}'.format(coord, self.edges))\n\n    for i in range(self._bins):\n        self._data[i][coord + i] += weight\n", "def fill(self, coord, weight=1):\n    if coord >= self.size:\n        raise IndexError('coord outside histogram')\n    self.data[coord] += weight\n", "def fill(self, coord, weight=1):\n    self._update(self._buf, coord, weight)\n", "def fill(self, coord, weight=1):\n    if not self._bin_edges:\n        raise ValueError(\"bin edges are not set\")\n    if coord in self:\n        raise ValueError(\"coordinate \" + str(coord) + \" already set\")\n\n    edges = self._bin_edges[1:] - self._bin_edges[:-1]\n    self._data[coord] = weight * np.diff(edges) / np.diff(self._bin_edges)\n", "def fill(self, coord, weight=1):\n    self._fill(self._find(coord), weight)\n", "def fill(self, coord, weight=1):\n    self._data[coord] = weight\n", "def fill(self, coord, weight=1):\n    if self._min <= coord < self._max:\n        self._hist[coord] += weight\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fill(self, coord, weight=1):\n    self._data[coord] = weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    self._fill(coord, weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    if self._bins is None:\n        raise ValueError(\"No histogram bin information available.\")\n    if coord >= self._bins or coord < 0:\n        raise ValueError(\"Invalid coordinate.\")\n    if coord < 0:\n        coord += self._bins\n    self._data[coord] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    if self._bins == 1:\n        return\n\n    if coord >= self._bins or coord < 0:\n        raise IndexError('coord {} outside histogram edges {}'.format(coord, self.edges))\n\n    for i in range(self._bins):\n        self._data[i][coord + i] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    if coord >= self.size:\n        raise IndexError('coord outside histogram')\n    self.data[coord] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    self._update(self._buf, coord, weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    if not self._bin_edges:\n        raise ValueError(\"bin edges are not set\")\n    if coord in self:\n        raise ValueError(\"coordinate \" + str(coord) + \" already set\")\n\n    edges = self._bin_edges[1:] - self._bin_edges[:-1]\n    self._data[coord] = weight * np.diff(edges) / np.diff(self._bin_edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    self._fill(self._find(coord), weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    self._data[coord] = weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n    if self._min <= coord < self._max:\n        self._hist[coord] += weight\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_validate_labels", "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]", "ques_id": "62b86aa3b4d922cb0e688d36", "code": ["def _validate_labels(labels):\n    for label_key, label_values in labels.items():\n        for label_value in label_values:\n            _validate_label(label_key, label_value)\n", "def _validate_labels(labels):\n    for key, regexps in labels.items():\n        for regexp in regexps:\n            if not re.match(regexp['regexp'], key):\n                raise ValidationError(\n                    \"Label key '{}' does not match regexp '{}'\".format(key, regexp['regexp'])\n                )\n", "def _validate_labels(labels):\n    for key, regex in labels.items():\n        messages = []\n        for err in labels.values():\n            try:\n                match = re.match(regex, err.messages[0][\"['invalid']\"])\n            except TypeError:\n                match = re.match(regex, err.messages[0])\n            if match:\n                messages.append(err.messages[0])\n        if not messages:\n            continue\n        error_msg = \"{} has an invalid regex in labels: {}\".format(key, ','.join(messages))\n        raise ValidationError(error_msg)\n", "def _validate_labels(labels):\n    for key, expected_error in labels.items():\n        actual_error = _validate_label(key, expected_error)\n        if actual_error is not None:\n            raise ValidationError(actual_error)\n", "def _validate_labels(labels):\n    for label_name, label_key in labels.items():\n        if label_name in _LABELS_SKIPPED:\n            continue\n        for expected_error in _LABEL_ERRORS[label_name]:\n            match_regex = expected_error[\"regex\"]\n            match_key = expected_error[\"key\"]\n            match_value = expected_error[\"value\"]\n            if not re.match(match_regex, match_key):\n                raise ValidationError(\"{} key '{}' does not match regex {}\".format(label_name, match_key, match_regex))\n            if not re.match(match_regex, match_value):\n                raise ValidationError(\"{} value '{}' does not match regex {}\".format(label_name, match_value, match_regex))\n", "def _validate_labels(labels):\n    for key, pattern in labels.items():\n        try:\n            m = re.match(pattern, labels[key])\n        except (TypeError, KeyError):\n            raise ValidationError(\n                \"Label key '{}' does not match regex {}\".format(key, pattern)\n            )\n\n        if not m:\n            raise ValidationError(\n                \"Label key '{}' does not match regex {}\".format(key, pattern)\n            )\n", "def _validate_labels(labels):\n    for key, expected in labels.items():\n        regex = get_regex_for_key(key)\n        actual = re.search(regex, expected)\n        if not actual:\n            raise ValidationError(\n                \"Label '{}' does not match regex '{}'\".format(\n                    key, regex))\n        errors = actual.groupdict()\n        if len(errors):\n            errors = '\\n'.join(\"{}: {}\".format(k, v) for k, v in errors.items())\n            raise ValidationError(errors)\n", "def _validate_labels(labels):\n    for key, regex in labels.items():\n        try:\n            assert re.search(regex, labels[key])\n        except AssertionError as err:\n            raise ValidationError(\n                \"'{}' key does not match the regex: {}\".format(key, err)\n            ) from err\n", "def _validate_labels(labels):\n    for key in labels:\n        if not isinstance(key, string_types):\n            raise ValidationError('labels[\"%s\"] keys must be strings.' % key)\n        for value in labels[key]:\n            m = re.match(value, key)\n            if not m:\n                raise ValidationError('labels[\"%s\"] values must be strings.' % key)\n", "def _validate_labels(labels):\n    for key, values in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"Keys in labels must be strings.\")\n\n        if any(not isinstance(v, str) for v in values):\n            raise ValueError(\"Values in labels must be strings.\")\n\n        for err in _validate_label_regexps(values):\n            err.validate_label(key, labels)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _validate_labels(labels):\n    for label_key, label_values in labels.items():\n        for label_value in label_values:\n            _validate_label(label_key, label_value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, regexps in labels.items():\n        for regexp in regexps:\n            if not re.match(regexp['regexp'], key):\n                raise ValidationError(\n                    \"Label key '{}' does not match regexp '{}'\".format(key, regexp['regexp'])\n                )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, regex in labels.items():\n        messages = []\n        for err in labels.values():\n            try:\n                match = re.match(regex, err.messages[0][\"['invalid']\"])\n            except TypeError:\n                match = re.match(regex, err.messages[0])\n            if match:\n                messages.append(err.messages[0])\n        if not messages:\n            continue\n        error_msg = \"{} has an invalid regex in labels: {}\".format(key, ','.join(messages))\n        raise ValidationError(error_msg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, expected_error in labels.items():\n        actual_error = _validate_label(key, expected_error)\n        if actual_error is not None:\n            raise ValidationError(actual_error)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for label_name, label_key in labels.items():\n        if label_name in _LABELS_SKIPPED:\n            continue\n        for expected_error in _LABEL_ERRORS[label_name]:\n            match_regex = expected_error[\"regex\"]\n            match_key = expected_error[\"key\"]\n            match_value = expected_error[\"value\"]\n            if not re.match(match_regex, match_key):\n                raise ValidationError(\"{} key '{}' does not match regex {}\".format(label_name, match_key, match_regex))\n            if not re.match(match_regex, match_value):\n                raise ValidationError(\"{} value '{}' does not match regex {}\".format(label_name, match_value, match_regex))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, pattern in labels.items():\n        try:\n            m = re.match(pattern, labels[key])\n        except (TypeError, KeyError):\n            raise ValidationError(\n                \"Label key '{}' does not match regex {}\".format(key, pattern)\n            )\n\n        if not m:\n            raise ValidationError(\n                \"Label key '{}' does not match regex {}\".format(key, pattern)\n            )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, expected in labels.items():\n        regex = get_regex_for_key(key)\n        actual = re.search(regex, expected)\n        if not actual:\n            raise ValidationError(\n                \"Label '{}' does not match regex '{}'\".format(\n                    key, regex))\n        errors = actual.groupdict()\n        if len(errors):\n            errors = '\\n'.join(\"{}: {}\".format(k, v) for k, v in errors.items())\n            raise ValidationError(errors)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, regex in labels.items():\n        try:\n            assert re.search(regex, labels[key])\n        except AssertionError as err:\n            raise ValidationError(\n                \"'{}' key does not match the regex: {}\".format(key, err)\n            ) from err\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key in labels:\n        if not isinstance(key, string_types):\n            raise ValidationError('labels[\"%s\"] keys must be strings.' % key)\n        for value in labels[key]:\n            m = re.match(value, key)\n            if not m:\n                raise ValidationError('labels[\"%s\"] values must be strings.' % key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n    for key, values in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"Keys in labels must be strings.\")\n\n        if any(not isinstance(v, str) for v in values):\n            raise ValueError(\"Values in labels must be strings.\")\n\n        for err in _validate_label_regexps(values):\n            err.validate_label(key, labels)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_get_resource_name_regex", "docstring": "Build or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.", "ques_id": "62b86a9eb4d922cb0e688d25", "code": ["def _get_resource_name_regex():\n    resource_name_regex = re.compile(r\"(?P<resource_name>.+?)\\.(?P<version>.+?)\\.(?P<type>.+?)$\")\n    resource_name_regex = resource_name_regex.replace(\"\\\\\", \"\\\\\\\\\")\n    resource_name_regex = resource_name_regex.replace(\"/\", \"\\\\/\")\n    resource_name_regex = resource_name_regex.replace(\"\\\"\", \"\\\\\\\"\")\n    resource_name_regex = re.compile(r\"(?P<name>.+?)\\.(?P<version>.+?)\\.(?P<type>.+?)$\")\n    resource_name_regex = resource_name_regex.replace(\"\\\\\", \"\\\\\\\\\")\n    resource_name_regex = resource_name_regex.replace(\"/\", \"\\\\/\")\n    resource_name_regex = resource_name_regex.replace(\"\\\"\", \"\\\\\\\"\")\n    return resource_name_regex\n", "def _get_resource_name_regex():\n    global _KRAKE_NAME_REGEX\n    if not _KRAKE_NAME_REGEX:\n        _KRAKE_NAME_REGEX = re.compile(r'^[a-z0-9\\-_]+[a-z0-9\\-_]+$')\n    return _KRAKE_NAME_REGEX\n", "def _get_resource_name_regex():\n    global _NAME_REGEX_CACHE\n    if _NAME_REGEX_CACHE is None:\n        _NAME_REGEX_CACHE = {}\n        for pattern in KrakeResourceNamePatterns:\n            _NAME_REGEX_CACHE[pattern.pattern] = re.compile(pattern.pattern)\n\n    return _NAME_REGEX_CACHE[_get_name_resource_name()]\n", "def _get_resource_name_regex():\n    if not Krake.NAME_REGEX:\n        Krake.NAME_REGEX = re.compile(r'[a-zA-Z0-9\\-]+')\n    return Krake.NAME_REGEX\n", "def _get_resource_name_regex():\n    resource_name_pattern = re.compile(r\"^[a-zA-Z0-9_]{1,}$\")\n    return resource_name_pattern\n", "def _get_resource_name_regex():\n    pattern = re.compile(r\"^(?P<api_id>\\w+)/(?P<api_name>\\w+)$\")\n    return pattern\n", "def _get_resource_name_regex():\n    return _get_regex(Krake.KR_NAME_PATTERN)\n", "def _get_resource_name_regex():\n    return _get_regex(Krake.NAME_REGEX)\n", "def _get_resource_name_regex():\n    return get_regex(Krake.NAME_REGEX)\n", "def _get_resource_name_regex():\n    global Krake_NAME_REGEX\n    if Krake_NAME_REGEX is None:\n        Krake_NAME_REGEX = re.compile(r\"^\" + Krake.RESOURCE_NAME_PATTERN + r\"$\")\n    return Krake_NAME_REGEX\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _get_resource_name_regex():\n    resource_name_regex = re.compile(r\"(?P<resource_name>.+?)\\.(?P<version>.+?)\\.(?P<type>.+?)$\")\n    resource_name_regex = resource_name_regex.replace(\"\\\\\", \"\\\\\\\\\")\n    resource_name_regex = resource_name_regex.replace(\"/\", \"\\\\/\")\n    resource_name_regex = resource_name_regex.replace(\"\\\"\", \"\\\\\\\"\")\n    resource_name_regex = re.compile(r\"(?P<name>.+?)\\.(?P<version>.+?)\\.(?P<type>.+?)$\")\n    resource_name_regex = resource_name_regex.replace(\"\\\\\", \"\\\\\\\\\")\n    resource_name_regex = resource_name_regex.replace(\"/\", \"\\\\/\")\n    resource_name_regex = resource_name_regex.replace(\"\\\"\", \"\\\\\\\"\")\n    return resource_name_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    global _KRAKE_NAME_REGEX\n    if not _KRAKE_NAME_REGEX:\n        _KRAKE_NAME_REGEX = re.compile(r'^[a-z0-9\\-_]+[a-z0-9\\-_]+$')\n    return _KRAKE_NAME_REGEX\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    global _NAME_REGEX_CACHE\n    if _NAME_REGEX_CACHE is None:\n        _NAME_REGEX_CACHE = {}\n        for pattern in KrakeResourceNamePatterns:\n            _NAME_REGEX_CACHE[pattern.pattern] = re.compile(pattern.pattern)\n\n    return _NAME_REGEX_CACHE[_get_name_resource_name()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    if not Krake.NAME_REGEX:\n        Krake.NAME_REGEX = re.compile(r'[a-zA-Z0-9\\-]+')\n    return Krake.NAME_REGEX\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    resource_name_pattern = re.compile(r\"^[a-zA-Z0-9_]{1,}$\")\n    return resource_name_pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    pattern = re.compile(r\"^(?P<api_id>\\w+)/(?P<api_name>\\w+)$\")\n    return pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_regex(Krake.KR_NAME_PATTERN)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return _get_regex(Krake.NAME_REGEX)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    return get_regex(Krake.NAME_REGEX)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n    global Krake_NAME_REGEX\n    if Krake_NAME_REGEX is None:\n        Krake_NAME_REGEX = re.compile(r\"^\" + Krake.RESOURCE_NAME_PATTERN + r\"$\")\n    return Krake_NAME_REGEX\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_value", "docstring": "Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.", "ques_id": "62b86a4fb4d922cb0e688cf8", "code": ["def validate_value(value):\n    pattern = re.compile(get_regex_string())\n\n    if pattern.match(value) is None:\n        raise ValidationError('Not a valid regular expression')\n", "def validate_value(value):\n    regex = re.compile(value)\n    if not regex.search(value):\n        raise ValidationError('Value \"{value}\" does not match the regular expression \"{regex}\"'.format(\n            value=value, regex=regex.pattern))\n", "def validate_value(value):\n    try:\n        re.compile(value)\n    except re.error as e:\n        raise ValidationError(\"Invalid regular expression: \" + str(e))\n", "def validate_value(value):\n    pattern = re.compile(value)\n    if pattern.match(value) is None:\n        raise ValidationError('Value \\'{}\\' is not a valid regular expression'.format(value))\n", "def validate_value(value):\n    pattern = re.compile(get_value_re())\n\n    if pattern.match(value) is None:\n        raise ValidationError('Invalid value for field {}: {}'.format(\n            get_name(), value))\n", "def validate_value(value):\n    regex = _get_regex()\n    if not regex.search(value):\n        raise ValidationError('{0} is not a valid value for regex'.format(value))\n", "def validate_value(value):\n    pattern = re.compile(value)\n    if pattern.match(value) is None:\n        raise ValidationError('Invalid value')\n", "def validate_value(value):\n    if value is None:\n        return\n\n    if not constants.ALL_VALUES.match(value):\n        raise ValidationError('Invalid value \"%s\". Valid values are '\n                              '%s' % (value, constants.ALL_VALUES.pattern))\n", "def validate_value(value):\n    match = _expression.match(value)\n    if match is None:\n        raise ValidationError('Invalid value given')\n", "def validate_value(value):\n    regex = _get_regex()\n    if not regex:\n        raise ValidationError('The regular expression is not configured.')\n    if not regex.match(value):\n        raise ValidationError('The value \"{}\" is not a valid regular expression'.format(value))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_value(value):\n    pattern = re.compile(get_regex_string())\n\n    if pattern.match(value) is None:\n        raise ValidationError('Not a valid regular expression')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    regex = re.compile(value)\n    if not regex.search(value):\n        raise ValidationError('Value \"{value}\" does not match the regular expression \"{regex}\"'.format(\n            value=value, regex=regex.pattern))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    try:\n        re.compile(value)\n    except re.error as e:\n        raise ValidationError(\"Invalid regular expression: \" + str(e))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    pattern = re.compile(value)\n    if pattern.match(value) is None:\n        raise ValidationError('Value \\'{}\\' is not a valid regular expression'.format(value))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    pattern = re.compile(get_value_re())\n\n    if pattern.match(value) is None:\n        raise ValidationError('Invalid value for field {}: {}'.format(\n            get_name(), value))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    regex = _get_regex()\n    if not regex.search(value):\n        raise ValidationError('{0} is not a valid value for regex'.format(value))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    pattern = re.compile(value)\n    if pattern.match(value) is None:\n        raise ValidationError('Invalid value')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    if value is None:\n        return\n\n    if not constants.ALL_VALUES.match(value):\n        raise ValidationError('Invalid value \"%s\". Valid values are '\n                              '%s' % (value, constants.ALL_VALUES.pattern))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    match = _expression.match(value)\n    if match is None:\n        raise ValidationError('Invalid value given')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n    regex = _get_regex()\n    if not regex:\n        raise ValidationError('The regular expression is not configured.')\n    if not regex.match(value):\n        raise ValidationError('The value \"{}\" is not a valid regular expression'.format(value))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_key", "docstring": "Validate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.", "ques_id": "62b86a4fb4d922cb0e688cf7", "code": ["def validate_key(key):\n    regex = _KEY_REGEX_PATTERN\n    match = re.match(regex, key)\n    if not match:\n        raise ValidationError('Invalid key: \"{}\"'.format(key))\n", "def validate_key(key):\n    key = key.replace('[', '').replace(']', '').replace(',', '')\n    key = key.replace('/', '.')\n    regex = _KEY_REGEX_PATTERN\n    if not re.match(regex, key):\n        raise ValidationError('Key \"{}\" is not conform to the regex: {}'.format(key, regex))\n", "def validate_key(key):\n    if not KEY_RE.match(key):\n        raise ValidationError('Invalid key: \"{}\"'.format(key))\n", "def validate_key(key):\n    key = re.sub(r'\\s+','', key)\n    regex = re.compile(r'^[a-zA-Z0-9\\-_]+$')\n    if not regex.match(key):\n        raise ValidationError('Invalid key: {}'.format(key))\n", "def validate_key(key):\n    regex = re.compile(REGEX_STR)\n    if not regex.match(key):\n        raise ValidationError('The given key is not conform to the regular expression.')\n", "def validate_key(key):\n    regex = KEY_REGEX_PATTERN\n    match = re.match(regex, key)\n    if not match:\n        raise ValidationError('Invalid key: {}'.format(key))\n", "def validate_key(key):\n    regex = get_regex(key)\n    if not regex:\n        raise ValidationError('Key \"{}\" is not conforming to the regex {}'.format(key, regex))\n", "def validate_key(key):\n    match = re.match(REGEX_PATTERN, key)\n    if not match:\n        raise ValidationError('Invalid key: {}'.format(key))\n", "def validate_key(key):\n    regex = _key_regex()\n    if not regex.match(key):\n        raise ValidationError(\n            'Regex does not match the key \"{}\"!'.format(key))\n", "def validate_key(key):\n    if not KEY_RE.match(key):\n        raise ValidationError('Invalid key')\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_key(key):\n    regex = _KEY_REGEX_PATTERN\n    match = re.match(regex, key)\n    if not match:\n        raise ValidationError('Invalid key: \"{}\"'.format(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    key = key.replace('[', '').replace(']', '').replace(',', '')\n    key = key.replace('/', '.')\n    regex = _KEY_REGEX_PATTERN\n    if not re.match(regex, key):\n        raise ValidationError('Key \"{}\" is not conform to the regex: {}'.format(key, regex))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    if not KEY_RE.match(key):\n        raise ValidationError('Invalid key: \"{}\"'.format(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    key = re.sub(r'\\s+','', key)\n    regex = re.compile(r'^[a-zA-Z0-9\\-_]+$')\n    if not regex.match(key):\n        raise ValidationError('Invalid key: {}'.format(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    regex = re.compile(REGEX_STR)\n    if not regex.match(key):\n        raise ValidationError('The given key is not conform to the regular expression.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    regex = KEY_REGEX_PATTERN\n    match = re.match(regex, key)\n    if not match:\n        raise ValidationError('Invalid key: {}'.format(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    regex = get_regex(key)\n    if not regex:\n        raise ValidationError('Key \"{}\" is not conforming to the regex {}'.format(key, regex))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    match = re.match(REGEX_PATTERN, key)\n    if not match:\n        raise ValidationError('Invalid key: {}'.format(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    regex = _key_regex()\n    if not regex.match(key):\n        raise ValidationError(\n            'Regex does not match the key \"{}\"!'.format(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n    if not KEY_RE.match(key):\n        raise ValidationError('Invalid key')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema_dict", "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.", "ques_id": "62b86a01b4d922cb0e688ccc", "code": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n\n    new_manifest_dict = copy.deepcopy(manifest_dict)\n    for key, value in new_manifest_dict.items():\n        if type(value) is dict:\n            new_manifest_dict[key] = generate_default_observer_schema_dict(\n                value, first_level=False)\n        elif type(value) is list:\n            new_manifest_dict[key] = [\n                generate_default_observer_schema_dict(item, first_level=False)\n                for item in value\n            ]\n        elif type(value) is str:\n            new_manifest_dict[key] = None\n    return new_manifest_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    # type: (dict, bool) -> dict\n    \"\"\"\n    Recursively generates part of a default ``observer_schema`` from part of a Kubernetes\n    resource, defined below.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resource first_level (bool, optional):\n            If True, indicates that the dictionary represents the whole observer schema of a\n            Kubernetes resource\n\n    Returns:\n        dict:\n            Generated partial observer_schema\n\n    \"\"\"\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n\n    return {\n        'name': manifest_dict.get('name', None),\n        'type': manifest_dict.get('type', None),\n       'metadata': copy.deepcopy(manifest_dict.get('metadata', {})),\n       'spec': copy.deepcopy(manifest_dict.get('spec', {})),\n       'status': manifest_dict.get('status', None)\n    }\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if not first_level:\n        return copy.deepcopy(manifest_dict)\n\n    if 'observerSchema' not in manifest_dict:\n        return manifest_dict\n\n    return copy.deepcopy(manifest_dict['observerSchema'])\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if manifest_dict is None:\n        return None\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n    return {k: generate_default_observer_schema_dict(v, first_level=False)\n            for k, v in manifest_dict.items()}\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    new_dict = manifest_dict.copy()\n    for k, v in new_dict.items():\n        if isinstance(v, dict):\n            new_dict[k] = generate_default_observer_schema_dict(v, first_level=first_level)\n        elif not isinstance(v, list):\n            new_dict[k] = None\n    return new_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if manifest_dict is None:\n        return None\n\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n\n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            ret = generate_default_observer_schema_dict(value, first_level=False)\n            if ret:\n                manifest_dict[key] = ret\n        elif isinstance(value, list):\n            for i, val in enumerate(value):\n                if i == 0:\n                    manifest_dict[key].append(val)\n                else:\n                    manifest_dict[key][i] = None\n        else:\n            manifest_dict[key] = None\n\n    return manifest_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n    else:\n        return copy.deepcopy(manifest_dict.get('observer_schema', {}))\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n    else:\n        return manifest_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        # If the manifest contains a full observer schema then we can use it to\n        # generate the default observer schema\n        manifest_dict = manifest_dict.copy()\n        manifest_dict.pop('observer_schema', None)\n        return manifest_dict\n    else:\n        # Otherwise we must be recursive and return the whole schema\n        return generate_default_observer_schema_list(manifest_dict)\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n\n    return {\n        k: generate_default_observer_schema_dict(v, False)\n        for k, v in manifest_dict.items()\n    }\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n\n    new_manifest_dict = copy.deepcopy(manifest_dict)\n    for key, value in new_manifest_dict.items():\n        if type(value) is dict:\n            new_manifest_dict[key] = generate_default_observer_schema_dict(\n                value, first_level=False)\n        elif type(value) is list:\n            new_manifest_dict[key] = [\n                generate_default_observer_schema_dict(item, first_level=False)\n                for item in value\n            ]\n        elif type(value) is str:\n            new_manifest_dict[key] = None\n    return new_manifest_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    # type: (dict, bool) -> dict\n    \"\"\"\n    Recursively generates part of a default ``observer_schema`` from part of a Kubernetes\n    resource, defined below.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resource first_level (bool, optional):\n            If True, indicates that the dictionary represents the whole observer schema of a\n            Kubernetes resource\n\n    Returns:\n        dict:\n            Generated partial observer_schema\n\n    \"\"\"\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n\n    return {\n        'name': manifest_dict.get('name', None),\n        'type': manifest_dict.get('type', None),\n       'metadata': copy.deepcopy(manifest_dict.get('metadata', {})),\n       'spec': copy.deepcopy(manifest_dict.get('spec', {})),\n       'status': manifest_dict.get('status', None)\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if not first_level:\n        return copy.deepcopy(manifest_dict)\n\n    if 'observerSchema' not in manifest_dict:\n        return manifest_dict\n\n    return copy.deepcopy(manifest_dict['observerSchema'])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if manifest_dict is None:\n        return None\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n    return {k: generate_default_observer_schema_dict(v, first_level=False)\n            for k, v in manifest_dict.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    new_dict = manifest_dict.copy()\n    for k, v in new_dict.items():\n        if isinstance(v, dict):\n            new_dict[k] = generate_default_observer_schema_dict(v, first_level=first_level)\n        elif not isinstance(v, list):\n            new_dict[k] = None\n    return new_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if manifest_dict is None:\n        return None\n\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n\n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            ret = generate_default_observer_schema_dict(value, first_level=False)\n            if ret:\n                manifest_dict[key] = ret\n        elif isinstance(value, list):\n            for i, val in enumerate(value):\n                if i == 0:\n                    manifest_dict[key].append(val)\n                else:\n                    manifest_dict[key][i] = None\n        else:\n            manifest_dict[key] = None\n\n    return manifest_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n    else:\n        return copy.deepcopy(manifest_dict.get('observer_schema', {}))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n    else:\n        return manifest_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        # If the manifest contains a full observer schema then we can use it to\n        # generate the default observer schema\n        manifest_dict = manifest_dict.copy()\n        manifest_dict.pop('observer_schema', None)\n        return manifest_dict\n    else:\n        # Otherwise we must be recursive and return the whole schema\n        return generate_default_observer_schema_list(manifest_dict)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    if first_level:\n        return copy.deepcopy(manifest_dict)\n\n    return {\n        k: generate_default_observer_schema_dict(v, False)\n        for k, v in manifest_dict.items()\n    }\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_list_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "ques_id": "62b869ebb4d922cb0e688cc6", "code": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recommendations for :func:``update_last_applied_manifest_dict_from_resp``\n\n    This function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubes API. This function go\n            through all observed fields, and initialized their value in last_applied_manifest\n            if they are not yet present in last_applied_manifest\n    \"\"\"\n    for obs in response:\n        if isinstance(obs, dict):\n            if 'last_applied_manifest' in obs:\n                obs['last_applied_manifest'] = update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest=obs['last_ap", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurrespects :func:``update_last_applied_manifest_dict_from_resp``,\n    this function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` being response from the Kubernetes API.\n        response (list): partial response from the Kubernetes API. This function go through all\n            observed fields, and initialized their value in last_applied_manifest\n            if they are not yet present in the Kubernetes API\n    \"\"\"\n    for observed_field in observer_schema:\n        if observed_field in response:\n            last_applied_manifest.append(\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest=last_ap", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Recursively update a partial ``last_applied_manifest`` from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` being updated\n        response (list): partial response from the Kubernetes API. This function\n        go through all observed fields, and initialized their value in last_applied_manifest\n        if they are not yet present\n    \"\"\"\n    for k, v in response.items():\n        if k not in last_applied_manifest:\n            last_applied_manifest[k] = v\n        elif isinstance(v, dict) and v.get(\"$ref\"):\n            last_applied_manifest[k] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[k], observer_schema, v\n            )\n    return last_applied_manifest\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recursively update the last_applied_manifest from a partial ``last_applied_manifest``\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` response\n        response (list): partial response from the Kubernetes API. This function\n            go through all observed fields, and initialized their value in last_applied_manifest\n            if they are not yet present\n    \"\"\"\n    for field in last_applied_manifest:\n        if field not in observer_schema:\n            last_applied_manifest[field] = response[field]\n        elif isinstance(observer_schema[field], list):\n            last_applied_manifest[field] = update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[field], observer_schema[field], response[field]\n            )\n      ", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recommendations are not supported for Kubernetes API\n    \"\"\"\n    # Get the last applied manifest from the response\n    last_applied_manifest_dict = parse_last_applied_manifest(\n        last_applied_manifest,\n        observer_schema,\n    )\n\n    # Update last_applied_manifest from the response\n    response[-1]['last_applied_manifest'] = last_applied_manifest_dict\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recommendations for :func:``update_last_applied_manifest_dict_from_resp``, this function is called recursively to update\n    a partial ``last_applied_manifest`` from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` being response\n        response (list): partial response from the Kubernetes API. This function go through all observed fields, and initialized\n            their value in last_applied_manifest it they are not yet present in\n    \"\"\"\n    for manifest in response:\n        if manifest[\"apiVersion\"]!= \"v1\":\n            continue\n\n        manifest[\"lastAppliedManifest\"] = update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest, observer_schema, manifest[\"name\"]\n        )\n\n    return response\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Recursively update a partial ``last_applied_manifest`` from a partial\n    Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema`` being updated\n        response (list): partial response from the Kubernetes API. This\n            function go through all observed fields, and initialized their\n            value in last_applied_manifest if they are not yet present\n    \"\"\"\n    for field in response:\n        if field not in last_applied_manifest:\n            last_applied_manifest.append(field)\n            continue\n        last_applied_manifest[-1][field] = update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest[-1], observer_schema, response[field]\n        )\n    return last_applied_manifest\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recursively calls to update the last_applied_manifest from a partial\n    Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` being response from the Kubernetes API\n        response (list): partial response from the Kubernetes API. This function go through\n        all observed fields, and initialized their value in last_applied_manifest if they are\n        not yet present in last_applied_manifest\n    \"\"\"\n    for field in observer_schema:\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = {}\n        update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest[field], observer_schema[field], response[field]\n        )\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Recursively update a partial ``last_applied_manifest`` from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API. This function go through\n            all observed fields, and initialized their value in last_applied_manifest\n            if they are not yet present\n    \"\"\"\n    for obs in response:\n        if obs.get(\"metadata\", {}).get(\"name\") in last_applied_manifest:\n            obs[\"metadata\"][\"name\"] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest, observer_schema, obs\n            )\n    return last_applied_manifest\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Recursively update a partial ``last_applied_manifest`` from a partial\n    Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API. This\n            function go through all observed fields, and initialized their\n            value in last_applied_manifest if they are not yet present\n    \"\"\"\n    for manifest in response:\n        if manifest.get(\"last_applied_manifest\") is None:\n            last_applied_manifest.append(manifest)\n        else:\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest=manifest.get(\"last_applied_manifest\"),\n                observer_schema=observer_schema,\n           "], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recommendations for :func:``update_last_applied_manifest_dict_from_resp``\n\n    This function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubes API. This function go\n            through all observed fields, and initialized their value in last_applied_manifest\n            if they are not yet present in last_applied_manifest\n    \"\"\"\n    for obs in response:\n        if isinstance(obs, dict):\n            if 'last_applied_manifest' in obs:\n                obs['last_applied_manifest'] = update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest=obs['last_ap", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurrespects :func:``update_last_applied_manifest_dict_from_resp``,\n    this function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` being response from the Kubernetes API.\n        response (list): partial response from the Kubernetes API. This function go through all\n            observed fields, and initialized their value in last_applied_manifest\n            if they are not yet present in the Kubernetes API\n    \"\"\"\n    for observed_field in observer_schema:\n        if observed_field in response:\n            last_applied_manifest.append(\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest=last_ap", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Recursively update a partial ``last_applied_manifest`` from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` being updated\n        response (list): partial response from the Kubernetes API. This function\n        go through all observed fields, and initialized their value in last_applied_manifest\n        if they are not yet present\n    \"\"\"\n    for k, v in response.items():\n        if k not in last_applied_manifest:\n            last_applied_manifest[k] = v\n        elif isinstance(v, dict) and v.get(\"$ref\"):\n            last_applied_manifest[k] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[k], observer_schema, v\n            )\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recursively update the last_applied_manifest from a partial ``last_applied_manifest``\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` response\n        response (list): partial response from the Kubernetes API. This function\n            go through all observed fields, and initialized their value in last_applied_manifest\n            if they are not yet present\n    \"\"\"\n    for field in last_applied_manifest:\n        if field not in observer_schema:\n            last_applied_manifest[field] = response[field]\n        elif isinstance(observer_schema[field], list):\n            last_applied_manifest[field] = update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[field], observer_schema[field], response[field]\n            )\n      ", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recommendations are not supported for Kubernetes API\n    \"\"\"\n    # Get the last applied manifest from the response\n    last_applied_manifest_dict = parse_last_applied_manifest(\n        last_applied_manifest,\n        observer_schema,\n    )\n\n    # Update last_applied_manifest from the response\n    response[-1]['last_applied_manifest'] = last_applied_manifest_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recommendations for :func:``update_last_applied_manifest_dict_from_resp``, this function is called recursively to update\n    a partial ``last_applied_manifest`` from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` being response\n        response (list): partial response from the Kubernetes API. This function go through all observed fields, and initialized\n            their value in last_applied_manifest it they are not yet present in\n    \"\"\"\n    for manifest in response:\n        if manifest[\"apiVersion\"]!= \"v1\":\n            continue\n\n        manifest[\"lastAppliedManifest\"] = update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest, observer_schema, manifest[\"name\"]\n        )\n\n    return response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Recursively update a partial ``last_applied_manifest`` from a partial\n    Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema`` being updated\n        response (list): partial response from the Kubernetes API. This\n            function go through all observed fields, and initialized their\n            value in last_applied_manifest if they are not yet present\n    \"\"\"\n    for field in response:\n        if field not in last_applied_manifest:\n            last_applied_manifest.append(field)\n            continue\n        last_applied_manifest[-1][field] = update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest[-1], observer_schema, response[field]\n        )\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Reurring recursively calls to update the last_applied_manifest from a partial\n    Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema`` being response from the Kubernetes API\n        response (list): partial response from the Kubernetes API. This function go through\n        all observed fields, and initialized their value in last_applied_manifest if they are\n        not yet present in last_applied_manifest\n    \"\"\"\n    for field in observer_schema:\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = {}\n        update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest[field], observer_schema[field], response[field]\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Recursively update a partial ``last_applied_manifest`` from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API. This function go through\n            all observed fields, and initialized their value in last_applied_manifest\n            if they are not yet present\n    \"\"\"\n    for obs in response:\n        if obs.get(\"metadata\", {}).get(\"name\") in last_applied_manifest:\n            obs[\"metadata\"][\"name\"] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest, observer_schema, obs\n            )\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Recursively update a partial ``last_applied_manifest`` from a partial\n    Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API. This\n            function go through all observed fields, and initialized their\n            value in last_applied_manifest if they are not yet present\n    \"\"\"\n    for manifest in response:\n        if manifest.get(\"last_applied_manifest\") is None:\n            last_applied_manifest.append(manifest)\n        else:\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest=manifest.get(\"last_applied_manifest\"),\n                observer_schema=observer_schema,\n           ", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_dict_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "ques_id": "62b869eab4d922cb0e688cc5", "code": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = response[field]\n        else:\n            last_applied_manifest[field] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field], observer_schema, response[field]\n            )\n    return last_applied_manifest\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = response[field]\n        else:\n            last_applied_manifest[field] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field], observer_schema, response[field]\n            )\n    return last_applied_manifest\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for k, v in response.items():\n        if k in last_applied_manifest:\n            last_applied_manifest[k].update_from_resp(\n                last_applied_manifest[k], observer_schema, v\n            )\n        else:\n            last_applied_manifest[k] = v\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field[\"name\"] not in last_applied_manifest:\n            last_applied_manifest[field[\"name\"]] = field[\"default\"]\n        else:\n            last_applied_manifest[field[\"name\"]] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field[\"name\"]],\n                field,\n                response,\n            )\n    return last_applied_manifest\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field[\"name\"] not in last_applied_manifest:\n            last_applied_manifest[field[\"name\"]] = field[\"value\"]\n        elif isinstance(last_applied_manifest[field[\"name\"]], list):\n            last_applied_manifest[field[\"name\"]].append(\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[field[\"name\"]],\n                    observer_schema,\n                    response\n                )\n            )\n        else:\n            last_applied_manifest[field[\"name\"]] = last_applied_manifest[field[\"name\"]]\n    return last_applied_manifest\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for key, value in response.items():\n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = {}\n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, observer_schema\n            )\n        elif isinstance(value, list):\n            for item in value:\n                if isinstance(item, dict):\n                    update_last_applied_manifest_dict_from_resp(\n                        last_applied_manifest[key], item, observer_schema\n                    )\n                else:\n                    update_last_applied_manifest_value_from_resp(\n                        last_applied_manifest[key], item, observer_schema\n                   ", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema['fields']:\n        if field['name'] not in last_applied_manifest:\n            last_applied_manifest[field['name']] = field['value']\n        elif field['type']!= 'object':\n            last_applied_manifest[field['name']] = field['value']\n        elif field['type'] == 'array':\n            last_applied_manifest[field['name']] = [\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest, observer_schema, result\n                ) for result in field['value']\n            ]\n    return last_applied_manifest\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema:\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = response[field]\n        else:\n            last_applied_manifest[field] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field], observer_schema, response[field]\n            )\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field[\"name\"] in last_applied_manifest:\n            if field[\"type\"] == \"object\":\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[field[\"name\"]],\n                    field,\n                    response,\n                )\n            else:\n                setattr(last_applied_manifest, field[\"name\"], field[\"type\"](response[field[\"name\"]]))\n    return last_applied_manifest\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field[\"name\"] not in last_applied_manifest:\n            last_applied_manifest[field[\"name\"]] = field[\"default\"]\n        else:\n            last_applied_manifest[field[\"name\"]] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field[\"name\"]], field, response\n            )\n\n    return last_applied_manifest\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = response[field]\n        else:\n            last_applied_manifest[field] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field], observer_schema, response[field]\n            )\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = response[field]\n        else:\n            last_applied_manifest[field] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field], observer_schema, response[field]\n            )\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for k, v in response.items():\n        if k in last_applied_manifest:\n            last_applied_manifest[k].update_from_resp(\n                last_applied_manifest[k], observer_schema, v\n            )\n        else:\n            last_applied_manifest[k] = v\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field[\"name\"] not in last_applied_manifest:\n            last_applied_manifest[field[\"name\"]] = field[\"default\"]\n        else:\n            last_applied_manifest[field[\"name\"]] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field[\"name\"]],\n                field,\n                response,\n            )\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field[\"name\"] not in last_applied_manifest:\n            last_applied_manifest[field[\"name\"]] = field[\"value\"]\n        elif isinstance(last_applied_manifest[field[\"name\"]], list):\n            last_applied_manifest[field[\"name\"]].append(\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[field[\"name\"]],\n                    observer_schema,\n                    response\n                )\n            )\n        else:\n            last_applied_manifest[field[\"name\"]] = last_applied_manifest[field[\"name\"]]\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for key, value in response.items():\n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = {}\n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, observer_schema\n            )\n        elif isinstance(value, list):\n            for item in value:\n                if isinstance(item, dict):\n                    update_last_applied_manifest_dict_from_resp(\n                        last_applied_manifest[key], item, observer_schema\n                    )\n                else:\n                    update_last_applied_manifest_value_from_resp(\n                        last_applied_manifest[key], item, observer_schema\n                   ", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema['fields']:\n        if field['name'] not in last_applied_manifest:\n            last_applied_manifest[field['name']] = field['value']\n        elif field['type']!= 'object':\n            last_applied_manifest[field['name']] = field['value']\n        elif field['type'] == 'array':\n            last_applied_manifest[field['name']] = [\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest, observer_schema, result\n                ) for result in field['value']\n            ]\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema:\n        if field not in last_applied_manifest:\n            last_applied_manifest[field] = response[field]\n        else:\n            last_applied_manifest[field] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field], observer_schema, response[field]\n            )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field[\"name\"] in last_applied_manifest:\n            if field[\"type\"] == \"object\":\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[field[\"name\"]],\n                    field,\n                    response,\n                )\n            else:\n                setattr(last_applied_manifest, field[\"name\"], field[\"type\"](response[field[\"name\"]]))\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for field in observer_schema[\"fields\"]:\n        if field[\"name\"] not in last_applied_manifest:\n            last_applied_manifest[field[\"name\"]] = field[\"default\"]\n        else:\n            last_applied_manifest[field[\"name\"]] = update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[field[\"name\"]], field, response\n            )\n\n    return last_applied_manifest\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema", "docstring": "Generate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema", "ques_id": "62b869eab4d922cb0e688cbf", "code": ["def generate_default_observer_schema(app):\n    for filename in app.spec.manifest.keys():\n        if filename == 'observer':\n            continue\n        yield generate_schema_observer(app, filename)\n", "def generate_default_observer_schema(app):\n    if app.config.get('KERBEROS_OBSERVER_SCHEMA'):\n        return\n\n    # Generate the schema for the manifest\n    manifest = app.manifest\n    resources = manifest.get('resources', {})\n\n    # Get the default observer schema\n    custom_observer_schema = get_custom_observer_schema(app)\n\n    # Iterate over each resource and append the default observer schema\n    for k, v in resources.items():\n        if k not in custom_observer_schema:\n            custom_observer_schema[k] = {\n                'type':'string',\n                'format': 'path',\n                'pattern': '^/(?P<resource_path>[^/]+)$'\n            }\n\n    app.config['KERBEROS_OBSERVER_SCHEMA'] = custom_observer_schema\n", "def generate_default_observer_schema(app):\n    for k, v in app.config.items():\n        if k in KNOWN_OBSERVER_SCHEMA:\n            app.config[k] = generate_observer_schema(k, v)\n", "def generate_default_observer_schema(app):\n    spec = app.spec\n    if spec.manifest.get('observer'):\n        # If a custom observer schema hasn't been specified for the given\n        # app yet, generate it\n        custom_schema = app.get_custom_observer_schema(app.name)\n        if custom_schema is not None:\n            spec.apply_default_observer_schema(custom_schema)\n", "def generate_default_observer_schema(app):\n    app.resource_registry.add_custom_type(\n        kubernetes.types.V1ObjectMeta(\n            kind=kubernetes.types.V1Object,\n            api_version=\"v1\",\n            metadata=kubernetes.client.V1ObjectMeta(),\n            spec=kubernetes.client.V1ObjectSpec(),\n        )\n    )\n", "def generate_default_observer_schema(app):\n    app.resources[\"observer\"].metadata.create_all(app.context.k8s_client)\n", "def generate_default_observer_schema(app):\n    for filename in app.spec.manifest.get('files', []):\n        if filename.endswith('.yaml'):\n            logger.info('Generating custom observer schema for %s', filename)\n            app.api.generate_custom_observer_schema(\n                filename=filename,\n            )\n", "def generate_default_observer_schema(app):\n    for resource in app.resources:\n        if resource.custom_observer_schema:\n            resource.custom_observer_schema.generate(app)\n", "def generate_default_observer_schema(app):\n    app.add_deserializers(kubernetes.core.OBSERVER_SCHEMA)\n", "def generate_default_observer_schema(app):\n    app.bind(\"HttpObserver\", HttpObserver)\n    app.bind(\"Observer\", Observer)\n    app.bind(\"ObserverSchema\", HttpObserverSchema)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema(app):\n    for filename in app.spec.manifest.keys():\n        if filename == 'observer':\n            continue\n        yield generate_schema_observer(app, filename)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    if app.config.get('KERBEROS_OBSERVER_SCHEMA'):\n        return\n\n    # Generate the schema for the manifest\n    manifest = app.manifest\n    resources = manifest.get('resources', {})\n\n    # Get the default observer schema\n    custom_observer_schema = get_custom_observer_schema(app)\n\n    # Iterate over each resource and append the default observer schema\n    for k, v in resources.items():\n        if k not in custom_observer_schema:\n            custom_observer_schema[k] = {\n                'type':'string',\n                'format': 'path',\n                'pattern': '^/(?P<resource_path>[^/]+)$'\n            }\n\n    app.config['KERBEROS_OBSERVER_SCHEMA'] = custom_observer_schema\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    for k, v in app.config.items():\n        if k in KNOWN_OBSERVER_SCHEMA:\n            app.config[k] = generate_observer_schema(k, v)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    spec = app.spec\n    if spec.manifest.get('observer'):\n        # If a custom observer schema hasn't been specified for the given\n        # app yet, generate it\n        custom_schema = app.get_custom_observer_schema(app.name)\n        if custom_schema is not None:\n            spec.apply_default_observer_schema(custom_schema)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    app.resource_registry.add_custom_type(\n        kubernetes.types.V1ObjectMeta(\n            kind=kubernetes.types.V1Object,\n            api_version=\"v1\",\n            metadata=kubernetes.client.V1ObjectMeta(),\n            spec=kubernetes.client.V1ObjectSpec(),\n        )\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    app.resources[\"observer\"].metadata.create_all(app.context.k8s_client)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    for filename in app.spec.manifest.get('files', []):\n        if filename.endswith('.yaml'):\n            logger.info('Generating custom observer schema for %s', filename)\n            app.api.generate_custom_observer_schema(\n                filename=filename,\n            )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    for resource in app.resources:\n        if resource.custom_observer_schema:\n            resource.custom_observer_schema.generate(app)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    app.add_deserializers(kubernetes.core.OBSERVER_SCHEMA)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n    app.bind(\"HttpObserver\", HttpObserver)\n    app.bind(\"Observer\", Observer)\n    app.bind(\"ObserverSchema\", HttpObserverSchema)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "format", "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).", "ques_id": "62b43427903eeb48555d3ea5", "code": ["def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter (:class:`str`\nor :class:`int`) to value. If :attr:`.SQLParams.in_style` is a named\nparameters style. then *params* must be a :class:`~collections.abc.Mapping`. If\n:attr:`.SQLParams.in_style` is an ordinal parameter style, then *params*\nmust be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n* The formatted SQL query (:class:`str` or :class:`bytes`).\n\n* The set of converted out-style parameters (:class:`dict` or :class:`list`).\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nout_style = self.get_params(params)\nparam_map = self.get_param_map(out_style)\n\nif param_map is not None:\nparams = param_map\n\nreturn self._format_sql(sql, params, out_style)\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nparam_set = self._param_to_set(params)\nif self._in_style is None:\nreturn self._format_plain(sql, param_set)\nelse:\nreturn self._format_ordinal(sql, param_set)\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nif self.in_style is not None:\nif isinstance(self.in_style, str):\nparams = self.in_style\nelse:\nparams = self.in_style(params)\n\nreturn self.out_style.format(sql=sql, params=params)\nelse:\nreturn self.out_style.format(sql=sql)\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter (:class:`str`\nor :class:`int`) to value.\n\nIf :attr:`.SQLParams.in_style` is a named parameter style. then\n*params* must be a :class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style`\nis an named parameter style, then *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- The set of converted out-style parameters (:class:`dict` or :class:`list`).\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`) contains\nthe set of in-style parameters. It maps each parameter (:class:`str` or\n:class:`int`) to value. If :attr:`.SQLParams.in_style` is a named parameter\nstyle. then *params* must be a :class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style`\nis an ordinal parameter style, then *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- The set of converted out-style parameters (:class:`dict` or :class:`list`).\n\"\"\"\n# Use the param style if it is a named parameter style.\nif self.param_style is not None:\nparams = self.param_style.format(params)\n\n# Use the in-style param style if it i", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nif self.in_style is None:\nreturn sql, params\n\nif isinstance(self.in_style, str):\nreturn sql, params\n\nif isinstance(self.in_style, int):\nreturn sql, {self.in_style: params}\n\nraise InvalidParamsError(self.in_style)\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n# Sql is formatted as :''.join(sql) ''.\n# Each sql parameter can be separated by a newline.\nreturn self._format_params(sql, params)\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nout_style = self.params.get(self.out_style, None)\n\nif out_style is not None:\nif isinstance(out_style, dict):\nout_style = out_style.get(self.out_param, self.default_out_param)\n\nreturn self._format_sql(sql, params, out_style)\nelse:\nreturn self._format_sql(sql, params)\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nreturn self._format_with_params(sql, params)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter (:class:`str`\nor :class:`int`) to value. If :attr:`.SQLParams.in_style` is a named\nparameters style. then *params* must be a :class:`~collections.abc.Mapping`. If\n:attr:`.SQLParams.in_style` is an ordinal parameter style, then *params*\nmust be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n* The formatted SQL query (:class:`str` or :class:`bytes`).\n\n* The set of converted out-style parameters (:class:`dict` or :class:`list`).\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nout_style = self.get_params(params)\nparam_map = self.get_param_map(out_style)\n\nif param_map is not None:\nparams = param_map\n\nreturn self._format_sql(sql, params, out_style)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nparam_set = self._param_to_set(params)\nif self._in_style is None:\nreturn self._format_plain(sql, param_set)\nelse:\nreturn self._format_ordinal(sql, param_set)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nif self.in_style is not None:\nif isinstance(self.in_style, str):\nparams = self.in_style\nelse:\nparams = self.in_style(params)\n\nreturn self.out_style.format(sql=sql, params=params)\nelse:\nreturn self.out_style.format(sql=sql)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter (:class:`str`\nor :class:`int`) to value.\n\nIf :attr:`.SQLParams.in_style` is a named parameter style. then\n*params* must be a :class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style`\nis an named parameter style, then *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- The set of converted out-style parameters (:class:`dict` or :class:`list`).\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`) contains\nthe set of in-style parameters. It maps each parameter (:class:`str` or\n:class:`int`) to value. If :attr:`.SQLParams.in_style` is a named parameter\nstyle. then *params* must be a :class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style`\nis an ordinal parameter style, then *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- The set of converted out-style parameters (:class:`dict` or :class:`list`).\n\"\"\"\n# Use the param style if it is a named parameter style.\nif self.param_style is not None:\nparams = self.param_style.format(params)\n\n# Use the in-style param style if it i", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nif self.in_style is None:\nreturn sql, params\n\nif isinstance(self.in_style, str):\nreturn sql, params\n\nif isinstance(self.in_style, int):\nreturn sql, {self.in_style: params}\n\nraise InvalidParamsError(self.in_style)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n# Sql is formatted as :''.join(sql) ''.\n# Each sql parameter can be separated by a newline.\nreturn self._format_params(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nout_style = self.params.get(self.out_style, None)\n\nif out_style is not None:\nif isinstance(out_style, dict):\nout_style = out_style.get(self.out_param, self.default_out_param)\n\nreturn self._format_sql(sql, params, out_style)\nelse:\nreturn self._format_sql(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\nreturn self._format_with_params(sql, params)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "formatmany", "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).", "ques_id": "62b43428903eeb48555d3eaa", "code": ["def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\nreturn self._formatmany(sql, many_params)\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n# TODO: if sql is a named parameter style, then it should be a map, and\n#       if sql is a sequence then it should be a list.\nparams = self._params(many_params)\nreturn self.format(sql, params)\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*).\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str` or\n:class:`int`) to value.\n\nIf :attr:`.SQLParams.in_style` is a named parameter style. then\n*params* must be a :class:`dict` or :class:`list`.\n\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n*params* must be a :class:`dict`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n (:class:`dict` or :class:`list`).\n\n@rtype: :class:`tuple` containing:\n\n* The formatted SQL query (:class:", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each\nSet of in-style parameters (*params*). - *params* (:class:`~collections.abc.Mapping` or\n:class:`~collections.abc.Sequence`) contains the set of in-style parameters.\nIf :attr:`.SQLParams.in_style` is a named parameter style.\n\n:param sql: The SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each\nSet of in-style parameters (*params*). - *params* (:class:`str` or\n:class:`int`) contains the set of in-style parameters.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n(:class:`dict` or :class:`list`).\n\n@types:\n- `many_params`: ``Iterable`` container of many-parameters\n- `params", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*).\n\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str` or\n:class:`int`) to value.  If :attr:`.SQLParams.in_style` is a named\nparameter style. then *params* must be a :class:`dict`.  If :attr:`.SQLParams.in_style`\nis an ordinal parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n(:class:`dict` or :class:`list`).\n\n\"\"\"\nif self.in_style is None:\nreturn self._formatmany(sql, many_para", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*).\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str` or\n:class:`int`) to value.  If :attr:`.SQLParams.in_style` is a named\nparameter style. then *params* must be a :class:`dict` or :class:`list`.  If\n:attr:`.SQLParams.in_style` is an ordinal parameter style. then\n*params* must be a :class:`~collections.abc.Mapping`.  If :attr:`.SQLParams.in_style`\nis an ordinal parameter style. then *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list`", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each\nSet of in-style parameters (*params*).\n\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`) contains\nthe set of in-style parameters. It maps each parameter (:class:`str` or\n:class:`int`) to value.\nIf :attr:`.SQLParams.in_style` is a named parameter style. then\n*params* must be a :class:`dict` or :class:`list`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n*params* must be a :class:`~collections.abc.Mapping`.\n\nReturns a :class:`tuple` containing:\n\n* The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters (:class:`dict`\nor :class:`list`).\n\n@rtype: :class:`tuple` containing:\n\n* The formatted SQ", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*). - *params* (:class:`~collections.abc.Mapping`\nor :class:`~collections.abc.Sequence`) contains the set of in-style\nparameters.      *params* (:class:`str` or :class:`int`) contains the set\nof in-style parameters.      *params* must be a :class:`dict` or :class:`list`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n (:class:`dict` or :class:`list`).\n\n\"\"\"\nparams = self._params_from_many_params(many_params)\n\nreturn self.format(sql, params)\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each\nset of in-style parameters (*params*).\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`dict`. If\n:attr:`.SQLParams.in_style` is an ordinal parameter style. then\n*params* must be a :class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style`\nis an ordinal parameter style.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n (:class:`dict` or :class:`list`).\n\n\"\"", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*).\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str`\nor :class:`int`) to value.  If :attr:`.SQLParams.in_style` is\na named parameter style. then *params* must be a :class:`dict`.\n\nIf :attr:`.SQLParams.in_style` is an named parameter style. then\n*params* must be a :class:`dict`.\n\nReturns a :class:`tuple` containing:\n\n* The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n(:class:`dict` or :class:`list`).\n\n@types:\n* `many_params` (:class:`~collections.abc.Iterable`) contains each set\nof in-styl"], "level": "project_runnable", "generate_results": [{"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\nreturn self._formatmany(sql, many_params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n# TODO: if sql is a named parameter style, then it should be a map, and\n#       if sql is a sequence then it should be a list.\nparams = self._params(many_params)\nreturn self.format(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*).\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str` or\n:class:`int`) to value.\n\nIf :attr:`.SQLParams.in_style` is a named parameter style. then\n*params* must be a :class:`dict` or :class:`list`.\n\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n*params* must be a :class:`dict`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n (:class:`dict` or :class:`list`).\n\n@rtype: :class:`tuple` containing:\n\n* The formatted SQL query (:class:", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each\nSet of in-style parameters (*params*). - *params* (:class:`~collections.abc.Mapping` or\n:class:`~collections.abc.Sequence`) contains the set of in-style parameters.\nIf :attr:`.SQLParams.in_style` is a named parameter style.\n\n:param sql: The SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each\nSet of in-style parameters (*params*). - *params* (:class:`str` or\n:class:`int`) contains the set of in-style parameters.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n(:class:`dict` or :class:`list`).\n\n@types:\n- `many_params`: ``Iterable`` container of many-parameters\n- `params", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*).\n\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str` or\n:class:`int`) to value.  If :attr:`.SQLParams.in_style` is a named\nparameter style. then *params* must be a :class:`dict`.  If :attr:`.SQLParams.in_style`\nis an ordinal parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n(:class:`dict` or :class:`list`).\n\n\"\"\"\nif self.in_style is None:\nreturn self._formatmany(sql, many_para", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*).\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str` or\n:class:`int`) to value.  If :attr:`.SQLParams.in_style` is a named\nparameter style. then *params* must be a :class:`dict` or :class:`list`.  If\n:attr:`.SQLParams.in_style` is an ordinal parameter style. then\n*params* must be a :class:`~collections.abc.Mapping`.  If :attr:`.SQLParams.in_style`\nis an ordinal parameter style. then *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list`", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each\nSet of in-style parameters (*params*).\n\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`) contains\nthe set of in-style parameters. It maps each parameter (:class:`str` or\n:class:`int`) to value.\nIf :attr:`.SQLParams.in_style` is a named parameter style. then\n*params* must be a :class:`dict` or :class:`list`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n*params* must be a :class:`~collections.abc.Mapping`.\n\nReturns a :class:`tuple` containing:\n\n* The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters (:class:`dict`\nor :class:`list`).\n\n@rtype: :class:`tuple` containing:\n\n* The formatted SQ", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*). - *params* (:class:`~collections.abc.Mapping`\nor :class:`~collections.abc.Sequence`) contains the set of in-style\nparameters.      *params* (:class:`str` or :class:`int`) contains the set\nof in-style parameters.      *params* must be a :class:`dict` or :class:`list`.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n (:class:`dict` or :class:`list`).\n\n\"\"\"\nparams = self._params_from_many_params(many_params)\n\nreturn self.format(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each\nset of in-style parameters (*params*).\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`dict`. If\n:attr:`.SQLParams.in_style` is an ordinal parameter style. then\n*params* must be a :class:`~collections.abc.Mapping`. If :attr:`.SQLParams.in_style`\nis an ordinal parameter style.\n\nReturns a :class:`tuple` containing:\n\n- The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n (:class:`dict` or :class:`list`).\n\n\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\"\"\"\nConvert the SQL query to use the out-style parameters instead of the in-style\nparameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set of\nin-style parameters (*params*).\n- *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters.  It maps each parameter (:class:`str`\nor :class:`int`) to value.  If :attr:`.SQLParams.in_style` is\na named parameter style. then *params* must be a :class:`dict`.\n\nIf :attr:`.SQLParams.in_style` is an named parameter style. then\n*params* must be a :class:`dict`.\n\nReturns a :class:`tuple` containing:\n\n* The formatted SQL query (:class:`str` or :class:`bytes`).\n- A :class:`list` containing each set of converted out-style parameters\n(:class:`dict` or :class:`list`).\n\n@types:\n* `many_params` (:class:`~collections.abc.Iterable`) contains each set\nof in-styl", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.", "ques_id": "62b45df05108cfac7f2109ce", "code": ["def validate(self, path):\n\n    if path is None:\n        path = self.root\n\n    return super(OCFL, self).validate(path) and \\\n        os.access(path, os.W_OK | os.X_OK)\n", "def validate(self, path):\n    try:\n        self.stat()\n    except IOError:\n        return False\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            return False\n        raise\n    else:\n        return True\n", "def validate(self, path):\n    if not is_string(path):\n        return False\n    if not is_root(path):\n        return False\n    return True\n", "def validate(self, path):\n    if self.warnings:\n        if not self.fs.file_exists(path):\n            self.fs.fatal(\"'{}' does not exist or is not a file\".format(path))\n        if not self.fs.is_file(path):\n            self.fs.fatal(\"'{}' is not a file\".format(path))\n\n    return True\n", "def validate(self, path):\n    if os.path.exists(path):\n        if not self.validate_path(path):\n            return False\n    elif self.root:\n        return self.validate_path(os.path.join(self.root, path))\n    else:\n        return self.validate_pyfs(path)\n", "def validate(self, path):\n    super(OCFL, self).validate(path)\n\n    if not self.root:\n        return False\n\n    if not os.path.isdir(self.root):\n        return False\n\n    return True\n", "def validate(self, path):\n    return True\n", "def validate(self, path):\n    if not self._validate_path(path):\n        return False\n\n    return True\n", "def validate(self, path):\n    if not is_valid_path(path):\n        return False\n\n    if self.warnings:\n        for entry in self.entries:\n            if entry.path == path:\n                return True\n\n    return super(OCFLFile, self).validate(path)\n", "def validate(self, path):\n\n    return True\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, path):\n\n    if path is None:\n        path = self.root\n\n    return super(OCFL, self).validate(path) and \\\n        os.access(path, os.W_OK | os.X_OK)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    try:\n        self.stat()\n    except IOError:\n        return False\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            return False\n        raise\n    else:\n        return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    if not is_string(path):\n        return False\n    if not is_root(path):\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    if self.warnings:\n        if not self.fs.file_exists(path):\n            self.fs.fatal(\"'{}' does not exist or is not a file\".format(path))\n        if not self.fs.is_file(path):\n            self.fs.fatal(\"'{}' is not a file\".format(path))\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    if os.path.exists(path):\n        if not self.validate_path(path):\n            return False\n    elif self.root:\n        return self.validate_path(os.path.join(self.root, path))\n    else:\n        return self.validate_pyfs(path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    super(OCFL, self).validate(path)\n\n    if not self.root:\n        return False\n\n    if not os.path.isdir(self.root):\n        return False\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    if not self._validate_path(path):\n        return False\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n    if not is_valid_path(path):\n        return False\n\n    if self.warnings:\n        for entry in self.entries:\n            if entry.path == path:\n                return True\n\n    return super(OCFLFile, self).validate(path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\n    return True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validation_logger.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string of validator status, with optional prefix.", "ques_id": "62b45df15108cfac7f2109dc", "code": ["def status_str(self, prefix=''):\n    s = 'validator '\n    if self.valid:\n        s += 'on '\n    else:\n        s += 'off '\n    s += 'with '\n    if self.has_errors:\n        s += 'errors'\n    else:\n        s += 'warnings'\n    s +='and '\n    if self.has_warnings:\n        s += 'warnings'\n    else:\n        s += 'no warnings'\n    s += '.'\n    return prefix + s\n", "def status_str(self, prefix=''):\n    if self.status in self.VALIDATION_STATUSES:\n        if prefix:\n            return '{}{}'.format(prefix, self.status.upper())\n        else:\n            return self.status.upper()\n    else:\n        return ''\n", "def status_str(self, prefix=''):\n    s = 'valid' if self.valid else 'invalid'\n    if prefix:\n        s = '%s %s' % (prefix, s)\n    return s\n", "def status_str(self, prefix=''):\n    if self.valid:\n        if len(prefix) > 0:\n            prefix += '.'\n        return '{0}valid'.format(prefix)\n    else:\n        if len(prefix) > 0:\n            prefix += '.'\n        return '{0}invalid'.format(prefix)\n", "def status_str(self, prefix=''):\n    s = 'valid' if self.valid else 'invalid'\n    if prefix:\n        s = '{0} ({1})'.format(s, prefix)\n    return s\n", "def status_str(self, prefix=''):\n    if self.is_valid():\n        if prefix:\n            return '{0}valid{1}'.format(prefix, self.status)\n        else:\n            return self.status\n    else:\n        return 'invalid'\n", "def status_str(self, prefix=''):\n    if self.valid:\n        if len(prefix) > 0:\n            prefix += ': '\n        else:\n            prefix = ''\n        return '{0}valid{1}'.format(prefix, self.status)\n    else:\n        return '{0}invalid{1}'.format(prefix, self.status)\n", "def status_str(self, prefix=''):\n    if self.status not in self.VALIDATOR_STATUSES:\n        raise InvalidValidator('Invalid validator status: %s' % self.status)\n    return '%sVALIDATOR_%s' % (prefix, self.status)\n", "def status_str(self, prefix=''):\n    if self.status in ('invalid', 'invalid (invalid)'):\n        return 'Invalid'\n    if self.status in ('invalid (unrecognized)', 'invalid (not recognized)'):\n        return 'Unrecognized'\n    if self.status in ('in progress', 'completed'):\n        return 'Completed'\n    if self.status in ('cancelled', 'cancelled (cancelled)'):\n        return 'Cancelled'\n    if self.status == 'failed':\n        return 'Failed'\n    if self.status == 'error':\n        return 'Error'\n    if prefix:\n        return '{} ({})'.format(self.status, prefix)\n    return self.status\n", "def status_str(self, prefix=''):\n    if self.status not in self.VALIDATOR_STATUSES:\n        raise ValueError('invalid status: %s' % self.status)\n    s = ['validator %s: %s' % (prefix, self.status)]\n    if self.details:\n        s.append('details: %s' % self.details)\n    return''.join(s)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n    s = 'validator '\n    if self.valid:\n        s += 'on '\n    else:\n        s += 'off '\n    s += 'with '\n    if self.has_errors:\n        s += 'errors'\n    else:\n        s += 'warnings'\n    s +='and '\n    if self.has_warnings:\n        s += 'warnings'\n    else:\n        s += 'no warnings'\n    s += '.'\n    return prefix + s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.status in self.VALIDATION_STATUSES:\n        if prefix:\n            return '{}{}'.format(prefix, self.status.upper())\n        else:\n            return self.status.upper()\n    else:\n        return ''\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = 'valid' if self.valid else 'invalid'\n    if prefix:\n        s = '%s %s' % (prefix, s)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.valid:\n        if len(prefix) > 0:\n            prefix += '.'\n        return '{0}valid'.format(prefix)\n    else:\n        if len(prefix) > 0:\n            prefix += '.'\n        return '{0}invalid'.format(prefix)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = 'valid' if self.valid else 'invalid'\n    if prefix:\n        s = '{0} ({1})'.format(s, prefix)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.is_valid():\n        if prefix:\n            return '{0}valid{1}'.format(prefix, self.status)\n        else:\n            return self.status\n    else:\n        return 'invalid'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.valid:\n        if len(prefix) > 0:\n            prefix += ': '\n        else:\n            prefix = ''\n        return '{0}valid{1}'.format(prefix, self.status)\n    else:\n        return '{0}invalid{1}'.format(prefix, self.status)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.status not in self.VALIDATOR_STATUSES:\n        raise InvalidValidator('Invalid validator status: %s' % self.status)\n    return '%sVALIDATOR_%s' % (prefix, self.status)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.status in ('invalid', 'invalid (invalid)'):\n        return 'Invalid'\n    if self.status in ('invalid (unrecognized)', 'invalid (not recognized)'):\n        return 'Unrecognized'\n    if self.status in ('in progress', 'completed'):\n        return 'Completed'\n    if self.status in ('cancelled', 'cancelled (cancelled)'):\n        return 'Cancelled'\n    if self.status == 'failed':\n        return 'Failed'\n    if self.status == 'error':\n        return 'Error'\n    if prefix:\n        return '{} ({})'.format(self.status, prefix)\n    return self.status\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    if self.status not in self.VALIDATOR_STATUSES:\n        raise ValueError('invalid status: %s' % self.status)\n    s = ['validator %s: %s' % (prefix, self.status)]\n    if self.details:\n        s.append('details: %s' % self.details)\n    return''.join(s)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string representation of validation log, with optional prefix.", "ques_id": "62b45df15108cfac7f2109dd", "code": ["def status_str(self, prefix=''):\n    s = ''\n    for k, v in self.items():\n        s += '{}{}: {} '.format(prefix, k, v)\n    return s\n", "def status_str(self, prefix=''):\n    return '{}ValidationLog: {}{}'.format(\n        prefix,\n        self._status_str(self.failed),\n        self._status_str(self.passed),\n    )\n", "def status_str(self, prefix=''):\n    return '{}{}'.format(prefix, self)\n", "def status_str(self, prefix=''):\n    s = '{}Validation log:\\n'.format(prefix)\n    for k, v in self._validation_log.items():\n        s +=' {} : {}\\n'.format(k, v)\n    return s\n", "def status_str(self, prefix=''):\n    l = []\n    for k, v in self.items():\n        s = '%s%s=%s' % (prefix, k, v)\n        l.append(s)\n    return ', '.join(l)\n", "def status_str(self, prefix=''):\n    result = []\n    for key, val in self.items():\n        if isinstance(val, ValidationLog):\n            val = val.status_str(prefix + key)\n        result.append(\"{}{}={}\".format(prefix, key, val))\n    return ', '.join(result)\n", "def status_str(self, prefix=''):\n    s = '{0}Validation log for {1}:\\n'.format(prefix, self.name)\n    for k, v in self.log.items():\n        s +=' {0} = {1}\\n'.format(k, v)\n    return s\n", "def status_str(self, prefix=''):\n    s = ''\n    if self.valid:\n        s += 'Valid: %s\\n' % self.valid\n    if self.invalid:\n        s += 'Invalid: %s\\n' % self.invalid\n    if self.time:\n        s += '%s (%s)\\n' % (prefix, self.time.strftime('%H:%M:%S'))\n    return s\n", "def status_str(self, prefix=''):\n    return '{}ValidationLog: {} [{}]'.format(prefix, self.message, self.status)\n", "def status_str(self, prefix=''):\n    return ''.join((prefix, ': ', self.message))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n    s = ''\n    for k, v in self.items():\n        s += '{}{}: {} '.format(prefix, k, v)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    return '{}ValidationLog: {}{}'.format(\n        prefix,\n        self._status_str(self.failed),\n        self._status_str(self.passed),\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    return '{}{}'.format(prefix, self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = '{}Validation log:\\n'.format(prefix)\n    for k, v in self._validation_log.items():\n        s +=' {} : {}\\n'.format(k, v)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    l = []\n    for k, v in self.items():\n        s = '%s%s=%s' % (prefix, k, v)\n        l.append(s)\n    return ', '.join(l)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    result = []\n    for key, val in self.items():\n        if isinstance(val, ValidationLog):\n            val = val.status_str(prefix + key)\n        result.append(\"{}{}={}\".format(prefix, key, val))\n    return ', '.join(result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = '{0}Validation log for {1}:\\n'.format(prefix, self.name)\n    for k, v in self.log.items():\n        s +=' {0} = {1}\\n'.format(k, v)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    s = ''\n    if self.valid:\n        s += 'Valid: %s\\n' % self.valid\n    if self.invalid:\n        s += 'Invalid: %s\\n' % self.invalid\n    if self.time:\n        s += '%s (%s)\\n' % (prefix, self.time.strftime('%H:%M:%S'))\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    return '{}ValidationLog: {} [{}]'.format(prefix, self.message, self.status)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n    return ''.join((prefix, ': ', self.message))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "is_valid", "docstring": "Return True if identifier is valid, always True in this base implementation.", "ques_id": "62b45e135108cfac7f2109f4", "code": ["def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n", "def is_valid(self, identifier):\n    return True\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n    return True\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.", "ques_id": "62b45e145108cfac7f210a07", "code": ["def validate(self, inventory, extract_spec_version=False):\n    if extract_spec_version:\n        try:\n            self.spec_version = inventory.get_type().version\n        except AttributeError:\n            self.spec_version = None\n    else:\n        self.spec_version = None\n\n    self._validate_hosts(inventory)\n    self._validate_groups(inventory)\n    self._validate_children(inventory)\n    self._validate_group_hosts(inventory)\n    self._validate_children_host_types(inventory)\n\n    return inventory\n", "def validate(self, inventory, extract_spec_version=False):\n    self._required_fields_in_inventory(inventory)\n    if extract_spec_version:\n        self._extract_spec_version(inventory)\n    self._validate_inventory(inventory)\n    return inventory\n", "def validate(self, inventory, extract_spec_version=False):\n    if not isinstance(inventory, dict):\n        raise AnsibleFilterError(\"The 'inventory' parameter must be a dictionary\")\n    if 'type' not in inventory:\n        raise AnsibleFilterError(\"The 'inventory' parameter must contain a 'type' key\")\n    if inventory['type'] not in self.TYPES:\n        raise AnsibleFilterError(\"'type' key must be one of %s\" % self.TYPES)\n    if'version' in inventory:\n        if inventory['version']!= self.spec_version:\n            raise AnsibleFilterError(\"'version' key must be equal to the specification version\")\n    if extract_spec_version:\n        if'version' not in inventory:\n            raise AnsibleFilterError(\"'extract_spec_version' flag must be set to True\")\n        if inventory['version']!= self.spec_version:\n            raise AnsibleFilterError(\"'extract_spec_version' flag must be equal to the specification version\")\n", "def validate(self, inventory, extract_spec_version=False):\n    inventory = copy.deepcopy(inventory)\n    if self.extract_spec_version:\n        if not inventory.get('type'):\n            inventory['type'] = self.spec_version\n        if not inventory.get('version'):\n            inventory['version'] = self.spec_version\n\n    try:\n        validate(inventory, self.spec)\n    except ValidationError as e:\n        raise InventoryValidationError(str(e))\n", "def validate(self, inventory, extract_spec_version=False):\n    # TODO: should be able to validate a list of items\n    #       based on their specification\n    if not self._validate_as_list(inventory):\n        return False\n\n    if extract_spec_version:\n        self._validate_spec_version(inventory)\n\n    return True\n", "def validate(self, inventory, extract_spec_version=False):\n    if not isinstance(inventory, dict):\n        raise AnsibleFilterError(\"'inventory' parameter must be a dict\")\n\n    if self._required_parameters_missing(inventory):\n        raise AnsibleFilterError(\"missing required parameters for %s test\" % self.name)\n\n    if self._unrecognized_parameters_errors(inventory):\n        raise AnsibleFilterError(\"unrecognized parameters for %s test\" % self.name)\n\n    if not extract_spec_version:\n        return self._extract_spec_version(inventory)\n\n    if self._validate_spec_version(inventory):\n        return inventory\n\n    raise AnsibleFilterError(\"%s is not a valid inventory format\" % self.name)\n", "def validate(self, inventory, extract_spec_version=False):\n    if self.is_dynamic:\n        raise AnsibleFilterError('Cannot test a dynamic inventory')\n    if self._is_valid_inventory(inventory):\n        return self._validate_inventory(inventory, extract_spec_version)\n    return super(InventoryModifier, self).validate(inventory, extract_spec_version)\n", "def validate(self, inventory, extract_spec_version=False):\n    if not self.validate_inventory(inventory):\n        return False\n\n    if extract_spec_version:\n        self.spec_version = self._extract_spec_version(inventory)\n\n    return True\n", "def validate(self, inventory, extract_spec_version=False):\n    if extract_spec_version:\n        self._validate_spec_version(inventory)\n\n    self._validate_inventory_items(inventory)\n    self._validate_groups(inventory)\n    self._validate_items(inventory)\n    self._validate_hosts(inventory)\n", "def validate(self, inventory, extract_spec_version=False):\n    if not inventory:\n        return\n    if extract_spec_version:\n        self.spec_version = self._extract_specification_version(inventory)\n    else:\n        self.spec_version = self._get_specification_version(inventory)\n    if self.spec_version is None:\n        raise Exception('Cannot determine inventory specification version.')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    if extract_spec_version:\n        try:\n            self.spec_version = inventory.get_type().version\n        except AttributeError:\n            self.spec_version = None\n    else:\n        self.spec_version = None\n\n    self._validate_hosts(inventory)\n    self._validate_groups(inventory)\n    self._validate_children(inventory)\n    self._validate_group_hosts(inventory)\n    self._validate_children_host_types(inventory)\n\n    return inventory\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    self._required_fields_in_inventory(inventory)\n    if extract_spec_version:\n        self._extract_spec_version(inventory)\n    self._validate_inventory(inventory)\n    return inventory\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    if not isinstance(inventory, dict):\n        raise AnsibleFilterError(\"The 'inventory' parameter must be a dictionary\")\n    if 'type' not in inventory:\n        raise AnsibleFilterError(\"The 'inventory' parameter must contain a 'type' key\")\n    if inventory['type'] not in self.TYPES:\n        raise AnsibleFilterError(\"'type' key must be one of %s\" % self.TYPES)\n    if'version' in inventory:\n        if inventory['version']!= self.spec_version:\n            raise AnsibleFilterError(\"'version' key must be equal to the specification version\")\n    if extract_spec_version:\n        if'version' not in inventory:\n            raise AnsibleFilterError(\"'extract_spec_version' flag must be set to True\")\n        if inventory['version']!= self.spec_version:\n            raise AnsibleFilterError(\"'extract_spec_version' flag must be equal to the specification version\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    inventory = copy.deepcopy(inventory)\n    if self.extract_spec_version:\n        if not inventory.get('type'):\n            inventory['type'] = self.spec_version\n        if not inventory.get('version'):\n            inventory['version'] = self.spec_version\n\n    try:\n        validate(inventory, self.spec)\n    except ValidationError as e:\n        raise InventoryValidationError(str(e))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    # TODO: should be able to validate a list of items\n    #       based on their specification\n    if not self._validate_as_list(inventory):\n        return False\n\n    if extract_spec_version:\n        self._validate_spec_version(inventory)\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    if not isinstance(inventory, dict):\n        raise AnsibleFilterError(\"'inventory' parameter must be a dict\")\n\n    if self._required_parameters_missing(inventory):\n        raise AnsibleFilterError(\"missing required parameters for %s test\" % self.name)\n\n    if self._unrecognized_parameters_errors(inventory):\n        raise AnsibleFilterError(\"unrecognized parameters for %s test\" % self.name)\n\n    if not extract_spec_version:\n        return self._extract_spec_version(inventory)\n\n    if self._validate_spec_version(inventory):\n        return inventory\n\n    raise AnsibleFilterError(\"%s is not a valid inventory format\" % self.name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    if self.is_dynamic:\n        raise AnsibleFilterError('Cannot test a dynamic inventory')\n    if self._is_valid_inventory(inventory):\n        return self._validate_inventory(inventory, extract_spec_version)\n    return super(InventoryModifier, self).validate(inventory, extract_spec_version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    if not self.validate_inventory(inventory):\n        return False\n\n    if extract_spec_version:\n        self.spec_version = self._extract_spec_version(inventory)\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    if extract_spec_version:\n        self._validate_spec_version(inventory)\n\n    self._validate_inventory_items(inventory)\n    self._validate_groups(inventory)\n    self._validate_items(inventory)\n    self._validate_hosts(inventory)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n    if not inventory:\n        return\n    if extract_spec_version:\n        self.spec_version = self._extract_specification_version(inventory)\n    else:\n        self.spec_version = self._get_specification_version(inventory)\n    if self.spec_version is None:\n        raise Exception('Cannot determine inventory specification version.')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "check_digests_present_and_used", "docstring": "Check all digests in manifest that are needed are present and used.", "ques_id": "62b45e145108cfac7f210a09", "code": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    for filename in manifest_files:\n        digest_file = self._get_digest_file_path(filename)\n        if not os.path.isfile(digest_file):\n            continue\n\n        with open(digest_file, \"r\") as f:\n            digest_contents = f.read()\n\n        for digest in digests_used:\n            if digest not in digest_contents:\n                raise Exception(\n                    \"Digest '{}' for file '{}' not found. You must use the following digests: {}\".format(\n                        digest, filename, digests_used\n                    )\n                )\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest_file in manifest_files:\n        found = False\n        for digest in digests_used:\n            if digest.endswith(digest_file):\n                found = True\n                break\n        if not found:\n            raise CheckError('Digest \"{}\" not found in manifest files'.format(digest_file))\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest_file in manifest_files:\n        self.digest_in_manifest(digest_file, digests_used)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    return self._check_digests(manifest_files, digests_used,\n                               self._digest_type)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest in digests_used:\n        if digest not in manifest_files:\n            msg = \"Digest '{}' is not present in manifest files: {}\"\n            raise CheckFailed(msg.format(digest, manifest_files))\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for filename in manifest_files:\n        self.assertTrue(\n            self._digest_is_in_manifest(filename, digests_used),\n            \"Digest '{}' is not present in manifest file '{}'\".format(\n                filename, digests_used\n            )\n        )\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest_name, digest_contents in digests_used.items():\n        if digest_name not in manifest_files:\n            raise CheckFailed(\n                \"Digest '%s' not found in manifest files\" % digest_name)\n\n        if digest_contents is None:\n            raise CheckFailed(\"Digest '%s' not found\" % digest_name)\n\n        for digest_content in digest_contents:\n            if digest_content not in manifest_files[digest_name]:\n                raise CheckFailed(\n                    \"Digest '%s' not found in manifest file '%s'\" %\n                    (digest_content, digest_name))\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for manifest_file in manifest_files:\n        digest = self.digest_for_file(manifest_file)\n        if digest in digests_used:\n            log.info(\"Found manifest file %s in %s\", digest, manifest_file)\n            self.used_digests.add(digest)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest in digests_used:\n        if digest not in manifest_files:\n            raise DigestNotFound(\n                \"Digest '{}' is not present in manifest files\".format(digest)\n            )\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest in digests_used:\n        if digest not in manifest_files:\n            self.log.warning(\n                \"Digest '{}' not found in manifest files, \"\n                \"skipping it.\".format(digest)\n            )\n        else:\n            self.log.info(\n                \"Found digest '{}' in manifest files, \"\n                \"checking it.\".format(digest)\n            )\n            self.check_digest_for_entry(digest, manifest_files[digest])\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for filename in manifest_files:\n        digest_file = self._get_digest_file_path(filename)\n        if not os.path.isfile(digest_file):\n            continue\n\n        with open(digest_file, \"r\") as f:\n            digest_contents = f.read()\n\n        for digest in digests_used:\n            if digest not in digest_contents:\n                raise Exception(\n                    \"Digest '{}' for file '{}' not found. You must use the following digests: {}\".format(\n                        digest, filename, digests_used\n                    )\n                )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest_file in manifest_files:\n        found = False\n        for digest in digests_used:\n            if digest.endswith(digest_file):\n                found = True\n                break\n        if not found:\n            raise CheckError('Digest \"{}\" not found in manifest files'.format(digest_file))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest_file in manifest_files:\n        self.digest_in_manifest(digest_file, digests_used)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    return self._check_digests(manifest_files, digests_used,\n                               self._digest_type)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest in digests_used:\n        if digest not in manifest_files:\n            msg = \"Digest '{}' is not present in manifest files: {}\"\n            raise CheckFailed(msg.format(digest, manifest_files))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for filename in manifest_files:\n        self.assertTrue(\n            self._digest_is_in_manifest(filename, digests_used),\n            \"Digest '{}' is not present in manifest file '{}'\".format(\n                filename, digests_used\n            )\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest_name, digest_contents in digests_used.items():\n        if digest_name not in manifest_files:\n            raise CheckFailed(\n                \"Digest '%s' not found in manifest files\" % digest_name)\n\n        if digest_contents is None:\n            raise CheckFailed(\"Digest '%s' not found\" % digest_name)\n\n        for digest_content in digest_contents:\n            if digest_content not in manifest_files[digest_name]:\n                raise CheckFailed(\n                    \"Digest '%s' not found in manifest file '%s'\" %\n                    (digest_content, digest_name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for manifest_file in manifest_files:\n        digest = self.digest_for_file(manifest_file)\n        if digest in digests_used:\n            log.info(\"Found manifest file %s in %s\", digest, manifest_file)\n            self.used_digests.add(digest)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest in digests_used:\n        if digest not in manifest_files:\n            raise DigestNotFound(\n                \"Digest '{}' is not present in manifest files\".format(digest)\n            )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n    for digest in digests_used:\n        if digest not in manifest_files:\n            self.log.warning(\n                \"Digest '{}' not found in manifest files, \"\n                \"skipping it.\".format(digest)\n            )\n        else:\n            self.log.info(\n                \"Found digest '{}' in manifest files, \"\n                \"checking it.\".format(digest)\n            )\n            self.check_digest_for_entry(digest, manifest_files[digest])\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_as_prior_version", "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.", "ques_id": "62b45e165108cfac7f210a16", "code": ["def validate_as_prior_version(self, prior):\n    if self.is_variant() and prior.is_variant():\n        if prior.min > self.min:\n            raise ValidationError(\n                _(\"Cannot change the minimum priority from %(old_min)s \"\n                  \"to %(new_min)s on a variant that already has a prior \"\n                  \"version of %(old_min)s.\") %\n                {'old_min': prior.min, 'new_min': self.min},\n                code='incompatible_prior',\n                params={'prior': prior, 'old_min': self.min, 'new_min': self.min},\n            )\n", "def validate_as_prior_version(self, prior):\n    if prior is None:\n        return\n\n    if not isinstance(prior, self.__class__):\n        raise TypeError(\"prior object must be an instance of {}\".format(self.__class__.__name__))\n\n    if prior.version!= self.version:\n        raise ValueError(\n            \"invalid prior version ({}!= {}) for {}\".format(prior.version, self.version, self.name)\n        )\n", "def validate_as_prior_version(self, prior):\n\n    if prior is not None and not isinstance(prior, InventoryValidator):\n        raise TypeError(\"Invalid type for 'prior' input. Expected an instance of {}\".format(\n            InventoryValidator))\n\n    return True\n", "def validate_as_prior_version(self, prior):\n\n    if prior is None:\n        return\n\n    if not isinstance(prior, InventoryValidator):\n        raise TypeError('prior must be an instance of InventoryValidator')\n\n    if prior.get_id()!= self.get_id():\n        raise ValueError('prior inventory ID does not match inventory ID'+ str(self.get_id()) + '.')\n\n    if prior.get_version()!= self.get_version():\n        raise ValueError('prior inventory version does not match inventory version'+ str(self.get_version()) + '.')\n\n    if prior.get_inventory_type()!= self.get_inventory_type():\n        raise ValueError('prior inventory type does not match inventory type'+ str(self.get_inventory_type()) + '.')\n\n    if prior.get_quantity()!= self.get_quantity():\n        raise ValueError('prior inventory quantity does not match inventory quantity'+ str(self.get_quantity()) + '.')\n", "def validate_as_prior_version(self, prior):\n    self.validate_as_valid(prior)\n    self._validate_inventory_value(prior)\n    return True\n", "def validate_as_prior_version(self, prior):\n\n    if not isinstance(prior, InventoryValidator):\n        raise TypeError(\"{} is not an instance of {}\".format(prior, InventoryValidator))\n\n    self._validate_fields(prior.field_list)\n    return prior\n", "def validate_as_prior_version(self, prior):\n    assert prior is not None\n    assert self is not None\n\n    if prior is not self:\n        msg = \"Unexpected inventory prior version '{}'\".format(prior)\n        raise ValueError(msg)\n", "def validate_as_prior_version(self, prior):\n    if not isinstance(prior, InventoryValidator):\n        raise TypeError(\"The prior object needs to be an instance of \"\n                        \"the InventoryValidator class.\")\n    # Check that the input object has all the required fields\n    if not self.name and not prior.name:\n        raise ValueError(\"The input object needs to have a name.\")\n    if not self.version and not prior.version:\n        raise ValueError(\"The input object needs to have a version.\")\n    # Validate the input object has all the required fields\n    if not self.name and not prior.name:\n        raise ValueError(\"The input object needs to have a name.\")\n    if not self.version and not prior.version:\n        raise ValueError(\"The input object needs to have a version.\")\n    if prior.version!= self.version:\n        raise ValueError(\"The input object does not have the same version \"\n                      ", "def validate_as_prior_version(self, prior):\n    if not isinstance(prior, self.__class__):\n        msg = (\n            \"'prior' must be an instance of {}, got {}.\".format(\n                self.__class__.__name__, prior.__class__.__name__\n            )\n        )\n        raise TypeError(msg)\n    if prior.version!= self.version:\n        msg = (\n            \"'prior' input variable prior must have prior version {}, but \"\n            \"got {}.\".format(self.version, prior.version)\n        )\n        raise ValueError(msg)\n    return prior\n", "def validate_as_prior_version(self, prior):\n    if prior is None:\n        return\n\n    assert isinstance(prior, inventory.InventoryValidator)\n\n    if prior.version!= self.version:\n        raise errors.ValidationError(\n            \"Inventory prior version '{0}' is not compatible with current \"\n            \"inventory version '{1}'.\".format(prior.version, self.version)\n        )\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_as_prior_version(self, prior):\n    if self.is_variant() and prior.is_variant():\n        if prior.min > self.min:\n            raise ValidationError(\n                _(\"Cannot change the minimum priority from %(old_min)s \"\n                  \"to %(new_min)s on a variant that already has a prior \"\n                  \"version of %(old_min)s.\") %\n                {'old_min': prior.min, 'new_min': self.min},\n                code='incompatible_prior',\n                params={'prior': prior, 'old_min': self.min, 'new_min': self.min},\n            )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    if prior is None:\n        return\n\n    if not isinstance(prior, self.__class__):\n        raise TypeError(\"prior object must be an instance of {}\".format(self.__class__.__name__))\n\n    if prior.version!= self.version:\n        raise ValueError(\n            \"invalid prior version ({}!= {}) for {}\".format(prior.version, self.version, self.name)\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\n    if prior is not None and not isinstance(prior, InventoryValidator):\n        raise TypeError(\"Invalid type for 'prior' input. Expected an instance of {}\".format(\n            InventoryValidator))\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\n    if prior is None:\n        return\n\n    if not isinstance(prior, InventoryValidator):\n        raise TypeError('prior must be an instance of InventoryValidator')\n\n    if prior.get_id()!= self.get_id():\n        raise ValueError('prior inventory ID does not match inventory ID'+ str(self.get_id()) + '.')\n\n    if prior.get_version()!= self.get_version():\n        raise ValueError('prior inventory version does not match inventory version'+ str(self.get_version()) + '.')\n\n    if prior.get_inventory_type()!= self.get_inventory_type():\n        raise ValueError('prior inventory type does not match inventory type'+ str(self.get_inventory_type()) + '.')\n\n    if prior.get_quantity()!= self.get_quantity():\n        raise ValueError('prior inventory quantity does not match inventory quantity'+ str(self.get_quantity()) + '.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    self.validate_as_valid(prior)\n    self._validate_inventory_value(prior)\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\n    if not isinstance(prior, InventoryValidator):\n        raise TypeError(\"{} is not an instance of {}\".format(prior, InventoryValidator))\n\n    self._validate_fields(prior.field_list)\n    return prior\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    assert prior is not None\n    assert self is not None\n\n    if prior is not self:\n        msg = \"Unexpected inventory prior version '{}'\".format(prior)\n        raise ValueError(msg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    if not isinstance(prior, InventoryValidator):\n        raise TypeError(\"The prior object needs to be an instance of \"\n                        \"the InventoryValidator class.\")\n    # Check that the input object has all the required fields\n    if not self.name and not prior.name:\n        raise ValueError(\"The input object needs to have a name.\")\n    if not self.version and not prior.version:\n        raise ValueError(\"The input object needs to have a version.\")\n    # Validate the input object has all the required fields\n    if not self.name and not prior.name:\n        raise ValueError(\"The input object needs to have a name.\")\n    if not self.version and not prior.version:\n        raise ValueError(\"The input object needs to have a version.\")\n    if prior.version!= self.version:\n        raise ValueError(\"The input object does not have the same version \"\n                      ", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    if not isinstance(prior, self.__class__):\n        msg = (\n            \"'prior' must be an instance of {}, got {}.\".format(\n                self.__class__.__name__, prior.__class__.__name__\n            )\n        )\n        raise TypeError(msg)\n    if prior.version!= self.version:\n        msg = (\n            \"'prior' input variable prior must have prior version {}, but \"\n            \"got {}.\".format(self.version, prior.version)\n        )\n        raise ValueError(msg)\n    return prior\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n    if prior is None:\n        return\n\n    assert isinstance(prior, inventory.InventoryValidator)\n\n    if prior.version!= self.version:\n        raise errors.ValidationError(\n            \"Inventory prior version '{0}' is not compatible with current \"\n            \"inventory version '{1}'.\".format(prior.version, self.version)\n        )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "get_logical_path_map", "docstring": "Get a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.", "ques_id": "62b45e165108cfac7f210a17", "code": ["def get_logical_path_map(inventory, version):\n    result = {}\n    for key in inventory:\n        if isinstance(inventory[key], dict):\n            if version not in inventory[key]:\n                inventory[key][version] = set()\n            result[key] = inventory[key]\n        elif inventory[key]:\n            result[key] = inventory[key]\n    return result\n", "def get_logical_path_map(inventory, version):\n    inventory_dir = inventory.get_inventory_directory()\n    if not inventory_dir:\n        return {}\n\n    result = {}\n    for inventory_file in os.listdir(inventory_dir):\n        inventory_path = os.path.join(inventory_dir, inventory_file)\n        if not os.path.isfile(inventory_path):\n            continue\n\n        content_files = set()\n        with open(inventory_path) as f:\n            for line in f:\n                if not line.startswith(\"#\"):\n                    content_files.add(line.strip())\n\n        if version not in content_files:\n            continue\n\n        result[inventory_file] = content_files\n\n    return result\n", "def get_logical_path_map(inventory, version):\n    inv = Inventory(inventory)\n    return {\n        os.path.join(inv.path, f): set(c.content_files)\n        for f, c in inv.contents.items()\n        if c.state == version\n    }\n", "def get_logical_path_map(inventory, version):\n    if version not in inventory:\n        return {}\n    return inventory[version]\n", "def get_logical_path_map(inventory, version):\n    content_file_map = {}\n    for path, content_files in inventory.items():\n        for content_file in content_files:\n            if content_file in content_file_map:\n                # Duplicate files are not allowed\n                raise exceptions.InventoryError(\n                    \"Duplicate content file {} in {}\".format(\n                        content_file, path))\n            content_file_map[content_file] = set()\n            path_parts = path.split(os.path.sep)\n            path_parts.reverse()\n            while path_parts:\n                prefix = '/'.join(path_parts)\n                if prefix == version:\n                    break\n                else:\n                    content_file_map[content_file].add(os.path.join(\n                        *pat", "def get_logical_path_map(inventory, version):\n    content_files = inventory.get_content_files(version)\n    logical_path_in_state = {}\n    for content_file in content_files:\n        logical_path = content_file.logical_path\n        if logical_path not in logical_path_in_state:\n            logical_path_in_state[logical_path] = set()\n        logical_path_in_state[logical_path].add(content_file)\n    return logical_path_in_state\n", "def get_logical_path_map(inventory, version):\n    content_files = set()\n    for group in inventory.groups.values():\n        for host in group.hosts.values():\n            if host.is_inventory_file:\n                content_files.add(host.path)\n    return {\n        path: content_files\n        for path, content_files in inventory.content_files.items()\n        if path in content_files\n    }\n", "def get_logical_path_map(inventory, version):\n    return {\n        path: set(content_files)\n        for path, content_files in inventory.items()\n        if version in content_files\n    }\n", "def get_logical_path_map(inventory, version):\n    return {\n        os.path.join(inventory.inventory_dir, path): set(content_files)\n        for path, content_files in inventory.content_files.items()\n    }\n", "def get_logical_path_map(inventory, version):\n    return inventory.get_logical_path_map(version)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get_logical_path_map(inventory, version):\n    result = {}\n    for key in inventory:\n        if isinstance(inventory[key], dict):\n            if version not in inventory[key]:\n                inventory[key][version] = set()\n            result[key] = inventory[key]\n        elif inventory[key]:\n            result[key] = inventory[key]\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    inventory_dir = inventory.get_inventory_directory()\n    if not inventory_dir:\n        return {}\n\n    result = {}\n    for inventory_file in os.listdir(inventory_dir):\n        inventory_path = os.path.join(inventory_dir, inventory_file)\n        if not os.path.isfile(inventory_path):\n            continue\n\n        content_files = set()\n        with open(inventory_path) as f:\n            for line in f:\n                if not line.startswith(\"#\"):\n                    content_files.add(line.strip())\n\n        if version not in content_files:\n            continue\n\n        result[inventory_file] = content_files\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    inv = Inventory(inventory)\n    return {\n        os.path.join(inv.path, f): set(c.content_files)\n        for f, c in inv.contents.items()\n        if c.state == version\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    if version not in inventory:\n        return {}\n    return inventory[version]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    content_file_map = {}\n    for path, content_files in inventory.items():\n        for content_file in content_files:\n            if content_file in content_file_map:\n                # Duplicate files are not allowed\n                raise exceptions.InventoryError(\n                    \"Duplicate content file {} in {}\".format(\n                        content_file, path))\n            content_file_map[content_file] = set()\n            path_parts = path.split(os.path.sep)\n            path_parts.reverse()\n            while path_parts:\n                prefix = '/'.join(path_parts)\n                if prefix == version:\n                    break\n                else:\n                    content_file_map[content_file].add(os.path.join(\n                        *pat", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    content_files = inventory.get_content_files(version)\n    logical_path_in_state = {}\n    for content_file in content_files:\n        logical_path = content_file.logical_path\n        if logical_path not in logical_path_in_state:\n            logical_path_in_state[logical_path] = set()\n        logical_path_in_state[logical_path].add(content_file)\n    return logical_path_in_state\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    content_files = set()\n    for group in inventory.groups.values():\n        for host in group.hosts.values():\n            if host.is_inventory_file:\n                content_files.add(host.path)\n    return {\n        path: content_files\n        for path, content_files in inventory.content_files.items()\n        if path in content_files\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    return {\n        path: set(content_files)\n        for path, content_files in inventory.items()\n        if version in content_files\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    return {\n        os.path.join(inventory.inventory_dir, path): set(content_files)\n        for path, content_files in inventory.content_files.items()\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n    return inventory.get_logical_path_map(version)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_fixity", "docstring": "Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.", "ques_id": "62b45e175108cfac7f210a19", "code": ["def validate_fixity(self, fixity, manifest_files):\n    for file_name, content in fixity.items():\n        if file_name not in manifest_files:\n            raise FixityError(\"{}: file {} is not in manifest\".format(self.file_name, file_name))\n", "def validate_fixity(self, fixity, manifest_files):\n    for filename, contents in fixity.items():\n        if filename in manifest_files:\n            self.assertEqual(\n                contents, manifest_files[filename]['contents']\n            )\n", "def validate_fixity(self, fixity, manifest_files):\n    referenced_files = {}\n    for file_name, file_content in fixity.items():\n        if file_name not in manifest_files:\n            referenced_files[file_name] = file_content\n            continue\n        if not file_content:\n            raise exceptions.FixityError(\n                \"Referenced file {} is empty\".format(file_name)\n            )\n        if file_content!= referenced_files[file_name]:\n            raise exceptions.FixityError(\n                \"Referenced file {} is not unique\".format(file_name)\n            )\n    return referenced_files\n", "def validate_fixity(self, fixity, manifest_files):\n    inventory = fixity.get(\"inventory\", {})\n    self._validate_fixity_inventory(inventory, manifest_files)\n    self._validate_fixity_requirements(inventory, manifest_files)\n    self._validate_fixity_extras(fixity, manifest_files)\n", "def validate_fixity(self, fixity, manifest_files):\n    if 'files' in fixity:\n        referenced = set(fixity['files'])\n    else:\n        referenced = set()\n\n    missing = referenced - manifest_files\n    if missing:\n        raise FixityError(\"Missing files: {}\".format(missing))\n", "def validate_fixity(self, fixity, manifest_files):\n    def referenced(f):\n        return f in manifest_files\n\n    for filename in fixity.get('files', []):\n        if referenced(filename):\n            continue\n        log.debug(\"%s: %s\", filename, fixity.get('type', 'unknown'))\n        yield self.error(\n            \"Fixity file {} is not referenced by any of the manifest files {}\".format(\n                filename, ', '.join(manifest_files)))\n", "def validate_fixity(self, fixity, manifest_files):\n    for path, contents in fixity.items():\n        if path not in manifest_files:\n            raise FixityError(\n                \"{} in fixity block does not contain file {}\".format(\n                    path, path))\n        for file_name in contents:\n            if file_name not in manifest_files[path]:\n                raise FixityError(\n                    \"{} in fixity block does not contain file {}\".format(\n                        path, file_name))\n", "def validate_fixity(self, fixity, manifest_files):\n    referenced_files = []\n    for filename, contents in fixity.items():\n        if filename not in manifest_files:\n            referenced_files.append(filename)\n    if referenced_files:\n        self.log.error('Fixity files {} are not referenced in the manifest files'\n                       .format(referenced_files))\n        raise ValueError('Fixity files not referenced in the manifest files')\n", "def validate_fixity(self, fixity, manifest_files):\n    for key in fixity.keys():\n        if key in manifest_files:\n            assert fixity[key] == manifest_files[key]\n", "def validate_fixity(self, fixity, manifest_files):\n    for file_name in fixity.keys():\n        if file_name in manifest_files:\n            raise FixityError(\n                \"Fixity file {} referenced as fixity block {}\".format(\n                    file_name, fixity[file_name]))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    for file_name, content in fixity.items():\n        if file_name not in manifest_files:\n            raise FixityError(\"{}: file {} is not in manifest\".format(self.file_name, file_name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    for filename, contents in fixity.items():\n        if filename in manifest_files:\n            self.assertEqual(\n                contents, manifest_files[filename]['contents']\n            )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    referenced_files = {}\n    for file_name, file_content in fixity.items():\n        if file_name not in manifest_files:\n            referenced_files[file_name] = file_content\n            continue\n        if not file_content:\n            raise exceptions.FixityError(\n                \"Referenced file {} is empty\".format(file_name)\n            )\n        if file_content!= referenced_files[file_name]:\n            raise exceptions.FixityError(\n                \"Referenced file {} is not unique\".format(file_name)\n            )\n    return referenced_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    inventory = fixity.get(\"inventory\", {})\n    self._validate_fixity_inventory(inventory, manifest_files)\n    self._validate_fixity_requirements(inventory, manifest_files)\n    self._validate_fixity_extras(fixity, manifest_files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    if 'files' in fixity:\n        referenced = set(fixity['files'])\n    else:\n        referenced = set()\n\n    missing = referenced - manifest_files\n    if missing:\n        raise FixityError(\"Missing files: {}\".format(missing))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    def referenced(f):\n        return f in manifest_files\n\n    for filename in fixity.get('files', []):\n        if referenced(filename):\n            continue\n        log.debug(\"%s: %s\", filename, fixity.get('type', 'unknown'))\n        yield self.error(\n            \"Fixity file {} is not referenced by any of the manifest files {}\".format(\n                filename, ', '.join(manifest_files)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    for path, contents in fixity.items():\n        if path not in manifest_files:\n            raise FixityError(\n                \"{} in fixity block does not contain file {}\".format(\n                    path, path))\n        for file_name in contents:\n            if file_name not in manifest_files[path]:\n                raise FixityError(\n                    \"{} in fixity block does not contain file {}\".format(\n                        path, file_name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    referenced_files = []\n    for filename, contents in fixity.items():\n        if filename not in manifest_files:\n            referenced_files.append(filename)\n    if referenced_files:\n        self.log.error('Fixity files {} are not referenced in the manifest files'\n                       .format(referenced_files))\n        raise ValueError('Fixity files not referenced in the manifest files')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    for key in fixity.keys():\n        if key in manifest_files:\n            assert fixity[key] == manifest_files[key]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n    for file_name in fixity.keys():\n        if file_name in manifest_files:\n            raise FixityError(\n                \"Fixity file {} referenced as fixity block {}\".format(\n                    file_name, fixity[file_name]))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list", "docstring": "Return the files in `path`", "ques_id": "62b463153879012d19481498", "code": ["def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    return os.listdir(path)\n", "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    return [os.path.join(path, f) for f in os.listdir(path)]\n", "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    return [os.path.join(path, f) for f in os.listdir(path)]\n", "def files_list(path):\n    return [os.path.join(path, f) for f in os.listdir(path)]\n", "def files_list(path):\n    if not os.path.isdir(path):\n        return []\n    return os.listdir(path)\n", "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n    return os.listdir(path)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return os.listdir(path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [os.path.join(path, f) for f in os.listdir(path)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [os.path.join(path, f) for f in os.listdir(path)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [os.path.join(path, f) for f in os.listdir(path)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    if not os.path.isdir(path):\n        return []\n    return os.listdir(path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n    return os.listdir(path)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_group_files_by_xml_filename", "docstring": "Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package", "ques_id": "62b463153879012d1948149a", "code": ["def _group_files_by_xml_filename(source, xmls, files):\n    file_groups = defaultdict(list)\n    for file in files:\n        xml_basename = os.path.splitext(os.path.basename(file))[0]\n        for xml in xmls:\n            if xml == xml_basename:\n                file_groups[xml].append(file)\n    return {xml: file_groups[xml] for xml in file_groups}\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for file in files:\n        data[file] = _group_files_by_xml_basename(source, xmls, [file])\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    groups = {}\n    for f in files:\n        basename = os.path.splitext(os.path.basename(f))[0]\n        for xml in xmls:\n            if xml in f:\n                groups.setdefault(basename, []).append(f)\n    return groups\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for filename in files:\n        data[filename] = _group_files_by_xml_basename(source, xmls, [filename])\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for filename in files:\n        data[filename] = _group_files_by_xml_basename(source, xmls, [filename])\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml in xmls:\n        name = os.path.splitext(os.path.basename(xml))[0]\n        data[name] = _group_files_by_xml_name(source, xml, files)\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n\n    data = {}\n    for xml_file in xmls:\n        data[xml_file] = _group_files_by_xml_basename(source, xml_file, files)\n\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        data[f] = {}\n        for xml in xmls:\n            if fnmatch.fnmatch(xml, xml):\n                data[f][xml] = source.get_xml_data(f, xml)\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        data[f] = _group_files_by_xml_basename(source, xmls, [f])\n    return data\n", "def _group_files_by_xml_filename(source, xmls, files):\n    groups = defaultdict(lambda: defaultdict(dict))\n    for filename in files:\n        if not source.is_zipfile(filename):\n            xml_basename = os.path.basename(filename)\n            for xml_name, data in groups[xml_basename].items():\n                data[xml_name] = {}\n                for package, data_files in source.get_xml_packages(filename).items():\n                    data[xml_name][package] = {}\n                    for data_file in data_files:\n                        data[xml_name][package][data_file] = _extract_xml_file_data(data_file)\n            groups[xml_basename] = data\n    return groups\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    file_groups = defaultdict(list)\n    for file in files:\n        xml_basename = os.path.splitext(os.path.basename(file))[0]\n        for xml in xmls:\n            if xml == xml_basename:\n                file_groups[xml].append(file)\n    return {xml: file_groups[xml] for xml in file_groups}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for file in files:\n        data[file] = _group_files_by_xml_basename(source, xmls, [file])\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    groups = {}\n    for f in files:\n        basename = os.path.splitext(os.path.basename(f))[0]\n        for xml in xmls:\n            if xml in f:\n                groups.setdefault(basename, []).append(f)\n    return groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for filename in files:\n        data[filename] = _group_files_by_xml_basename(source, xmls, [filename])\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for filename in files:\n        data[filename] = _group_files_by_xml_basename(source, xmls, [filename])\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for xml in xmls:\n        name = os.path.splitext(os.path.basename(xml))[0]\n        data[name] = _group_files_by_xml_name(source, xml, files)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\n    data = {}\n    for xml_file in xmls:\n        data[xml_file] = _group_files_by_xml_basename(source, xml_file, files)\n\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        data[f] = {}\n        for xml in xmls:\n            if fnmatch.fnmatch(xml, xml):\n                data[f][xml] = source.get_xml_data(f, xml)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    data = {}\n    for f in files:\n        data[f] = _group_files_by_xml_basename(source, xmls, [f])\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n    groups = defaultdict(lambda: defaultdict(dict))\n    for filename in files:\n        if not source.is_zipfile(filename):\n            xml_basename = os.path.basename(filename)\n            for xml_name, data in groups[xml_basename].items():\n                data[xml_name] = {}\n                for package, data_files in source.get_xml_packages(filename).items():\n                    data[xml_name][package] = {}\n                    for data_file in data_files:\n                        data[xml_name][package][data_file] = _extract_xml_file_data(data_file)\n            groups[xml_basename] = data\n    return groups\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "match_file_by_prefix", "docstring": "Identify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package", "ques_id": "62b463153879012d1948149b", "code": ["def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix + os.sep)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    for root, _, files in os.walk(prefix):\n        for file_ in files:\n            if file_.startswith(file_path):\n                return True\n\n    return False\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix + os.sep)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    for root, _, files in os.walk(prefix):\n        for file_ in files:\n            if file_.startswith(file_path):\n                return True\n\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n    return file_path.startswith(prefix)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "select_filenames_by_prefix", "docstring": "Get files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix", "ques_id": "62b463153879012d1948149c", "code": ["def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n    filenames = []\n    for f in files:\n        if f.startswith(prefix):\n            filenames.append(f)\n    return filenames\n", "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n    files = [f for f in files if f.startswith(prefix)]\n    return files\n", "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n    return [filename for filename in files if filename.startswith(prefix)]\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    filenames = []\n    for f in files:\n        if f.startswith(prefix):\n            filenames.append(f)\n    return filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    files = [f for f in files if f.startswith(prefix)]\n    return files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [f for f in files if f.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n    return [filename for filename in files if filename.startswith(prefix)]\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_folder", "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict", "ques_id": "62b463153879012d1948149d", "code": ["def _explore_folder(folder):\n    data = {}\n    for file in os.listdir(folder):\n        filepath = os.path.join(folder, file)\n        if os.path.isfile(filepath):\n            if file.endswith('.xml'):\n                data[file[:-4]] = _get_package_xml_data(filepath)\n    return data\n", "def _explore_folder(folder):\n    data = {}\n    for file in os.listdir(folder):\n        if file.endswith(\".xml\"):\n            data.update(_explore_file(os.path.join(folder, file)))\n    return data\n", "def _explore_folder(folder):\n    pkgs = {}\n\n    for pkg in os.listdir(folder):\n        if pkg.endswith(\".xml\"):\n            pkgs[pkg] = {}\n\n            for filename in os.listdir(os.path.join(folder, pkg)):\n                if filename.endswith(\".xml\"):\n                    with open(os.path.join(folder, pkg, filename)) as f:\n                        pkgs[pkg][filename[:-4]] = f.read()\n\n    return pkgs\n", "def _explore_folder(folder):\n    data = {}\n    for filename in os.listdir(folder):\n        if filename.endswith(\".xml\"):\n            try:\n                data[filename] = _explore_file(folder, filename)\n            except Exception as e:\n                logger.error(\"Failed to load xml file %s: %s\", filename, e)\n    return data\n", "def _explore_folder(folder):\n    result = {}\n    for f in os.listdir(folder):\n        if os.path.isdir(os.path.join(folder, f)):\n            name = os.path.basename(f)\n            if name.endswith(\".xml\"):\n                data = _explore_folder(os.path.join(folder, f))\n                result[name] = data\n    return result\n", "def _explore_folder(folder):\n    for name in os.listdir(folder):\n        pkg = Package(os.path.join(folder, name))\n        if pkg.is_group:\n            data = {}\n            for data_file in pkg.data_files:\n                data_file_name = os.path.splitext(data_file)[0]\n                data_file_path = os.path.join(folder, data_file_name)\n                data = _explore_folder(data_file_path)\n                if data:\n                    pkg.data.update(data)\n            pkg.data['groups'] = pkg.data['groups'] + 1\n            yield data\n", "def _explore_folder(folder):\n    data = {}\n    for root, _, files in os.walk(folder):\n        for f in files:\n            if f.endswith(\".xml\"):\n                data[os.path.basename(os.path.splitext(f)[0])] = _read_xml(os.path.join(root, f))\n    return data\n", "def _explore_folder(folder):\n    data = OrderedDict()\n    for f in os.listdir(folder):\n        if not f.endswith('.xml'):\n            continue\n        pkg = pkgutil.get_loader(f)\n        if pkg is None:\n            continue\n        data[pkg.__name__] = pkg.__dict__\n    return data\n", "def _explore_folder(folder):\n    data = {}\n    for xml_file in os.listdir(folder):\n        xml_path = os.path.join(folder, xml_file)\n        if os.path.isfile(xml_path):\n            package_data = _get_package_data(xml_path)\n            if package_data:\n                data.update(package_data)\n    return data\n", "def _explore_folder(folder):\n    data = {}\n    for f in os.listdir(folder):\n        xml_name = os.path.basename(f).replace('.xml', '')\n        xml_path = os.path.join(folder, f)\n        data[xml_name] = _read_xml(xml_path)\n    return data\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_folder(folder):\n    data = {}\n    for file in os.listdir(folder):\n        filepath = os.path.join(folder, file)\n        if os.path.isfile(filepath):\n            if file.endswith('.xml'):\n                data[file[:-4]] = _get_package_xml_data(filepath)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    data = {}\n    for file in os.listdir(folder):\n        if file.endswith(\".xml\"):\n            data.update(_explore_file(os.path.join(folder, file)))\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    pkgs = {}\n\n    for pkg in os.listdir(folder):\n        if pkg.endswith(\".xml\"):\n            pkgs[pkg] = {}\n\n            for filename in os.listdir(os.path.join(folder, pkg)):\n                if filename.endswith(\".xml\"):\n                    with open(os.path.join(folder, pkg, filename)) as f:\n                        pkgs[pkg][filename[:-4]] = f.read()\n\n    return pkgs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    data = {}\n    for filename in os.listdir(folder):\n        if filename.endswith(\".xml\"):\n            try:\n                data[filename] = _explore_file(folder, filename)\n            except Exception as e:\n                logger.error(\"Failed to load xml file %s: %s\", filename, e)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    result = {}\n    for f in os.listdir(folder):\n        if os.path.isdir(os.path.join(folder, f)):\n            name = os.path.basename(f)\n            if name.endswith(\".xml\"):\n                data = _explore_folder(os.path.join(folder, f))\n                result[name] = data\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    for name in os.listdir(folder):\n        pkg = Package(os.path.join(folder, name))\n        if pkg.is_group:\n            data = {}\n            for data_file in pkg.data_files:\n                data_file_name = os.path.splitext(data_file)[0]\n                data_file_path = os.path.join(folder, data_file_name)\n                data = _explore_folder(data_file_path)\n                if data:\n                    pkg.data.update(data)\n            pkg.data['groups'] = pkg.data['groups'] + 1\n            yield data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    data = {}\n    for root, _, files in os.walk(folder):\n        for f in files:\n            if f.endswith(\".xml\"):\n                data[os.path.basename(os.path.splitext(f)[0])] = _read_xml(os.path.join(root, f))\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    data = OrderedDict()\n    for f in os.listdir(folder):\n        if not f.endswith('.xml'):\n            continue\n        pkg = pkgutil.get_loader(f)\n        if pkg is None:\n            continue\n        data[pkg.__name__] = pkg.__dict__\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    data = {}\n    for xml_file in os.listdir(folder):\n        xml_path = os.path.join(folder, xml_file)\n        if os.path.isfile(xml_path):\n            package_data = _get_package_data(xml_path)\n            if package_data:\n                data.update(package_data)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n    data = {}\n    for f in os.listdir(folder):\n        xml_name = os.path.basename(f).replace('.xml', '')\n        xml_path = os.path.join(folder, f)\n        data[xml_name] = _read_xml(xml_path)\n    return data\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_eval_file", "docstring": "Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict", "ques_id": "62b463153879012d1948149f", "code": ["def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n    if filename.endswith('.xml'):\n        packages = {'asset': 'asset.xml'}\n    elif filename.endswith('.zip'):\n        packages = {'rendition':'rendition.zip'}\n    else:\n        raise ValueError('File must be.xml or.zip')\n\n    return {prefix: filename, **packages}\n", "def _eval_file(prefix, file_path):\n    regex = re.compile(r'^{}\\.xml$'.format(prefix))\n    match = regex.match(file_path)\n    if not match:\n        return {}\n    else:\n        return {'packages': match.group(1)}\n", "def _eval_file(prefix, file_path):\n    file_path = os.path.join(file_path, 'packages')\n    file_name = os.path.join(file_path, prefix + '.xml')\n    with open(file_name, 'r') as f:\n        xml_tree = ElementTree.parse(f)\n        root = xml_tree.getroot()\n        type = root.tag\n        if type == 'asset':\n            package = AssetPackage(xml_tree)\n        elif type =='rendition':\n            package = RenditionPackage(xml_tree)\n        else:\n            raise ValueError('Unknown file type: {}'.format(type))\n        packages[prefix] = package\n    return packages\n", "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path).replace('.xml', '')\n    if filename in ('asset','rendition'):\n        return {'packages': '/'.join([prefix, filename])}\n", "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n\n    if filename.endswith('.xml'):\n        package_type = 'asset'\n    elif filename.endswith('.rst'):\n        package_type ='rendition'\n    else:\n        raise ValueError('File must end with.xml or.rst')\n\n    file_name = os.path.splitext(filename)[0]\n    package_name = '{}{}'.format(prefix, file_name)\n\n    file_path = os.path.join(os.path.dirname(file_path), package_name)\n\n    return {package_type: package_name, 'file_path': file_path}\n", "def _eval_file(prefix, file_path):\n    file_path = os.path.join(file_path, 'packages')\n    if not os.path.exists(file_path):\n        os.makedirs(file_path)\n    file_path = os.path.join(file_path, prefix + '.xml')\n    if os.path.exists(file_path):\n        return {\n            'package': prefix,\n            'file': file_path,\n            'endereco': os.environ['RENDICO_EMAIL']\n        }\n    else:\n        return {\n            'package': prefix,\n            'file': file_path,\n            'endereco': None\n        }\n", "def _eval_file(prefix, file_path):\n    file_name = os.path.basename(file_path)\n    if re.match(r'asset', file_name):\n        packages = _asset_package(file_name)\n    else:\n        packages = _rendition_package(file_name)\n    return {\n        'packages': packages,\n        'file_name': file_name,\n        'file_path': file_path\n    }\n", "def _eval_file(prefix, file_path):\n    eval_str = ''\n    if os.path.isfile(file_path):\n        eval_str = 'asset'\n    elif os.path.isdir(file_path):\n        eval_str ='rendition'\n\n    return {\n        'packages': {\n            'asset': eval_str,\n           'rendition': eval_str\n        }\n    }\n", "def _eval_file(prefix, file_path):\n    file_path = os.path.join(file_path, 'packages')\n    file_name = os.path.basename(file_path)\n    file_name = os.path.splitext(file_name)[0]\n    file_name = os.path.split(file_name)[0]\n    file_name = 'packages.' + file_name\n    file_path = os.path.join(file_path, file_name)\n    if not os.path.exists(file_path):\n        _create_package(file_path)\n    elif not os.path.isfile(file_path):\n        _delete_package(file_path)\n\n    result = eval(os.path.splitext(file_path)[0])\n\n    return result\n", "def _eval_file(prefix, file_path):\n    packages = {}\n    filename = os.path.join(file_path, prefix + '.xml')\n    with open(filename) as f:\n        root = ElementTree.fromstring(f.read())\n\n        for package in root.iter('package'):\n            packages[package.attrib['name']] = package.attrib['endereco']\n\n    return packages\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n    if filename.endswith('.xml'):\n        packages = {'asset': 'asset.xml'}\n    elif filename.endswith('.zip'):\n        packages = {'rendition':'rendition.zip'}\n    else:\n        raise ValueError('File must be.xml or.zip')\n\n    return {prefix: filename, **packages}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    regex = re.compile(r'^{}\\.xml$'.format(prefix))\n    match = regex.match(file_path)\n    if not match:\n        return {}\n    else:\n        return {'packages': match.group(1)}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    file_path = os.path.join(file_path, 'packages')\n    file_name = os.path.join(file_path, prefix + '.xml')\n    with open(file_name, 'r') as f:\n        xml_tree = ElementTree.parse(f)\n        root = xml_tree.getroot()\n        type = root.tag\n        if type == 'asset':\n            package = AssetPackage(xml_tree)\n        elif type =='rendition':\n            package = RenditionPackage(xml_tree)\n        else:\n            raise ValueError('Unknown file type: {}'.format(type))\n        packages[prefix] = package\n    return packages\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path).replace('.xml', '')\n    if filename in ('asset','rendition'):\n        return {'packages': '/'.join([prefix, filename])}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    filename = os.path.basename(file_path)\n\n    if filename.endswith('.xml'):\n        package_type = 'asset'\n    elif filename.endswith('.rst'):\n        package_type ='rendition'\n    else:\n        raise ValueError('File must end with.xml or.rst')\n\n    file_name = os.path.splitext(filename)[0]\n    package_name = '{}{}'.format(prefix, file_name)\n\n    file_path = os.path.join(os.path.dirname(file_path), package_name)\n\n    return {package_type: package_name, 'file_path': file_path}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    file_path = os.path.join(file_path, 'packages')\n    if not os.path.exists(file_path):\n        os.makedirs(file_path)\n    file_path = os.path.join(file_path, prefix + '.xml')\n    if os.path.exists(file_path):\n        return {\n            'package': prefix,\n            'file': file_path,\n            'endereco': os.environ['RENDICO_EMAIL']\n        }\n    else:\n        return {\n            'package': prefix,\n            'file': file_path,\n            'endereco': None\n        }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    file_name = os.path.basename(file_path)\n    if re.match(r'asset', file_name):\n        packages = _asset_package(file_name)\n    else:\n        packages = _rendition_package(file_name)\n    return {\n        'packages': packages,\n        'file_name': file_name,\n        'file_path': file_path\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    eval_str = ''\n    if os.path.isfile(file_path):\n        eval_str = 'asset'\n    elif os.path.isdir(file_path):\n        eval_str ='rendition'\n\n    return {\n        'packages': {\n            'asset': eval_str,\n           'rendition': eval_str\n        }\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    file_path = os.path.join(file_path, 'packages')\n    file_name = os.path.basename(file_path)\n    file_name = os.path.splitext(file_name)[0]\n    file_name = os.path.split(file_name)[0]\n    file_name = 'packages.' + file_name\n    file_path = os.path.join(file_path, file_name)\n    if not os.path.exists(file_path):\n        _create_package(file_path)\n    elif not os.path.isfile(file_path):\n        _delete_package(file_path)\n\n    result = eval(os.path.splitext(file_path)[0])\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n    packages = {}\n    filename = os.path.join(file_path, prefix + '.xml')\n    with open(filename) as f:\n        root = ElementTree.fromstring(f.read())\n\n        for package in root.iter('package'):\n            packages[package.attrib['name']] = package.attrib['endereco']\n\n    return packages\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_rendition", "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}", "ques_id": "62b463153879012d194814a1", "code": ["def add_rendition(self, lang, file_path):\n    self.renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    self.renditions.setdefault(lang, []).append(file_path)\n", "def add_rendition(self, lang, file_path):\n    self.renditions[lang] = self._get_rendition(file_path)\n", "def add_rendition(self, lang, file_path):\n    url = self.__get_url(\"add-rendition\", {\"lang\": lang, \"file_path\": file_path})\n    html = self.__get_html(url)\n    if not html:\n        return False\n    return True\n", "def add_rendition(self, lang, file_path):\n    if not file_path:\n        return\n\n    self.renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    if not self.renditions:\n        self.renditions = {}\n    self.renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    if self._validate_lang(lang):\n        self.renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    if not lang or lang not in self.supported_languages:\n        raise UnsupportedLanguage('Language %s is not supported.' % lang)\n\n    if self._has_errors:\n        return\n\n    if not os.path.exists(file_path):\n        raise FileNotFoundError('File \"%s\" does not exist' % file_path)\n\n    self._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    if not file_path:\n        raise Exception(\"No file path provided\")\n\n    if not isinstance(file_path, str):\n        raise Exception(\"File path must be a string\")\n\n    self.renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n    self.renditions[lang] = file_path\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_rendition(self, lang, file_path):\n    self.renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    self.renditions.setdefault(lang, []).append(file_path)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    self.renditions[lang] = self._get_rendition(file_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n    url = self.__get_url(\"add-rendition\", {\"lang\": lang, \"file_path\": file_path})\n    html = self.__get_html(url)\n    if not html:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n    if not file_path:\n        return\n\n    self.renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    if not self.renditions:\n        self.renditions = {}\n    self.renditions[lang] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n    if self._validate_lang(lang):\n        self.renditions[lang] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n    if not lang or lang not in self.supported_languages:\n        raise UnsupportedLanguage('Language %s is not supported.' % lang)\n\n    if self._has_errors:\n        return\n\n    if not os.path.exists(file_path):\n        raise FileNotFoundError('File \"%s\" does not exist' % file_path)\n\n    self._renditions[lang] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n    if not file_path:\n        raise Exception(\"No file path provided\")\n\n    if not isinstance(file_path, str):\n        raise Exception(\"File path must be a string\")\n\n    self.renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n    self.renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_asset", "docstring": "\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}", "ques_id": "62b463163879012d194814a2", "code": ["def add_asset(self, basename, file_path):\n    self.assets.update({basename: file_path})\n", "def add_asset(self, basename, file_path):\n    self.assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n    self.assets.update({basename: file_path})\n", "def add_asset(self, basename, file_path):\n    if basename in self.assets:\n        raise ValueError(\"Asset \" + basename + \" already exists!\")\n    self.assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n    asset_path = self.asset_dir / basename\n    if asset_path.exists():\n        raise FileExistsError(\n            \"Asset '{}' already exists. Cannot add it again\".format(\n                asset_path.name))\n    else:\n        with open(str(asset_path), \"wb\") as f:\n            f.write(file_path)\n        self.assets[basename] = asset_path\n", "def add_asset(self, basename, file_path):\n    if basename not in self.assets:\n        self.assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n    asset = Asset(basename, self.asset_type, self._asset_storage)\n    asset.add_file(file_path)\n    self._assets[basename] = asset\n", "def add_asset(self, basename, file_path):\n    self.assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n    asset_path = self.assets_path / basename\n    asset_path.parent.mkdir(parents=True, exist_ok=True)\n\n    if file_path.endswith(\".jpg\"):\n        with open(str(asset_path), \"rb\") as f:\n            img = f.read()\n    elif file_path.endswith((\".png\", \".gif\")):\n        with open(str(asset_path), \"rb\") as f:\n            img = f.read()\n    else:\n        with open(str(asset_path), \"rb\") as f:\n            img = f.read()\n\n    with asset_path.open(\"wb\") as f:\n        f.write(img)\n", "def add_asset(self, basename, file_path):\n    asset_path = self.get_asset_path(basename)\n    self.assets.update({asset_path: file_path})\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_asset(self, basename, file_path):\n    self.assets.update({basename: file_path})\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    self.assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    self.assets.update({basename: file_path})\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    if basename in self.assets:\n        raise ValueError(\"Asset \" + basename + \" already exists!\")\n    self.assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset_path = self.asset_dir / basename\n    if asset_path.exists():\n        raise FileExistsError(\n            \"Asset '{}' already exists. Cannot add it again\".format(\n                asset_path.name))\n    else:\n        with open(str(asset_path), \"wb\") as f:\n            f.write(file_path)\n        self.assets[basename] = asset_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n    if basename not in self.assets:\n        self.assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset = Asset(basename, self.asset_type, self._asset_storage)\n    asset.add_file(file_path)\n    self._assets[basename] = asset\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n    self.assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset_path = self.assets_path / basename\n    asset_path.parent.mkdir(parents=True, exist_ok=True)\n\n    if file_path.endswith(\".jpg\"):\n        with open(str(asset_path), \"rb\") as f:\n            img = f.read()\n    elif file_path.endswith((\".png\", \".gif\")):\n        with open(str(asset_path), \"rb\") as f:\n            img = f.read()\n    else:\n        with open(str(asset_path), \"rb\") as f:\n            img = f.read()\n\n    with asset_path.open(\"wb\") as f:\n        f.write(img)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n    asset_path = self.get_asset_path(basename)\n    self.assets.update({asset_path: file_path})\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_zipfile", "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict", "ques_id": "62b463163879012d194814a4", "code": ["def _explore_zipfile(zip_path):\n    data = {}\n    for pkg in ZIP_PACKAGE_NAMES:\n        data[pkg] = _get_package_data(zip_path, pkg)\n    return data\n", "def _explore_zipfile(zip_path):\n    data = {}\n    for basename in os.listdir(zip_path):\n        if basename.endswith(\".xml\"):\n            data[basename[:-4]] = _explore_xmlfile(zip_path, basename)\n    return data\n", "def _explore_zipfile(zip_path):\n    zip_path = _os.path.basename(zip_path)\n    zip_data = {}\n    for file_ in _os.listdir(zip_path):\n        data = _explore_zipfile_data(zip_path, file_)\n        if data:\n            zip_data.update(data)\n    return zip_data\n", "def _explore_zipfile(zip_path):\n    # remove compression from zipfile path\n    zip_path = re.sub(r'\\.zip$', '', zip_path)\n    # find data in zipfile\n    data = {}\n    with zipfile.ZipFile(zip_path) as zf:\n        for fname in zf.namelist():\n            if fname.endswith('.xml'):\n                data[fname.split('/')[-1]] = zf.open(fname).read()\n    return data\n", "def _explore_zipfile(zip_path):\n    zip_path = _clean_zip_path(zip_path)\n    data = {}\n    for xml_basename, files in _get_xml_files(zip_path).items():\n        data[xml_basename] = files\n\n    return data\n", "def _explore_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        data = {}\n        for name in zf.namelist():\n            if '.xml' in name:\n                data[name] = zf.read(name)\n    return data\n", "def _explore_zipfile(zip_path):\n    if not zip_path:\n        return {}\n\n    zip_data = zipfile.ZipFile(zip_path).read(zip_path)\n    return _explore_xml(zip_data)\n", "def _explore_zipfile(zip_path):\n    zip_file = zipfile.ZipFile(zip_path)\n    for xml_file in zip_file.namelist():\n        if not xml_file.endswith('.xml'):\n            continue\n\n        # Get XML data\n        xml_data = zip_file.open(xml_file)\n        xml_data_str = xml_data.read()\n        xml_data.close()\n\n        # Parse XML data\n        xml_data_dict = _parse_xml_data(xml_data_str)\n\n        # Add to package\n        _package[xml_file[:-4]] = xml_data_dict\n\n    # Get package data\n    package_data = {}\n    for xml_file in _package.keys():\n        xml_data_dict = _package[xml_file]\n        package_data[xml_file] = xml_data_dict\n\n    return package_data\n", "def _explore_zipfile(zip_path):\n    data = {}\n    for root, dirs, files in os.walk(zip_path):\n        for file in files:\n            if '.xml' not in file:\n                continue\n            zip_file = os.path.join(root, file)\n            data[os.path.basename(zip_file)] = _read_xml(zip_file)\n    return data\n", "def _explore_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        xml_data = {}\n        for f in zf.namelist():\n            if f.endswith('.xml'):\n                xml_data[f[:-5]] = zf.open(f).read()\n\n        return xml_data\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_zipfile(zip_path):\n    data = {}\n    for pkg in ZIP_PACKAGE_NAMES:\n        data[pkg] = _get_package_data(zip_path, pkg)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    data = {}\n    for basename in os.listdir(zip_path):\n        if basename.endswith(\".xml\"):\n            data[basename[:-4]] = _explore_xmlfile(zip_path, basename)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    zip_path = _os.path.basename(zip_path)\n    zip_data = {}\n    for file_ in _os.listdir(zip_path):\n        data = _explore_zipfile_data(zip_path, file_)\n        if data:\n            zip_data.update(data)\n    return zip_data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    # remove compression from zipfile path\n    zip_path = re.sub(r'\\.zip$', '', zip_path)\n    # find data in zipfile\n    data = {}\n    with zipfile.ZipFile(zip_path) as zf:\n        for fname in zf.namelist():\n            if fname.endswith('.xml'):\n                data[fname.split('/')[-1]] = zf.open(fname).read()\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    zip_path = _clean_zip_path(zip_path)\n    data = {}\n    for xml_basename, files in _get_xml_files(zip_path).items():\n        data[xml_basename] = files\n\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        data = {}\n        for name in zf.namelist():\n            if '.xml' in name:\n                data[name] = zf.read(name)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    if not zip_path:\n        return {}\n\n    zip_data = zipfile.ZipFile(zip_path).read(zip_path)\n    return _explore_xml(zip_data)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    zip_file = zipfile.ZipFile(zip_path)\n    for xml_file in zip_file.namelist():\n        if not xml_file.endswith('.xml'):\n            continue\n\n        # Get XML data\n        xml_data = zip_file.open(xml_file)\n        xml_data_str = xml_data.read()\n        xml_data.close()\n\n        # Parse XML data\n        xml_data_dict = _parse_xml_data(xml_data_str)\n\n        # Add to package\n        _package[xml_file[:-4]] = xml_data_dict\n\n    # Get package data\n    package_data = {}\n    for xml_file in _package.keys():\n        xml_data_dict = _package[xml_file]\n        package_data[xml_file] = xml_data_dict\n\n    return package_data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    data = {}\n    for root, dirs, files in os.walk(zip_path):\n        for file in files:\n            if '.xml' not in file:\n                continue\n            zip_file = os.path.join(root, file)\n            data[os.path.basename(zip_file)] = _read_xml(zip_file)\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        xml_data = {}\n        for f in zf.namelist():\n            if f.endswith('.xml'):\n                xml_data[f[:-5]] = zf.open(f).read()\n\n        return xml_data\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list_from_zipfile", "docstring": "Return the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```", "ques_id": "62b463163879012d194814a6", "code": ["def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zip:\n        return [fname.filename for fname in zip.infolist()\n                if is_file_in_zip(fname)]\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files = zf.namelist()\n    return files\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files = zf.namelist()\n\n    return files\n", "def files_list_from_zipfile(zip_path):\n    with ZipFile(zip_path) as zf:\n        files = list(zf.namelist())\n        files.sort()\n    return files\n", "def files_list_from_zipfile(zip_path):\n    return [fname for fname in os.listdir(zip_path)\n            if is_file_in_zip(os.path.join(zip_path, fname))]\n", "def files_list_from_zipfile(zip_path):\n    return [f for f in os.listdir(zip_path) if is_file_in_zipfile(f)]\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zip:\n        return [\n            filename\n            for filename in zip.namelist()\n            if filename.endswith(\".xml\")\n        ]\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        names = zf.namelist()\n        return [f for f in names if f.endswith('.xml')]\n", "def files_list_from_zipfile(zip_path):\n    with ZipFile(zip_path, 'r') as zip_:\n        files_list = []\n        for item in zip_.namelist():\n            if item.endswith('.xml'):\n                files_list.append(zip_path + '/' + item)\n            else:\n                files_list.append(item)\n        return files_list\n", "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files = []\n        for name in zf.namelist():\n            # remove.pdf extension, if present\n            if name.endswith('.pdf'):\n                name = name[:-4]\n            files.append(name)\n        return files\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zip:\n        return [fname.filename for fname in zip.infolist()\n                if is_file_in_zip(fname)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files = zf.namelist()\n    return files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files = zf.namelist()\n\n    return files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with ZipFile(zip_path) as zf:\n        files = list(zf.namelist())\n        files.sort()\n    return files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    return [fname for fname in os.listdir(zip_path)\n            if is_file_in_zip(os.path.join(zip_path, fname))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    return [f for f in os.listdir(zip_path) if is_file_in_zipfile(f)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zip:\n        return [\n            filename\n            for filename in zip.namelist()\n            if filename.endswith(\".xml\")\n        ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        names = zf.namelist()\n        return [f for f in names if f.endswith('.xml')]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with ZipFile(zip_path, 'r') as zip_:\n        files_list = []\n        for item in zip_.namelist():\n            if item.endswith('.xml'):\n                files_list.append(zip_path + '/' + item)\n            else:\n                files_list.append(item)\n        return files_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n    with zipfile.ZipFile(zip_path) as zf:\n        files = []\n        for name in zf.namelist():\n            # remove.pdf extension, if present\n            if name.endswith('.pdf'):\n                name = name[:-4]\n            files.append(name)\n        return files\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "fix_namespace_prefix_w", "docstring": "Convert text that defaults to 'w:st=\"' to 'w-st=\"'", "ques_id": "62b4631b3879012d194814dd", "code": ["def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "def fix_namespace_prefix_w(content):\n    return re.sub(r'(?<!w:):(?!st=\")(.+?)(?=\")', r'w-st=\"\\1\"', content)\n", "def fix_namespace_prefix_w(content):\n    return re.sub(r'w:(\\w+)st=\"([^\"]+)\"', r'w-st=\"\\1\"', content)\n", "def fix_namespace_prefix_w(content):\n    return re.sub(r\"w:(.*?)st='(.*?)\" r\"-(.*?)st\", r\"\\1-\\2-w-st\", content)\n", "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "def fix_namespace_prefix_w(content):\n    return re.sub(\n        r'w:(\\w+):st=\"',\n        r'w-st=\"',\n        content\n    )\n", "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "def fix_namespace_prefix_w(content):\n    return re.sub(r'w:(\\w+)-st=\"(.*)\"', r'w-st=\"\\1\"', content)\n", "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(r'(?<!w:):(?!st=\")(.+?)(?=\")', r'w-st=\"\\1\"', content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(r'w:(\\w+)st=\"([^\"]+)\"', r'w-st=\"\\1\"', content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(r\"w:(.*?)st='(.*?)\" r\"-(.*?)st\", r\"\\1-\\2-w-st\", content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(\n        r'w:(\\w+):st=\"',\n        r'w-st=\"',\n        content\n    )\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return re.sub(r'w:(\\w+)-st=\"(.*)\"', r'w-st=\"\\1\"', content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n    return content.replace('w:st=\"', 'w-st=\"')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "match_pubdate", "docstring": "Returns the first match in the pubdate_xpaths list", "ques_id": "62b463283879012d1948153d", "code": ["def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        try:\n            return next(\n                e for e in node.xpath(pubdate_xpath) if e.text is not None\n            )\n        except StopIteration:\n            continue\n    return None\n", "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        try:\n            return etree.XPath(pubdate_xpath, namespaces=node.nsmap)[0].text\n        except IndexError:\n            pass\n    return None\n", "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        try:\n            return next(node.xpath(pubdate_xpath, namespaces=NSMAP))\n        except StopIteration:\n            pass\n    return None\n", "def match_pubdate(node, pubdate_xpaths):\n    return next(iter(\n        (match_pubdate_xpath(node, pubdate_xpath), pubdate_xpath)\n        for pubdate_xpath in pubdate_xpaths\n    ))\n", "def match_pubdate(node, pubdate_xpaths):\n    return next(iter(matching_nodes(node, pubdate_xpaths)), None)\n", "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        match = pubdate_xpath(node)\n        if match:\n            return match\n    return None\n", "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        match = pubdate_xpath.search(node)\n        if match:\n            return match\n"], "level": "self_contained", "generate_results": [{"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        try:\n            return next(\n                e for e in node.xpath(pubdate_xpath) if e.text is not None\n            )\n        except StopIteration:\n            continue\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        try:\n            return etree.XPath(pubdate_xpath, namespaces=node.nsmap)[0].text\n        except IndexError:\n            pass\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        try:\n            return next(node.xpath(pubdate_xpath, namespaces=NSMAP))\n        except StopIteration:\n            pass\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next(iter(\n        (match_pubdate_xpath(node, pubdate_xpath), pubdate_xpath)\n        for pubdate_xpath in pubdate_xpaths\n    ))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next(iter(matching_nodes(node, pubdate_xpaths)), None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    return next((x for x in pubdate_xpaths if x.match(node)), None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        match = pubdate_xpath(node)\n        if match:\n            return match\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n    for pubdate_xpath in pubdate_xpaths:\n        match = pubdate_xpath.search(node)\n        if match:\n            return match\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/front_articlemeta_issue.py", "project": "scieloorg/packtools", "name": "_extract_number_and_supplment_from_issue_element", "docstring": "Extract the possible values of number and suppl from the contents of issue.", "ques_id": "62b463303879012d19481579", "code": ["def _extract_number_and_supplment_from_issue_element(issue):\n    number = _extract_text_from_issue_element(issue, './/number')\n    if number is not None:\n        number = number.strip()\n    suppl = _extract_text_from_issue_element(issue, './/suppl')\n    if suppl is not None:\n        suppl = suppl.strip()\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = ''\n    suppl = ''\n    for el in issue:\n        if el.tag == '{%s}number' % RELATIONSHIP:\n            number = el.text\n        elif el.tag == '{%s}suppl' % RELATIONSHIP:\n            suppl = el.text\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('.//number')\n    if number is None:\n        raise ValueError('issue does not have a <number> element')\n    number_text = number.text\n    if not number_text.strip():\n        raise ValueError('issue does not contain a number')\n\n    number_value = number_text.split('/')\n    number_value = '/'.join(number_value)\n    if number_value.isdigit():\n        number_value = int(number_value)\n    else:\n        raise ValueError('issue does not contain a number in the format XXX/YYYY')\n\n    suppl = issue.find('.//suppl')\n    if suppl is None:\n        raise ValueError('issue does not contain a <suppl> element')\n    suppl_text = suppl.text\n    if not suppl_text.strip():\n        raise ValueError('issue does not contain a <suppl> element')\n\n    suppl_value = suppl_text.split('/')\n    suppl_value = '/'.join(suppl_value)\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    if issue.tag == 'Number':\n        number = issue.text\n    if issue.tag == 'Supplement':\n        suppl = issue.text\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    for element in issue:\n        if element.tag == 'number':\n            number = element.text\n        if element.tag =='suppl':\n            suppl = element.text\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    return _extract_number_and_supplment_from_element(issue.find('number')), _extract_supplement_from_element(issue)\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('Number').text\n    suppl = issue.find('Supply').text\n\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('number').text\n    if issue.find('suppl') is not None:\n        suppl = issue.find('suppl').text\n    else:\n        suppl = None\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    for element in issue:\n        if element.tag == 'number':\n            number = element.text\n        if element.tag =='suppl':\n            suppl = element.text\n    return number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n    return (\n        issue.find('number').text,\n        issue.find('suppl').text\n    )\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = _extract_text_from_issue_element(issue, './/number')\n    if number is not None:\n        number = number.strip()\n    suppl = _extract_text_from_issue_element(issue, './/suppl')\n    if suppl is not None:\n        suppl = suppl.strip()\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = ''\n    suppl = ''\n    for el in issue:\n        if el.tag == '{%s}number' % RELATIONSHIP:\n            number = el.text\n        elif el.tag == '{%s}suppl' % RELATIONSHIP:\n            suppl = el.text\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('.//number')\n    if number is None:\n        raise ValueError('issue does not have a <number> element')\n    number_text = number.text\n    if not number_text.strip():\n        raise ValueError('issue does not contain a number')\n\n    number_value = number_text.split('/')\n    number_value = '/'.join(number_value)\n    if number_value.isdigit():\n        number_value = int(number_value)\n    else:\n        raise ValueError('issue does not contain a number in the format XXX/YYYY')\n\n    suppl = issue.find('.//suppl')\n    if suppl is None:\n        raise ValueError('issue does not contain a <suppl> element')\n    suppl_text = suppl.text\n    if not suppl_text.strip():\n        raise ValueError('issue does not contain a <suppl> element')\n\n    suppl_value = suppl_text.split('/')\n    suppl_value = '/'.join(suppl_value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    if issue.tag == 'Number':\n        number = issue.text\n    if issue.tag == 'Supplement':\n        suppl = issue.text\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    for element in issue:\n        if element.tag == 'number':\n            number = element.text\n        if element.tag =='suppl':\n            suppl = element.text\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    return _extract_number_and_supplment_from_element(issue.find('number')), _extract_supplement_from_element(issue)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('Number').text\n    suppl = issue.find('Supply').text\n\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = issue.find('number').text\n    if issue.find('suppl') is not None:\n        suppl = issue.find('suppl').text\n    else:\n        suppl = None\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    number = None\n    suppl = None\n    for element in issue:\n        if element.tag == 'number':\n            number = element.text\n        if element.tag =='suppl':\n            suppl = element.text\n    return number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n    return (\n        issue.find('number').text,\n        issue.find('suppl').text\n    )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "pretty", "docstring": "Return a pretty formatted representation of self.", "ques_id": "62b46740d2f69a53b466171a", "code": ["def pretty(self, indent=0, debug=False):\n    return self.to_string(indent=indent, debug=debug)\n", "def pretty(self, indent=0, debug=False):\n    lines = []\n    indent_str = \" \" * indent\n    if debug:\n        lines.append(indent_str + \"__pretty__()\")\n    try:\n        lines.append(indent_str + \"repr(self)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__class__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__dict__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__weakref__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__dict__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__module__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__dict__.keys())\")\n    except Exception:\n        pass\n    try", "def pretty(self, indent=0, debug=False):\n    s = ''\n    if self.__full_name is not None:\n        s += self.__full_name\n        s +='' * indent + '-->'\n\n    if self.__alias is not None:\n        s += self.__alias\n        s +='' * indent + '-->'\n\n    if self.__type is not None:\n        s += self.__type\n        s +='' * indent + '-->'\n\n    if self.__init_params is not None:\n        s +='' * indent + '--->'\n        s +=''.join(self.__init_params)\n        s +='' * indent + '--->'\n\n    if self.__required is not None:\n        s +='' * indent + '--->'\n        s +=''.join(self.__required)\n        s +='' * indent + '--->'\n\n    if self.__optional is not None:\n        s +='' * indent", "def pretty(self, indent=0, debug=False):\n    s = \"\"\n    for c in self:\n        s += c.pretty(indent, debug) + \"\\n\"\n    return s\n", "def pretty(self, indent=0, debug=False):\n    indent_str ='' * indent\n    pretty_str = '{0}{1}: {2}'.format(indent_str, self.__class__.__name__, self.__str__())\n    if debug:\n        return pretty_str\n    else:\n        return yaml.dump(self.__dict__, default_flow_style=False, indent=indent)\n", "def pretty(self, indent=0, debug=False):\n    return super(Var, self).pretty(indent, debug, f'{self.name} = {self.value}')\n", "def pretty(self, indent=0, debug=False):\n    return pretty_format(self, indent=indent, debug=debug)\n", "def pretty(self, indent=0, debug=False):\n    output = io.StringIO()\n    self._pretty(output, indent, debug)\n    return output.getvalue()\n", "def pretty(self, indent=0, debug=False):\n    indent_str ='' * indent\n    s = '{}{}{}'.format(indent_str, self.__class__.__name__, self.name)\n    if debug:\n        s = '{} = {}'.format(s, self.value)\n    return s\n", "def pretty(self, indent=0, debug=False):\n    indent_str = \" \" * indent\n    string = \"{}{}\".format(indent_str, self.name)\n    if self.args:\n        string += \"({})\".format(\", \".join(map(pretty, self.args)))\n    return string\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pretty(self, indent=0, debug=False):\n    return self.to_string(indent=indent, debug=debug)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    lines = []\n    indent_str = \" \" * indent\n    if debug:\n        lines.append(indent_str + \"__pretty__()\")\n    try:\n        lines.append(indent_str + \"repr(self)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__class__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__dict__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__weakref__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__dict__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__module__)\")\n    except Exception:\n        pass\n    try:\n        lines.append(indent_str + \"repr(self.__dict__.keys())\")\n    except Exception:\n        pass\n    try", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    s = ''\n    if self.__full_name is not None:\n        s += self.__full_name\n        s +='' * indent + '-->'\n\n    if self.__alias is not None:\n        s += self.__alias\n        s +='' * indent + '-->'\n\n    if self.__type is not None:\n        s += self.__type\n        s +='' * indent + '-->'\n\n    if self.__init_params is not None:\n        s +='' * indent + '--->'\n        s +=''.join(self.__init_params)\n        s +='' * indent + '--->'\n\n    if self.__required is not None:\n        s +='' * indent + '--->'\n        s +=''.join(self.__required)\n        s +='' * indent + '--->'\n\n    if self.__optional is not None:\n        s +='' * indent", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    s = \"\"\n    for c in self:\n        s += c.pretty(indent, debug) + \"\\n\"\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    indent_str ='' * indent\n    pretty_str = '{0}{1}: {2}'.format(indent_str, self.__class__.__name__, self.__str__())\n    if debug:\n        return pretty_str\n    else:\n        return yaml.dump(self.__dict__, default_flow_style=False, indent=indent)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    return super(Var, self).pretty(indent, debug, f'{self.name} = {self.value}')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    return pretty_format(self, indent=indent, debug=debug)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    output = io.StringIO()\n    self._pretty(output, indent, debug)\n    return output.getvalue()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    indent_str ='' * indent\n    s = '{}{}{}'.format(indent_str, self.__class__.__name__, self.name)\n    if debug:\n        s = '{} = {}'.format(s, self.value)\n    return s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n    indent_str = \" \" * indent\n    string = \"{}{}\".format(indent_str, self.name)\n    if self.args:\n        string += \"({})\".format(\", \".join(map(pretty, self.args)))\n    return string\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "absorb", "docstring": "Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B", "ques_id": "62b46746d2f69a53b4661722", "code": ["def absorb(self, args):\n    return [arg for arg in args if arg!= self]\n", "def absorb(self, args):\n    if not args:\n        return args\n\n    absorbables = []\n    for arg in args:\n        if not self._predicate(arg):\n            absorbables.append(arg)\n\n    return absorbables\n", "def absorb(self, args):\n    if self.is_non_negative:\n        return [self.a & arg for arg in args]\n    else:\n        return [self.a | arg for arg in args]\n", "def absorb(self, args):\n    assert len(args) >= 1\n    for arg in args:\n        if self.is_absorption(arg):\n            args = [arg] + self.negative_absorption(arg)\n        else:\n            args = [arg]\n    return args\n", "def absorb(self, args):\n    return [arg for arg in args if arg not in self.absorptions]\n", "def absorb(self, args):\n    return [a | b for a, b in zip(args, args)]\n", "def absorb(self, args):\n    return [self._assoc(arg) for arg in args]\n", "def absorb(self, args):\n\n    if len(args) == 1:\n        return args[0]\n\n    if len(args) == 2:\n        if self.is_bit_set(self.get_sign(args[0])):\n            return self._absorb_bit(args[0], args[1])\n        return self._absorb_neg(args[0], args[1])\n\n    if len(args) == 3:\n        if self.is_bit_set(self.get_sign(args[0])):\n            return self._absorb_bit(self.get_sign(args[1]), self.get_sign(args[2]))\n        return self._absorb_neg(self.get_sign(args[0]), self.get_sign(args[1]))\n\n    raise ValueError(\"Invalid absorption expression sequence.\")\n", "def absorb(self, args):\n    if not args:\n        return []\n    # If no arguments are given, assume A as absorption.\n    if len(args) == 1:\n        return [args[0]]\n    # If two arguments are given, assume (A & B) as absorption.\n    if len(args) == 2:\n        return [self._substitute(args[0], args[1])]\n    # Otherwise, assume (~A & B) as negative absorption.\n    if not self._include:\n        return [self._substitute(~args[0], args[1])]\n    return self._substitute(args[0], ~args[1]) + self.absorb(args[2:])\n", "def absorb(self, args):\n\n    new_args = []\n\n    for arg in args:\n        if arg == self._args[-1]:\n            new_args.append(not self._args[-1])\n        else:\n            new_args.append(arg)\n\n    return self.__class__(*new_args)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def absorb(self, args):\n    return [arg for arg in args if arg!= self]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def absorb(self, args):\n    if not args:\n        return args\n\n    absorbables = []\n    for arg in args:\n        if not self._predicate(arg):\n            absorbables.append(arg)\n\n    return absorbables\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    if self.is_non_negative:\n        return [self.a & arg for arg in args]\n    else:\n        return [self.a | arg for arg in args]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    assert len(args) >= 1\n    for arg in args:\n        if self.is_absorption(arg):\n            args = [arg] + self.negative_absorption(arg)\n        else:\n            args = [arg]\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    return [arg for arg in args if arg not in self.absorptions]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    return [a | b for a, b in zip(args, args)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    return [self._assoc(arg) for arg in args]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\n    if len(args) == 1:\n        return args[0]\n\n    if len(args) == 2:\n        if self.is_bit_set(self.get_sign(args[0])):\n            return self._absorb_bit(args[0], args[1])\n        return self._absorb_neg(args[0], args[1])\n\n    if len(args) == 3:\n        if self.is_bit_set(self.get_sign(args[0])):\n            return self._absorb_bit(self.get_sign(args[1]), self.get_sign(args[2]))\n        return self._absorb_neg(self.get_sign(args[0]), self.get_sign(args[1]))\n\n    raise ValueError(\"Invalid absorption expression sequence.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n    if not args:\n        return []\n    # If no arguments are given, assume A as absorption.\n    if len(args) == 1:\n        return [args[0]]\n    # If two arguments are given, assume (A & B) as absorption.\n    if len(args) == 2:\n        return [self._substitute(args[0], args[1])]\n    # Otherwise, assume (~A & B) as negative absorption.\n    if not self._include:\n        return [self._substitute(~args[0], args[1])]\n    return self._substitute(args[0], ~args[1]) + self.absorb(args[2:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\n    new_args = []\n\n    for arg in args:\n        if arg == self._args[-1]:\n            new_args.append(not self._args[-1])\n        else:\n            new_args.append(arg)\n\n    return self.__class__(*new_args)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "on", "docstring": "Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.", "ques_id": "62b86707b4d922cb0e688c2a", "code": ["def on(self, hook):\n    def decorator(func):\n        if hook not in self._registry:\n            self._registry[hook] = []\n\n        self._registry[hook].append(func)\n        return func\n\n    return decorator\n", "def on(self, hook):\n    def register_listener(hook_type, listener):\n        if hook_type not in self._hooks:\n            self._hooks[hook_type] = {}\n\n        if listener not in self._hooks[hook_type]:\n            self._hooks[hook_type][listener] = []\n\n        self._hooks[hook_type][listener].append(hook)\n\n    return register_listener\n", "def on(self, hook):\n\n    def decorator(func):\n        if hook not in self._listeners:\n            self._listeners[hook] = []\n        self._listeners[hook].append(func)\n        return func\n\n    return decorator\n", "def on(self, hook):\n    def register_listener(listener):\n        if hook in self._registry:\n            self._registry[hook].append(listener)\n        else:\n            self._registry[hook] = [listener]\n        return listener\n\n    return register_listener\n", "def on(self, hook):\n\n    def register_listener(listener):\n        if listener not in self._listeners[hook]:\n            self._listeners[hook].append(listener)\n\n    return register_listener\n", "def on(self, hook):\n    def decorator(listener):\n        if hook not in self._registry:\n            self._registry[hook] = [listener]\n        else:\n            self._registry[hook].append(listener)\n        return listener\n    return decorator\n", "def on(self, hook):\n    def register_listener(hook_obj):\n        if hook_obj is None:\n            # If the hook is not registered, we ignore it.\n            return\n\n        if hook_obj not in self._hooks:\n            # Add the hook to the registry.\n            self._hooks[hook_obj] = [hook_obj]\n\n        # Add the hook to the listeners of the handler.\n        for listener in self._hooks[hook_obj]:\n            if listener not in self._listeners:\n                self._listeners.append(listener)\n\n        # Update the handler state.\n        hook_obj._registry = self\n        hook_obj._initialize()\n\n    return register_listener\n", "def on(self, hook):\n    def decorator(func):\n        if hook not in self._listeners:\n            self._listeners[hook] = []\n        self._listeners[hook].append(func)\n        return func\n    return decorator\n", "def on(self, hook):\n    def decorator(listener):\n        if hook not in self._listeners[hook]:\n            self._listeners[hook].append(listener)\n        return listener\n\n    return decorator\n", "def on(self, hook):\n    def register_listener(listener):\n        if hook not in self._hooks[hook]:\n            self._hooks[hook].append(listener)\n        return listener\n\n    return register_listener\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def on(self, hook):\n    def decorator(func):\n        if hook not in self._registry:\n            self._registry[hook] = []\n\n        self._registry[hook].append(func)\n        return func\n\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def register_listener(hook_type, listener):\n        if hook_type not in self._hooks:\n            self._hooks[hook_type] = {}\n\n        if listener not in self._hooks[hook_type]:\n            self._hooks[hook_type][listener] = []\n\n        self._hooks[hook_type][listener].append(hook)\n\n    return register_listener\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\n    def decorator(func):\n        if hook not in self._listeners:\n            self._listeners[hook] = []\n        self._listeners[hook].append(func)\n        return func\n\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def register_listener(listener):\n        if hook in self._registry:\n            self._registry[hook].append(listener)\n        else:\n            self._registry[hook] = [listener]\n        return listener\n\n    return register_listener\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\n    def register_listener(listener):\n        if listener not in self._listeners[hook]:\n            self._listeners[hook].append(listener)\n\n    return register_listener\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def decorator(listener):\n        if hook not in self._registry:\n            self._registry[hook] = [listener]\n        else:\n            self._registry[hook].append(listener)\n        return listener\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def register_listener(hook_obj):\n        if hook_obj is None:\n            # If the hook is not registered, we ignore it.\n            return\n\n        if hook_obj not in self._hooks:\n            # Add the hook to the registry.\n            self._hooks[hook_obj] = [hook_obj]\n\n        # Add the hook to the listeners of the handler.\n        for listener in self._hooks[hook_obj]:\n            if listener not in self._listeners:\n                self._listeners.append(listener)\n\n        # Update the handler state.\n        hook_obj._registry = self\n        hook_obj._initialize()\n\n    return register_listener\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def decorator(func):\n        if hook not in self._listeners:\n            self._listeners[hook] = []\n        self._listeners[hook].append(func)\n        return func\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def decorator(listener):\n        if hook not in self._listeners[hook]:\n            self._listeners[hook].append(listener)\n        return listener\n\n    return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n    def register_listener(listener):\n        if hook not in self._hooks[hook]:\n            self._hooks[hook].append(listener)\n        return listener\n\n    return register_listener\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/tests/conftest.py", "project": "rak-n-rok/Krake", "name": "base_config", "docstring": "Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.", "ques_id": "62b86729b4d922cb0e688c2f", "code": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"name\": \"test\",\n        \"user\": user,\n        \"password\": \"pass\",\n        \"host\": etcd_host,\n        \"port\": etcd_port,\n        \"database\": \"test-database\",\n        \"ssl\": False,\n        \"verify\": False,\n        \"certificate\": \"\",\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"password\": \"\",\n        \"key\": \"\",\n        \"database\": {\n            \"name\": \"\",\n            \"version\": \"\",\n            \"root_certificate\": \"\",\n            \"root_key\": \"\",\n            \"certificates\": {},\n            \"keys\": [],\n            \"database_password\": \"\",\n            \"status\": \"\",\n        },\n        \"schemes\": [\"http\", \"https\", \"tcp\"],\n        \"ssl\": {\n            \"ca_certs\": None,\n            \"cert_file\": None,\n            \"key_file\": None,\n            \"cert_reqs\": \"CERT_NONE\",\n            \"ca_certs_dir\": None,\n        },\n     ", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"\",\n        \"database\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        },\n        \"authentication\": {\n            \"type\": \"password\",\n            \"username\": \"\",\n            \"password\": \"\"\n        }\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"secret\",\n        \"host\": etcd_host,\n        \"port\": etcd_port,\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"secret\",\n        \"host\": etcd_host,\n        \"port\": etcd_port,\n        \"root\": \"/\",\n        \"db\": \"test\",\n        \"auth\": {\n            \"username\": \"test\",\n            \"password\": \"test\",\n            \"type\": \"password\"\n        }\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"test\",\n        \"host\": etcd_host,\n        \"port\": etcd_port,\n        \"ca_certs\": None,\n        \"client\": {\"cert\": None, \"key\": None},\n        \"additional_auth\": {\"basic\": {\"username\": \"test\", \"password\": \"test\"}},\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"api_key\": None,\n        \"api_secret\": None,\n        \"token\": None,\n        \"token_secret\": None,\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"name\": \"test\",\n        \"user\": user,\n        \"password\": \"test\",\n        \"hosts\": [{\"host\": etcd_host, \"port\": etcd_port}],\n        \"security\": {\"allow\": [\"*\"], \"deny\": []},\n        \"storage\": {\"type\": \"filesystem\"},\n        \"templates\": {\"type\": \"filesystem\"},\n        \"options\": {\"ttl\": 600},\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"static_key\": \"\",\n        \"static_secret\": \"\",\n        \"key\": \"\",\n        \"secret\": \"\",\n        \"password\": \"\",\n        \"user_groups\": [],\n        \"groups\": [],\n        \"password_encrypted\": False,\n        \"encrypted_secret\": False,\n    }\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    config = {\n        \"name\": \"test\",\n        \"user\": user,\n        \"encrypted\": False,\n        \"secrets\": {\n            \"secret_one\": \"secret_value_one\",\n            \"secret_two\": \"secret_value_two\",\n        },\n        \"ssh_keys\": {\n            \"key_one\": {\n                \"path\": \"key_path\",\n                \"type\": \"SSH_KEY_FILE\",\n                \"value\": \"/tmp/test_key_file\",\n            },\n            \"key_two\": {\n                \"path\": \"key_path\",\n                \"type\": \"SSH_KEY_FILE\",\n                \"value\": \"/tmp/test_key_file\",\n            },\n        },\n        \"private_keys\": [\n            {\n                \"key_file\": \"/tmp/test_key_file\","], "level": "self_contained", "generate_results": [{"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"name\": \"test\",\n        \"user\": user,\n        \"password\": \"pass\",\n        \"host\": etcd_host,\n        \"port\": etcd_port,\n        \"database\": \"test-database\",\n        \"ssl\": False,\n        \"verify\": False,\n        \"certificate\": \"\",\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"password\": \"\",\n        \"key\": \"\",\n        \"database\": {\n            \"name\": \"\",\n            \"version\": \"\",\n            \"root_certificate\": \"\",\n            \"root_key\": \"\",\n            \"certificates\": {},\n            \"keys\": [],\n            \"database_password\": \"\",\n            \"status\": \"\",\n        },\n        \"schemes\": [\"http\", \"https\", \"tcp\"],\n        \"ssl\": {\n            \"ca_certs\": None,\n            \"cert_file\": None,\n            \"key_file\": None,\n            \"cert_reqs\": \"CERT_NONE\",\n            \"ca_certs_dir\": None,\n        },\n     ", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"\",\n        \"database\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        },\n        \"authentication\": {\n            \"type\": \"password\",\n            \"username\": \"\",\n            \"password\": \"\"\n        }\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"secret\",\n        \"host\": etcd_host,\n        \"port\": etcd_port,\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"secret\",\n        \"host\": etcd_host,\n        \"port\": etcd_port,\n        \"root\": \"/\",\n        \"db\": \"test\",\n        \"auth\": {\n            \"username\": \"test\",\n            \"password\": \"test\",\n            \"type\": \"password\"\n        }\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"password\": \"test\",\n        \"host\": etcd_host,\n        \"port\": etcd_port,\n        \"ca_certs\": None,\n        \"client\": {\"cert\": None, \"key\": None},\n        \"additional_auth\": {\"basic\": {\"username\": \"test\", \"password\": \"test\"}},\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"api_key\": None,\n        \"api_secret\": None,\n        \"token\": None,\n        \"token_secret\": None,\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"name\": \"test\",\n        \"user\": user,\n        \"password\": \"test\",\n        \"hosts\": [{\"host\": etcd_host, \"port\": etcd_port}],\n        \"security\": {\"allow\": [\"*\"], \"deny\": []},\n        \"storage\": {\"type\": \"filesystem\"},\n        \"templates\": {\"type\": \"filesystem\"},\n        \"options\": {\"ttl\": 600},\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port,\n        \"static_key\": \"\",\n        \"static_secret\": \"\",\n        \"key\": \"\",\n        \"secret\": \"\",\n        \"password\": \"\",\n        \"user_groups\": [],\n        \"groups\": [],\n        \"password_encrypted\": False,\n        \"encrypted_secret\": False,\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    config = {\n        \"name\": \"test\",\n        \"user\": user,\n        \"encrypted\": False,\n        \"secrets\": {\n            \"secret_one\": \"secret_value_one\",\n            \"secret_two\": \"secret_value_two\",\n        },\n        \"ssh_keys\": {\n            \"key_one\": {\n                \"path\": \"key_path\",\n                \"type\": \"SSH_KEY_FILE\",\n                \"value\": \"/tmp/test_key_file\",\n            },\n            \"key_two\": {\n                \"path\": \"key_path\",\n                \"type\": \"SSH_KEY_FILE\",\n                \"value\": \"/tmp/test_key_file\",\n            },\n        },\n        \"private_keys\": [\n            {\n                \"key_file\": \"/tmp/test_key_file\",", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "_fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "ques_id": "62b8a4a4755ee91dce50a3d2", "code": ["def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt\n\n    return dt.astimezone(self.tz)\n", "def _fromutc(self, dt):\n    if self._ambiguous:\n        dt = dt.astimezone(self._tz)\n    return dt.replace(tzinfo=None)\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=self.__class__.__tzinfo__)\n    return dt.astimezone(self.__class__.__fromutc_tz__)\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=self.tz)\n    else:\n        return dt.astimezone(self.tz)\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:\n        raise ValueError('fromutc requires a timezone-aware datetime')\n    # TODO: if dt.tzinfo is UTC, return a copy of the original datetime object\n    #       instead of modifying it. Assume that dt is in UTC.\n    return dt.replace(tzinfo=self)\n", "def _fromutc(self, dt):\n    if self._is_freeze(dt):\n        return dt\n    else:\n        return self._fromutc_tz(dt)\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt\n\n    return dt.astimezone(self.tz)\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=self.__class__.__timezone)\n    else:\n        return dt.astimezone(self.__class__.__timezone)\n", "def _fromutc(self, dt):\n\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=self.tz)\n\n    return dt.astimezone(self.tz)\n", "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=self._tzinfo)\n\n    # If the datetime is in the zone, we can assume it's already fromutc\n    if dt.tzinfo == self._tzinfo:\n        return dt\n\n    # Otherwise, we must be in a fold state where the fold has to be handled\n    # by the replacement function.\n    dt = dt.astimezone(self._tzinfo)\n    return dt.replace(tzinfo=None)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt\n\n    return dt.astimezone(self.tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if self._ambiguous:\n        dt = dt.astimezone(self._tz)\n    return dt.replace(tzinfo=None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=self.__class__.__tzinfo__)\n    return dt.astimezone(self.__class__.__fromutc_tz__)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=self.tz)\n    else:\n        return dt.astimezone(self.tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:\n        raise ValueError('fromutc requires a timezone-aware datetime')\n    # TODO: if dt.tzinfo is UTC, return a copy of the original datetime object\n    #       instead of modifying it. Assume that dt is in UTC.\n    return dt.replace(tzinfo=self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if self._is_freeze(dt):\n        return dt\n    else:\n        return self._fromutc_tz(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt\n\n    return dt.astimezone(self.tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=self.__class__.__timezone)\n    else:\n        return dt.astimezone(self.__class__.__timezone)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=self.tz)\n\n    return dt.astimezone(self.tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=self._tzinfo)\n\n    # If the datetime is in the zone, we can assume it's already fromutc\n    if dt.tzinfo == self._tzinfo:\n        return dt\n\n    # Otherwise, we must be in a fold state where the fold has to be handled\n    # by the replacement function.\n    dt = dt.astimezone(self._tzinfo)\n    return dt.replace(tzinfo=None)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/relativedelta.py", "project": "pexip/os-python-dateutil", "name": "normalized", "docstring": "Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.", "ques_id": "62b8982f755ee91dce50a241", "code": ["def normalized(self):\n    return relativedelta(**dict(self.items()))\n", "def normalized(self):\n    if self.days or self.hours or self.minutes or self.seconds:\n        return relativedelta(self.relative, self)\n    else:\n        return relativedelta(self.relative, datetime.timedelta(0))\n", "def normalized(self):\n    return relativedelta(self.relative, self)\n", "def normalized(self):\n\n    return relativedelta(\n        self.relative, self.absolute,\n        days=self.days, hours=self.hours)\n", "def normalized(self):\n    return relativedelta(self.relative, self)\n", "def normalized(self):\n\n    return relativedelta(\n        **dict(\n            (k, int(v))\n            for k, v in self.iteritems()\n            if not k.startswith(\"_\")\n        )\n    )\n", "def normalized(self):\n    return relativedelta(self.relative, self)\n", "def normalized(self):\n    return self.__class__(self.relativedelta)\n", "def normalized(self):\n    return relativedelta(self.relative, self)\n", "def normalized(self):\n    return relativedelta(self.relative, self.absolute)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def normalized(self):\n    return relativedelta(**dict(self.items()))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n    if self.days or self.hours or self.minutes or self.seconds:\n        return relativedelta(self.relative, self)\n    else:\n        return relativedelta(self.relative, datetime.timedelta(0))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n    return relativedelta(self.relative, self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\n    return relativedelta(\n        self.relative, self.absolute,\n        days=self.days, hours=self.hours)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n    return relativedelta(self.relative, self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\n    return relativedelta(\n        **dict(\n            (k, int(v))\n            for k, v in self.iteritems()\n            if not k.startswith(\"_\")\n        )\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n    return relativedelta(self.relative, self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n    return self.__class__(self.relativedelta)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n    return relativedelta(self.relative, self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n    return relativedelta(self.relative, self.absolute)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "tzname_in_python2", "docstring": "Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings", "ques_id": "62b89640755ee91dce50a114", "code": ["def tzname_in_python2(namefunc):\n    def wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if not PY3 and isinstance(name, bytes):\n            name = name.decode('utf-8', errors='replace')\n        return name\n    return wrapper\n", "def tzname_in_python2(namefunc):\n    namefunc = namefunc.__name__\n\n    @wraps(namefunc)\n    def wrapper(self, *args, **kwargs):\n        name = namefunc(self, *args, **kwargs)\n        if not PY3:\n            name = name.encode(self.encoding,'replace')\n        return name\n\n    return wrapper\n", "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self, *args, **kwargs):\n        name = namefunc(self, *args, **kwargs)\n        if not PY3:\n            name = name.encode('utf-8')\n        return name\n\n    return wrapper\n", "def tzname_in_python2(namefunc):\n    @wraps(namefunc)\n    def wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if six.PY3:\n            name = name.encode(self.encoding, errors='replace')\n        return name\n    return wrapper\n", "def tzname_in_python2(namefunc):\n\n    @functools.wraps(namefunc)\n    def wrapper(self, name, *args, **kwargs):\n        name = name.encode('utf-8')\n        return namefunc(self, name, *args, **kwargs)\n\n    return wrapper\n", "def tzname_in_python2(namefunc):\n    if PY2:\n        @wraps(namefunc)\n        def wrapper(self, *args, **kwargs):\n            name = namefunc(self, *args, **kwargs)\n            if name is not None:\n                name = name.encode(self.encoding)\n            return name\n        return wrapper\n    else:\n        return namefunc\n", "def tzname_in_python2(namefunc):\n    def _wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if name is not None:\n            name = name.encode('utf-8', 'backslashreplace')\n        return name\n    return _wrapper\n", "def tzname_in_python2(namefunc):\n    name = namefunc()\n\n    if six.PY3:\n        if isinstance(name, bytes):\n            name = name.decode(\"utf-8\", errors=\"replace\")\n\n    return name\n", "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if name is not None:\n            name = name.encode('utf-8')\n        return name\n\n    return wrapper\n", "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if sys.version_info < (3, 0):\n            return name\n        if name is not None:\n            name = name.encode(\"utf-8\", \"backslashreplace\")\n        return name\n\n    return wrapper\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def tzname_in_python2(namefunc):\n    def wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if not PY3 and isinstance(name, bytes):\n            name = name.decode('utf-8', errors='replace')\n        return name\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    namefunc = namefunc.__name__\n\n    @wraps(namefunc)\n    def wrapper(self, *args, **kwargs):\n        name = namefunc(self, *args, **kwargs)\n        if not PY3:\n            name = name.encode(self.encoding,'replace')\n        return name\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self, *args, **kwargs):\n        name = namefunc(self, *args, **kwargs)\n        if not PY3:\n            name = name.encode('utf-8')\n        return name\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    @wraps(namefunc)\n    def wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if six.PY3:\n            name = name.encode(self.encoding, errors='replace')\n        return name\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\n    @functools.wraps(namefunc)\n    def wrapper(self, name, *args, **kwargs):\n        name = name.encode('utf-8')\n        return namefunc(self, name, *args, **kwargs)\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    if PY2:\n        @wraps(namefunc)\n        def wrapper(self, *args, **kwargs):\n            name = namefunc(self, *args, **kwargs)\n            if name is not None:\n                name = name.encode(self.encoding)\n            return name\n        return wrapper\n    else:\n        return namefunc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    def _wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if name is not None:\n            name = name.encode('utf-8', 'backslashreplace')\n        return name\n    return _wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n    name = namefunc()\n\n    if six.PY3:\n        if isinstance(name, bytes):\n            name = name.decode(\"utf-8\", errors=\"replace\")\n\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if name is not None:\n            name = name.encode('utf-8')\n        return name\n\n    return wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\n    @wraps(namefunc)\n    def wrapper(self, name, *args, **kwargs):\n        name = namefunc(self, name, *args, **kwargs)\n        if sys.version_info < (3, 0):\n            return name\n        if name is not None:\n            name = name.encode(\"utf-8\", \"backslashreplace\")\n        return name\n\n    return wrapper\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_versions", "docstring": "Get version information or return default if unable to do so.", "ques_id": "62b87d24d292efb640a55670", "code": ["def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the.git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "render", "docstring": "Render the given version pieces into the requested style.", "ques_id": "62b87d24d292efb640a5566f", "code": ["def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else"], "level": "file_runnable", "generate_results": [{"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "plus_or_dot", "docstring": "Return a + if we don't already have one, else return a .", "ques_id": "62b87d24d292efb640a5566d", "code": ["def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n"], "level": "self_contained", "generate_results": [{"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "run_command", "docstring": "Call the given command(s).", "ques_id": "62b87d23d292efb640a5566b", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_config", "docstring": "Create, populate and return the VersioneerConfig() object.", "ques_id": "62b87d23d292efb640a55668", "code": ["def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"apertium/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/datalad/version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-post\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/parse/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-post\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/seminario/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"simple-hello/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"ete_source/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"pypcg/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-post\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"node_finder/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/twilio/twilio_pro_sms/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"natsu_src/_version.py\"\n    cfg.verbose = False\n    return cfg\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"apertium/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/datalad/version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-post\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/parse/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-post\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/seminario/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"simple-hello/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"ete_source/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"pypcg/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-post\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"node_finder/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/twilio/twilio_pro_sms/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n    # these strings are filled in when'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"natsu_src/_version.py\"\n    cfg.verbose = False\n    return cfg\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "register_vcs_handler", "docstring": "Create decorator to mark a method as the handler of a VCS.", "ques_id": "62b87d23d292efb640a55667", "code": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n"], "level": "self_contained", "generate_results": [{"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "validate_hierarchy", "docstring": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid", "ques_id": "62b45e945108cfac7f210a4a", "code": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects, good_objects, found_objects = self._validate(\n        self.storage_root_path,\n        validate_objects=validate_objects,\n        check_digests=check_digests,\n        show_warnings=show_warnings\n    )\n\n    if found_objects and not found_objects % 2:\n        raise StorageError(\"Invalid object tree.\")\n\n    return num_objects, good_objects, found_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    validation_errors = []\n    storage_roots = self.storage_roots\n    if validate_objects:\n        storage_roots = self.validate_objects(check_digests)\n\n    if check_digests:\n        self.validate_digests(storage_roots, show_warnings)\n\n    for storage_root in storage_roots:\n        if not storage_root.is_valid:\n            validation_errors.append(storage_root.path)\n\n    if validate_objects and len(validation_errors) > 0:\n        return len(validation_errors), validation_errors\n    else:\n        return 0, validation_errors\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects, good_objects = self._validate_storage_root(\n        self.storage_root, validate_objects=validate_objects,\n        check_digests=check_digests, show_warnings=show_warnings\n    )\n    return num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects = 0\n    good_objects = 0\n    for storage_root in self.storage_roots:\n        num_objects += storage_root.validate_hierarchy(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)\n        if check_digests:\n            good_objects += len(storage_root.check_digests(validate_objects=validate_objects, show_warnings=show_warnings))\n    return num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    validate_objects = self._validate_objects(validate_objects)\n    checked_objects = self._check_digests(check_digests)\n    found_to_be_valid = self._found_to_be_valid(validate_objects, checked_objects, show_warnings)\n    num_objects = len(found_to_be_valid)\n    return num_objects, found_to_be_valid, checked_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if self.storage_root:\n        validation_results = self.storage_root.validate(\n            validate_objects=validate_objects,\n            check_digests=check_digests,\n            show_warnings=show_warnings\n        )\n\n        if validation_results.num_objects == 0:\n            return 0, 0\n        elif validation_results.num_objects > 0:\n            return validation_results.good_objects, validation_results.num_objects\n\n    return 0, 0\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if self.is_root:\n        return 0, []\n\n    results = self._validate_storage_root()\n    num_objects = len(results)\n    good_objects = 0\n    if validate_objects:\n        for storage_root in results:\n            good_objects += self._validate_storage_object(storage_root, check_digests)\n    if show_warnings:\n        for storage_root in results:\n            if storage_root.is_warning():\n                print(\"Warning: %s is a warning storage.\" % storage_root.name)\n                if check_digests:\n                    self._validate_digests(storage_root)\n    if check_digests:\n        for storage_root in results:\n            good_objects += self._validate_digests(storage_root)\n    return num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if not validate_objects:\n        return 0, 0, 0\n    validated = 0\n    for object_path, root in self.store.items():\n        validated += self._validate_object(\n            object_path, root, check_digests=check_digests, show_warnings=show_warnings)\n    return validated, self.store.values(), validated / self.store.count()\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    self._validate_root_objects()\n    self._validate_storage_roots()\n    self._validate_root_digests()\n    self._validate_all_digests()\n    self._validate_objects_found()\n\n    if validate_objects:\n        self._validate_objects_checked(check_digests=check_digests, show_warnings=show_warnings)\n\n    return self.num_objects, self.good_objects, self.found_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects = 0\n    good_objects = 0\n    for storage_root in self.storage_roots:\n        num_objects += self._validate_storage_root(storage_root, validate_objects, check_digests, show_warnings)\n        if validate_objects:\n            good_objects += len(storage_root.get_manifest().get_all_good_objects())\n    return num_objects, good_objects\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects, good_objects, found_objects = self._validate(\n        self.storage_root_path,\n        validate_objects=validate_objects,\n        check_digests=check_digests,\n        show_warnings=show_warnings\n    )\n\n    if found_objects and not found_objects % 2:\n        raise StorageError(\"Invalid object tree.\")\n\n    return num_objects, good_objects, found_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    validation_errors = []\n    storage_roots = self.storage_roots\n    if validate_objects:\n        storage_roots = self.validate_objects(check_digests)\n\n    if check_digests:\n        self.validate_digests(storage_roots, show_warnings)\n\n    for storage_root in storage_roots:\n        if not storage_root.is_valid:\n            validation_errors.append(storage_root.path)\n\n    if validate_objects and len(validation_errors) > 0:\n        return len(validation_errors), validation_errors\n    else:\n        return 0, validation_errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects, good_objects = self._validate_storage_root(\n        self.storage_root, validate_objects=validate_objects,\n        check_digests=check_digests, show_warnings=show_warnings\n    )\n    return num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects = 0\n    good_objects = 0\n    for storage_root in self.storage_roots:\n        num_objects += storage_root.validate_hierarchy(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)\n        if check_digests:\n            good_objects += len(storage_root.check_digests(validate_objects=validate_objects, show_warnings=show_warnings))\n    return num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    validate_objects = self._validate_objects(validate_objects)\n    checked_objects = self._check_digests(check_digests)\n    found_to_be_valid = self._found_to_be_valid(validate_objects, checked_objects, show_warnings)\n    num_objects = len(found_to_be_valid)\n    return num_objects, found_to_be_valid, checked_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if self.storage_root:\n        validation_results = self.storage_root.validate(\n            validate_objects=validate_objects,\n            check_digests=check_digests,\n            show_warnings=show_warnings\n        )\n\n        if validation_results.num_objects == 0:\n            return 0, 0\n        elif validation_results.num_objects > 0:\n            return validation_results.good_objects, validation_results.num_objects\n\n    return 0, 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if self.is_root:\n        return 0, []\n\n    results = self._validate_storage_root()\n    num_objects = len(results)\n    good_objects = 0\n    if validate_objects:\n        for storage_root in results:\n            good_objects += self._validate_storage_object(storage_root, check_digests)\n    if show_warnings:\n        for storage_root in results:\n            if storage_root.is_warning():\n                print(\"Warning: %s is a warning storage.\" % storage_root.name)\n                if check_digests:\n                    self._validate_digests(storage_root)\n    if check_digests:\n        for storage_root in results:\n            good_objects += self._validate_digests(storage_root)\n    return num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    if not validate_objects:\n        return 0, 0, 0\n    validated = 0\n    for object_path, root in self.store.items():\n        validated += self._validate_object(\n            object_path, root, check_digests=check_digests, show_warnings=show_warnings)\n    return validated, self.store.values(), validated / self.store.count()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    self._validate_root_objects()\n    self._validate_storage_roots()\n    self._validate_root_digests()\n    self._validate_all_digests()\n    self._validate_objects_found()\n\n    if validate_objects:\n        self._validate_objects_checked(check_digests=check_digests, show_warnings=show_warnings)\n\n    return self.num_objects, self.good_objects, self.found_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    num_objects = 0\n    good_objects = 0\n    for storage_root in self.storage_roots:\n        num_objects += self._validate_storage_root(storage_root, validate_objects, check_digests, show_warnings)\n        if validate_objects:\n            good_objects += len(storage_root.get_manifest().get_all_good_objects())\n    return num_objects, good_objects\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "initialize", "docstring": "Create and initialize a new OCFL storage root.", "ques_id": "62b45e515108cfac7f210a3c", "code": ["def initialize(self):\n    self._storage_root = self._create_storage_root()\n", "def initialize(self):\n    self._create_storage_root()\n    self._initialize_object_store_backend()\n", "def initialize(self):\n    self._backend = _Backend(\n        self._name, self._type, self._length, self._block_size)\n    self._root = self._backend.root\n    self._tree = self._backend.tree\n", "def initialize(self):\n    super(OCFLStorageRoot, self).initialize()\n    self._storage_root = self._initialize_storage_root()\n", "def initialize(self):\n    self._storage_root = self._engine.create_storage_root(self._engine.get_storage_root())\n", "def initialize(self):\n    self._logger.debug(\"Initializing OCFL storage root '%s'\", self._root_name)\n\n    self._create_root()\n    self._set_root_schema(self._root_name, OCFL_SCHEMA_VERSION)\n    self._set_root_version(self._root_name, OCFL_ROOT_VERSION)\n    self._set_root_schema_version(self._root_name, OCFL_SCHEMA_VERSION)\n    self._set_root_db_version(self._root_name, OCFL_SCHEMA_VERSION)\n", "def initialize(self):\n    self._storage_root = OCFLStorageRoot(self)\n", "def initialize(self):\n    self._create_root()\n    self._initialize_storage_root()\n", "def initialize(self):\n    self._storage_root = self._create_storage_root()\n    self._write_roots(self._storage_root)\n", "def initialize(self):\n    self._create_storage_root()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def initialize(self):\n    self._storage_root = self._create_storage_root()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self._create_storage_root()\n    self._initialize_object_store_backend()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self._backend = _Backend(\n        self._name, self._type, self._length, self._block_size)\n    self._root = self._backend.root\n    self._tree = self._backend.tree\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    super(OCFLStorageRoot, self).initialize()\n    self._storage_root = self._initialize_storage_root()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self._storage_root = self._engine.create_storage_root(self._engine.get_storage_root())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self._logger.debug(\"Initializing OCFL storage root '%s'\", self._root_name)\n\n    self._create_root()\n    self._set_root_schema(self._root_name, OCFL_SCHEMA_VERSION)\n    self._set_root_version(self._root_name, OCFL_ROOT_VERSION)\n    self._set_root_schema_version(self._root_name, OCFL_SCHEMA_VERSION)\n    self._set_root_db_version(self._root_name, OCFL_SCHEMA_VERSION)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self._storage_root = OCFLStorageRoot(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self._create_root()\n    self._initialize_storage_root()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self._storage_root = self._create_storage_root()\n    self._write_roots(self._storage_root)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n    self._create_storage_root()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "next_version", "docstring": "Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.", "ques_id": "62b45e2eb89c9fd354170232", "code": ["def next_version(version):\n    if version is None:\n        return '0'\n    if re.match(r'^\\d+$', version):\n        return version\n    next = ''\n    for i, c in enumerate(version):\n        if c.isdigit():\n            next += c\n        else:\n            break\n    return int(next) + 1\n", "def next_version(version):\n    if version == 0:\n        return 1\n    else:\n        return int(version + 1)\n", "def next_version(version):\n    next = ''\n    if version!= 0:\n        next = '-%s' % version\n    return next\n", "def next_version(version):\n    if version == 0:\n        return 1\n    return 2 * version + 1\n", "def next_version(version):\n    if version == 0:\n        return 2\n    if version == 1:\n        return 1\n    return 0\n", "def next_version(version):\n    if version == 0:\n        return 1\n    return 2 + int(version.lstrip('v'))\n", "def next_version(version):\n    match = NEXT_VERSION_RE.match(version)\n    if match is None:\n        return version\n    else:\n        next_version = match.group(1)\n        if next_version is None or next_version == '0':\n            next_version = None\n        else:\n            next_version = int(next_version)\n        return '%s.0' % next_version\n", "def next_version(version):\n    if version == 0:\n        return 1\n    else:\n        return int(version + 1)\n", "def next_version(version):\n    if version == 0:\n        return 1\n    else:\n        return int(version.lstrip('v')) + 1\n", "def next_version(version):\n    if version == 0:\n        return 2\n    else:\n        return int(version + 1)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def next_version(version):\n    if version is None:\n        return '0'\n    if re.match(r'^\\d+$', version):\n        return version\n    next = ''\n    for i, c in enumerate(version):\n        if c.isdigit():\n            next += c\n        else:\n            break\n    return int(next) + 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == 0:\n        return 1\n    else:\n        return int(version + 1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    next = ''\n    if version!= 0:\n        next = '-%s' % version\n    return next\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == 0:\n        return 1\n    return 2 * version + 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == 0:\n        return 2\n    if version == 1:\n        return 1\n    return 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == 0:\n        return 1\n    return 2 + int(version.lstrip('v'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    match = NEXT_VERSION_RE.match(version)\n    if match is None:\n        return version\n    else:\n        next_version = match.group(1)\n        if next_version is None or next_version == '0':\n            next_version = None\n        else:\n            next_version = int(next_version)\n        return '%s.0' % next_version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == 0:\n        return 1\n    else:\n        return int(version + 1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == 0:\n        return 1\n    else:\n        return int(version.lstrip('v')) + 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n    if version == 0:\n        return 2\n    else:\n        return int(version + 1)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate_version_inventories", "docstring": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).", "ques_id": "62b45e23e0d4551b0392c90a", "code": ["def validate_version_inventories(self, version_dirs):\n    pass\n", "def validate_version_inventories(self, version_dirs):\n    pass\n", "def validate_version_inventories(self, version_dirs):\n    pass\n", "def validate_version_inventories(self, version_dirs):\n    pass\n", "def validate_version_inventories(self, version_dirs):\n    # TODO: this method should be merged into the InventoryValidator\n    # validation and make it more dynamic.\n    for version_dir in version_dirs:\n        self._validate_version_inventories_in_dir(version_dir)\n", "def validate_version_inventories(self, version_dirs):\n    pass\n", "def validate_version_inventories(self, version_dirs):\n    inventory_filename = self.inventory_filename\n    if inventory_filename is None:\n        return\n\n    # If there's a single version directory, we should have a valid inventory\n    if len(version_dirs) == 1:\n        self.log.info(\"%s is already in an inventory\" % inventory_filename)\n        return\n\n    # The inventory is not valid if it's a single directory, but it's actually a list of version directories.\n    # If there is more than one version directory, then the list of version directories needs to be sorted before\n    # the inventory. This is because the inventory could be compared to a list of different version directories.\n    version_dirs = [os.path.relpath(d, version_dirs[0]) for d in version_dirs]\n\n    self.log.info(\"Validating inventory from %s\" % inventory_filename)\n    inventory = Inventory(inventory_filename)\n    self._validate_version_inventories_from_inventory(inventory, version_dirs)\n", "def validate_version_inventories(self, version_dirs):\n    pass\n", "def validate_version_inventories(self, version_dirs):\n    inventory_root = self.get_inventory_root()\n    if inventory_root is None:\n        return\n\n    for version_dir in version_dirs:\n        version_path = os.path.join(inventory_root, version_dir)\n        if not os.path.exists(version_path):\n            raise errors.CannotValidate(\n                'Inventory is out of date. '\n                'Expected a version in the root of the inventory. '\n                'Found {}'.format(version_path)\n            )\n\n        # Make sure the path is absolute.\n        if not os.path.isabs(version_path):\n            version_path = os.path.join(inventory_root, version_path)\n\n        # Check the digests in the contents of the path.\n        contents = self.read_inventory_file(version_path)\n        for digest in contents.get('__ALL__', []):\n            self.validate_digest(version_path, digest)\n", "def validate_version_inventories(self, version_dirs):\n    pass\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_version_inventories(self, version_dirs):\n    pass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    pass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    pass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    pass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    # TODO: this method should be merged into the InventoryValidator\n    # validation and make it more dynamic.\n    for version_dir in version_dirs:\n        self._validate_version_inventories_in_dir(version_dir)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    pass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    inventory_filename = self.inventory_filename\n    if inventory_filename is None:\n        return\n\n    # If there's a single version directory, we should have a valid inventory\n    if len(version_dirs) == 1:\n        self.log.info(\"%s is already in an inventory\" % inventory_filename)\n        return\n\n    # The inventory is not valid if it's a single directory, but it's actually a list of version directories.\n    # If there is more than one version directory, then the list of version directories needs to be sorted before\n    # the inventory. This is because the inventory could be compared to a list of different version directories.\n    version_dirs = [os.path.relpath(d, version_dirs[0]) for d in version_dirs]\n\n    self.log.info(\"Validating inventory from %s\" % inventory_filename)\n    inventory = Inventory(inventory_filename)\n    self._validate_version_inventories_from_inventory(inventory, version_dirs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    pass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    inventory_root = self.get_inventory_root()\n    if inventory_root is None:\n        return\n\n    for version_dir in version_dirs:\n        version_path = os.path.join(inventory_root, version_dir)\n        if not os.path.exists(version_path):\n            raise errors.CannotValidate(\n                'Inventory is out of date. '\n                'Expected a version in the root of the inventory. '\n                'Found {}'.format(version_path)\n            )\n\n        # Make sure the path is absolute.\n        if not os.path.isabs(version_path):\n            version_path = os.path.join(inventory_root, version_path)\n\n        # Check the digests in the contents of the path.\n        contents = self.read_inventory_file(version_path)\n        for digest in contents.get('__ALL__', []):\n            self.validate_digest(version_path, digest)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n    pass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "find_path_type", "docstring": "Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.", "ques_id": "62b45e21e0d4551b0392c8ed", "code": ["def find_path_type(path):\n    if os.path.isdir(path):\n        return 'root'\n    if os.path.isfile(path):\n        return 'object'\n    if not os.path.isdir(path):\n        raise ValueError('path must be a directory or a file')\n    for d in os.listdir(path):\n        d = os.path.join(path, d)\n        if os.path.isdir(d):\n            return 'directory'\n        if os.path.isfile(d):\n            return 'file'\n    raise IOError('path does not appear to be a OCFL or OCFL file')\n", "def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'root'\n    else:\n        try:\n            content_type = head(path).split('=')[1].strip().lower()\n            return 'object' if content_type == 'application/octet-stream' else 'file'\n        except Exception:\n            return 'error looking for path type'\n", "def find_path_type(path):\n    if is_file_like(path):\n        return 'file'\n    elif is_storage_root(path):\n        return 'root'\n    else:\n        return 'object'\n", "def find_path_type(path):\n    if is_file(path):\n        return 'file'\n    elif is_directory(path):\n        return 'directory'\n    elif is_non_existent_file(path):\n        return 'root'\n    else:\n        return 'unknown'\n", "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'file'\n    elif os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(os.path.join(path, '0')):\n        return 'root'\n    else:\n        for name in ['0', '1', '2', '3']:\n            if os.path.isdir(os.path.join(path, name)):\n                return 'dir'\n    return 'file'\n", "def find_path_type(path):\n    for type_name in ['root', 'object', 'file']:\n        if os.path.isdir(path):\n            return type_name\n    return 'error detected path type'\n", "def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'directory'\n    else:\n        return 'root'\n", "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'file'\n    elif os.stat(path).st_size == 0:\n        return 'object'\n    else:\n        fn = os.path.basename(path)\n        if re.match('^0*$', fn):\n            return 'file'\n        else:\n            return 'object'\n", "def find_path_type(path):\n    if os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'object'\n    elif '0=' in path:\n        return 'file'\n    else:\n        raise ValueError('Not a valid OCFL path: {}'.format(path))\n", "def find_path_type(path):\n    if os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'object'\n    elif os.path.islink(path):\n        return 'file'\n    else:\n        description = ''\n        if os.path.isdir(path):\n            description = 'Directory'\n        elif os.path.isfile(path):\n            description = 'File'\n        elif os.path.islink(path):\n            description = 'Link'\n        return description\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_path_type(path):\n    if os.path.isdir(path):\n        return 'root'\n    if os.path.isfile(path):\n        return 'object'\n    if not os.path.isdir(path):\n        raise ValueError('path must be a directory or a file')\n    for d in os.listdir(path):\n        d = os.path.join(path, d)\n        if os.path.isdir(d):\n            return 'directory'\n        if os.path.isfile(d):\n            return 'file'\n    raise IOError('path does not appear to be a OCFL or OCFL file')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'root'\n    else:\n        try:\n            content_type = head(path).split('=')[1].strip().lower()\n            return 'object' if content_type == 'application/octet-stream' else 'file'\n        except Exception:\n            return 'error looking for path type'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if is_file_like(path):\n        return 'file'\n    elif is_storage_root(path):\n        return 'root'\n    else:\n        return 'object'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if is_file(path):\n        return 'file'\n    elif is_directory(path):\n        return 'directory'\n    elif is_non_existent_file(path):\n        return 'root'\n    else:\n        return 'unknown'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'file'\n    elif os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(os.path.join(path, '0')):\n        return 'root'\n    else:\n        for name in ['0', '1', '2', '3']:\n            if os.path.isdir(os.path.join(path, name)):\n                return 'dir'\n    return 'file'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    for type_name in ['root', 'object', 'file']:\n        if os.path.isdir(path):\n            return type_name\n    return 'error detected path type'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if os.path.isfile(path):\n        return 'file'\n    elif os.path.isdir(path):\n        return 'directory'\n    else:\n        return 'root'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if not os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'file'\n    elif os.stat(path).st_size == 0:\n        return 'object'\n    else:\n        fn = os.path.basename(path)\n        if re.match('^0*$', fn):\n            return 'file'\n        else:\n            return 'object'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'object'\n    elif '0=' in path:\n        return 'file'\n    else:\n        raise ValueError('Not a valid OCFL path: {}'.format(path))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n    if os.path.isdir(path):\n        return 'root'\n    elif os.path.isfile(path):\n        return 'object'\n    elif os.path.islink(path):\n        return 'file'\n    else:\n        description = ''\n        if os.path.isdir(path):\n            description = 'Directory'\n        elif os.path.isfile(path):\n            description = 'File'\n        elif os.path.islink(path):\n            description = 'Link'\n        return description\n", "is_pass": false, "return_code": 1}]}
{"file_path": "auto_nag/bzcleaner.py", "project": "mozilla/relman-auto-nag", "name": "amend_bzparams", "docstring": "Amend the Bugzilla params", "ques_id": "62b45b396decaeff903e1001", "code": ["def amend_bzparams(self, params, bug_ids):\n    if isinstance(bug_ids, int):\n        bug_ids = [bug_ids]\n    params['bugzilla_id'] = bug_ids\n    return params\n", "def amend_bzparams(self, params, bug_ids):\n    for bug_id in bug_ids:\n        params[\"bz_id\"] = bug_id\n        yield params\n", "def amend_bzparams(self, params, bug_ids):\n    for id in bug_ids:\n        params['id'] = id\n    return params\n", "def amend_bzparams(self, params, bug_ids):\n    if not params.get('bz_ids'):\n        return\n\n    for bz_id in bug_ids:\n        bz_params = params.copy()\n        bz_params['bz_id'] = bz_id\n\n        del bz_params['product_id']\n        del bz_params['product_name']\n        del bz_params['product_displayname']\n        del bz_params['product_version']\n        del bz_params['platform']\n\n        del bz_params['checked_in']\n        del bz_params['created_at']\n        del bz_params['updated_at']\n\n        del bz_params['status']\n\n        del bz_params['bug_id']\n        del bz_params['bz_ids']\n\n        params['bz_params'] = bz_params\n", "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        return\n\n    if isinstance(bug_ids, list):\n        bug_ids = ';'.join(bug_ids)\n\n    params['ids'] = bug_ids\n\n    return params\n", "def amend_bzparams(self, params, bug_ids):\n    if isinstance(bug_ids, list):\n        bug_ids = ','.join(bug_ids)\n    if isinstance(params, dict):\n        params = params.copy()\n        params['bug_ids[]'] = bug_ids\n        return params\n", "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        return\n\n    for id_ in bug_ids:\n        params['ids'].append(str(id_))\n", "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        return\n    # If there are no bug_ids, we can just return\n    if not bug_ids:\n        return\n    # Make sure we have a list\n    if not isinstance(bug_ids, list):\n        bug_ids = [bug_ids]\n    # Now we can add the params\n    params['bug_id'] = bug_ids\n", "def amend_bzparams(self, params, bug_ids):\n    if params is None:\n        params = {}\n    params['ids'] = \",\".join(bug_ids)\n    return params\n", "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        return\n\n    if isinstance(bug_ids, str):\n        bug_ids = [bug_ids]\n\n    for bug_id in bug_ids:\n        params['bugzilla_id'] = bug_id\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if isinstance(bug_ids, int):\n        bug_ids = [bug_ids]\n    params['bugzilla_id'] = bug_ids\n    return params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    for bug_id in bug_ids:\n        params[\"bz_id\"] = bug_id\n        yield params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    for id in bug_ids:\n        params['id'] = id\n    return params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not params.get('bz_ids'):\n        return\n\n    for bz_id in bug_ids:\n        bz_params = params.copy()\n        bz_params['bz_id'] = bz_id\n\n        del bz_params['product_id']\n        del bz_params['product_name']\n        del bz_params['product_displayname']\n        del bz_params['product_version']\n        del bz_params['platform']\n\n        del bz_params['checked_in']\n        del bz_params['created_at']\n        del bz_params['updated_at']\n\n        del bz_params['status']\n\n        del bz_params['bug_id']\n        del bz_params['bz_ids']\n\n        params['bz_params'] = bz_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        return\n\n    if isinstance(bug_ids, list):\n        bug_ids = ';'.join(bug_ids)\n\n    params['ids'] = bug_ids\n\n    return params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if isinstance(bug_ids, list):\n        bug_ids = ','.join(bug_ids)\n    if isinstance(params, dict):\n        params = params.copy()\n        params['bug_ids[]'] = bug_ids\n        return params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        return\n\n    for id_ in bug_ids:\n        params['ids'].append(str(id_))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        return\n    # If there are no bug_ids, we can just return\n    if not bug_ids:\n        return\n    # Make sure we have a list\n    if not isinstance(bug_ids, list):\n        bug_ids = [bug_ids]\n    # Now we can add the params\n    params['bug_id'] = bug_ids\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if params is None:\n        params = {}\n    params['ids'] = \",\".join(bug_ids)\n    return params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n    if not bug_ids:\n        return\n\n    if isinstance(bug_ids, str):\n        bug_ids = [bug_ids]\n\n    for bug_id in bug_ids:\n        params['bugzilla_id'] = bug_id\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/atticmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "ques_id": "62b4567ed7d32e5b55cc83d9", "code": ["def deep_merge_nodes(nodes):\n\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n\n    return result\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in result:\n                result[result.index(node.value)].append(deep_merge_node(node))\n            else:\n                result.append(deep_merge_node(node))\n        elif isinstance(node, ScalarNode):\n            result.append(node)\n        else:\n            raise TypeError('Unexpected node type: {}'.format(type(node)))\n    return result\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_mapping_nodes(node))\n        elif isinstance(node, ScalarNode):\n            result.append(node)\n        elif isinstance(node, Node):\n            if node.value in ('null', 'null:true', 'null:false'):\n                result[-1] = node.value\n            else:\n                result.append(node)\n        else:\n            raise ValueError('Unexpected node type: {}'.format(type(node)))\n    return result\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node_list in nodes:\n        if isinstance(node_list, list):\n            for node in node_list:\n                if isinstance(node, MappingNode):\n                    result.append(deep_merge_nodes(node))\n                else:\n                    for key in node.keys():\n                        if key in node_list:\n                            node_list[key] = deep_merge_node(node_list[key], node[key])\n                        else:\n                            node_list[key] = node[key]\n        else:\n            result.append(node_list)\n    return result\n", "def deep_merge_nodes(nodes):\n\n    def _deep_merge(node, nodes):\n        if node is None:\n            return nodes\n        elif isinstance(node, MappingNode):\n            for k, v in six.iteritems(node.value):\n                if k in nodes and isinstance(v, MappingNode):\n                    nodes[k] = _deep_merge(v, nodes[k])\n                else:\n                    nodes[k] = v\n            return nodes\n        else:\n            return [node, _deep_merge(node.value, nodes)]\n\n    return _deep_merge(nodes, nodes)\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n\n    return result\n", "def deep_merge_nodes(nodes):\n    # type: (List[Tuple[Union[Node, MappingNode], MappingNode]]) -> List[Union[Node, MappingNode]]\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the\n    form of:\n\n        (\n            ruamel.yaml.nodes.ScalarNode as a key,\n            ruamel.yaml.nodes.MappingNode or other Node as a value,\n        )\n    \"\"\"\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append((node, node))\n\n    return result\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in [None, '']:\n                result.append(node)\n            elif any(\n                    isinstance(val, MappingNode) and val.value in [None, ''] for val in result\n            ):\n                result.append(node)\n            else:\n                result.extend(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n\n    result = []\n\n    for node in nodes:\n        if isinstance(node, list):\n            result.append(deep_merge_nodes(node))\n        else:\n            result.append(node)\n\n    return result\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in result:\n                result[result.index(node.value)].append(deep_merge_node(node))\n            else:\n                result.append(deep_merge_node(node))\n        elif isinstance(node, ScalarNode):\n            result.append(node)\n        else:\n            raise TypeError('Unexpected node type: {}'.format(type(node)))\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_mapping_nodes(node))\n        elif isinstance(node, ScalarNode):\n            result.append(node)\n        elif isinstance(node, Node):\n            if node.value in ('null', 'null:true', 'null:false'):\n                result[-1] = node.value\n            else:\n                result.append(node)\n        else:\n            raise ValueError('Unexpected node type: {}'.format(type(node)))\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node_list in nodes:\n        if isinstance(node_list, list):\n            for node in node_list:\n                if isinstance(node, MappingNode):\n                    result.append(deep_merge_nodes(node))\n                else:\n                    for key in node.keys():\n                        if key in node_list:\n                            node_list[key] = deep_merge_node(node_list[key], node[key])\n                        else:\n                            node_list[key] = node[key]\n        else:\n            result.append(node_list)\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n\n    def _deep_merge(node, nodes):\n        if node is None:\n            return nodes\n        elif isinstance(node, MappingNode):\n            for k, v in six.iteritems(node.value):\n                if k in nodes and isinstance(v, MappingNode):\n                    nodes[k] = _deep_merge(v, nodes[k])\n                else:\n                    nodes[k] = v\n            return nodes\n        else:\n            return [node, _deep_merge(node.value, nodes)]\n\n    return _deep_merge(nodes, nodes)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    # type: (List[Tuple[Union[Node, MappingNode], MappingNode]]) -> List[Union[Node, MappingNode]]\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the\n    form of:\n\n        (\n            ruamel.yaml.nodes.ScalarNode as a key,\n            ruamel.yaml.nodes.MappingNode or other Node as a value,\n        )\n    \"\"\"\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append((node, node))\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in [None, '']:\n                result.append(node)\n            elif any(\n                    isinstance(val, MappingNode) and val.value in [None, ''] for val in result\n            ):\n                result.append(node)\n            else:\n                result.extend(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\n    result = []\n\n    for node in nodes:\n        if isinstance(node, list):\n            result.append(deep_merge_nodes(node))\n        else:\n            result.append(node)\n\n    return result\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "ques_id": "62b4567ad7d32e5b55cc83af", "code": ["def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Check output of a single command from a script.')\n    parser.add_argument(\n        '-v', '--verbose',\n        action='store_true',\n        help='Show extra information about the script.')\n    parser.add_argument(\n       'script',\n        metavar='PATH',\n        nargs='?',\n        type=str,\n        help='The script to check out.')\n    parser.add_argument(\n        '--workdir',\n        action='store',\n        help='The working directory for the output of the script.')\n    parser.add_argument(\n        '--recursive',\n        action='store_true',\n        help='Only check the subdirectories of the script.')\n    parser.add_argument(\n        '--stdout',\n        action='store_true',\n        help='Output a single stdout record (or stderr record).')\n   ", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='A simple script that parses command-line arguments.'\n    )\n\n    parser.add_argument(\n        '-v', '--version',\n        action='store_true',\n        help='Show the version number of the program.'\n    )\n\n    parser.add_argument(\n        '-a', '--attributes',\n        help='A file containing the attribute map from the XML document.'\n    )\n\n    parser.add_argument(\n        '-o', '--output-directory',\n        help='The directory to write the XML report.'\n    )\n\n    parser.add_argument(\n        '-p', '--prefix',\n        help='The prefix to use in the output file.'\n    )\n\n    parser.add_argument(\n        '-s', '--suffix',\n        help='The suffix to use in the output file.'\n    )\n\n    parser.add_argument(\n        '--success',", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n\n    # Allow the script to specify files to load.\n    parser.add_argument('-i', '--input', type=argparse.FileType('r'),\n                        help='Input file.')\n\n    # Allow the script to specify the output format (json, yaml, etc).\n    parser.add_argument('-o', '--output', type=str,\n                        help='Output file format.')\n\n    # Allow the script to specify a default format for output.\n    parser.add_argument('-f', '--format', default='yaml',\n                        help='Output format.')\n\n    # Allow the script to specify a default schema.\n    parser.add_argument('-s', '--schema', default='schema.json',\n                        help='Output schema file.')\n\n    # Allow the script to specify the default parser.\n    parser.add_argument('-p', '-", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        parser.add_argument(*argument)\n\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        try:\n            parser.add_argument(arg)\n        except argparse.ArgumentParserError as e:\n            raise Exception(\"Unable to parse argument: {}\".format(str(e)))\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return parser\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Check output of a single command from a script.')\n    parser.add_argument(\n        '-v', '--verbose',\n        action='store_true',\n        help='Show extra information about the script.')\n    parser.add_argument(\n       'script',\n        metavar='PATH',\n        nargs='?',\n        type=str,\n        help='The script to check out.')\n    parser.add_argument(\n        '--workdir',\n        action='store',\n        help='The working directory for the output of the script.')\n    parser.add_argument(\n        '--recursive',\n        action='store_true',\n        help='Only check the subdirectories of the script.')\n    parser.add_argument(\n        '--stdout',\n        action='store_true',\n        help='Output a single stdout record (or stderr record).')\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='A simple script that parses command-line arguments.'\n    )\n\n    parser.add_argument(\n        '-v', '--version',\n        action='store_true',\n        help='Show the version number of the program.'\n    )\n\n    parser.add_argument(\n        '-a', '--attributes',\n        help='A file containing the attribute map from the XML document.'\n    )\n\n    parser.add_argument(\n        '-o', '--output-directory',\n        help='The directory to write the XML report.'\n    )\n\n    parser.add_argument(\n        '-p', '--prefix',\n        help='The prefix to use in the output file.'\n    )\n\n    parser.add_argument(\n        '-s', '--suffix',\n        help='The suffix to use in the output file.'\n    )\n\n    parser.add_argument(\n        '--success',", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n\n    # Allow the script to specify files to load.\n    parser.add_argument('-i', '--input', type=argparse.FileType('r'),\n                        help='Input file.')\n\n    # Allow the script to specify the output format (json, yaml, etc).\n    parser.add_argument('-o', '--output', type=str,\n                        help='Output file format.')\n\n    # Allow the script to specify a default format for output.\n    parser.add_argument('-f', '--format', default='yaml',\n                        help='Output format.')\n\n    # Allow the script to specify a default schema.\n    parser.add_argument('-s', '--schema', default='schema.json',\n                        help='Output schema file.')\n\n    # Allow the script to specify the default parser.\n    parser.add_argument('-p', '-", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        parser.add_argument(*argument)\n\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        try:\n            parser.add_argument(arg)\n        except argparse.ArgumentParserError as e:\n            raise Exception(\"Unable to parse argument: {}\".format(str(e)))\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return parser\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/atticmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62b45679d7d32e5b55cc83a9", "code": ["def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        flags +='' + flag._group_actions[0]._group_actions[1].option_strings[0]\n    return flags\n", "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if isinstance(arg, argparse.ArgumentParser):\n            flags.extend(arg.args)\n        else:\n            flags.append(arg)\n    return''.join(str(flag) for flag in flags)\n", "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        if flag._action_group_name == 'tornado.generator':\n            flags +='' + flag._action_flags\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups['general']._group_actions:\n        if flag._action_group_name == 'examples':\n            flags +='-x {}'.format(flag._action_group_title)\n        else:\n            flags +='{}'.format(flag._action_group_title)\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        flags +='' + arg.flags\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        if flag._group_name == '__flags__':\n            flags += flag._action_group_action\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        flags +='' + arg.flags\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            continue\n        flags +='{}'.format(arg.dest)\n    return flags\n", "def parser_flags(parser):\n    return''.join([str(flag) for flag in parser.flags])\n", "def parser_flags(parser):\n    return''.join(parser._flags)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        flags +='' + flag._group_actions[0]._group_actions[1].option_strings[0]\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if isinstance(arg, argparse.ArgumentParser):\n            flags.extend(arg.args)\n        else:\n            flags.append(arg)\n    return''.join(str(flag) for flag in flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        if flag._action_group_name == 'tornado.generator':\n            flags +='' + flag._action_flags\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups['general']._group_actions:\n        if flag._action_group_name == 'examples':\n            flags +='-x {}'.format(flag._action_group_title)\n        else:\n            flags +='{}'.format(flag._action_group_title)\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        flags +='' + arg.flags\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        if flag._group_name == '__flags__':\n            flags += flag._action_group_action\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        flags +='' + arg.flags\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            continue\n        flags +='{}'.format(arg.dest)\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join([str(flag) for flag in parser.flags])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(parser._flags)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "ques_id": "62b45665d7d32e5b55cc8365", "code": ["def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, default='./config/config.json')\n    parser.add_argument('--log-level', type=str, default='WARNING')\n    parser.add_argument('--report', action='store_true')\n    parser.add_argument('--report-file', type=str, default='report.txt')\n    parser.add_argument('--debug', action='store_true')\n    parser.add_argument('--debug-file', type=str, default='debug.txt')\n    parser.add_argument('--no-color', action='store_true')\n\n    args = parser.parse_args(unparsed_arguments)\n\n    return {\n        'config': args.config,\n        'log_level': args.log_level,\n       'report': args.report,\n       'report_file': args.report_file,\n        'debug': args.debug,\n        'debug_file': args.debug_file,\n        'no_color': args.no_color,\n    }\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        if argument.startswith('--'):\n            parser.add_argument(*argument.split(' ', 1))\n        else:\n            parser.add_argument(*argument.split())\n    return vars(parser.parse_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for arg in unparsed_arguments:\n        if arg[0] == '-':\n            parser.add_argument(*arg[1:], **_unparsed_arg(arg))\n        else:\n            parser.add_argument(*arg, **_unparsed_arg(arg))\n    return vars(parser.parse_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    args = parser.parse_args(unparsed_arguments)\n    return args\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description=\"This script generates a random sequence of data from\"\n        \" the given sequence of files and writes them to stdout.\"\n    )\n    parser.add_argument(\n        \"files\",\n        nargs=\"+\",\n        help=\"One or more files to process\",\n        metavar=\"PATH\",\n    )\n    parser.add_argument(\n        \"-v\", \"--verbose\", help=\"be more verbose\", action=\"store_true\"\n    )\n    parser.add_argument(\n        \"-t\", \"--template\", help=\"the template to use to render the files\"\n    )\n    parser.add_argument(\n        \"-o\", \"--output\", help=\"the output file to write the sequence to\"\n    )\n    parser.add_argument(\n        \"-c\", \"--check-format\",\n        help=\"if provided, the output file will be written as a json file \"\n        \"with the same name as the input file. Otherwise, the output ", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description='A simple command-line tool that can be used to test '\n                    'the command line interface.')\n    subparsers = parser.add_subparsers(dest='subcommand')\n    for subcommand, (subcommand_parser, subcommand_name) in _SUBCOMMANDS.items():\n        parser.add_subparser(subcommand, prog=subcommand_name)\n        subcommand_parser.add_arguments(parser, *unparsed_arguments)\n\n    return vars(parser.parse_args(unparsed_arguments))\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for unparsed_argument in unparsed_arguments:\n        parser.add_argument(*unparsed_argument.args, **unparsed_argument.kwargs)\n    args = parser.parse_args()\n    return vars(args)\n", "def parse_arguments(*unparsed_arguments):\n    parser = create_argparser()\n    args = parser.parse_args(unparsed_arguments)\n    return vars(args)\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for arg in unparsed_arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return dict(parser.parse_args().__dict__)\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return dict(parser.parse_known_args())\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, default='./config/config.json')\n    parser.add_argument('--log-level', type=str, default='WARNING')\n    parser.add_argument('--report', action='store_true')\n    parser.add_argument('--report-file', type=str, default='report.txt')\n    parser.add_argument('--debug', action='store_true')\n    parser.add_argument('--debug-file', type=str, default='debug.txt')\n    parser.add_argument('--no-color', action='store_true')\n\n    args = parser.parse_args(unparsed_arguments)\n\n    return {\n        'config': args.config,\n        'log_level': args.log_level,\n       'report': args.report,\n       'report_file': args.report_file,\n        'debug': args.debug,\n        'debug_file': args.debug_file,\n        'no_color': args.no_color,\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        if argument.startswith('--'):\n            parser.add_argument(*argument.split(' ', 1))\n        else:\n            parser.add_argument(*argument.split())\n    return vars(parser.parse_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for arg in unparsed_arguments:\n        if arg[0] == '-':\n            parser.add_argument(*arg[1:], **_unparsed_arg(arg))\n        else:\n            parser.add_argument(*arg, **_unparsed_arg(arg))\n    return vars(parser.parse_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    args = parser.parse_args(unparsed_arguments)\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description=\"This script generates a random sequence of data from\"\n        \" the given sequence of files and writes them to stdout.\"\n    )\n    parser.add_argument(\n        \"files\",\n        nargs=\"+\",\n        help=\"One or more files to process\",\n        metavar=\"PATH\",\n    )\n    parser.add_argument(\n        \"-v\", \"--verbose\", help=\"be more verbose\", action=\"store_true\"\n    )\n    parser.add_argument(\n        \"-t\", \"--template\", help=\"the template to use to render the files\"\n    )\n    parser.add_argument(\n        \"-o\", \"--output\", help=\"the output file to write the sequence to\"\n    )\n    parser.add_argument(\n        \"-c\", \"--check-format\",\n        help=\"if provided, the output file will be written as a json file \"\n        \"with the same name as the input file. Otherwise, the output ", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description='A simple command-line tool that can be used to test '\n                    'the command line interface.')\n    subparsers = parser.add_subparsers(dest='subcommand')\n    for subcommand, (subcommand_parser, subcommand_name) in _SUBCOMMANDS.items():\n        parser.add_subparser(subcommand, prog=subcommand_name)\n        subcommand_parser.add_arguments(parser, *unparsed_arguments)\n\n    return vars(parser.parse_args(unparsed_arguments))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for unparsed_argument in unparsed_arguments:\n        parser.add_argument(*unparsed_argument.args, **unparsed_argument.kwargs)\n    args = parser.parse_args()\n    return vars(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = create_argparser()\n    args = parser.parse_args(unparsed_arguments)\n    return vars(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for arg in unparsed_arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return dict(parser.parse_args().__dict__)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return dict(parser.parse_known_args())\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "ques_id": "62b45665d7d32e5b55cc8364", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    not_parsed_arguments = []\n    for subparser_name, subparser in six.iteritems(subparsers):\n        if subparser_name in unparsed_arguments:\n            parsed_arguments[subparser_name] = subparser.parse_args(unparsed_arguments[subparser_name])\n        else:\n            not_parsed_arguments.append(subparser_name)\n    return parsed_arguments, not_parsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for subparser_name, subparser in six.iteritems(subparsers):\n        parsed_arguments[subparser_name] = parse_argument_line(subparser,\n                                                             unparsed_arguments)\n        if parsed_arguments[subparser_name] is None:\n            remaining_arguments.append(subparser_name)\n\n    return parsed_arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_args = []\n    for argument in unparsed_arguments:\n        parsed_arguments[argument.name] = parse_argument(argument, subparsers)\n        if argument.name not in parsed_arguments:\n            unparsed_args.append(argument)\n    return parsed_arguments, unparsed_args\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parse_args = {}\n    not_parsed = []\n    for name, parser in subparsers.items():\n        try:\n            parser_args = parser.parse_args(unparsed_arguments)\n            parse_args.update(parser_args)\n            parse_args[name] = parser_args.pop(name)\n        except ArgumentParserError as e:\n            not_parsed.append(name)\n            e.parser = parser\n            e.arguments = []\n            e.parser_args = []\n            e.unparsed_arguments = unparsed_arguments\n            raise\n    return parse_args, not_parsed\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser in subparsers._subparsers._actions:\n        args = unparsed_arguments.pop(0)\n        remaining_arguments.append(args)\n        arguments.update(subparser.parse_args(args))\n\n    return (arguments, remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser in subparsers:\n        if subparser.name not in unparsed_arguments:\n            continue\n        args, _ = parse_args(unparsed_arguments[subparser.name], subparser)\n        arguments[subparser.name] = args\n        remaining_arguments.extend(args)\n    return (arguments, remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    unparsed_arguments = unparsed_arguments or []\n    for subparser_name in subparsers:\n        parser = subparsers[subparser_name]\n        arguments[subparser_name] = parser.parse_args(unparsed_arguments)\n        unparsed_arguments.pop(0)\n    return arguments, unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_args = []\n    remaining_arguments = []\n    for arg in unparsed_arguments:\n        arg_parser = subparsers.add_parser(arg.replace('-', '_'))\n        arg_parser.set_defaults(**args_to_kwargs(arg))\n        parsed_arguments[arg] = arg_parser\n        remaining_arguments.append(arg_parser)\n        unparsed_args.append(arg)\n    for arg in unparsed_args:\n        arg_parser = subparsers.add_parser(arg.replace('-', '_'))\n        parsed_arguments[arg] = arg_parser\n        remaining_arguments.append(arg_parser)\n        unparsed_args.remove(arg)\n    return parsed_arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_args = []\n    for parser in subparsers._actions:\n        parsed_arguments.update(parser.parse_args(unparsed_arguments))\n        if parsed_arguments[parser.dest] is None:\n            remaining_args.append(parser.dest)\n    return parsed_arguments, remaining_args\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    unparsed_args = []\n    for arg in unparsed_arguments:\n        if arg.name not in arguments:\n            arguments[arg.name] = parse_argparse_action(arg, subparsers)\n        else:\n            arguments[arg.name] = arguments[arg.name].parse(arg, subparsers)\n        unparsed_args.append(arguments[arg.name])\n    return (arguments, unparsed_args)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    not_parsed_arguments = []\n    for subparser_name, subparser in six.iteritems(subparsers):\n        if subparser_name in unparsed_arguments:\n            parsed_arguments[subparser_name] = subparser.parse_args(unparsed_arguments[subparser_name])\n        else:\n            not_parsed_arguments.append(subparser_name)\n    return parsed_arguments, not_parsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for subparser_name, subparser in six.iteritems(subparsers):\n        parsed_arguments[subparser_name] = parse_argument_line(subparser,\n                                                             unparsed_arguments)\n        if parsed_arguments[subparser_name] is None:\n            remaining_arguments.append(subparser_name)\n\n    return parsed_arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_args = []\n    for argument in unparsed_arguments:\n        parsed_arguments[argument.name] = parse_argument(argument, subparsers)\n        if argument.name not in parsed_arguments:\n            unparsed_args.append(argument)\n    return parsed_arguments, unparsed_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parse_args = {}\n    not_parsed = []\n    for name, parser in subparsers.items():\n        try:\n            parser_args = parser.parse_args(unparsed_arguments)\n            parse_args.update(parser_args)\n            parse_args[name] = parser_args.pop(name)\n        except ArgumentParserError as e:\n            not_parsed.append(name)\n            e.parser = parser\n            e.arguments = []\n            e.parser_args = []\n            e.unparsed_arguments = unparsed_arguments\n            raise\n    return parse_args, not_parsed\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser in subparsers._subparsers._actions:\n        args = unparsed_arguments.pop(0)\n        remaining_arguments.append(args)\n        arguments.update(subparser.parse_args(args))\n\n    return (arguments, remaining_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser in subparsers:\n        if subparser.name not in unparsed_arguments:\n            continue\n        args, _ = parse_args(unparsed_arguments[subparser.name], subparser)\n        arguments[subparser.name] = args\n        remaining_arguments.extend(args)\n    return (arguments, remaining_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    unparsed_arguments = unparsed_arguments or []\n    for subparser_name in subparsers:\n        parser = subparsers[subparser_name]\n        arguments[subparser_name] = parser.parse_args(unparsed_arguments)\n        unparsed_arguments.pop(0)\n    return arguments, unparsed_arguments\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_args = []\n    remaining_arguments = []\n    for arg in unparsed_arguments:\n        arg_parser = subparsers.add_parser(arg.replace('-', '_'))\n        arg_parser.set_defaults(**args_to_kwargs(arg))\n        parsed_arguments[arg] = arg_parser\n        remaining_arguments.append(arg_parser)\n        unparsed_args.append(arg)\n    for arg in unparsed_args:\n        arg_parser = subparsers.add_parser(arg.replace('-', '_'))\n        parsed_arguments[arg] = arg_parser\n        remaining_arguments.append(arg_parser)\n        unparsed_args.remove(arg)\n    return parsed_arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_args = []\n    for parser in subparsers._actions:\n        parsed_arguments.update(parser.parse_args(unparsed_arguments))\n        if parsed_arguments[parser.dest] is None:\n            remaining_args.append(parser.dest)\n    return parsed_arguments, remaining_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    unparsed_args = []\n    for arg in unparsed_arguments:\n        if arg.name not in arguments:\n            arguments[arg.name] = parse_argparse_action(arg, subparsers)\n        else:\n            arguments[arg.name] = arguments[arg.name].parse(arg, subparsers)\n        unparsed_args.append(arguments[arg.name])\n    return (arguments, unparsed_args)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "ques_id": "62b45665d7d32e5b55cc8363", "code": ["def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Parse the results of a SMC analysis run.\"\n    )\n\n    subparsers = parser.add_subparsers(dest=\"subcommand\")\n    create_parser = subparsers.add_parser(\"create\")\n    create_parser.add_argument(\"--smc\", help=\"the SMC analysis run\", required=True)\n    create_parser.add_argument(\n        \"--output-directory\",\n        help=\"the directory in which the output should be saved\",\n        default=\".\",\n    )\n    create_parser.set_defaults(func=create)\n\n    return (parser, subparsers)\n", "def make_parsers():\n    parser = argparse.ArgumentParser(description=\"Testing for docopt output\")\n    subparsers = parser.add_subparsers(dest=\"subcommand\")\n    test_init = subparsers.add_parser(\"test_init\", help=\"initialize test scenarios\")\n    test_init.add_argument(\"--config\", type=str, default=\"./config.json\", help=\"path to configuration file\")\n    test_init.add_argument(\"--cluster\", type=str, default=\"cluster.json\", help=\"path to the cluster configuration file\")\n    test_init.add_argument(\"--scenarios\", type=str, default=\"scenarios.json\", help=\"path to the scenarios configuration file\")\n    test_init.add_argument(\"--output\", type=str, default=\"output.json\", help=\"path to output file\")\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Simple test command.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    subparsers = parser.add_subparsers(dest='command')\n\n    add_command(subparsers)\n    test_command(subparsers)\n    test_help(subparsers)\n    test_run(subparsers)\n    test_run_help(subparsers)\n    test_version(subparsers)\n    test_version_help(subparsers)\n    test_run_details(subparsers)\n    test_test_run(subparsers)\n    test_status(subparsers)\n    test_unknown(subparsers)\n    test_help_all(subparsers)\n    test_help_all(subparsers)\n    test_explain(subparsers)\n    test_help_all(subparsers)\n    test_help_choices(subparsers)\n    test_help_choices(subparsers)\n    test_help_boolean(subparsers)\n    test_help_choices_all(subparsers)\n    test_help_strings(subparsers)\n    test_help_strings_all(subparsers)\n    test_help_strings_enum(subparsers)\n    test_help_strings_enum(subparsers)\n    test_hel", "def make_parsers():\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers()\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Test that scanner arguments work.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    subparsers = parser.add_subparsers(dest=\"test_command\")\n\n    test_scanner_parser = subparsers.add_parser(\n        \"test_scanner\",\n        help=\"Scanner to test for arguments.\",\n    )\n    test_scanner_parser.add_argument(\n        \"-a\", \"--a\", type=int, default=5, help=\"argument to scan\"\n    )\n    test_scanner_parser.add_argument(\n        \"-b\", \"--b\", type=int, default=10, help=\"argument to scan\"\n    )\n    test_scanner_parser.add_argument(\n        \"--c\", type=str, default=\"hello\", help=\"argument to scan\"\n    )\n    test_scanner_parser.add_argument(\n        \"--d\", type=str, default=\"world\", help=\"argument to scan\"\n    )\n    test_scanner_parser.add_argument(\n        \"--e\", type=str, default=", "def make_parsers():\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='command')\n    build_command_parser(subparsers)\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser('my_tool')\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    for command in COMMANDS:\n        subparser = subparsers.add_parser(command['name'])\n        command['args'](subparser)\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        prog='vcstools',\n        description='A tool to manage VCS repositories.'\n    )\n\n    parser.add_argument(\n        '--version',\n        action='version',\n        version='%(prog)s {}'.format(__version__),\n        help='Show the version of this program and its version and exit.'\n    )\n\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    create_vcstools_parser(subparsers)\n    update_vcstools_parser(subparsers)\n    remove_vcstools_parser(subparsers)\n    list_vcstools_parser(subparsers)\n    list_vcstools_files(subparsers)\n    list_vcstools_dirs(subparsers)\n    list_vcstools_repos(subparsers)\n    download_vcstools_repos(subparsers)\n    download_vcstools_archives(subparsers)\n    download_vcstools_upstream(subparsers)\n    download_vcstools_vcstools(subparsers)\n    download_vcstools_vcstools_archives(subparsers)\n    download_vcstools_git(subparsers)\n    download_", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='A simple flake8 style checker.'\n    )\n    subparsers = parser.add_subparsers(dest='subcommand')\n    subparsers.required = True\n    add_flake8_lint(subparsers)\n    add_black_check(subparsers)\n    add_pyflakes(subparsers)\n    add_jinja_render(subparsers)\n    add_nose(subparsers)\n    add_coverage(subparsers)\n    add_pyflakes_lint(subparsers)\n    add_tsdocstyle(subparsers)\n    add_pylint(subparsers)\n    add_tests(subparsers)\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"A simple command-line tool to parse and transform \"\n        \"a JSON file into a Python dictionary.\")\n    parser.add_argument(\n        \"-v\", \"--version\", action=\"store_true\", help=\"Show the version of this \"\n        \"program.\")\n    parser.add_argument(\n        \"file\", nargs=\"?\", default=sys.argv[1], help=\"The JSON file to be \"\n        \"parsed.\")\n    return (parser, parser.add_argument_group(\"Options\"))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Parse the results of a SMC analysis run.\"\n    )\n\n    subparsers = parser.add_subparsers(dest=\"subcommand\")\n    create_parser = subparsers.add_parser(\"create\")\n    create_parser.add_argument(\"--smc\", help=\"the SMC analysis run\", required=True)\n    create_parser.add_argument(\n        \"--output-directory\",\n        help=\"the directory in which the output should be saved\",\n        default=\".\",\n    )\n    create_parser.set_defaults(func=create)\n\n    return (parser, subparsers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(description=\"Testing for docopt output\")\n    subparsers = parser.add_subparsers(dest=\"subcommand\")\n    test_init = subparsers.add_parser(\"test_init\", help=\"initialize test scenarios\")\n    test_init.add_argument(\"--config\", type=str, default=\"./config.json\", help=\"path to configuration file\")\n    test_init.add_argument(\"--cluster\", type=str, default=\"cluster.json\", help=\"path to the cluster configuration file\")\n    test_init.add_argument(\"--scenarios\", type=str, default=\"scenarios.json\", help=\"path to the scenarios configuration file\")\n    test_init.add_argument(\"--output\", type=str, default=\"output.json\", help=\"path to output file\")\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Simple test command.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    subparsers = parser.add_subparsers(dest='command')\n\n    add_command(subparsers)\n    test_command(subparsers)\n    test_help(subparsers)\n    test_run(subparsers)\n    test_run_help(subparsers)\n    test_version(subparsers)\n    test_version_help(subparsers)\n    test_run_details(subparsers)\n    test_test_run(subparsers)\n    test_status(subparsers)\n    test_unknown(subparsers)\n    test_help_all(subparsers)\n    test_help_all(subparsers)\n    test_explain(subparsers)\n    test_help_all(subparsers)\n    test_help_choices(subparsers)\n    test_help_choices(subparsers)\n    test_help_boolean(subparsers)\n    test_help_choices_all(subparsers)\n    test_help_strings(subparsers)\n    test_help_strings_all(subparsers)\n    test_help_strings_enum(subparsers)\n    test_help_strings_enum(subparsers)\n    test_hel", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers()\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Test that scanner arguments work.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    subparsers = parser.add_subparsers(dest=\"test_command\")\n\n    test_scanner_parser = subparsers.add_parser(\n        \"test_scanner\",\n        help=\"Scanner to test for arguments.\",\n    )\n    test_scanner_parser.add_argument(\n        \"-a\", \"--a\", type=int, default=5, help=\"argument to scan\"\n    )\n    test_scanner_parser.add_argument(\n        \"-b\", \"--b\", type=int, default=10, help=\"argument to scan\"\n    )\n    test_scanner_parser.add_argument(\n        \"--c\", type=str, default=\"hello\", help=\"argument to scan\"\n    )\n    test_scanner_parser.add_argument(\n        \"--d\", type=str, default=\"world\", help=\"argument to scan\"\n    )\n    test_scanner_parser.add_argument(\n        \"--e\", type=str, default=", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='command')\n    build_command_parser(subparsers)\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser('my_tool')\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    for command in COMMANDS:\n        subparser = subparsers.add_parser(command['name'])\n        command['args'](subparser)\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        prog='vcstools',\n        description='A tool to manage VCS repositories.'\n    )\n\n    parser.add_argument(\n        '--version',\n        action='version',\n        version='%(prog)s {}'.format(__version__),\n        help='Show the version of this program and its version and exit.'\n    )\n\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    create_vcstools_parser(subparsers)\n    update_vcstools_parser(subparsers)\n    remove_vcstools_parser(subparsers)\n    list_vcstools_parser(subparsers)\n    list_vcstools_files(subparsers)\n    list_vcstools_dirs(subparsers)\n    list_vcstools_repos(subparsers)\n    download_vcstools_repos(subparsers)\n    download_vcstools_archives(subparsers)\n    download_vcstools_upstream(subparsers)\n    download_vcstools_vcstools(subparsers)\n    download_vcstools_vcstools_archives(subparsers)\n    download_vcstools_git(subparsers)\n    download_", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='A simple flake8 style checker.'\n    )\n    subparsers = parser.add_subparsers(dest='subcommand')\n    subparsers.required = True\n    add_flake8_lint(subparsers)\n    add_black_check(subparsers)\n    add_pyflakes(subparsers)\n    add_jinja_render(subparsers)\n    add_nose(subparsers)\n    add_coverage(subparsers)\n    add_pyflakes_lint(subparsers)\n    add_tsdocstyle(subparsers)\n    add_pylint(subparsers)\n    add_tests(subparsers)\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"A simple command-line tool to parse and transform \"\n        \"a JSON file into a Python dictionary.\")\n    parser.add_argument(\n        \"-v\", \"--version\", action=\"store_true\", help=\"Show the version of this \"\n        \"program.\")\n    parser.add_argument(\n        \"file\", nargs=\"?\", default=sys.argv[1], help=\"The JSON file to be \"\n        \"parsed.\")\n    return (parser, parser.add_argument_group(\"Options\"))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/borgmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "ques_id": "62b438ba66fea644fe22cca2", "code": ["def deep_merge_nodes(nodes):\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            new_node = deep_merge_nodes(node.value)\n            if node.value is not None:\n                new_node.value = None\n            result.append((node, new_node))\n        else:\n            result.append(node)\n\n    return result\n", "def deep_merge_nodes(nodes):\n    # type: (List[Tuple[Node, MappingNode, Node]]) -> List[Node]\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the\n    form of:\n\n        (\n            ruamel.yaml.nodes.ScalarNode as a key,\n            ruamel.yaml.nodes.MappingNode or other Node as a value,\n        )\n\n   ...\n    deep merge any node values corresponding to duplicate keys and return\n    the result.\n\n    If there are colliding keys with non-MappingNode values (e.g., integers or\n    strings), the last of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[(\n                    ScalarNode(tag='tag:yaml.org,2002:str'", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, (MappingNode, ScalarNode)):\n            new_node = deep_merge_mapping_node(node)\n        else:\n            new_node = node\n\n        if isinstance(new_node, MappingNode):\n            new_node = deep_merge_mapping_node(new_node)\n\n        if isinstance(new_node, ScalarNode):\n            new_node.tag = node.tag\n\n        result.append(new_node)\n\n    return result\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n    for key, value in nodes:\n        if isinstance(value, MappingNode):\n            new_value = deep_merge_nodes(value)\n            if new_value is not value:\n                raise ValueError('Deep merging mismatched mapping nodes')\n            else:\n                value.parent = key\n                value.key = key\n        else:\n            yield (key, value)\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in result:\n                raise ValueError('Duplicate key {} in {}'.format(\n                    node.value, node.__class__.__name__))\n            result.append(node)\n        else:\n            result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value not in result:\n                result.append(node.value)\n        else:\n            if node not in result:\n                result.append(node)\n\n    return result\n", "def deep_merge_nodes(nodes):\n\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in result:\n                result[node.value].append(node)\n            else:\n                result.append(node)\n        else:\n            result.append(node)\n    return result\n", "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value not in result:\n                result.append(node.value)\n        elif isinstance(node, ScalarNode):\n            if node.value not in result:\n                result.append(node.value)\n        else:\n            if node.value not in result:\n                result.append(node.value)\n    return result\n", "def deep_merge_nodes(nodes):\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        elif isinstance(node, ScalarNode):\n            if node.value is None:\n                result.append(node)\n            else:\n                result.append(ScalarNode(node.tag, node.value))\n        else:\n            result.append(node)\n\n    return result\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            new_node = deep_merge_nodes(node.value)\n            if node.value is not None:\n                new_node.value = None\n            result.append((node, new_node))\n        else:\n            result.append(node)\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    # type: (List[Tuple[Node, MappingNode, Node]]) -> List[Node]\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the\n    form of:\n\n        (\n            ruamel.yaml.nodes.ScalarNode as a key,\n            ruamel.yaml.nodes.MappingNode or other Node as a value,\n        )\n\n   ...\n    deep merge any node values corresponding to duplicate keys and return\n    the result.\n\n    If there are colliding keys with non-MappingNode values (e.g., integers or\n    strings), the last of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[(\n                    ScalarNode(tag='tag:yaml.org,2002:str'", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, (MappingNode, ScalarNode)):\n            new_node = deep_merge_mapping_node(node)\n        else:\n            new_node = node\n\n        if isinstance(new_node, MappingNode):\n            new_node = deep_merge_mapping_node(new_node)\n\n        if isinstance(new_node, ScalarNode):\n            new_node.tag = node.tag\n\n        result.append(new_node)\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        else:\n            result.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    for key, value in nodes:\n        if isinstance(value, MappingNode):\n            new_value = deep_merge_nodes(value)\n            if new_value is not value:\n                raise ValueError('Deep merging mismatched mapping nodes')\n            else:\n                value.parent = key\n                value.key = key\n        else:\n            yield (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in result:\n                raise ValueError('Duplicate key {} in {}'.format(\n                    node.value, node.__class__.__name__))\n            result.append(node)\n        else:\n            result.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value not in result:\n                result.append(node.value)\n        else:\n            if node not in result:\n                result.append(node)\n\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value in result:\n                result[node.value].append(node)\n            else:\n                result.append(node)\n        else:\n            result.append(node)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            if node.value not in result:\n                result.append(node.value)\n        elif isinstance(node, ScalarNode):\n            if node.value not in result:\n                result.append(node.value)\n        else:\n            if node.value not in result:\n                result.append(node.value)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n    result = []\n\n    for node in nodes:\n        if isinstance(node, MappingNode):\n            result.append(deep_merge_nodes(node.value))\n        elif isinstance(node, ScalarNode):\n            if node.value is None:\n                result.append(node)\n            else:\n                result.append(ScalarNode(node.tag, node.value))\n        else:\n            result.append(node)\n\n    return result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "ques_id": "62b438b666fea644fe22cc78", "code": ["def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        if is_argument_instance(argument):\n            parser.add_argument(argument.get_name(), **argument.get_default_values())\n        else:\n            parser.add_argument(argument)\n    return parser\n", "def parse_arguments(*arguments):\n    parser = ArgumentParser(prog='prosper')\n\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Generate a coverage report for a module.'\n    )\n\n    parser.add_argument(\n       'module',\n        help='Module to generate coverage report for'\n    )\n\n    parser.add_argument(\n        '--junitxml', action='store_true',\n        help='Output the junit xml file for the module.'\n    )\n\n    parser.add_argument(\n        '--failed', action='store_true',\n        help='Output the failed tests status for the module.'\n    )\n\n    parser.add_argument(\n        '--elapsed', action='store_true',\n        help='Output the elapsed time for the module.'\n    )\n\n    parser.add_argument(\n        '--max-failures', action='store',\n        default='0',\n        type=int,\n        help='Maximum number of failures to run the test suite for.'\n    )\n\n    parse", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return parser\n", "def parse_arguments(*arguments):\n\n    parser = argparse.ArgumentParser(\n        description='Test the installation of a remote SSH key on an '\n                    'SSH server.'\n    )\n\n    parser.add_argument(\n        '--ssh-key',\n        metavar='<ssh-key>',\n        help='The SSH key to test for.'\n    )\n\n    parser.add_argument(\n        '--ssh-user',\n        metavar='<ssh-user>',\n        help='The SSH user to test for.'\n    )\n\n    parser.add_argument(\n        '--ssh-key-file',\n        metavar='<ssh-key-file>',\n        help='The SSH key file to use for testing.'\n    )\n\n    parser.add_argument(\n        '--ssh-key-pass',\n        metavar='<ssh-key-pass>',\n        help='The SSH password to use for testing.'\n    )\n\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test that the command line tool works.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"--version\",\n        action=\"store_true\",\n        help=\"Print the version number and exit.\",\n    )\n    parser.add_argument(\n        \"--no-coverage\",\n        action=\"store_true\",\n        help=\"Do not run coverage when testing.\",\n    )\n    parser.add_argument(\n        \"--failfast\",\n        action=\"store_true\",\n        help=\"Run the test suite in fail-fast.\",\n    )\n    parser.add_argument(\n        \"--no-pdb\",\n        action=\"store_true\",\n        help=\"Do not print debugging information about the process of executing.\",\n    )\n    parser.add_argument(\n        \"--failure-regex\",\n        action=\"append\",\n        help=\"Regex for failing tests (e.g. 'test_", "def parse_arguments(*arguments):\n\n    parser = argparse.ArgumentParser(\n        description=\"Just a Fibonnaci demonstration\")\n\n    for arg in arguments:\n        parser.add_argument(*arg[0], **arg[1])\n\n    return parser\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Just a Fibonnaci demonstration\")\n\n    parser.add_argument(\n        'n', type=int, help='The sum of the n most fibonnaci numbers')\n\n    return parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Test script for converting a PNG to JPEG.'\n    )\n\n    parser.add_argument(\n        '-o', '--output-file',\n        help='Output file to write the JPEG to.',\n        required=False,\n        default='-',\n        type=argparse.FileType('w')\n    )\n\n    parser.add_argument(\n        '-i', '--input-file',\n        help='Input PNG file.',\n        required=False,\n        default='-',\n        type=argparse.FileType('rb')\n    )\n\n    parser.add_argument(\n        '-s', '--size',\n        help='Size of the JPEG to convert.',\n        required=False,\n        default='200',\n        type=int\n    )\n\n    parser.add_argument(\n        '-v', '--verbose',\n        help='Verbose mode.',\n  "], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        if is_argument_instance(argument):\n            parser.add_argument(argument.get_name(), **argument.get_default_values())\n        else:\n            parser.add_argument(argument)\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = ArgumentParser(prog='prosper')\n\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for arg in arguments:\n        parser.add_argument(*arg.args, **arg.kwargs)\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Generate a coverage report for a module.'\n    )\n\n    parser.add_argument(\n       'module',\n        help='Module to generate coverage report for'\n    )\n\n    parser.add_argument(\n        '--junitxml', action='store_true',\n        help='Output the junit xml file for the module.'\n    )\n\n    parser.add_argument(\n        '--failed', action='store_true',\n        help='Output the failed tests status for the module.'\n    )\n\n    parser.add_argument(\n        '--elapsed', action='store_true',\n        help='Output the elapsed time for the module.'\n    )\n\n    parser.add_argument(\n        '--max-failures', action='store',\n        default='0',\n        type=int,\n        help='Maximum number of failures to run the test suite for.'\n    )\n\n    parse", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser()\n    for argument in arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\n    parser = argparse.ArgumentParser(\n        description='Test the installation of a remote SSH key on an '\n                    'SSH server.'\n    )\n\n    parser.add_argument(\n        '--ssh-key',\n        metavar='<ssh-key>',\n        help='The SSH key to test for.'\n    )\n\n    parser.add_argument(\n        '--ssh-user',\n        metavar='<ssh-user>',\n        help='The SSH user to test for.'\n    )\n\n    parser.add_argument(\n        '--ssh-key-file',\n        metavar='<ssh-key-file>',\n        help='The SSH key file to use for testing.'\n    )\n\n    parser.add_argument(\n        '--ssh-key-pass',\n        metavar='<ssh-key-pass>',\n        help='The SSH password to use for testing.'\n    )\n\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test that the command line tool works.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"--version\",\n        action=\"store_true\",\n        help=\"Print the version number and exit.\",\n    )\n    parser.add_argument(\n        \"--no-coverage\",\n        action=\"store_true\",\n        help=\"Do not run coverage when testing.\",\n    )\n    parser.add_argument(\n        \"--failfast\",\n        action=\"store_true\",\n        help=\"Run the test suite in fail-fast.\",\n    )\n    parser.add_argument(\n        \"--no-pdb\",\n        action=\"store_true\",\n        help=\"Do not print debugging information about the process of executing.\",\n    )\n    parser.add_argument(\n        \"--failure-regex\",\n        action=\"append\",\n        help=\"Regex for failing tests (e.g. 'test_", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\n    parser = argparse.ArgumentParser(\n        description=\"Just a Fibonnaci demonstration\")\n\n    for arg in arguments:\n        parser.add_argument(*arg[0], **arg[1])\n\n    return parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Just a Fibonnaci demonstration\")\n\n    parser.add_argument(\n        'n', type=int, help='The sum of the n most fibonnaci numbers')\n\n    return parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n    parser = argparse.ArgumentParser(\n        description='Test script for converting a PNG to JPEG.'\n    )\n\n    parser.add_argument(\n        '-o', '--output-file',\n        help='Output file to write the JPEG to.',\n        required=False,\n        default='-',\n        type=argparse.FileType('w')\n    )\n\n    parser.add_argument(\n        '-i', '--input-file',\n        help='Input PNG file.',\n        required=False,\n        default='-',\n        type=argparse.FileType('rb')\n    )\n\n    parser.add_argument(\n        '-s', '--size',\n        help='Size of the JPEG to convert.',\n        required=False,\n        default='200',\n        type=int\n    )\n\n    parser.add_argument(\n        '-v', '--verbose',\n        help='Verbose mode.',\n  ", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62b438b666fea644fe22cc72", "code": ["def parser_flags(parser):\n    return''.join(a.description for a in parser._actions)\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +='{}'.format(arg.flags)\n        else:\n            flags +='{} {}'.format(arg.flags, arg.dest)\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +=''\n        else:\n            flags += arg.dest +''\n    return flags\n", "def parser_flags(parser):\n    return''.join(v for v in vars(parser).values() if v)\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg._long_opts:\n            flags +='' + arg._long_opts[0]\n        if arg._opt_actions:\n            flags +='' + arg._opt_actions[0]\n    return flags\n", "def parser_flags(parser):\n    return''.join(str(f) for f in parser.flags)\n", "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags.append(arg.default)\n        else:\n            flags.append(arg.takes_arg(arg.dest))\n    return''.join(flags)\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.nargs == 0:\n            flags +=''\n        else:\n            flags +='-'\n        flags += str(arg.default)\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        if flag._group_name == '__flags':\n            flags ='' + flags\n    return flags\n", "def parser_flags(parser):\n    return''.join(str(f) for f in parser._flags)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n    return''.join(a.description for a in parser._actions)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +='{}'.format(arg.flags)\n        else:\n            flags +='{} {}'.format(arg.flags, arg.dest)\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +=''\n        else:\n            flags += arg.dest +''\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(v for v in vars(parser).values() if v)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg._long_opts:\n            flags +='' + arg._long_opts[0]\n        if arg._opt_actions:\n            flags +='' + arg._opt_actions[0]\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(str(f) for f in parser.flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags.append(arg.default)\n        else:\n            flags.append(arg.takes_arg(arg.dest))\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.nargs == 0:\n            flags +=''\n        else:\n            flags +='-'\n        flags += str(arg.default)\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        if flag._group_name == '__flags':\n            flags ='' + flags\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(str(f) for f in parser._flags)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "bash_completion", "docstring": "Return a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.", "ques_id": "62b438b566fea644fe22cc70", "code": ["def bash_completion():\n    return '\\n'.join(_bash_completion())\n", "def bash_completion():\n    bash_completion = \"\"\"\\\n        bash -c'source {}'\n    \"\"\".format(\n        os.path.join(BASH_COMMAND_PREFIX, 'bin', 'bash.sh')\n    )\n    return bash_completion\n", "def bash_completion():\n    return '\\n'.join('  --{} {}'.format(k, v) for k, v in vars(argparse.ArgumentParser()).items() if not k.startswith('--'))\n", "def bash_completion():\n    return '\\n'.join('    --{} {}'.format(k, v) for k, v in sorted(vars(argparse.ArgumentParser()).items()) if k.startswith('bash_'))\n", "def bash_completion():\n    return '\\n'.join('    --{} {}'.format(k.replace('_', '-'), v)\n                     for k, v in sorted(vars(bash).items()) if not k.startswith('_'))\n", "def bash_completion():\n    return \"\\n\".join([\n        \"%%bash -c '%s'\" % p for p in _bash_completion_prompts()\n    ])\n", "def bash_completion():\n    bash_completion_template = textwrap.dedent(\"\"\"\\\n        #!/usr/bin/env bash\n        export PATH=$PATH:$PATH\n        export BORG_EXECUTABLE=$PATH:$PATH\n        export PYTHONPATH=$PYTHONPATH:$PYTHONPATH\n        \"\"\")\n\n    return bash_completion_template\n", "def bash_completion():\n    return '\\n'.join(_bash_completion)\n", "def bash_completion():\n    return '\\n'.join(_bash_completion.format(name=name)\n                     for name, _, _ in _bash_completion_inputs())\n", "def bash_completion():\n    bash_completion_template = \"\"\"\n            # Bash completion for the {0} command.\n            set -e\n            set -z\n            set -d\n            export PATH=$PATH:$PATH\n            export BORGAUTH_PYTHON=$PYTHONPATH\n            export BORGAUTH_PYTHON_VERSION=$PYTHON_VERSION\n            export PYTHONPATH=$PYTHONPATH\n            \"\"\"\n    return bash_completion_template.format(__command__)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def bash_completion():\n    return '\\n'.join(_bash_completion())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    bash_completion = \"\"\"\\\n        bash -c'source {}'\n    \"\"\".format(\n        os.path.join(BASH_COMMAND_PREFIX, 'bin', 'bash.sh')\n    )\n    return bash_completion\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return '\\n'.join('  --{} {}'.format(k, v) for k, v in vars(argparse.ArgumentParser()).items() if not k.startswith('--'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return '\\n'.join('    --{} {}'.format(k, v) for k, v in sorted(vars(argparse.ArgumentParser()).items()) if k.startswith('bash_'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return '\\n'.join('    --{} {}'.format(k.replace('_', '-'), v)\n                     for k, v in sorted(vars(bash).items()) if not k.startswith('_'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return \"\\n\".join([\n        \"%%bash -c '%s'\" % p for p in _bash_completion_prompts()\n    ])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    bash_completion_template = textwrap.dedent(\"\"\"\\\n        #!/usr/bin/env bash\n        export PATH=$PATH:$PATH\n        export BORG_EXECUTABLE=$PATH:$PATH\n        export PYTHONPATH=$PYTHONPATH:$PYTHONPATH\n        \"\"\")\n\n    return bash_completion_template\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return '\\n'.join(_bash_completion)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    return '\\n'.join(_bash_completion.format(name=name)\n                     for name, _, _ in _bash_completion_inputs())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n    bash_completion_template = \"\"\"\n            # Bash completion for the {0} command.\n            set -e\n            set -z\n            set -d\n            export PATH=$PATH:$PATH\n            export BORGAUTH_PYTHON=$PYTHONPATH\n            export BORGAUTH_PYTHON_VERSION=$PYTHON_VERSION\n            export PYTHONPATH=$PYTHONPATH\n            \"\"\"\n    return bash_completion_template.format(__command__)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "ques_id": "62b438a266fea644fe22cc2e", "code": ["def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for unparsed_argument in unparsed_arguments:\n        parser.add_argument(*unparsed_argument.args, **unparsed_argument.kwargs)\n    return vars(parser.parse_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for name, argument_list in unparsed_arguments:\n        for argument in argument_list:\n            parser.add_argument(*argument.args, **argument.kwargs)\n    return vars(parser.parse_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test the command line tool.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    parser.add_argument(\n        \"--test-path\",\n        default=\".\",\n        help=\"Path to the directory containing the test files. \"\n        \"Defaults to the directory containing the.helloworld \"\n        \"file, or the current directory if no directory is given.\",\n    )\n\n    parsed_arguments = parser.parse_args(*unparsed_arguments)\n\n    if parsed_arguments.test_path is None:\n        parsed_arguments.test_path = os.getcwd()\n\n    return {\n        \"test_path\": parsed_arguments.test_path,\n    }\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return vars(parser.parse_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = create_argument_parser()\n    return vars(parser.parse_args(unparsed_arguments))\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n\n    return vars(parser.parse_args())\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test that the command line tool outputs the expected output.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return vars(parser.parse_args(unparsed_arguments))\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test that the --test-output-dir option works correctly\"\n    )\n    subparsers = parser.add_subparsers(dest=\"subcommand\")\n    for subcommand, subcommand_parser in six.iteritems(PARSERS):\n        parser = subparsers.add_parser(subcommand)\n        subcommand_parser.add_arguments(parser)\n    for argument in unparsed_arguments:\n        subparser_name = PARSERS[argument.command][\"subcommand\"]\n        parser_name = PARSERS[argument.command][\"global\"]\n        subparser = subparsers.add_parser(subparser_name)\n        parser.add_argument(*argument.args, **argument.kwargs)\n        parser.set_defaults(**parser_namespace(argument))\n    return {subcommand: parser.parse_args() for subcommand, parser in six.iteritems(PARSERS)}\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    args = parser.parse_args()\n    return {arg.name: arg for arg in args.__args__}\n", "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    for (subcommand, parser) in PARSERS:\n        parser = parser(\n            parents=[subparsers],\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter\n        )\n        parser.add_argument('--%s' % subcommand, default=None, required=False)\n        parser.add_argument('--%s-args' % subcommand, default=None)\n\n    for argument in unparsed_arguments:\n        parser = parser.add_argument(\n            '--%s-args' % argument.replace('-', '_'), default=None)\n        parser.add_argument('--%s-kwargs' % argument.replace('-', '_'), default=None)\n\n    return dict((subcommand, parser.parse_args(*unparsed_arguments))\n                for subcommand, parser in PARSERS)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for unparsed_argument in unparsed_arguments:\n        parser.add_argument(*unparsed_argument.args, **unparsed_argument.kwargs)\n    return vars(parser.parse_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for name, argument_list in unparsed_arguments:\n        for argument in argument_list:\n            parser.add_argument(*argument.args, **argument.kwargs)\n    return vars(parser.parse_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test the command line tool.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    parser.add_argument(\n        \"--test-path\",\n        default=\".\",\n        help=\"Path to the directory containing the test files. \"\n        \"Defaults to the directory containing the.helloworld \"\n        \"file, or the current directory if no directory is given.\",\n    )\n\n    parsed_arguments = parser.parse_args(*unparsed_arguments)\n\n    if parsed_arguments.test_path is None:\n        parsed_arguments.test_path = os.getcwd()\n\n    return {\n        \"test_path\": parsed_arguments.test_path,\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return vars(parser.parse_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = create_argument_parser()\n    return vars(parser.parse_args(unparsed_arguments))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n\n    return vars(parser.parse_args())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test that the command line tool outputs the expected output.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    return vars(parser.parse_args(unparsed_arguments))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Test that the --test-output-dir option works correctly\"\n    )\n    subparsers = parser.add_subparsers(dest=\"subcommand\")\n    for subcommand, subcommand_parser in six.iteritems(PARSERS):\n        parser = subparsers.add_parser(subcommand)\n        subcommand_parser.add_arguments(parser)\n    for argument in unparsed_arguments:\n        subparser_name = PARSERS[argument.command][\"subcommand\"]\n        parser_name = PARSERS[argument.command][\"global\"]\n        subparser = subparsers.add_parser(subparser_name)\n        parser.add_argument(*argument.args, **argument.kwargs)\n        parser.set_defaults(**parser_namespace(argument))\n    return {subcommand: parser.parse_args() for subcommand, parser in six.iteritems(PARSERS)}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    for argument in unparsed_arguments:\n        parser.add_argument(*argument.args, **argument.kwargs)\n    args = parser.parse_args()\n    return {arg.name: arg for arg in args.__args__}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    for (subcommand, parser) in PARSERS:\n        parser = parser(\n            parents=[subparsers],\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter\n        )\n        parser.add_argument('--%s' % subcommand, default=None, required=False)\n        parser.add_argument('--%s-args' % subcommand, default=None)\n\n    for argument in unparsed_arguments:\n        parser = parser.add_argument(\n            '--%s-args' % argument.replace('-', '_'), default=None)\n        parser.add_argument('--%s-kwargs' % argument.replace('-', '_'), default=None)\n\n    return dict((subcommand, parser.parse_args(*unparsed_arguments))\n                for subcommand, parser in PARSERS)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "ques_id": "62b438a266fea644fe22cc2d", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    not_parsed_arguments = []\n    for subparser in subparsers.values():\n        try:\n            parsed_arguments = subparser.parse_args(unparsed_arguments)\n        except SystemExit:\n            raise\n        except Exception as exc:\n            not_parsed_arguments.append(subparser.name)\n            raise exc\n\n        if parsed_arguments is not None:\n            return (parsed_arguments, not_parsed_arguments)\n        else:\n            not_parsed_arguments.clear()\n    raise SystemExit(\n        \"Unable to parse arguments from arguments provided by subparsers: {}\"\n       .format(not_parsed_arguments)\n    )\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_args = []\n    for subparser in subparsers:\n        parser = subparser.add_subparsers(dest=subparser.dest, help=subparser.help)\n        parsed_arguments[subparser.name] = parser\n        unparsed_args.extend(subparser.parse_args(unparsed_arguments))\n    return parsed_arguments, unparsed_args\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_arguments.update(parsed_arguments)\n    remaining_arguments = []\n    unparsed_arguments.pop(\"--\", None)\n    for name, parser in subparsers.items():\n        parser_name = parser.name\n        parsed_arguments[name] = parser.parse_args(unparsed_arguments[name])\n        remaining_arguments.append(parsed_arguments[name])\n        if parser.description is not None:\n            parsed_arguments[name].description = parser.description\n        if parser.epilog is not None:\n            parsed_arguments[name].epilog = parser.epilog\n        if parser.formatter_class is not None:\n            parsed_arguments[name].formatter_class = parser.formatter_class\n        if parser.error_handler is not None:\n            parsed_arguments[name].error_handler = parser.error_handler\n        if parser.version is not None:\n            parsed_arguments[name].version = parse", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    unparsed_arguments = set(unparsed_arguments)\n\n    for namespace_parser in subparsers.values():\n        for name in unparsed_arguments:\n            arguments[name] = namespace_parser.parse_args(unparsed_arguments.pop(name))\n        if not unparsed_arguments:\n            break\n\n        remaining_arguments.append(namespace_parser)\n\n    for namespace_parser in remaining_arguments:\n        namespace_parser.parse_args(unparsed_arguments)\n\n    return arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for arg in unparsed_arguments:\n        parser_name = \"--{}\".format(arg)\n        parser = subparsers[parser_name].add_argument_group(\n            description=arg,\n        )\n        arguments[arg] = parser.parse_args(unparsed_arguments[arg])\n        remaining_arguments.append(arguments[arg])\n        parser.set_defaults(**arguments[arg])\n    return (arguments, remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser in subparsers._actions:\n        args = unparsed_arguments.pop(0)\n        if args:\n            subparser_name = \"--%s\" % subparser._dest\n            parser = subparsers._actions[subparser_name]\n            arguments[subparser_name] = parser.parse_args(args)\n            remaining_arguments.extend(unparsed_arguments)\n            unparsed_arguments.clear()\n        else:\n            unparsed_arguments.append(subparser)\n\n    return arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for subparser_name, subparser in subparsers.items():\n        if subparser_name not in unparsed_arguments:\n            continue\n        args = unparsed_arguments[subparser_name]\n        args = subparser.parse_args(args)\n        parsed_arguments[subparser_name] = args\n        remaining_arguments.extend(args.args)\n    return parsed_arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n\n    for arg in unparsed_arguments:\n        parser = subparsers.add_parser(arg.name, help=arg.help)\n        parser.set_defaults(func=partial(arg.func, arguments, remaining_arguments))\n        arguments[arg.name] = parser\n        remaining_arguments.append(arg)\n\n    return (arguments, remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    not_claimed_arguments = []\n    for name, parser in subparsers.items():\n        if name not in unparsed_arguments:\n            continue\n        parser_arguments = parser.parse_args(unparsed_arguments[name])\n        parsed_arguments.update(parser_arguments)\n        not_claimed_arguments.append(name)\n    return parsed_arguments, not_claimed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    unparsed_arguments = list(unparsed_arguments)\n\n    while unparsed_arguments:\n        parser_name = unparsed_arguments.pop(0)\n        if parser_name in subparsers:\n            parser = subparsers[parser_name]\n            parsed_arguments = parser.parse_args(unparsed_arguments)\n            remaining_arguments.append(parsed_arguments)\n        else:\n            parsed_arguments = {}\n            for subparser_name in subparsers:\n                if subparser_name == parser_name:\n                    continue\n                parser = subparsers[subparser_name]\n                parsed_arguments = parser.parse_args(unparsed_arguments)\n                remaining_arguments.append(parsed_arguments)\n            unparsed_arguments = remaining_arguments\n  "], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    not_parsed_arguments = []\n    for subparser in subparsers.values():\n        try:\n            parsed_arguments = subparser.parse_args(unparsed_arguments)\n        except SystemExit:\n            raise\n        except Exception as exc:\n            not_parsed_arguments.append(subparser.name)\n            raise exc\n\n        if parsed_arguments is not None:\n            return (parsed_arguments, not_parsed_arguments)\n        else:\n            not_parsed_arguments.clear()\n    raise SystemExit(\n        \"Unable to parse arguments from arguments provided by subparsers: {}\"\n       .format(not_parsed_arguments)\n    )\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_args = []\n    for subparser in subparsers:\n        parser = subparser.add_subparsers(dest=subparser.dest, help=subparser.help)\n        parsed_arguments[subparser.name] = parser\n        unparsed_args.extend(subparser.parse_args(unparsed_arguments))\n    return parsed_arguments, unparsed_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    unparsed_arguments.update(parsed_arguments)\n    remaining_arguments = []\n    unparsed_arguments.pop(\"--\", None)\n    for name, parser in subparsers.items():\n        parser_name = parser.name\n        parsed_arguments[name] = parser.parse_args(unparsed_arguments[name])\n        remaining_arguments.append(parsed_arguments[name])\n        if parser.description is not None:\n            parsed_arguments[name].description = parser.description\n        if parser.epilog is not None:\n            parsed_arguments[name].epilog = parser.epilog\n        if parser.formatter_class is not None:\n            parsed_arguments[name].formatter_class = parser.formatter_class\n        if parser.error_handler is not None:\n            parsed_arguments[name].error_handler = parser.error_handler\n        if parser.version is not None:\n            parsed_arguments[name].version = parse", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    unparsed_arguments = set(unparsed_arguments)\n\n    for namespace_parser in subparsers.values():\n        for name in unparsed_arguments:\n            arguments[name] = namespace_parser.parse_args(unparsed_arguments.pop(name))\n        if not unparsed_arguments:\n            break\n\n        remaining_arguments.append(namespace_parser)\n\n    for namespace_parser in remaining_arguments:\n        namespace_parser.parse_args(unparsed_arguments)\n\n    return arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for arg in unparsed_arguments:\n        parser_name = \"--{}\".format(arg)\n        parser = subparsers[parser_name].add_argument_group(\n            description=arg,\n        )\n        arguments[arg] = parser.parse_args(unparsed_arguments[arg])\n        remaining_arguments.append(arguments[arg])\n        parser.set_defaults(**arguments[arg])\n    return (arguments, remaining_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n    for subparser in subparsers._actions:\n        args = unparsed_arguments.pop(0)\n        if args:\n            subparser_name = \"--%s\" % subparser._dest\n            parser = subparsers._actions[subparser_name]\n            arguments[subparser_name] = parser.parse_args(args)\n            remaining_arguments.extend(unparsed_arguments)\n            unparsed_arguments.clear()\n        else:\n            unparsed_arguments.append(subparser)\n\n    return arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    for subparser_name, subparser in subparsers.items():\n        if subparser_name not in unparsed_arguments:\n            continue\n        args = unparsed_arguments[subparser_name]\n        args = subparser.parse_args(args)\n        parsed_arguments[subparser_name] = args\n        remaining_arguments.extend(args.args)\n    return parsed_arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    arguments = {}\n    remaining_arguments = []\n\n    for arg in unparsed_arguments:\n        parser = subparsers.add_parser(arg.name, help=arg.help)\n        parser.set_defaults(func=partial(arg.func, arguments, remaining_arguments))\n        arguments[arg.name] = parser\n        remaining_arguments.append(arg)\n\n    return (arguments, remaining_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    not_claimed_arguments = []\n    for name, parser in subparsers.items():\n        if name not in unparsed_arguments:\n            continue\n        parser_arguments = parser.parse_args(unparsed_arguments[name])\n        parsed_arguments.update(parser_arguments)\n        not_claimed_arguments.append(name)\n    return parsed_arguments, not_claimed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    parsed_arguments = {}\n    remaining_arguments = []\n    unparsed_arguments = list(unparsed_arguments)\n\n    while unparsed_arguments:\n        parser_name = unparsed_arguments.pop(0)\n        if parser_name in subparsers:\n            parser = subparsers[parser_name]\n            parsed_arguments = parser.parse_args(unparsed_arguments)\n            remaining_arguments.append(parsed_arguments)\n        else:\n            parsed_arguments = {}\n            for subparser_name in subparsers:\n                if subparser_name == parser_name:\n                    continue\n                parser = subparsers[subparser_name]\n                parsed_arguments = parser.parse_args(unparsed_arguments)\n                remaining_arguments.append(parsed_arguments)\n            unparsed_arguments = remaining_arguments\n  ", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "ques_id": "62b438a266fea644fe22cc2c", "code": ["def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test command line tool for generating test files.')\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    test_gen_parser = subparsers.add_parser(\n        'test-gen',\n        help='Generate test files for the given project.')\n    test_gen_parser.add_argument(\n        '--output-dir',\n        type=str,\n        default='./',\n        help='Directory to output generated files.')\n    test_gen_parser.add_argument(\n        '--source-dir',\n        type=str,\n        default='./',\n        help='Directory to load test source files.')\n    test_gen_parser.add_argument(\n        '--version',\n        action='store_true',\n        help='Print the version number and exit.')\n    test_gen_parser.set_defaults(func=gen_files)\n\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='A command line tool to parse and understand the '\n                    'GitHub API documentation.')\n\n    subparsers = parser.add_subparsers(dest='command')\n\n    parser_help = (\n        'A command line tool to parse and understand the '\n        'GitHub API documentation.'\n    )\n\n    subparser = subparsers.add_parser(\n        'help',\n        help=parser_help,\n        description='A command line tool to display the '\n                    'documentation of a command.'\n    )\n    subparser.add_argument(\n        'command',\n        metavar='COMMAND',\n        nargs='?',\n        help='The command to be documented.'\n    )\n\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Convert a CSV to YAML from a given JSON file.')\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    convert_to_yaml.add_parser(\n        subparsers, help='Convert a CSV to YAML from a given JSON file.')\n\n    split_args = parser.add_mutually_exclusive_group(required=False)\n    split_args.add_argument(\n        '-o', '--output', type=str, help='Output directory.',\n        required=False, default='.')\n    split_args.add_argument(\n        '-s', '--split', type=str, help='Split files by comma.',\n        required=False, default='')\n\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Simple test to verify that the CLI works properly\")\n    subparsers = parser.add_subparsers(dest=\"command\")\n    add_data(parser, subparsers)\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser(description='Generate a new Test List.')\n    subparsers = parser.add_subparsers()\n    add_test_files(subparsers)\n    return (parser, subparsers)\n", "def make_parsers():\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers()\n\n    upload_parser = subparsers.add_parser(\n        'upload', description='Upload a new document to an existing '\n       'repository.')\n    upload_parser.add_argument(\n        '--repository', type=str, help='Repository to upload to.',\n        default='.', required=False)\n    upload_parser.add_argument(\n        '--schema', type=str, help='The schema file to use.', default=None,\n        required=False)\n\n    download_parser = subparsers.add_parser(\n        'download', description='Download a new document from an '\n        'existing repository.')\n    download_parser.add_argument(\n        '--repository', type=str, help='Repository to download from.',\n        default='.', required=False)\n    download_parser.add_argument(\n        '--schema', type=str, help='The schema file to use.', default=None,\n ", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Test that subcommands can be called with argparse \"\n        \"options.\")\n    subparsers = parser.add_subparsers(dest=\"subcommand\")\n\n    for i in range(2):\n        subparser = subparsers.add_parser(\"subcommand-{}\".format(i))\n        subparser.add_argument(\"--arg1\", type=int)\n        subparser.add_argument(\"--arg2\", type=int)\n\n    return parser, subparsers\n", "def make_parsers():\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='command')\n\n    build_parser = subparsers.add_parser(\n        'build',\n        help='Build a single package'\n    )\n    build_parser.add_argument(\n        'package',\n        help='Package name to build'\n    )\n    build_parser.set_defaults(\n        func=build\n    )\n\n    clean_parser = subparsers.add_parser(\n        'clean',\n        help='Clean up the package'\n    )\n    clean_parser.add_argument(\n        'package',\n        help='Package name to clean'\n    )\n    clean_parser.set_defaults(\n        func=clean\n    )\n\n    install_parser = subparsers.add_parser(\n        'install',\n        help='Install a package'\n    )\n    install_parser.add_argument(\n        'package',\n        help='Package name to install'\n    )\n    install", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Commands related to a given project.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    subparsers = parser.add_subparsers(dest='subcommand')\n    subparsers.required = True\n    build_commands = subparsers.add_parser(\n        'build', help='Build the given project.'\n    )\n    build_commands.add_argument(\n        '--all', action='store_true', help='Build all packages.'\n    )\n    build_commands.add_argument(\n        '--skip-existing', action='store_true', help='Skip previous builds.'\n    )\n    build_commands.add_argument(\n        '--no-output', action='store_true', help='Do not print output to stdout.'\n    )\n    build_commands.set_defaults(func=build)\n    install_commands = subparsers.add_parser(\n        'install', help='Install the given project.'\n    )\n    install_commands.add_argument(\n        '--all', action=", "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Program that parses JSON files from a given URL')\n    parser.add_argument('url', help='URL of the JSON file to parse')\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    parse = subparsers.add_parser('parse')\n    parse.add_argument('--json', dest='jsonfile', required=True,\n                        help='Name of the JSON file to parse')\n    parse.set_defaults(func=parse)\n\n    return parser, subparsers\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Test command line tool for generating test files.')\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    test_gen_parser = subparsers.add_parser(\n        'test-gen',\n        help='Generate test files for the given project.')\n    test_gen_parser.add_argument(\n        '--output-dir',\n        type=str,\n        default='./',\n        help='Directory to output generated files.')\n    test_gen_parser.add_argument(\n        '--source-dir',\n        type=str,\n        default='./',\n        help='Directory to load test source files.')\n    test_gen_parser.add_argument(\n        '--version',\n        action='store_true',\n        help='Print the version number and exit.')\n    test_gen_parser.set_defaults(func=gen_files)\n\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='A command line tool to parse and understand the '\n                    'GitHub API documentation.')\n\n    subparsers = parser.add_subparsers(dest='command')\n\n    parser_help = (\n        'A command line tool to parse and understand the '\n        'GitHub API documentation.'\n    )\n\n    subparser = subparsers.add_parser(\n        'help',\n        help=parser_help,\n        description='A command line tool to display the '\n                    'documentation of a command.'\n    )\n    subparser.add_argument(\n        'command',\n        metavar='COMMAND',\n        nargs='?',\n        help='The command to be documented.'\n    )\n\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Convert a CSV to YAML from a given JSON file.')\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    convert_to_yaml.add_parser(\n        subparsers, help='Convert a CSV to YAML from a given JSON file.')\n\n    split_args = parser.add_mutually_exclusive_group(required=False)\n    split_args.add_argument(\n        '-o', '--output', type=str, help='Output directory.',\n        required=False, default='.')\n    split_args.add_argument(\n        '-s', '--split', type=str, help='Split files by comma.',\n        required=False, default='')\n\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Simple test to verify that the CLI works properly\")\n    subparsers = parser.add_subparsers(dest=\"command\")\n    add_data(parser, subparsers)\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(description='Generate a new Test List.')\n    subparsers = parser.add_subparsers()\n    add_test_files(subparsers)\n    return (parser, subparsers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers()\n\n    upload_parser = subparsers.add_parser(\n        'upload', description='Upload a new document to an existing '\n       'repository.')\n    upload_parser.add_argument(\n        '--repository', type=str, help='Repository to upload to.',\n        default='.', required=False)\n    upload_parser.add_argument(\n        '--schema', type=str, help='The schema file to use.', default=None,\n        required=False)\n\n    download_parser = subparsers.add_parser(\n        'download', description='Download a new document from an '\n        'existing repository.')\n    download_parser.add_argument(\n        '--repository', type=str, help='Repository to download from.',\n        default='.', required=False)\n    download_parser.add_argument(\n        '--schema', type=str, help='The schema file to use.', default=None,\n ", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description=\"Test that subcommands can be called with argparse \"\n        \"options.\")\n    subparsers = parser.add_subparsers(dest=\"subcommand\")\n\n    for i in range(2):\n        subparser = subparsers.add_parser(\"subcommand-{}\".format(i))\n        subparser.add_argument(\"--arg1\", type=int)\n        subparser.add_argument(\"--arg2\", type=int)\n\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='command')\n\n    build_parser = subparsers.add_parser(\n        'build',\n        help='Build a single package'\n    )\n    build_parser.add_argument(\n        'package',\n        help='Package name to build'\n    )\n    build_parser.set_defaults(\n        func=build\n    )\n\n    clean_parser = subparsers.add_parser(\n        'clean',\n        help='Clean up the package'\n    )\n    clean_parser.add_argument(\n        'package',\n        help='Package name to clean'\n    )\n    clean_parser.set_defaults(\n        func=clean\n    )\n\n    install_parser = subparsers.add_parser(\n        'install',\n        help='Install a package'\n    )\n    install_parser.add_argument(\n        'package',\n        help='Package name to install'\n    )\n    install", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Commands related to a given project.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    subparsers = parser.add_subparsers(dest='subcommand')\n    subparsers.required = True\n    build_commands = subparsers.add_parser(\n        'build', help='Build the given project.'\n    )\n    build_commands.add_argument(\n        '--all', action='store_true', help='Build all packages.'\n    )\n    build_commands.add_argument(\n        '--skip-existing', action='store_true', help='Skip previous builds.'\n    )\n    build_commands.add_argument(\n        '--no-output', action='store_true', help='Do not print output to stdout.'\n    )\n    build_commands.set_defaults(func=build)\n    install_commands = subparsers.add_parser(\n        'install', help='Install the given project.'\n    )\n    install_commands.add_argument(\n        '--all', action=", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n    parser = argparse.ArgumentParser(\n        description='Program that parses JSON files from a given URL')\n    parser.add_argument('url', help='URL of the JSON file to parse')\n    subparsers = parser.add_subparsers(dest='subcommand')\n\n    parse = subparsers.add_parser('parse')\n    parse.add_argument('--json', dest='jsonfile', required=True,\n                        help='Name of the JSON file to parse')\n    parse.set_defaults(func=parse)\n\n    return parser, subparsers\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_10.py", "name": "paging", "docstring": "Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.", "ques_id": "62ece4982e6aefcf4aabbd5f", "code": ["def paging(response, max_results):\n\n    return (response[i:i+max_results] for i in range(0, len(response), max_results))\n", "def paging(response, max_results):\n\n    if not isinstance(response, list):\n        raise ValueError(\"response must be a list\")\n\n    if len(response) > max_results:\n        response = response[:max_results]\n\n    for item in response:\n        yield item\n", "def paging(response, max_results):\n    if len(response) < max_results:\n        yield response\n        return\n\n    start_index = 0\n    while start_index < len(response):\n        end_index = start_index + max_results\n        if end_index < len(response):\n            yield response[start_index:end_index]\n            start_index = end_index\n        else:\n            break\n", "def paging(response, max_results):\n\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]\n", "def paging(response, max_results):\n    offset = 0\n    while offset < max_results:\n        for item in response:\n            yield item\n        offset += max_results\n", "def paging(response, max_results):\n    return (item for item in response[0:max_results])\n", "def paging(response, max_results):\n\n    start = 0\n    end = 0\n    next_start = start\n    next_end = start\n\n    while start < max_results:\n\n        if end == max_results:\n            return\n\n        yield response[start:end]\n\n        if next_start >= end:\n            return\n\n        start += 1\n        next_start = end\n        next_end = start\n", "def paging(response, max_results):\n\n    if max_results <= 0:\n        raise ValueError('Maximum number of results must be greater than zero')\n\n    if len(response) < max_results:\n        for i in range(len(response), max_results):\n            yield response[i:]\n    else:\n        for i in range(len(response), max_results, -1):\n            yield response[i:]\n", "def paging(response, max_results):\n    page = 1\n    while True:\n        if len(response) <= max_results:\n            yield response\n            response = []\n        for item in response:\n            if item[\"meta\"][\"page\"] >= page:\n                response.append(item)\n        page += 1\n        if len(response) >= max_results:\n            break\n", "def paging(response, max_results):\n    if not response:\n        raise ValueError('Response is empty')\n\n    page = 1\n    last_results = None\n    while page <= max_results:\n        for result in response:\n            yield result\n        if last_results and page <= last_results:\n            return\n        page += 1\n        last_results = len(response)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def paging(response, max_results):\n\n    return (response[i:i+max_results] for i in range(0, len(response), max_results))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\n    if not isinstance(response, list):\n        raise ValueError(\"response must be a list\")\n\n    if len(response) > max_results:\n        response = response[:max_results]\n\n    for item in response:\n        yield item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    if len(response) < max_results:\n        yield response\n        return\n\n    start_index = 0\n    while start_index < len(response):\n        end_index = start_index + max_results\n        if end_index < len(response):\n            yield response[start_index:end_index]\n            start_index = end_index\n        else:\n            break\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n    offset = 0\n    while offset < max_results:\n        for item in response:\n            yield item\n        offset += max_results\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    return (item for item in response[0:max_results])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\n    start = 0\n    end = 0\n    next_start = start\n    next_end = start\n\n    while start < max_results:\n\n        if end == max_results:\n            return\n\n        yield response[start:end]\n\n        if next_start >= end:\n            return\n\n        start += 1\n        next_start = end\n        next_end = start\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\n    if max_results <= 0:\n        raise ValueError('Maximum number of results must be greater than zero')\n\n    if len(response) < max_results:\n        for i in range(len(response), max_results):\n            yield response[i:]\n    else:\n        for i in range(len(response), max_results, -1):\n            yield response[i:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    page = 1\n    while True:\n        if len(response) <= max_results:\n            yield response\n            response = []\n        for item in response:\n            if item[\"meta\"][\"page\"] >= page:\n                response.append(item)\n        page += 1\n        if len(response) >= max_results:\n            break\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n    if not response:\n        raise ValueError('Response is empty')\n\n    page = 1\n    last_results = None\n    while page <= max_results:\n        for result in response:\n            yield result\n        if last_results and page <= last_results:\n            return\n        page += 1\n        last_results = len(response)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_20.py", "name": "size_to_bytes", "docstring": "Convert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000", "ques_id": "62ece4982e6aefcf4aabbd60", "code": ["def size_to_bytes(size: str) -> int:\n    return humanfriendly.parse_size(size)\n", "def size_to_bytes(size: str) -> int:\n    for x in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if size.endswith(x):\n            num = float(size[:-len(x)])\n            return int(num * 1024)\n    num = float(size[:-len(\"B\")])\n    den = float(size[-len(\"B\"):])\n    return int(num / den)\n", "def size_to_bytes(size: str) -> int:\n    match = _FILE_SIZE_RE.match(size)\n    if not match:\n        raise ValueError(\"Unable to parse file size '%s'\" % size)\n\n    num, unit = match.groups()\n    num = int(num)\n    if unit == \"B\":\n        return num * _BYTES_IN_BYTES\n    elif unit == \"KB\":\n        return num * _BYTES_IN_KB\n    elif unit == \"MB\":\n        return num * _BYTES_IN_MB\n    elif unit == \"GB\":\n        return num * _BYTES_IN_GB\n    elif unit == \"TB\":\n        return num * _BYTES_IN_TB\n    elif unit == \"PB\":\n        return num * _BYTES_IN_PB\n    elif unit == \"EB\":\n        return num * _BYTES_IN_EB\n    else:\n        raise ValueError(\"Unknown unit %s\" % unit)\n", "def size_to_bytes(size: str) -> int:\n    for x in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            num = int(size[:-len(x)])\n            return num * (1024 ** x)\n    raise ValueError('Invalid file size \"%s\".' % size)\n", "def size_to_bytes(size: str) -> int:\n    if size is None or not isinstance(size, str):\n        raise ValueError(\"Invalid input format\")\n\n    try:\n        size = int(size)\n    except ValueError:\n        raise ValueError(\"Invalid input format\")\n\n    for unit in (\"B\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if size < 1024:\n            return size\n        size /= 1024.0\n\n    raise ValueError(\"Invalid input format\")\n", "def size_to_bytes(size: str) -> int:\n    converter = humanfriendly.Unit(\"B\")\n    return int(round(converter.to_base_unit(float(size))))\n", "def size_to_bytes(size: str) -> int:\n    for x in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            return int(float(size.rstrip(x)))\n    raise ValueError('Could not convert size to bytes')\n", "def size_to_bytes(size: str) -> int:\n    return humanfriendly.parse_size(size)\n", "def size_to_bytes(size: str) -> int:\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            num = float(size.strip(x))\n            if num > 10:\n                return int(num) * 1024\n            else:\n                return int(num)\n    return int(size)\n", "def size_to_bytes(size: str) -> int:\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            size = size[:-len(x)]\n            return int(float(size))\n    raise ValueError(\"Invalid file size {}\".format(size))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def size_to_bytes(size: str) -> int:\n    return humanfriendly.parse_size(size)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for x in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if size.endswith(x):\n            num = float(size[:-len(x)])\n            return int(num * 1024)\n    num = float(size[:-len(\"B\")])\n    den = float(size[-len(\"B\"):])\n    return int(num / den)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    match = _FILE_SIZE_RE.match(size)\n    if not match:\n        raise ValueError(\"Unable to parse file size '%s'\" % size)\n\n    num, unit = match.groups()\n    num = int(num)\n    if unit == \"B\":\n        return num * _BYTES_IN_BYTES\n    elif unit == \"KB\":\n        return num * _BYTES_IN_KB\n    elif unit == \"MB\":\n        return num * _BYTES_IN_MB\n    elif unit == \"GB\":\n        return num * _BYTES_IN_GB\n    elif unit == \"TB\":\n        return num * _BYTES_IN_TB\n    elif unit == \"PB\":\n        return num * _BYTES_IN_PB\n    elif unit == \"EB\":\n        return num * _BYTES_IN_EB\n    else:\n        raise ValueError(\"Unknown unit %s\" % unit)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for x in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            num = int(size[:-len(x)])\n            return num * (1024 ** x)\n    raise ValueError('Invalid file size \"%s\".' % size)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    if size is None or not isinstance(size, str):\n        raise ValueError(\"Invalid input format\")\n\n    try:\n        size = int(size)\n    except ValueError:\n        raise ValueError(\"Invalid input format\")\n\n    for unit in (\"B\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if size < 1024:\n            return size\n        size /= 1024.0\n\n    raise ValueError(\"Invalid input format\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    converter = humanfriendly.Unit(\"B\")\n    return int(round(converter.to_base_unit(float(size))))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for x in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            return int(float(size.rstrip(x)))\n    raise ValueError('Could not convert size to bytes')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    return humanfriendly.parse_size(size)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            num = float(size.strip(x))\n            if num > 10:\n                return int(num) * 1024\n            else:\n                return int(num)\n    return int(size)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if size.endswith(x):\n            size = size[:-len(x)]\n            return int(float(size))\n    raise ValueError(\"Invalid file size {}\".format(size))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_14.py", "name": "_dictsum", "docstring": "Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}", "ques_id": "62ece4982e6aefcf4aabbd61", "code": ["def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            if k not in result:\n                result[k] = v\n            else:\n                result[k] += v\n    return result\n", "def _dictsum(dicts):\n    return dict(reduce(lambda d, kv: d.update({k: d[k] + kv[0]}), dicts, {}))\n", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "def _dictsum(dicts):\n    return dict(reduce(operator.add, map(dict.items, dicts)))\n", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "def _dictsum(dicts):\n    return dict(sum(d.items(), ()) for d in dicts)\n", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result.setdefault(k, 0)\n            result[k] += v\n    return result\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            if k not in result:\n                result[k] = v\n            else:\n                result[k] += v\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n    return dict(reduce(lambda d, kv: d.update({k: d[k] + kv[0]}), dicts, {}))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(reduce(operator.add, map(dict.items, dicts)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    return dict(sum(d.items(), ()) for d in dicts)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result.setdefault(k, 0)\n            result[k] += v\n    return result\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_15.py", "name": "_replace_url_args", "docstring": "Replace any custom string URL items with values in args", "ques_id": "62ece4982e6aefcf4aabbd62", "code": ["def _replace_url_args(url, url_args):\n    if url is None:\n        return None\n\n    # Replace any custom string URL items with values in args\n    return url.format(**url_args)\n", "def _replace_url_args(url, url_args):\n    return _replace_args(url, url_args, URL_REGEX)\n", "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        url = re.sub('{{{0}}}'.format(key), value, url)\n    return url\n", "def _replace_url_args(url, url_args):\n    # We need to handle non-strings here because otherwise requests will\n    # pass through unmodified data when sending requests to the API. See\n    # https://github.com/shazow/requests/issues/2373\n    for k, v in url_args.items():\n        if type(v) == str:\n            url.args[k] = v.format(**url.args)\n        else:\n            url.args[k] = v\n    return url\n", "def _replace_url_args(url, url_args):\n    return _replace_variables(url, url_args, URL_REGEX)\n", "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        url = re.sub(\"{{{0}}}\".format(key), value, url)\n    return url\n", "def _replace_url_args(url, url_args):\n    return _replace_args(url, urlparse(url).query, url_args)\n", "def _replace_url_args(url, url_args):\n\n    if not isinstance(url, string_types):\n        return url\n\n    for key, value in url_args.items():\n        url = url.replace('{{' + key + '}}', value)\n\n    return url\n", "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        url = re.sub(r'{{{0}}}'.format(key), value, url)\n    return url\n", "def _replace_url_args(url, url_args):\n    return _replace_variables(url, url_args, URL_REGEX)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_url_args(url, url_args):\n    if url is None:\n        return None\n\n    # Replace any custom string URL items with values in args\n    return url.format(**url_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return _replace_args(url, url_args, URL_REGEX)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        url = re.sub('{{{0}}}'.format(key), value, url)\n    return url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    # We need to handle non-strings here because otherwise requests will\n    # pass through unmodified data when sending requests to the API. See\n    # https://github.com/shazow/requests/issues/2373\n    for k, v in url_args.items():\n        if type(v) == str:\n            url.args[k] = v.format(**url.args)\n        else:\n            url.args[k] = v\n    return url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return _replace_variables(url, url_args, URL_REGEX)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        url = re.sub(\"{{{0}}}\".format(key), value, url)\n    return url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return _replace_args(url, urlparse(url).query, url_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\n    if not isinstance(url, string_types):\n        return url\n\n    for key, value in url_args.items():\n        url = url.replace('{{' + key + '}}', value)\n\n    return url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    for key, value in url_args.items():\n        url = re.sub(r'{{{0}}}'.format(key), value, url)\n    return url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n    return _replace_variables(url, url_args, URL_REGEX)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_5.py", "name": "is_none_string", "docstring": "Check if a string represents a None value.", "ques_id": "62ece4982e6aefcf4aabbd63", "code": ["def is_none_string(val: any) -> bool:\n    return val is None or (isinstance(val, str) and val.lower() == 'none')\n", "def is_none_string(val: any) -> bool:\n    return isinstance(val, string_types) and val == 'None'\n", "def is_none_string(val: any) -> bool:\n    return isinstance(val, str) and val.lower() in (\"none\", \"null\")\n", "def is_none_string(val: any) -> bool:\n    return val in ('None', 'null', 'null.')\n", "def is_none_string(val: any) -> bool:\n    return isinstance(val, str) and val.lower() in _NONE_STRING\n", "def is_none_string(val: any) -> bool:\n    return val is None or isinstance(val, str) and val.lower() in _NONE_VALUES\n", "def is_none_string(val: any) -> bool:\n    return val is None\n", "def is_none_string(val: any) -> bool:\n    return val is None\n", "def is_none_string(val: any) -> bool:\n    return val is None or isinstance(val, str) and val.lower() in _none_strings\n", "def is_none_string(val: any) -> bool:\n    return val is None or isinstance(val, str) and val.lower() in _NONE_STRING\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_none_string(val: any) -> bool:\n    return val is None or (isinstance(val, str) and val.lower() == 'none')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n    return isinstance(val, string_types) and val == 'None'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n    return isinstance(val, str) and val.lower() in (\"none\", \"null\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n    return val in ('None', 'null', 'null.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n    return isinstance(val, str) and val.lower() in _NONE_STRING\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n    return val is None or isinstance(val, str) and val.lower() in _NONE_VALUES\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n    return val is None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n    return val is None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n    return val is None or isinstance(val, str) and val.lower() in _none_strings\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n    return val is None or isinstance(val, str) and val.lower() in _NONE_STRING\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_1.py", "name": "strip_root", "docstring": "Remove root from path, throw exception on failure.", "ques_id": "62ece4982e6aefcf4aabbd64", "code": ["def strip_root(path, root):\n    if not path:\n        return root\n    if path == root:\n        raise NoSuchPath(\"'%s' is the root of '%s'\" % (path, root))\n    return path.rstrip(root)\n", "def strip_root(path, root):\n    if path == root:\n        return path\n    elif path.startswith(root + os.sep):\n        return path[len(root) + 1:]\n    else:\n        return None\n", "def strip_root(path, root):\n    if not root:\n        return path\n    if path == root:\n        raise ValueError('Root cannot be the same as the path')\n    return path[len(root):]\n", "def strip_root(path, root):\n    if not root:\n        return path\n    if path == root:\n        return ''\n    if path.startswith(root + os.sep):\n        return path[len(root) + 1:]\n    return ''\n", "def strip_root(path, root):\n    if not root:\n        return path\n    if path.startswith(root):\n        return path[len(root):]\n    return path\n", "def strip_root(path, root):\n    if path == root:\n        return path\n\n    if path.startswith(root + os.sep):\n        return path[len(root) + 1:]\n\n    return path\n", "def strip_root(path, root):\n    if not root:\n        return path\n    if path == root:\n        raise Exception(\"Cannot remove root from path: {0}\".format(path))\n    return path[len(root):]\n", "def strip_root(path, root):\n    if path == root:\n        return path\n    elif path.startswith(root + os.sep):\n        return path[len(root):]\n    else:\n        return None\n", "def strip_root(path, root):\n    if not path:\n        return root\n    if path == root:\n        raise ValueError('path is the same as root')\n    if path[0] == root[0]:\n        path = path[1:]\n    return path\n", "def strip_root(path, root):\n    if not root:\n        return path\n    if path == root:\n        # Sanity check.\n        if not os.path.exists(os.path.join(path, '.git')):\n            return path\n        raise Exception('Root \"%s\" is not clean' % root)\n    if path == root + '/':\n        return path\n    return strip_root(path, root + '/')\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def strip_root(path, root):\n    if not path:\n        return root\n    if path == root:\n        raise NoSuchPath(\"'%s' is the root of '%s'\" % (path, root))\n    return path.rstrip(root)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if path == root:\n        return path\n    elif path.startswith(root + os.sep):\n        return path[len(root) + 1:]\n    else:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if not root:\n        return path\n    if path == root:\n        raise ValueError('Root cannot be the same as the path')\n    return path[len(root):]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if not root:\n        return path\n    if path == root:\n        return ''\n    if path.startswith(root + os.sep):\n        return path[len(root) + 1:]\n    return ''\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if not root:\n        return path\n    if path.startswith(root):\n        return path[len(root):]\n    return path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if path == root:\n        return path\n\n    if path.startswith(root + os.sep):\n        return path[len(root) + 1:]\n\n    return path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if not root:\n        return path\n    if path == root:\n        raise Exception(\"Cannot remove root from path: {0}\".format(path))\n    return path[len(root):]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if path == root:\n        return path\n    elif path.startswith(root + os.sep):\n        return path[len(root):]\n    else:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if not path:\n        return root\n    if path == root:\n        raise ValueError('path is the same as root')\n    if path[0] == root[0]:\n        path = path[1:]\n    return path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n    if not root:\n        return path\n    if path == root:\n        # Sanity check.\n        if not os.path.exists(os.path.join(path, '.git')):\n            return path\n        raise Exception('Root \"%s\" is not clean' % root)\n    if path == root + '/':\n        return path\n    return strip_root(path, root + '/')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_8.py", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62ece4982e6aefcf4aabbd65", "code": ["def parser_flags(parser):\n    flags = ''\n    for flag in parser._actions:\n        if flag.nargs == 0:\n            flags +=''\n        flags += flag.dest +''\n    return flags.strip()\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +=''\n        else:\n            flags += arg.dest +''\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.nargs == 0:\n            flags +=''\n        else:\n            flags +='-'\n        flags += arg.dest\n    return flags\n", "def parser_flags(parser):\n    flags = []\n    for flag in parser._get_args():\n        if flag.nargs == 0:\n            flags.append('-')\n        else:\n            flags.append('--' + flag.flag_name)\n    return''.join(flags)\n", "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        if flag._action_name == '_':\n            continue\n        flags +='' + flag._action_flags\n    return flags\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +='{0}'.format(arg.help)\n        else:\n            flags +='{0} {1}'.format(arg.help, arg.dest)\n    return flags\n", "def parser_flags(parser):\n    return''.join(a.argparse_args[0] for a in parser._actions)\n", "def parser_flags(parser):\n    flags = []\n    for flag in parser._actions:\n        if flag.dest is None:\n            flags.append('-' + flag.default)\n        else:\n            flags.append('-' + flag.dest)\n    return''.join(flags)\n", "def parser_flags(parser):\n    return''.join(flag.name for flag in parser._flags)\n", "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            continue\n        if arg.default is not None and arg.default is not False:\n            flags +='{}'.format(arg.dest)\n        elif arg.default is True:\n            flags +='-{}'.format(arg.dest)\n        else:\n            flags +='{}'.format(arg.dest)\n    return flags\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n    flags = ''\n    for flag in parser._actions:\n        if flag.nargs == 0:\n            flags +=''\n        flags += flag.dest +''\n    return flags.strip()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +=''\n        else:\n            flags += arg.dest +''\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.nargs == 0:\n            flags +=''\n        else:\n            flags +='-'\n        flags += arg.dest\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for flag in parser._get_args():\n        if flag.nargs == 0:\n            flags.append('-')\n        else:\n            flags.append('--' + flag.flag_name)\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for flag in parser._action_groups:\n        if flag._action_name == '_':\n            continue\n        flags +='' + flag._action_flags\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            flags +='{0}'.format(arg.help)\n        else:\n            flags +='{0} {1}'.format(arg.help, arg.dest)\n    return flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(a.argparse_args[0] for a in parser._actions)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = []\n    for flag in parser._actions:\n        if flag.dest is None:\n            flags.append('-' + flag.default)\n        else:\n            flags.append('-' + flag.dest)\n    return''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    return''.join(flag.name for flag in parser._flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n    flags = ''\n    for arg in parser._actions:\n        if arg.dest is None:\n            continue\n        if arg.default is not None and arg.default is not False:\n            flags +='{}'.format(arg.dest)\n        elif arg.default is True:\n            flags +='-{}'.format(arg.dest)\n        else:\n            flags +='{}'.format(arg.dest)\n    return flags\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_19.py", "name": "was_processed", "docstring": "Check if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.", "ques_id": "62ece4982e6aefcf4aabbd66", "code": ["def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"ERROR: '%s' already processed. It will be overwritten.\" % path_name)\n        return False\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Adding %s to processed set.\" % path_name)\n        if os.path.isdir(path_name):\n            if verbose:\n                print(\"Adding directory to processed set.\")\n            for entry in os.listdir(path_name):\n                was_processed(processed, os.path.join(path_name, entry), verbose)\n        elif os.path.isfile(path_name):\n            if verbose:\n                print(\"Adding file to processed set.\")\n            if os.path.islink(path_name):\n                if verbose:\n                    print(\"Linking file to processed set.\")\n    ", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print('{0} already processed {1}'.format(path_name, processed))\n        return True\n\n    processed.add(path_name)\n    if os.path.isdir(path_name):\n        if verbose:\n            print('{0} already processed {1}'.format(path_name, processed))\n        return True\n\n    if verbose:\n        print('Adding {0} to processed set'.format(path_name))\n\n    if os.path.isfile(path_name):\n        if verbose:\n            print('{0} already processed {1}'.format(path_name, processed))\n        return False\n\n    processed.add(os.path.join(path_name, os.path.basename(path_name)))\n    if verbose:\n        print('Adding {0} to processed set'.format(os.path.join(path_name, os.path.basename(path_name))))\n\n    if os.path.isdir(path_name):\n        if verbose:\n            print('{0} already", "def was_processed(processed, path_name, verbose):\n    if processed.get(path_name):\n        if verbose:\n            print(\"Skipping due to recursion.\")\n        return False\n    processed[path_name] = True\n    if verbose:\n        print(\"Processing:\", path_name)\n    if os.path.isdir(path_name):\n        for entry in os.listdir(path_name):\n            was_processed(processed, os.path.join(path_name, entry), verbose)\n    elif os.path.isfile(path_name):\n        if verbose:\n            print(\"Skipping entry:\", path_name)\n        return False\n    return True\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"{} already processed. Returning False\".format(path_name))\n        return False\n    if os.path.isdir(path_name):\n        if verbose:\n            print(\"{} already processed. Returning False\".format(path_name))\n        return False\n    processed.add(path_name)\n    if verbose:\n        print(\"Adding {} to processed\".format(path_name))\n    return True\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(f\"Adding {path_name} to processed set\")\n        return False\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"WARNING: It seems that the file \"\n                  \"'{}' already exists and is being processed.\".format(\n                      path_name))\n        return True\n    processed.add(path_name)\n    if verbose:\n        print(\"Adding {} to the set.\".format(path_name))\n    if os.path.isdir(path_name):\n        if verbose:\n            print(\"WARNING: It seems that the directory \"\n                  \"'{}' already exists and is being processed.\".format(\n                      path_name))\n        return False\n    if verbose:\n        print(\"Adding {} to the set.\".format(path_name))\n    processed.add(os.path.join(path_name, os.path.basename(path_name)))\n    if verbose:\n        print(\"Returning True if it's already in the set.\")\n    return True", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"Skipping {0} because it already exists\".format(path_name))\n        return True\n    if os.path.isdir(path_name):\n        if verbose:\n            print(\"Skipping {0} because it is a directory\".format(path_name))\n        return True\n    if os.path.isfile(path_name):\n        if verbose:\n            print(\"Skipping {0} because it already exists\".format(path_name))\n        return True\n    processed.add(path_name)\n    if verbose:\n        print(\"Adding {0} to the set\".format(path_name))\n    return False\n", "def was_processed(processed, path_name, verbose):\n    # Get the path to the entry\n    entry = get_entry_path(path_name, verbose)\n\n    # If the entry isn't in the set, add it\n    if entry not in processed:\n        processed.add(entry)\n        if verbose:\n            print(\"Adding\", entry)\n\n        # Try to expand the path name to an abssolution path call so that\n        # the absolute paths return the same values for every entry\n        expanded_path_name = os.path.expanduser(entry)\n\n        # Try to add the entry to the set\n        if expanded_path_name not in processed:\n            processed.add(expanded_path_name)\n            if verbose:\n                print(\"Adding\", expanded_path_name)\n\n    return True\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"The file '{}' already in the set.\".format(path_name))\n        return False\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Adding the file '{}' to the set.\".format(path_name))\n        return True\n", "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"File already processed: %s\" % path_name)\n        return True\n    processed.add(path_name)\n    if verbose:\n        print(\"Adding file: %s\" % path_name)\n    try:\n        if os.path.isdir(path_name):\n            for entry in os.listdir(path_name):\n                was_processed(processed, os.path.join(path_name, entry), verbose)\n        else:\n            if os.path.isfile(path_name):\n                processed.add(path_name)\n    except OSError:\n        print(\"Error occured when processing file: %s\" % path_name)\n        if verbose:\n            print(\"Error message: %s\" % sys.exc_info()[1])\n    return False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"ERROR: '%s' already processed. It will be overwritten.\" % path_name)\n        return False\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Adding %s to processed set.\" % path_name)\n        if os.path.isdir(path_name):\n            if verbose:\n                print(\"Adding directory to processed set.\")\n            for entry in os.listdir(path_name):\n                was_processed(processed, os.path.join(path_name, entry), verbose)\n        elif os.path.isfile(path_name):\n            if verbose:\n                print(\"Adding file to processed set.\")\n            if os.path.islink(path_name):\n                if verbose:\n                    print(\"Linking file to processed set.\")\n    ", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print('{0} already processed {1}'.format(path_name, processed))\n        return True\n\n    processed.add(path_name)\n    if os.path.isdir(path_name):\n        if verbose:\n            print('{0} already processed {1}'.format(path_name, processed))\n        return True\n\n    if verbose:\n        print('Adding {0} to processed set'.format(path_name))\n\n    if os.path.isfile(path_name):\n        if verbose:\n            print('{0} already processed {1}'.format(path_name, processed))\n        return False\n\n    processed.add(os.path.join(path_name, os.path.basename(path_name)))\n    if verbose:\n        print('Adding {0} to processed set'.format(os.path.join(path_name, os.path.basename(path_name))))\n\n    if os.path.isdir(path_name):\n        if verbose:\n            print('{0} already", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if processed.get(path_name):\n        if verbose:\n            print(\"Skipping due to recursion.\")\n        return False\n    processed[path_name] = True\n    if verbose:\n        print(\"Processing:\", path_name)\n    if os.path.isdir(path_name):\n        for entry in os.listdir(path_name):\n            was_processed(processed, os.path.join(path_name, entry), verbose)\n    elif os.path.isfile(path_name):\n        if verbose:\n            print(\"Skipping entry:\", path_name)\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"{} already processed. Returning False\".format(path_name))\n        return False\n    if os.path.isdir(path_name):\n        if verbose:\n            print(\"{} already processed. Returning False\".format(path_name))\n        return False\n    processed.add(path_name)\n    if verbose:\n        print(\"Adding {} to processed\".format(path_name))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        return True\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(f\"Adding {path_name} to processed set\")\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"WARNING: It seems that the file \"\n                  \"'{}' already exists and is being processed.\".format(\n                      path_name))\n        return True\n    processed.add(path_name)\n    if verbose:\n        print(\"Adding {} to the set.\".format(path_name))\n    if os.path.isdir(path_name):\n        if verbose:\n            print(\"WARNING: It seems that the directory \"\n                  \"'{}' already exists and is being processed.\".format(\n                      path_name))\n        return False\n    if verbose:\n        print(\"Adding {} to the set.\".format(path_name))\n    processed.add(os.path.join(path_name, os.path.basename(path_name)))\n    if verbose:\n        print(\"Returning True if it's already in the set.\")\n    return True", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"Skipping {0} because it already exists\".format(path_name))\n        return True\n    if os.path.isdir(path_name):\n        if verbose:\n            print(\"Skipping {0} because it is a directory\".format(path_name))\n        return True\n    if os.path.isfile(path_name):\n        if verbose:\n            print(\"Skipping {0} because it already exists\".format(path_name))\n        return True\n    processed.add(path_name)\n    if verbose:\n        print(\"Adding {0} to the set\".format(path_name))\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    # Get the path to the entry\n    entry = get_entry_path(path_name, verbose)\n\n    # If the entry isn't in the set, add it\n    if entry not in processed:\n        processed.add(entry)\n        if verbose:\n            print(\"Adding\", entry)\n\n        # Try to expand the path name to an abssolution path call so that\n        # the absolute paths return the same values for every entry\n        expanded_path_name = os.path.expanduser(entry)\n\n        # Try to add the entry to the set\n        if expanded_path_name not in processed:\n            processed.add(expanded_path_name)\n            if verbose:\n                print(\"Adding\", expanded_path_name)\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"The file '{}' already in the set.\".format(path_name))\n        return False\n    else:\n        processed.add(path_name)\n        if verbose:\n            print(\"Adding the file '{}' to the set.\".format(path_name))\n        return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n    if path_name in processed:\n        if verbose:\n            print(\"File already processed: %s\" % path_name)\n        return True\n    processed.add(path_name)\n    if verbose:\n        print(\"Adding file: %s\" % path_name)\n    try:\n        if os.path.isdir(path_name):\n            for entry in os.listdir(path_name):\n                was_processed(processed, os.path.join(path_name, entry), verbose)\n        else:\n            if os.path.isfile(path_name):\n                processed.add(path_name)\n    except OSError:\n        print(\"Error occured when processing file: %s\" % path_name)\n        if verbose:\n            print(\"Error message: %s\" % sys.exc_info()[1])\n    return False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_9.py", "name": "vertex3tuple", "docstring": "return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices", "ques_id": "62ece4982e6aefcf4aabbd67", "code": ["def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        raise ValueError(\"vertices length is less than 3\")\n    elif len(vertices) == 3:\n        return (vertices[0], vertices[1], vertices[2])\n    else:\n        a, b, c = vertices\n        return (a, b, c, a+b+c)\n", "def vertex3tuple(vertices):\n    (A, B, C, D) = vertices\n    return tuple([A, B, C, D, D, B, A])\n", "def vertex3tuple(vertices):\n\n    (x1, y1, z1) = vertices\n    (x2, y2, z2) = (x1 + x2, y1 + y2, z1 + z2)\n    return (x1, y1, z1), (x2, y2, z2), (x2, y2, z1)\n", "def vertex3tuple(vertices):\n\n    return tuple([vertex3pt(v) for v in vertices])\n", "def vertex3tuple(vertices):\n\n    A = vertices[0::3]\n    B = vertices[1::3]\n    C = vertices[2::3]\n\n    ABCD = (A[0] + B[0], A[1] + B[1], A[2] + B[2])\n    CDA = (A[0] + C[0], A[1] + C[1], A[2] + C[2])\n    BCD = (A[0] + C[0] + B[0], A[1] + C[1] + B[1], A[2] + C[2] + B[2])\n    CDB = (A[0] + C[0] + B[0] + B[0], A[1] + C[1] + B[1] + B[1],\n           A[2] + C[2] + B[2] + B[2])\n\n    return ABCD, CDA, BCD, CDB\n", "def vertex3tuple(vertices):\n\n    vertices = np.array(vertices)\n    nvertices = vertices.shape[0]\n\n    C = vertices[:, 0]\n    B = vertices[:, 1]\n    A = vertices[:, 2]\n\n    ABCD = np.cross(A, B)\n\n    CDA = np.cross(B, A)\n\n    BCD = np.cross(A, C)\n    C = B + np.cross(A, C)\n\n    return (A, B, C, ABCD, CDA, BCD)\n", "def vertex3tuple(vertices):\n    return tuple([vertices[0], vertices[1], vertices[2]])\n", "def vertex3tuple(vertices):\n    return tuple([vertex3tuple1(vertices[0]),\n                 vertex3tuple2(vertices[1]),\n                 vertex3tuple2(vertices[2])])\n", "def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        return None\n\n    a = vertices[0]\n    b = vertices[1]\n    c = vertices[2]\n\n    A = a.coordinates\n    B = b.coordinates\n    C = c.coordinates\n\n    ABC = (A[0], B[1], B[2])\n    CDA = (A[1], C[0], C[1])\n    CDB = (B[0], C[1], A[2])\n\n    return (ABC, CDA, CDB)\n", "def vertex3tuple(vertices):\n    a, b, c = vertices\n    return ((a.y, b.y, c.y), (b.y, a.y, c.y), (c.y, b.y, a.y))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        raise ValueError(\"vertices length is less than 3\")\n    elif len(vertices) == 3:\n        return (vertices[0], vertices[1], vertices[2])\n    else:\n        a, b, c = vertices\n        return (a, b, c, a+b+c)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n    (A, B, C, D) = vertices\n    return tuple([A, B, C, D, D, B, A])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    (x1, y1, z1) = vertices\n    (x2, y2, z2) = (x1 + x2, y1 + y2, z1 + z2)\n    return (x1, y1, z1), (x2, y2, z2), (x2, y2, z1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    return tuple([vertex3pt(v) for v in vertices])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    A = vertices[0::3]\n    B = vertices[1::3]\n    C = vertices[2::3]\n\n    ABCD = (A[0] + B[0], A[1] + B[1], A[2] + B[2])\n    CDA = (A[0] + C[0], A[1] + C[1], A[2] + C[2])\n    BCD = (A[0] + C[0] + B[0], A[1] + C[1] + B[1], A[2] + C[2] + B[2])\n    CDB = (A[0] + C[0] + B[0] + B[0], A[1] + C[1] + B[1] + B[1],\n           A[2] + C[2] + B[2] + B[2])\n\n    return ABCD, CDA, BCD, CDB\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    vertices = np.array(vertices)\n    nvertices = vertices.shape[0]\n\n    C = vertices[:, 0]\n    B = vertices[:, 1]\n    A = vertices[:, 2]\n\n    ABCD = np.cross(A, B)\n\n    CDA = np.cross(B, A)\n\n    BCD = np.cross(A, C)\n    C = B + np.cross(A, C)\n\n    return (A, B, C, ABCD, CDA, BCD)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n    return tuple([vertices[0], vertices[1], vertices[2]])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n    return tuple([vertex3tuple1(vertices[0]),\n                 vertex3tuple2(vertices[1]),\n                 vertex3tuple2(vertices[2])])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\n    if len(vertices) < 3:\n        return None\n\n    a = vertices[0]\n    b = vertices[1]\n    c = vertices[2]\n\n    A = a.coordinates\n    B = b.coordinates\n    C = c.coordinates\n\n    ABC = (A[0], B[1], B[2])\n    CDA = (A[1], C[0], C[1])\n    CDB = (B[0], C[1], A[2])\n\n    return (ABC, CDA, CDB)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n    a, b, c = vertices\n    return ((a.y, b.y, c.y), (b.y, a.y, c.y), (c.y, b.y, a.y))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_6.py", "name": "int_to_string", "docstring": "Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.", "ques_id": "62ece4982e6aefcf4aabbd68", "code": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if not alphabet:\n        raise ValueError(\"Alphabet must have at least one element.\")\n    if len(alphabet) == 1 and padding is None:\n        raise ValueError(\"Padding must be specified if one of the alphabets is specified.\")\n\n    if padding is None:\n        padding = 0\n\n    number_str = str(number)\n    if padding > 0:\n        number_str = number_str[-padding:]\n    result = [number_str[i:i + padding] for i in range(0, len(number_str), padding)]\n    return \"\".join(result)\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if padding is None:\n        padding = len(alphabet) * 2\n    result = []\n    while number!= 0:\n        number, mod = divmod(number, len(alphabet))\n        result.append(alphabet[mod])\n    result.reverse()\n    return \"\".join(result[padding:])\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    output = []\n    while number!= 0:\n        output.append(alphabet[number % len(alphabet)])\n        number //= len(alphabet)\n    if padding is not None:\n        for i in range(padding):\n            output.append(alphabet[0])\n    output = ''.join(output[::-1])\n    return output\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if not padding:\n        padding = len(alphabet) * 2\n    str_number = str(number)\n    if len(str_number) < padding:\n        return alphabet[0] + str_number\n    for i in range(1, len(alphabet)):\n        if str_number[-(i + 1)] == alphabet[-(i + 1)]:\n            str_number = str_number[:-(i + 1)]\n            break\n    if len(str_number) < padding:\n        return alphabet[0] + str_number\n    return alphabet[0] + str_number[:-padding]\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    base = len(alphabet)\n    str_number = (number * base) + (padding or 0)\n    result = ''\n    while str_number >= base:\n        div, mod = divmod(str_number, base)\n        result = alphabet[mod] + result\n        str_number = div\n    result = alphabet[str_number] + result\n    return result\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if padding is None:\n        padding = len(alphabet) * 2\n\n    result = []\n    while number:\n        result.append(alphabet[number % len(alphabet)])\n        number //= len(alphabet)\n    result.reverse()\n    if not number:\n        return ''.join(result)\n    return ''.join(result[:padding])\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    string = \"\"\n    while number!= 0:\n        number, mod = divmod(number, len(alphabet))\n        string = alphabet[mod] + string\n    if padding is not None:\n        for i in range(padding):\n            string = alphabet[0] + string\n    return string\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    str_number = str(number)\n    if padding is not None:\n        str_number = str_number[-padding:]\n    for i, alphabet_char in enumerate(alphabet):\n        if str_number[-(i + 1)] in alphabet_char:\n            str_number = str_number[:-(i + 1)] + alphabet_char\n    return str_number\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if padding is None:\n        padding = 0\n\n    base = len(alphabet)\n    output = []\n    while number > 0:\n        number, mod = divmod(number, base)\n        output.append(alphabet[mod])\n    if mod:\n        output.append(alphabet[mod - 1])\n    if padding:\n        output.extend([\"0\", \"1\"][padding:])\n    return \"\".join(reversed(output))\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if number == 0:\n        return \"0\"\n    if padding is None:\n        padding = len(alphabet) - len(alphabet.index(\"1\"))\n    result = []\n    while number:\n        result.append(alphabet[number % len(alphabet)])\n        number //= len(alphabet)\n    result.reverse()\n    return \"\".join(result[padding:])\n"], "level": "self_contained", "generate_results": [{"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if not alphabet:\n        raise ValueError(\"Alphabet must have at least one element.\")\n    if len(alphabet) == 1 and padding is None:\n        raise ValueError(\"Padding must be specified if one of the alphabets is specified.\")\n\n    if padding is None:\n        padding = 0\n\n    number_str = str(number)\n    if padding > 0:\n        number_str = number_str[-padding:]\n    result = [number_str[i:i + padding] for i in range(0, len(number_str), padding)]\n    return \"\".join(result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if padding is None:\n        padding = len(alphabet) * 2\n    result = []\n    while number!= 0:\n        number, mod = divmod(number, len(alphabet))\n        result.append(alphabet[mod])\n    result.reverse()\n    return \"\".join(result[padding:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    output = []\n    while number!= 0:\n        output.append(alphabet[number % len(alphabet)])\n        number //= len(alphabet)\n    if padding is not None:\n        for i in range(padding):\n            output.append(alphabet[0])\n    output = ''.join(output[::-1])\n    return output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if not padding:\n        padding = len(alphabet) * 2\n    str_number = str(number)\n    if len(str_number) < padding:\n        return alphabet[0] + str_number\n    for i in range(1, len(alphabet)):\n        if str_number[-(i + 1)] == alphabet[-(i + 1)]:\n            str_number = str_number[:-(i + 1)]\n            break\n    if len(str_number) < padding:\n        return alphabet[0] + str_number\n    return alphabet[0] + str_number[:-padding]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    base = len(alphabet)\n    str_number = (number * base) + (padding or 0)\n    result = ''\n    while str_number >= base:\n        div, mod = divmod(str_number, base)\n        result = alphabet[mod] + result\n        str_number = div\n    result = alphabet[str_number] + result\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if padding is None:\n        padding = len(alphabet) * 2\n\n    result = []\n    while number:\n        result.append(alphabet[number % len(alphabet)])\n        number //= len(alphabet)\n    result.reverse()\n    if not number:\n        return ''.join(result)\n    return ''.join(result[:padding])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    string = \"\"\n    while number!= 0:\n        number, mod = divmod(number, len(alphabet))\n        string = alphabet[mod] + string\n    if padding is not None:\n        for i in range(padding):\n            string = alphabet[0] + string\n    return string\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    str_number = str(number)\n    if padding is not None:\n        str_number = str_number[-padding:]\n    for i, alphabet_char in enumerate(alphabet):\n        if str_number[-(i + 1)] in alphabet_char:\n            str_number = str_number[:-(i + 1)] + alphabet_char\n    return str_number\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if padding is None:\n        padding = 0\n\n    base = len(alphabet)\n    output = []\n    while number > 0:\n        number, mod = divmod(number, base)\n        output.append(alphabet[mod])\n    if mod:\n        output.append(alphabet[mod - 1])\n    if padding:\n        output.extend([\"0\", \"1\"][padding:])\n    return \"\".join(reversed(output))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    if number == 0:\n        return \"0\"\n    if padding is None:\n        padding = len(alphabet) - len(alphabet.index(\"1\"))\n    result = []\n    while number:\n        result.append(alphabet[number % len(alphabet)])\n        number //= len(alphabet)\n    result.reverse()\n    return \"\".join(result[padding:])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_12.py", "name": "_replace_register", "docstring": "Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number", "ques_id": "62ece4982e6aefcf4aabbd69", "code": ["def _replace_register(flow_params, register_number, register_value):\n    for key in flow_params.keys():\n        if key.startswith(register_number + '_'):\n            flow_params[key] = flow_params[key].replace(register_number, register_value)\n", "def _replace_register(flow_params, register_number, register_value):\n    for k, v in flow_params.items():\n        if k == 'idle_timeout':\n            flow_params[k] = flow_params[k].replace(\n                idle_timeout=register_value)\n        else:\n            flow_params[k][register_number] = register_value\n", "def _replace_register(flow_params, register_number, register_value):\n    if register_value in flow_params[register_number]:\n        flow_params[register_number] = flow_params[register_number].replace(register_value, \"\")\n", "def _replace_register(flow_params, register_number, register_value):\n    for key in flow_params:\n        if key =='register_number':\n            flow_params[key] = flow_params[key].replace(register_number, register_value)\n        else:\n            flow_params[key] = flow_params[key].replace(flow_params[key], register_value)\n", "def _replace_register(flow_params, register_number, register_value):\n    if flow_params is None:\n        return\n    if register_number is None:\n        return\n    if register_value is None:\n        return\n    for key in flow_params:\n        if key == \"flows\":\n            flow_params[key] = flow_params[key].replace(flow_params[key], register_value)\n        if key == \"registers\":\n            flow_params[key] = flow_params[key].replace(register_params[register_number], register_value)\n", "def _replace_register(flow_params, register_number, register_value):\n    for item in flow_params['registers']:\n        if item['register_number'] == register_number:\n            item['value'] = register_value\n", "def _replace_register(flow_params, register_number, register_value):\n\n    if register_value in flow_params:\n        flow_params[register_value] = flow_params[register_number]\n", "def _replace_register(flow_params, register_number, register_value):\n    for key in flow_params:\n        if register_number == 0:\n            flow_params[key] = flow_params[key].replace(register_number, register_value)\n        else:\n            flow_params[key] = flow_params[key].replace(register_number, register_value)\n", "def _replace_register(flow_params, register_number, register_value):\n    for key, value in flow_params.items():\n        if key == \"instructions\":\n            flow_params[key] = [instruction.replace(register_number, register_value) for instruction in value]\n        elif key == \"stack\":\n            flow_params[key] = [stack.replace(register_number, register_value) for stack in value]\n        elif key == \"locals\":\n            flow_params[key] = {k: local.replace(register_number, register_value) for k, local in value.items()}\n        elif key == \"assignments\":\n            flow_params[key] = {k: assignment.replace(register_number, register_value) for k, assignment in value.items()}\n        elif key == \"program\":\n            flow_params[key] = [program.replace(register_number, register_value) for program in value]\n        elif key == \"constants\":\n            flow_params[key] = [constant.replace(register_number, register_value) for constant in valu", "def _replace_register(flow_params, register_number, register_value):\n    if register_value in flow_params[register_number]:\n        flow_params[register_number] = flow_params[register_number].replace(\n            register_value,\n            flow_params[register_number].replace(register_value, ''))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for key in flow_params.keys():\n        if key.startswith(register_number + '_'):\n            flow_params[key] = flow_params[key].replace(register_number, register_value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for k, v in flow_params.items():\n        if k == 'idle_timeout':\n            flow_params[k] = flow_params[k].replace(\n                idle_timeout=register_value)\n        else:\n            flow_params[k][register_number] = register_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    if register_value in flow_params[register_number]:\n        flow_params[register_number] = flow_params[register_number].replace(register_value, \"\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for key in flow_params:\n        if key =='register_number':\n            flow_params[key] = flow_params[key].replace(register_number, register_value)\n        else:\n            flow_params[key] = flow_params[key].replace(flow_params[key], register_value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    if flow_params is None:\n        return\n    if register_number is None:\n        return\n    if register_value is None:\n        return\n    for key in flow_params:\n        if key == \"flows\":\n            flow_params[key] = flow_params[key].replace(flow_params[key], register_value)\n        if key == \"registers\":\n            flow_params[key] = flow_params[key].replace(register_params[register_number], register_value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for item in flow_params['registers']:\n        if item['register_number'] == register_number:\n            item['value'] = register_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\n    if register_value in flow_params:\n        flow_params[register_value] = flow_params[register_number]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for key in flow_params:\n        if register_number == 0:\n            flow_params[key] = flow_params[key].replace(register_number, register_value)\n        else:\n            flow_params[key] = flow_params[key].replace(register_number, register_value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    for key, value in flow_params.items():\n        if key == \"instructions\":\n            flow_params[key] = [instruction.replace(register_number, register_value) for instruction in value]\n        elif key == \"stack\":\n            flow_params[key] = [stack.replace(register_number, register_value) for stack in value]\n        elif key == \"locals\":\n            flow_params[key] = {k: local.replace(register_number, register_value) for k, local in value.items()}\n        elif key == \"assignments\":\n            flow_params[key] = {k: assignment.replace(register_number, register_value) for k, assignment in value.items()}\n        elif key == \"program\":\n            flow_params[key] = [program.replace(register_number, register_value) for program in value]\n        elif key == \"constants\":\n            flow_params[key] = [constant.replace(register_number, register_value) for constant in valu", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n    if register_value in flow_params[register_number]:\n        flow_params[register_number] = flow_params[register_number].replace(\n            register_value,\n            flow_params[register_number].replace(register_value, ''))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_2.py", "name": "replace_dots", "docstring": "Replaces all values of '.' to arg from the given string", "ques_id": "62ece4982e6aefcf4aabbd6a", "code": ["def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "def replace_dots(value, arg):\n    return value.replace('.', arg)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n    return value.replace('.', arg)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_3.py", "name": "subclasses", "docstring": "Return all subclasses of a class, recursively", "ques_id": "62ece4982e6aefcf4aabbd6b", "code": ["def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n    yield cls\n", "def subclasses(cls):\n    for subcls in cls.__subclasses__():\n        yield subcls\n        yield from subclasses(subcls)\n", "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield subclass\n        yield from subclasses(subclass)\n", "def subclasses(cls):\n    for subcls in cls.__subclasses__():\n        yield subcls\n        yield from subclasses(subcls)\n", "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n    yield cls\n", "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield subclass\n        yield from subclasses(subclass)\n", "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n    yield cls\n", "def subclasses(cls):\n    for subcls in cls.__subclasses__():\n        yield subcls\n        yield from subclasses(subcls)\n", "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n    yield cls\n"], "level": "self_contained", "generate_results": [{"generate_code": "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n    yield cls\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    for subcls in cls.__subclasses__():\n        yield subcls\n        yield from subclasses(subcls)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield subclass\n        yield from subclasses(subclass)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    for subcls in cls.__subclasses__():\n        yield subcls\n        yield from subclasses(subcls)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n    yield cls\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield subclass\n        yield from subclasses(subclass)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n    yield cls\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    for subcls in cls.__subclasses__():\n        yield subcls\n        yield from subclasses(subcls)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n    yield cls\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_7.py", "name": "string_to_int", "docstring": "Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.", "ques_id": "62ece4982e6aefcf4aabbd6d", "code": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    output = 0\n    for char in string:\n        output *= len(alphabet)\n        output += alphabet.index(char)\n    return output\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    int_value = 0\n    for c in string:\n        int_value *= len(alphabet)\n        int_value += alphabet.index(c)\n    return int_value\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    number = 0\n    for letter in string:\n        number *= len(alphabet)\n        number += alphabet.index(letter)\n    return number\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    if not alphabet:\n        raise ValueError(\"Alphabet must contain at least one character.\")\n    if len(alphabet) == 1 and string == \"\":\n        return 0\n    num = 0\n    for c in string:\n        num = num * len(alphabet) + alphabet.index(c)\n    return num\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    base = len(alphabet)\n    num = 0\n\n    for c in string:\n        num *= base\n        num += alphabet.index(c)\n\n    return num\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    if not string:\n        return 0\n\n    result = 0\n    for c in string:\n        result = result * len(alphabet) + alphabet.index(c)\n    return result\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    assert 0 <= len(string) <= 9\n    alphabet = alphabet or DEFAULT_ALPHABET\n    result = 0\n    for c in string:\n        result = alphabet.index(c) * result + alphabet.index('0')\n    return result % len(alphabet)\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    result = 0\n    for c in string:\n        result = alphabet.index(c) * result + alphabet.index('0')\n    return result\n"], "level": "self_contained", "generate_results": [{"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    output = 0\n    for char in string:\n        output *= len(alphabet)\n        output += alphabet.index(char)\n    return output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    int_value = 0\n    for c in string:\n        int_value *= len(alphabet)\n        int_value += alphabet.index(c)\n    return int_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    number = 0\n    for letter in string:\n        number *= len(alphabet)\n        number += alphabet.index(letter)\n    return number\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    if not alphabet:\n        raise ValueError(\"Alphabet must contain at least one character.\")\n    if len(alphabet) == 1 and string == \"\":\n        return 0\n    num = 0\n    for c in string:\n        num = num * len(alphabet) + alphabet.index(c)\n    return num\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    base = len(alphabet)\n    num = 0\n\n    for c in string:\n        num *= base\n        num += alphabet.index(c)\n\n    return num\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    if not string:\n        return 0\n\n    result = 0\n    for c in string:\n        result = result * len(alphabet) + alphabet.index(c)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    assert 0 <= len(string) <= 9\n    alphabet = alphabet or DEFAULT_ALPHABET\n    result = 0\n    for c in string:\n        result = alphabet.index(c) * result + alphabet.index('0')\n    return result % len(alphabet)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    result = 0\n    for c in string:\n        result *= len(alphabet)\n        result += alphabet.index(c)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n    result = 0\n    for c in string:\n        result = alphabet.index(c) * result + alphabet.index('0')\n    return result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_24.py", "name": "get_repo_archive", "docstring": "Given an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.", "ques_id": "62ece4982e6aefcf4aabbd6f", "code": ["def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_path = destination_path / \"desc\"\n    if not archive_path.is_file():\n        tar = tarfile.open(name=archive_path.name, mode=\"w:gz\")\n        tar.add(url, arcname=str(destination_path))\n        tar.close()\n        click.echo(f\"Extracted archive to {archive_path}\")\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    repo_archive_name = get_repo_archive_name(url)\n    archive_path = destination_path / repo_archive_name\n\n    if not archive_path.exists():\n        logger.info(\n            \"Extracting repo archive {} to {}\".format(repo_archive_name, destination_path)\n        )\n        archive_path.parent.mkdir(parents=True, exist_ok=True)\n        tar = tarfile.open(repo_archive_name, \"r:gz\")\n        tar.extractall(path=destination_path)\n        tar.close()\n    else:\n        logger.info(\"Repo archive {} already exists, skipping\".format(repo_archive_name))\n\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    response = requests.get(url, stream=True)\n    archive_path = destination_path.with_suffix(\".tar.gz\")\n    for i, path in enumerate(archive_path.glob(\"*.tar.gz\")):\n        if i == 0:\n            prefix = Path(url) / \"core\"\n        else:\n            prefix = Path(url) / \"extra\"\n        prefix.mkdir(parents=True, exist_ok=True)\n        path.write_text(response.content)\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_path = destination_path.joinpath('desc')\n    response = requests.get(url, stream=True)\n    with archive_path.open('wb') as f:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_name = get_repo_archive_name(url)\n    archive_path = destination_path / archive_name\n\n    if not archive_path.is_file():\n        download_tar_gz(url, archive_path)\n        extract_tar_xz(archive_path, destination_path)\n\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    tar_filename = f\"{url.split('/')[-1]}.tar.gz\"\n    tar_path = destination_path / tar_filename\n\n    response = requests.get(url, stream=True)\n    with tarfile.open(fileobj=response.raw) as tar:\n        tar.extractall(destination_path)\n\n    return Path(destination_path) / tar_filename\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_name = url.split(\"/\")[-1]\n    destination_path = destination_path.with_suffix(\".tar.gz\")\n\n    with tarfile.open(str(destination_path), \"w:gz\") as tar:\n        tar.add(str(url), arcname=archive_name)\n\n    return destination_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    repo_name = _get_repo_name(url)\n    archive_path = destination_path / Path(repo_name + '.tar.gz')\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    repo_path = get_repo_path(url)\n    archive_path = destination_path / \"desc.tar.gz\"\n\n    if not archive_path.exists():\n        _download_repo(url, repo_path)\n        extract_tar_gz(repo_path, destination_path)\n\n    return archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    tar_archive_path = get_repo_tar_archive(url, destination_path)\n    return extract_tar_archive(tar_archive_path, destination_path)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_path = destination_path / \"desc\"\n    if not archive_path.is_file():\n        tar = tarfile.open(name=archive_path.name, mode=\"w:gz\")\n        tar.add(url, arcname=str(destination_path))\n        tar.close()\n        click.echo(f\"Extracted archive to {archive_path}\")\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    repo_archive_name = get_repo_archive_name(url)\n    archive_path = destination_path / repo_archive_name\n\n    if not archive_path.exists():\n        logger.info(\n            \"Extracting repo archive {} to {}\".format(repo_archive_name, destination_path)\n        )\n        archive_path.parent.mkdir(parents=True, exist_ok=True)\n        tar = tarfile.open(repo_archive_name, \"r:gz\")\n        tar.extractall(path=destination_path)\n        tar.close()\n    else:\n        logger.info(\"Repo archive {} already exists, skipping\".format(repo_archive_name))\n\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    response = requests.get(url, stream=True)\n    archive_path = destination_path.with_suffix(\".tar.gz\")\n    for i, path in enumerate(archive_path.glob(\"*.tar.gz\")):\n        if i == 0:\n            prefix = Path(url) / \"core\"\n        else:\n            prefix = Path(url) / \"extra\"\n        prefix.mkdir(parents=True, exist_ok=True)\n        path.write_text(response.content)\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_path = destination_path.joinpath('desc')\n    response = requests.get(url, stream=True)\n    with archive_path.open('wb') as f:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_name = get_repo_archive_name(url)\n    archive_path = destination_path / archive_name\n\n    if not archive_path.is_file():\n        download_tar_gz(url, archive_path)\n        extract_tar_xz(archive_path, destination_path)\n\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    tar_filename = f\"{url.split('/')[-1]}.tar.gz\"\n    tar_path = destination_path / tar_filename\n\n    response = requests.get(url, stream=True)\n    with tarfile.open(fileobj=response.raw) as tar:\n        tar.extractall(destination_path)\n\n    return Path(destination_path) / tar_filename\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    archive_name = url.split(\"/\")[-1]\n    destination_path = destination_path.with_suffix(\".tar.gz\")\n\n    with tarfile.open(str(destination_path), \"w:gz\") as tar:\n        tar.add(str(url), arcname=archive_name)\n\n    return destination_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    repo_name = _get_repo_name(url)\n    archive_path = destination_path / Path(repo_name + '.tar.gz')\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    repo_path = get_repo_path(url)\n    archive_path = destination_path / \"desc.tar.gz\"\n\n    if not archive_path.exists():\n        _download_repo(url, repo_path)\n        extract_tar_gz(repo_path, destination_path)\n\n    return archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n    tar_archive_path = get_repo_tar_archive(url, destination_path)\n    return extract_tar_archive(tar_archive_path, destination_path)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_20.py", "name": "os_is_mac", "docstring": "Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool", "ques_id": "62ece4982e6aefcf4aabbd70", "code": ["def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "def os_is_mac():\n    return sys.platform.startswith('darwin')\n", "def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n", "def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "def os_is_mac():\n\n    return sys.platform.startswith('darwin')\n", "def os_is_mac():\n\n    return platform.system() == \"Darwin\"\n", "def os_is_mac():\n    return sys.platform == \"darwin\"\n", "def os_is_mac():\n    return sys.platform.startswith('darwin')\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n    return sys.platform.startswith('darwin')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n    return sys.platform.startswith(\"darwin\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n    return platform.system() == \"Darwin\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\n    return sys.platform.startswith('darwin')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\n    return platform.system() == \"Darwin\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n    return sys.platform == \"darwin\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n    return sys.platform.startswith('darwin')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_14.py", "name": "regex_dict", "docstring": "Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes", "ques_id": "62ece4982e6aefcf4aabbd71", "code": ["def regex_dict(item):\n    return {\n        key: re.compile(key_pattern.format(key))\n        for key, key_pattern in REGEX_KEY_PATTERNS.items()\n    }\n", "def regex_dict(item):\n    return {**item, **convert_wildcards(item)}\n", "def regex_dict(item):\n    return {k: re.compile(str(k).replace('.', r'\\.') + '$') for k in item}\n", "def regex_dict(item):\n    return {pattern: re.compile(fnmatch.translate(pattern)) for pattern in item}\n", "def regex_dict(item):\n    return {\n        key: _regex_pattern.sub(\n            lambda match_group: _convert_key(match_group.group(1)), value\n        )\n        for key, value in item.items()\n    }\n", "def regex_dict(item):\n    return {\n        key: re.compile(regex)\n        for key, regex in item.items()\n    }\n", "def regex_dict(item):\n    result = {}\n    for key, value in item.items():\n        # If a wildcard key is found, convert it into the corresponding regex\n        if fnmatch.fnmatchcase(key, wildcard_keys):\n            regex_key = f'{key}_regex'\n            regex = re.compile(f'(\\\\b{regex_key}\\\\b)|({regex_key}\\\\.{regex_key}\\\\b)')\n            value = regex.sub(r'\\1\\2\\3', value)\n        # Leave the key as-is otherwise\n        elif fnmatch.fnmatchcase(key, wildcard_keys):\n            continue\n        else:\n            # Leave it as-is otherwise\n            value = '.*'\n        result[key] = value\n    return result\n", "def regex_dict(item):\n    return {\n        key: _regex_pattern.format(key)\n        for key in item\n    }\n", "def regex_dict(item):\n    return {k: re.compile(regex, re.IGNORECASE) for k, regex in item.items()}\n", "def regex_dict(item):\n    return {\n        key: re.compile(value)\n        for key, value in item.items()\n        if is_regex_key(key)\n    }\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def regex_dict(item):\n    return {\n        key: re.compile(key_pattern.format(key))\n        for key, key_pattern in REGEX_KEY_PATTERNS.items()\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {**item, **convert_wildcards(item)}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {k: re.compile(str(k).replace('.', r'\\.') + '$') for k in item}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {pattern: re.compile(fnmatch.translate(pattern)) for pattern in item}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {\n        key: _regex_pattern.sub(\n            lambda match_group: _convert_key(match_group.group(1)), value\n        )\n        for key, value in item.items()\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {\n        key: re.compile(regex)\n        for key, regex in item.items()\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    result = {}\n    for key, value in item.items():\n        # If a wildcard key is found, convert it into the corresponding regex\n        if fnmatch.fnmatchcase(key, wildcard_keys):\n            regex_key = f'{key}_regex'\n            regex = re.compile(f'(\\\\b{regex_key}\\\\b)|({regex_key}\\\\.{regex_key}\\\\b)')\n            value = regex.sub(r'\\1\\2\\3', value)\n        # Leave the key as-is otherwise\n        elif fnmatch.fnmatchcase(key, wildcard_keys):\n            continue\n        else:\n            # Leave it as-is otherwise\n            value = '.*'\n        result[key] = value\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {\n        key: _regex_pattern.format(key)\n        for key in item\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {k: re.compile(regex, re.IGNORECASE) for k, regex in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n    return {\n        key: re.compile(value)\n        for key, value in item.items()\n        if is_regex_key(key)\n    }\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_4.py", "name": "unquote", "docstring": "Remove quote from the given name.", "ques_id": "62ece4982e6aefcf4aabbd72", "code": ["def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] in \"'\\\"\":\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] == name[-1] == '\"' and name[1:2] in ('\"', \"'\"):\n        name = name[1:-1]\n    return name\n", "def unquote(name):\n    if name[0] in ('\"', \"'\") and name[-1] == name[0]:\n        name = name[1:-1]\n    return name\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == '\"':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] in \"'\\\"\":\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] in '\"\\'':\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] == name[-1] == '\"' and name[1:2] in ('\"', \"'\"):\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n    if name[0] in ('\"', \"'\") and name[-1] == name[0]:\n        name = name[1:-1]\n    return name\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_21.py", "name": "split", "docstring": "Multi-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)", "ques_id": "62ece4982e6aefcf4aabbd73", "code": ["def split(s, platform='this'):\n    if platform == 'c':\n        return shlex.split(s)\n    elif platform == 'win32' or platform.startswith('win'):\n        return shlex.split('\"' + s + '\"')\n    elif platform == 'darwin':\n        # this is a workaround for a bug in shlex on macOS\n        # http://bugs.python.org/issue6926\n        p = shlex.split('\"' + s + '\"')\n        p.insert(0, '/usr/bin/')\n        return p\n    else:\n        return shlex.split(s)\n", "def split(s, platform='this'):\n    if platform == 'c':\n        return shlex.split(s)\n\n    # Python 2\n    if isinstance(s, str):\n        return shlex.split(s)\n\n    # Python 3\n    if hasattr(s, '__iter__'):\n        return shlex.split(s)\n\n    raise TypeError(\"Unsupported type to split: %r\" % s)\n", "def split(s, platform='this'):\n    if platform == 'Posix':\n        return shlex.split(s)\n    else:\n        return re.split(r'\\W', s)\n", "def split(s, platform='this'):\n    if platform == 'this':\n        splitter = '/'\n    elif platform == 'posix':\n        splitter = '\\\\'\n    elif platform == 'cmd':\n        splitter = ':'\n    else:\n        raise ValueError('invalid platform: %r' % platform)\n    return shlex.split(s, posix=platform == 'cmd' or is_win, splitter=splitter)\n", "def split(s, platform='this'):\n    return shlex.split(s, posix=platform!= 'no')\n", "def split(s, platform='this'):\n    if platform =='shlex':\n        # Note: shlex is the default implementation of posix on Unix,\n        # and can be overridden using the '--' syntax.\n        return shlex.split(s)\n    else:\n        # Note: Python shlex does not support multi-platform splits,\n        # but it does not support shell-style splits on Windows, so\n        # we need to make sure the shell is in place when we try to split.\n        # This avoids creating a split twice in the same command (see #1178).\n        # See https://github.com/kachayev/shlex/pull/611\n        if platform == 'windows':\n            return shlex.split(s, posix=True)\n        else:\n            return shlex.split(s)\n", "def split(s, platform='this'):\n    if platform == 'Posix':\n        shlex.split = shlex.split_posix\n    else:\n        shlex.split = shlex.split\n    return shlex.split(s)\n", "def split(s, platform='this'):\n    if platform == '':\n        # Do not use shlex here, because it doesn't support shlex.split()\n        # return shlex.split(s)\n        return shlex.split(s)\n    if platform == 'this':\n        return shlex.split(s, posix=True)\n    if platform == 'posix':\n        return shlex.split(s, posix=False)\n    raise ValueError('invalid platform:'+ repr(platform))\n", "def split(s, platform='this'):\n    if platform == 'this':\n        # Use current platform for split\n        split_tokens = shlex.split(s, posix=True)\n    else:\n        # Use others (e.g. windows/cmd) for split\n        split_tokens = shlex.split(s, posix=False)\n    return split_tokens\n", "def split(s, platform='this'):\n    if platform == 'this':\n        # Auto-detect POSIX system\n        platform = os.name\n    if platform == 'win32':\n        # Python 2.6 and below are supported\n        if not sys.version_info >= (2, 6, 0):\n            return shlex.split(s)\n        # Python 2.7 and below are not supported on Windows\n        split = shlex.split\n    elif platform == 'posix':\n        split = shlex.split\n    elif platform == 'cmd':\n        split = shlex.split\n    else:\n        raise ValueError('Unsupported platform: %r' % platform)\n    return split(s, posix=platform == 'cmd')\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def split(s, platform='this'):\n    if platform == 'c':\n        return shlex.split(s)\n    elif platform == 'win32' or platform.startswith('win'):\n        return shlex.split('\"' + s + '\"')\n    elif platform == 'darwin':\n        # this is a workaround for a bug in shlex on macOS\n        # http://bugs.python.org/issue6926\n        p = shlex.split('\"' + s + '\"')\n        p.insert(0, '/usr/bin/')\n        return p\n    else:\n        return shlex.split(s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'c':\n        return shlex.split(s)\n\n    # Python 2\n    if isinstance(s, str):\n        return shlex.split(s)\n\n    # Python 3\n    if hasattr(s, '__iter__'):\n        return shlex.split(s)\n\n    raise TypeError(\"Unsupported type to split: %r\" % s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'Posix':\n        return shlex.split(s)\n    else:\n        return re.split(r'\\W', s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'this':\n        splitter = '/'\n    elif platform == 'posix':\n        splitter = '\\\\'\n    elif platform == 'cmd':\n        splitter = ':'\n    else:\n        raise ValueError('invalid platform: %r' % platform)\n    return shlex.split(s, posix=platform == 'cmd' or is_win, splitter=splitter)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    return shlex.split(s, posix=platform!= 'no')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform =='shlex':\n        # Note: shlex is the default implementation of posix on Unix,\n        # and can be overridden using the '--' syntax.\n        return shlex.split(s)\n    else:\n        # Note: Python shlex does not support multi-platform splits,\n        # but it does not support shell-style splits on Windows, so\n        # we need to make sure the shell is in place when we try to split.\n        # This avoids creating a split twice in the same command (see #1178).\n        # See https://github.com/kachayev/shlex/pull/611\n        if platform == 'windows':\n            return shlex.split(s, posix=True)\n        else:\n            return shlex.split(s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'Posix':\n        shlex.split = shlex.split_posix\n    else:\n        shlex.split = shlex.split\n    return shlex.split(s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == '':\n        # Do not use shlex here, because it doesn't support shlex.split()\n        # return shlex.split(s)\n        return shlex.split(s)\n    if platform == 'this':\n        return shlex.split(s, posix=True)\n    if platform == 'posix':\n        return shlex.split(s, posix=False)\n    raise ValueError('invalid platform:'+ repr(platform))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'this':\n        # Use current platform for split\n        split_tokens = shlex.split(s, posix=True)\n    else:\n        # Use others (e.g. windows/cmd) for split\n        split_tokens = shlex.split(s, posix=False)\n    return split_tokens\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n    if platform == 'this':\n        # Auto-detect POSIX system\n        platform = os.name\n    if platform == 'win32':\n        # Python 2.6 and below are supported\n        if not sys.version_info >= (2, 6, 0):\n            return shlex.split(s)\n        # Python 2.7 and below are not supported on Windows\n        split = shlex.split\n    elif platform == 'posix':\n        split = shlex.split\n    elif platform == 'cmd':\n        split = shlex.split\n    else:\n        raise ValueError('Unsupported platform: %r' % platform)\n    return split(s, posix=platform == 'cmd')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_15.py", "name": "prepare_repository_from_archive", "docstring": "Given an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.", "ques_id": "62ece4982e6aefcf4aabbd74", "code": ["def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not\n    exist.\n    \"\"\"\n    logger.info(\"Unpacking archive %s to %s\", archive_path, tmp_path)\n\n    # Get the archive data as a byte stream\n    with open_archive(archive_path, \"rb\") as archive:\n        data = archive.read()\n\n    # Remove the temporary directory if it exists\n    if tmp_path:\n        shutil.rmtree(tmp_path)\n\n    # Create the temporary directory\n    tmp_path = tempfile.mkdtemp(prefix=\"aws-s3-\")\n\n    # Uncompress the data\n    with tarfile.open(fileobj=io.BytesIO(data)) as tar:\n        tar.extractall(path=tmp_path)\n\n    # Return the url of the repo url with the temporary directory\n    return url_from_repo_url(tmp_path.as_posix())\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url.\n    \"\"\"\n    # TODO: Improve this logic using repo.url\n    if not filename:\n        filename = archive_path.split(os.sep)[-1]\n\n    # TODO: Improve this logic using repo.url\n    # TODO: Improve this logic using repo.url\n    if not os.path.exists(archive_path):\n        raise ValueError(\"Invalid path provided\")\n\n    archive_path = os.path.abspath(archive_path)\n    if not os.path.exists(archive_path):\n        raise ValueError(\"Invalid archive path provided\")\n\n    if not os.path.exists(archive_path.replace(os.sep, \"/\")):\n        raise ValueError(\"Invalid archive path provided\")\n\n    tmp_path = tmp_path.replace(\"/\", \"-\")\n    repo_url = f\"https://github.com/{archive_path}/{tmp_path}\"\n    return repo_url\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url.\n    \"\"\"\n    if not os.path.exists(archive_path):\n        raise FileNotFoundError(\"The given archive_path does not exist.\")\n    # For backward compatibility with bzr < 2.0, this returns a file repo\n    # url which can be used as origin url.\n    return _prepare_repository_from_archive_url(\n        archive_path, filename=filename, tmp_path=tmp_path\n    )\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n\n    Returns a file repo url which can be used as origin url.\n    This does not deal with the case where the archive passed along\n    does not exist.\n    \"\"\"\n    # TODO: Better error handling for this?\n    archive = Archive(archive_path)\n    if not archive.exists:\n        raise ValueError(\"No such archive: %s\" % archive_path)\n    archive_url = archive.url\n    if not archive.is_dir:\n        return archive_url\n    if filename is not None:\n        archive_url = archive_url.replace(filename, \"\")\n    return \"{}/{}\".format(tmp_path, archive_url)\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not\n    exist. \"\"\"\n    if not os.path.exists(archive_path):\n        raise RuntimeError(\n            f\"No such archive_path {archive_path!r} found in \"\n            f\"current directory {os.getcwd()!r}\"\n        )\n\n    # TODO: use tarfile.open instead of tarfile.extract to avoid passing file\n    # objects directly to tarfile instead of directly being able to open\n    # the file when extracting to a temporary directory\n    with tarfile.open(archive_path, \"r:*\") as archive:\n        return archive.extract_filename(filename)\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    This does not deal with the case where the archive passed along\n    does not exist.\n    \"\"\"\n    if not os.path.exists(archive_path):\n        raise FileNotFoundError(\n            \"The given archive_path '{}' does not exist\".format(archive_path)\n        )\n    repo_url = prepare_repo_url_from_archive(\n        archive_path=archive_path,\n        filename=filename,\n        tmp_path=tmp_path,\n    )\n    return repo_url\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url.\n    \"\"\"\n    archive = Archive(archive_path)\n    if filename is None:\n        filename = archive.name\n    return archive.prepare_repository_from_archive(filename, tmp_path=tmp_path)\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url.\n    \"\"\"\n    if not os.path.exists(archive_path):\n        raise FileNotFoundError(\n            \"'{}' does not exist. Please check your archive path.\".format(\n                archive_path\n            )\n        )\n\n    if not filename:\n        filename = os.path.basename(archive_path)\n\n    tmp_path = os.path.join(tmp_path, filename)\n\n    if not os.path.exists(tmp_path):\n        os.makedirs(tmp_path)\n\n    archive_data = open(archive_path, \"rb\").read()\n    repo_url = _uncompress(archive_data, tmp_path)\n    return repo_url\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n\n    Returns a file repo url which can be used as origin url.\n    This does not deal with the case where the archive passed along\n    does not exist.\n    \"\"\"\n    if not os.path.isfile(archive_path):\n        raise ValueError(\"Archive path does not exist.\")\n\n    if filename:\n        archive_path = f\"{tmp_path}/{filename}\"\n\n    with tarfile.open(archive_path) as tar:\n        members = tar.getmembers()\n        members = [member.name for member in members if member.isfile()]\n        if not members:\n            raise ValueError(\"No repository file found at {}.\".format(archive_path))\n\n    repo_url = f\"{archive_path}/{\"/\".join(members)}.tar.gz\"\n\n    return repo_url\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case\n    where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    if not archive_path.exists():\n        raise OSError(\n            \"Unable to prepare repository from file {}\".format(archive_path)\n        )\n\n    if filename:\n        archive_path = archive_path.with_name(filename)\n\n    if not archive_path.is_file():\n        raise OSError(\"Unable to prepare repository from file {}\".format(archive_path))\n\n    try:\n        tar = tarfile.open(str(archive_path), \"r:*\")\n    except tarfile.TarError as e:\n        raise OSError(\"Unable to prepare repository from file {}\".format(archive_path)) from e\n\n    try:\n        members = tar.getmembers()\n    except tarfile.TarError as e:\n        raise"], "level": "slib_runnable", "generate_results": [{"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not\n    exist.\n    \"\"\"\n    logger.info(\"Unpacking archive %s to %s\", archive_path, tmp_path)\n\n    # Get the archive data as a byte stream\n    with open_archive(archive_path, \"rb\") as archive:\n        data = archive.read()\n\n    # Remove the temporary directory if it exists\n    if tmp_path:\n        shutil.rmtree(tmp_path)\n\n    # Create the temporary directory\n    tmp_path = tempfile.mkdtemp(prefix=\"aws-s3-\")\n\n    # Uncompress the data\n    with tarfile.open(fileobj=io.BytesIO(data)) as tar:\n        tar.extractall(path=tmp_path)\n\n    # Return the url of the repo url with the temporary directory\n    return url_from_repo_url(tmp_path.as_posix())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url.\n    \"\"\"\n    # TODO: Improve this logic using repo.url\n    if not filename:\n        filename = archive_path.split(os.sep)[-1]\n\n    # TODO: Improve this logic using repo.url\n    # TODO: Improve this logic using repo.url\n    if not os.path.exists(archive_path):\n        raise ValueError(\"Invalid path provided\")\n\n    archive_path = os.path.abspath(archive_path)\n    if not os.path.exists(archive_path):\n        raise ValueError(\"Invalid archive path provided\")\n\n    if not os.path.exists(archive_path.replace(os.sep, \"/\")):\n        raise ValueError(\"Invalid archive path provided\")\n\n    tmp_path = tmp_path.replace(\"/\", \"-\")\n    repo_url = f\"https://github.com/{archive_path}/{tmp_path}\"\n    return repo_url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url.\n    \"\"\"\n    if not os.path.exists(archive_path):\n        raise FileNotFoundError(\"The given archive_path does not exist.\")\n    # For backward compatibility with bzr < 2.0, this returns a file repo\n    # url which can be used as origin url.\n    return _prepare_repository_from_archive_url(\n        archive_path, filename=filename, tmp_path=tmp_path\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n\n    Returns a file repo url which can be used as origin url.\n    This does not deal with the case where the archive passed along\n    does not exist.\n    \"\"\"\n    # TODO: Better error handling for this?\n    archive = Archive(archive_path)\n    if not archive.exists:\n        raise ValueError(\"No such archive: %s\" % archive_path)\n    archive_url = archive.url\n    if not archive.is_dir:\n        return archive_url\n    if filename is not None:\n        archive_url = archive_url.replace(filename, \"\")\n    return \"{}/{}\".format(tmp_path, archive_url)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not\n    exist. \"\"\"\n    if not os.path.exists(archive_path):\n        raise RuntimeError(\n            f\"No such archive_path {archive_path!r} found in \"\n            f\"current directory {os.getcwd()!r}\"\n        )\n\n    # TODO: use tarfile.open instead of tarfile.extract to avoid passing file\n    # objects directly to tarfile instead of directly being able to open\n    # the file when extracting to a temporary directory\n    with tarfile.open(archive_path, \"r:*\") as archive:\n        return archive.extract_filename(filename)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    This does not deal with the case where the archive passed along\n    does not exist.\n    \"\"\"\n    if not os.path.exists(archive_path):\n        raise FileNotFoundError(\n            \"The given archive_path '{}' does not exist\".format(archive_path)\n        )\n    repo_url = prepare_repo_url_from_archive(\n        archive_path=archive_path,\n        filename=filename,\n        tmp_path=tmp_path,\n    )\n    return repo_url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url.\n    \"\"\"\n    archive = Archive(archive_path)\n    if filename is None:\n        filename = archive.name\n    return archive.prepare_repository_from_archive(filename, tmp_path=tmp_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url.\n    \"\"\"\n    if not os.path.exists(archive_path):\n        raise FileNotFoundError(\n            \"'{}' does not exist. Please check your archive path.\".format(\n                archive_path\n            )\n        )\n\n    if not filename:\n        filename = os.path.basename(archive_path)\n\n    tmp_path = os.path.join(tmp_path, filename)\n\n    if not os.path.exists(tmp_path):\n        os.makedirs(tmp_path)\n\n    archive_data = open(archive_path, \"rb\").read()\n    repo_url = _uncompress(archive_data, tmp_path)\n    return repo_url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n\n    Returns a file repo url which can be used as origin url.\n    This does not deal with the case where the archive passed along\n    does not exist.\n    \"\"\"\n    if not os.path.isfile(archive_path):\n        raise ValueError(\"Archive path does not exist.\")\n\n    if filename:\n        archive_path = f\"{tmp_path}/{filename}\"\n\n    with tarfile.open(archive_path) as tar:\n        members = tar.getmembers()\n        members = [member.name for member in members if member.isfile()]\n        if not members:\n            raise ValueError(\"No repository file found at {}.\".format(archive_path))\n\n    repo_url = f\"{archive_path}/{\"/\".join(members)}.tar.gz\"\n\n    return repo_url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it. Returns a file repo url\n    which can be used as origin url. This does not deal with the case\n    where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    if not archive_path.exists():\n        raise OSError(\n            \"Unable to prepare repository from file {}\".format(archive_path)\n        )\n\n    if filename:\n        archive_path = archive_path.with_name(filename)\n\n    if not archive_path.is_file():\n        raise OSError(\"Unable to prepare repository from file {}\".format(archive_path))\n\n    try:\n        tar = tarfile.open(str(archive_path), \"r:*\")\n    except tarfile.TarError as e:\n        raise OSError(\"Unable to prepare repository from file {}\".format(archive_path)) from e\n\n    try:\n        members = tar.getmembers()\n    except tarfile.TarError as e:\n        raise", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_5.py", "name": "addignored", "docstring": "Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.", "ques_id": "62ece4982e6aefcf4aabbd75", "code": ["def addignored(ignored):\n    ignored = git._iterate_git_command(ignored)\n    ignored = [os.path.basename(f) for f in ignored]\n    ignored = ','.join(ignored)\n    return ignored\n", "def addignored(ignored):\n    ignored = gitignore.normalize(ignored)\n    ignored = gitignore.split(ignored)\n    ignored.sort()\n    return ','.join(ignored)\n", "def addignored(ignored):\n    filenames = []\n    ignored_str = \"\"\n    for i in ignored:\n        if i == \"-\":\n            ignored_str += \"*\"\n        else:\n            ignored_str += i\n        filenames.append(i)\n    filenames.sort()\n    return \"\\n\".join(filenames) + \"\\n\" + ignored_str\n", "def addignored(ignored):\n    ignored = [f for f in ignored if not f.startswith(\"*\")]\n    ignored.sort()\n    return \"\\n\".join(ignored)\n", "def addignored(ignored):\n    ignored = git._run_git_command(['ls-files', '-z'], ignored)\n    ignored = ignored.split('\\x00'.decode('utf-8'))\n    ignored = [f for f in ignored if not f.startswith('.')]\n    ignored.sort()\n    return ','.join(ignored)\n", "def addignored(ignored):\n    ignored = subprocess.Popen([\"git\", \"ls-files\"],\n                                stdin=subprocess.PIPE,\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE).communicate(ignored)[0].decode(\"utf-8\").split(os.linesep)\n    ignored = [f for f in ignored if f]\n    return ignored\n", "def addignored(ignored):\n    ignored = [\n        os.path.basename(f)\n        for f in ignored\n        if not re.match(r'.*\\.git$', f)\n    ]\n    ignored = ','.join(ignored)\n    ignored = '[' + ignored + ']'\n    return ignored\n", "def addignored(ignored):\n    ignore_files = []\n    with open(ignored, 'r') as f:\n        for line in f:\n            ignore_files.append(line)\n    ignore_files.sort()\n    return ','.join(ignore_files)\n", "def addignored(ignored):\n    ignored = subprocess.Popen(\n        ['git', 'diff', '--name-only', ignored],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    ).communicate()[0].decode('utf-8')\n    ignored = ignored.split('\\n')\n    ignored.sort()\n    return ignored\n", "def addignored(ignored):\n    ignored = subprocess.check_output([\"git\", \"ls-files\", \"-z\", \"--ignored\"]).decode(\"utf-8\").split(\"\\n\")\n    ignored = [x for x in ignored if x!= \"\"]\n    ignored.sort()\n    return ignored\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def addignored(ignored):\n    ignored = git._iterate_git_command(ignored)\n    ignored = [os.path.basename(f) for f in ignored]\n    ignored = ','.join(ignored)\n    return ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = gitignore.normalize(ignored)\n    ignored = gitignore.split(ignored)\n    ignored.sort()\n    return ','.join(ignored)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    filenames = []\n    ignored_str = \"\"\n    for i in ignored:\n        if i == \"-\":\n            ignored_str += \"*\"\n        else:\n            ignored_str += i\n        filenames.append(i)\n    filenames.sort()\n    return \"\\n\".join(filenames) + \"\\n\" + ignored_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = [f for f in ignored if not f.startswith(\"*\")]\n    ignored.sort()\n    return \"\\n\".join(ignored)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = git._run_git_command(['ls-files', '-z'], ignored)\n    ignored = ignored.split('\\x00'.decode('utf-8'))\n    ignored = [f for f in ignored if not f.startswith('.')]\n    ignored.sort()\n    return ','.join(ignored)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.Popen([\"git\", \"ls-files\"],\n                                stdin=subprocess.PIPE,\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE).communicate(ignored)[0].decode(\"utf-8\").split(os.linesep)\n    ignored = [f for f in ignored if f]\n    return ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = [\n        os.path.basename(f)\n        for f in ignored\n        if not re.match(r'.*\\.git$', f)\n    ]\n    ignored = ','.join(ignored)\n    ignored = '[' + ignored + ']'\n    return ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignore_files = []\n    with open(ignored, 'r') as f:\n        for line in f:\n            ignore_files.append(line)\n    ignore_files.sort()\n    return ','.join(ignore_files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.Popen(\n        ['git', 'diff', '--name-only', ignored],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    ).communicate()[0].decode('utf-8')\n    ignored = ignored.split('\\n')\n    ignored.sort()\n    return ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n    ignored = subprocess.check_output([\"git\", \"ls-files\", \"-z\", \"--ignored\"]).decode(\"utf-8\").split(\"\\n\")\n    ignored = [x for x in ignored if x!= \"\"]\n    ignored.sort()\n    return ignored\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_11.py", "name": "match", "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported", "ques_id": "62ece4982e6aefcf4aabbd76", "code": ["def match(filename):\n    return (\n        filename.endswith(\".bz2\") or\n        filename.endswith(\".gz\") or\n        filename.endswith(\".zip\") or\n        filename.endswith(\".tar.gz\") or\n        filename.endswith(\".tgz\") or\n        filename.endswith(\".tar.bz2\")\n    )\n", "def match(filename):\n    return (\n        filename.endswith(\".so\")\n        or filename.endswith(\".dll\")\n        or filename.endswith(\".dylib\")\n        or filename.endswith(\".so.2\")\n        or filename.endswith(\".dylib\")\n    )\n", "def match(filename):\n    return (\n        filename is None or\n        filename.endswith((\".yaml\", \".yml\")) or\n        (filename.endswith((\".yml\", \".yaml.bz2\")) and not filename.endswith(\".yaml.gz\"))\n    )\n", "def match(filename):\n    return (filename in SUPPORTED_FORMATS or\n            filename.endswith('.gz') or\n            filename.endswith('.bz2') or\n            filename.endswith('.zip'))\n", "def match(filename):\n    return (filename.endswith(\".pp\") or\n            filename.endswith(\".h\") or\n            filename.endswith(\".cpp\") or\n            filename.endswith(\"_config.h\") or\n            filename.endswith(\"_config.cpp\"))\n", "def match(filename):\n    return (\n        filename.endswith(\".py\")\n        or filename.endswith(\".pyw\")\n        or filename.endswith(\".pyd\")\n        or filename.endswith(\".pyo\")\n        or not filename.endswith(\".pyc\")\n    )\n", "def match(filename):\n    return filename.lower().endswith(\".png\")\n", "def match(filename):\n    return filename.lower().endswith(\".pdf\")\n", "def match(filename):\n\n    if not filename:\n        return False\n\n    if filename.endswith(\".py\"):\n        return True\n\n    return False\n", "def match(filename):\n    return filename.lower().endswith((\".md\", \".markdown\"))\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match(filename):\n    return (\n        filename.endswith(\".bz2\") or\n        filename.endswith(\".gz\") or\n        filename.endswith(\".zip\") or\n        filename.endswith(\".tar.gz\") or\n        filename.endswith(\".tgz\") or\n        filename.endswith(\".tar.bz2\")\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return (\n        filename.endswith(\".so\")\n        or filename.endswith(\".dll\")\n        or filename.endswith(\".dylib\")\n        or filename.endswith(\".so.2\")\n        or filename.endswith(\".dylib\")\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return (\n        filename is None or\n        filename.endswith((\".yaml\", \".yml\")) or\n        (filename.endswith((\".yml\", \".yaml.bz2\")) and not filename.endswith(\".yaml.gz\"))\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return (filename in SUPPORTED_FORMATS or\n            filename.endswith('.gz') or\n            filename.endswith('.bz2') or\n            filename.endswith('.zip'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return (filename.endswith(\".pp\") or\n            filename.endswith(\".h\") or\n            filename.endswith(\".cpp\") or\n            filename.endswith(\"_config.h\") or\n            filename.endswith(\"_config.cpp\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return (\n        filename.endswith(\".py\")\n        or filename.endswith(\".pyw\")\n        or filename.endswith(\".pyd\")\n        or filename.endswith(\".pyo\")\n        or not filename.endswith(\".pyc\")\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return filename.lower().endswith(\".png\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return filename.lower().endswith(\".pdf\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\n    if not filename:\n        return False\n\n    if filename.endswith(\".py\"):\n        return True\n\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n    return filename.lower().endswith((\".md\", \".markdown\"))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_1.py", "name": "parse_frequency", "docstring": "Given a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.", "ques_id": "62ece4982e6aefcf4aabbd77", "code": ["def parse_frequency(frequency):\n    if frequency not in FREQ:\n        raise ValueError('Unrecognized frequency: \"%s\"' % frequency)\n\n    if frequency == 'always':\n        return timedelta(weeks=1)\n\n    try:\n        return timedelta(**FREQ[frequency])\n    except KeyError:\n        raise ValueError('Unknown frequency: \"%s\"' % frequency)\n", "def parse_frequency(frequency):\n    if frequency not in FREQUENCIES:\n        raise ValueError(\"Invalid frequency '%s'\" % frequency)\n    return FREQUENCIES[frequency]\n", "def parse_frequency(frequency):\n    if frequency in FREQ_ALIASES:\n        return datetime.timedelta(**{frequency: 1})\n    if frequency in FREQ_NEVER:\n        return datetime.timedelta()\n    raise ValueError('Unknown frequency \"%s\"' % frequency)\n", "def parse_frequency(frequency):\n    if frequency in _FREQ_MAP:\n        return timedelta(**_FREQ_MAP[frequency])\n\n    raise ValueError('frequency \"%s\" is not supported' % frequency)\n", "def parse_frequency(frequency):\n    if frequency is None or frequency == 'always':\n        return timedelta(weeks=1)\n    try:\n        return timedelta(days=int(frequency))\n    except (ValueError, TypeError):\n        raise ValueError('frequency {!r} not a valid frequency'.format(frequency))\n", "def parse_frequency(frequency):\n    if frequency in FREQ_NEVER:\n        return None\n    else:\n        try:\n            return timedelta(**{frequency.lower(): 1})\n        except KeyError:\n            raise ValueError('Unknown frequency: {!r}'.format(frequency))\n", "def parse_frequency(frequency):\n    frequency = frequency.strip()\n    if frequency in FREQ_ALIASES:\n        return getattr(timedelta, FREQ_ALIASES[frequency])()\n    elif frequency in ('always', 'weeks'):\n        return timedelta(days=7)\n    else:\n        raise ValueError('Invalid frequency: %r' % frequency)\n", "def parse_frequency(frequency):\n    if frequency not in FREQ_TABLE:\n        raise ValueError(\n            \"Frequency '{}' is not a valid frequency. Valid frequency are \"\n            \"{}.\".format(frequency, FREQ_TABLE.keys())\n        )\n\n    return FREQ_TABLE[frequency]\n", "def parse_frequency(frequency):\n    if frequency in FREQ:\n        return timedelta(**FREQ[frequency])\n    elif frequency == \"always\":\n        return timedelta(seconds=0)\n    else:\n        raise ValueError(\"frequency '%s' not recognized\" % frequency)\n", "def parse_frequency(frequency):\n    if frequency in (None, 'always'):\n        return None\n\n    frequency = frequency.upper()\n\n    if frequency in ('1', 'WEEKLY', 'MONTHLY', 'YEARLY'):\n        return timedelta(weeks=1)\n\n    if frequency in ('7', 'MONTHLY', 'YEARLY'):\n        return timedelta(weeks=7)\n\n    if frequency in ('DURATION', 'ONLY'):\n        return timedelta(days=0)\n\n    if frequency in ('YEARLY', 'ALL'):\n        return timedelta(days=365)\n\n    raise ValueError('Unknown frequency \"{0}\"'.format(frequency))\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def parse_frequency(frequency):\n    if frequency not in FREQ:\n        raise ValueError('Unrecognized frequency: \"%s\"' % frequency)\n\n    if frequency == 'always':\n        return timedelta(weeks=1)\n\n    try:\n        return timedelta(**FREQ[frequency])\n    except KeyError:\n        raise ValueError('Unknown frequency: \"%s\"' % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency not in FREQUENCIES:\n        raise ValueError(\"Invalid frequency '%s'\" % frequency)\n    return FREQUENCIES[frequency]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency in FREQ_ALIASES:\n        return datetime.timedelta(**{frequency: 1})\n    if frequency in FREQ_NEVER:\n        return datetime.timedelta()\n    raise ValueError('Unknown frequency \"%s\"' % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency in _FREQ_MAP:\n        return timedelta(**_FREQ_MAP[frequency])\n\n    raise ValueError('frequency \"%s\" is not supported' % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency is None or frequency == 'always':\n        return timedelta(weeks=1)\n    try:\n        return timedelta(days=int(frequency))\n    except (ValueError, TypeError):\n        raise ValueError('frequency {!r} not a valid frequency'.format(frequency))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency in FREQ_NEVER:\n        return None\n    else:\n        try:\n            return timedelta(**{frequency.lower(): 1})\n        except KeyError:\n            raise ValueError('Unknown frequency: {!r}'.format(frequency))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    frequency = frequency.strip()\n    if frequency in FREQ_ALIASES:\n        return getattr(timedelta, FREQ_ALIASES[frequency])()\n    elif frequency in ('always', 'weeks'):\n        return timedelta(days=7)\n    else:\n        raise ValueError('Invalid frequency: %r' % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency not in FREQ_TABLE:\n        raise ValueError(\n            \"Frequency '{}' is not a valid frequency. Valid frequency are \"\n            \"{}.\".format(frequency, FREQ_TABLE.keys())\n        )\n\n    return FREQ_TABLE[frequency]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency in FREQ:\n        return timedelta(**FREQ[frequency])\n    elif frequency == \"always\":\n        return timedelta(seconds=0)\n    else:\n        raise ValueError(\"frequency '%s' not recognized\" % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n    if frequency in (None, 'always'):\n        return None\n\n    frequency = frequency.upper()\n\n    if frequency in ('1', 'WEEKLY', 'MONTHLY', 'YEARLY'):\n        return timedelta(weeks=1)\n\n    if frequency in ('7', 'MONTHLY', 'YEARLY'):\n        return timedelta(weeks=7)\n\n    if frequency in ('DURATION', 'ONLY'):\n        return timedelta(days=0)\n\n    if frequency in ('YEARLY', 'ALL'):\n        return timedelta(days=365)\n\n    raise ValueError('Unknown frequency \"{0}\"'.format(frequency))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_18.py", "name": "is_local", "docstring": "Checks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost", "ques_id": "62ece4992e6aefcf4aabbd78", "code": ["def is_local(host):\n    return host == '127.0.0.1'\n", "def is_local(host):\n    return host == '127.0.0.1'\n", "def is_local(host):\n    if host is None:\n        return False\n\n    return host == '127.0.0.1'\n", "def is_local(host):\n    if not isinstance(host, string_types):\n        return False\n    return host == 'localhost'\n", "def is_local(host):\n    if not isinstance(host, str):\n        return False\n\n    return host == \"127.0.0.1\"\n", "def is_local(host):\n    return host == '127.0.0.1'\n", "def is_local(host):\n    if host.startswith('127.'):\n        return True\n    return False\n", "def is_local(host):\n    return (host == '127.0.0.1')\n", "def is_local(host):\n    return host == \"127.0.0.1\"\n", "def is_local(host):\n    return host == '127.0.0.1'\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    if host is None:\n        return False\n\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    if not isinstance(host, string_types):\n        return False\n    return host == 'localhost'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    if not isinstance(host, str):\n        return False\n\n    return host == \"127.0.0.1\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    if host.startswith('127.'):\n        return True\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return (host == '127.0.0.1')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == \"127.0.0.1\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n    return host == '127.0.0.1'\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_8.py", "name": "make_find_paths", "docstring": "Given a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']", "ques_id": "62ece4992e6aefcf4aabbd79", "code": ["def make_find_paths(find_paths):\n    return [make_find_path(p) for p in find_paths]\n", "def make_find_paths(find_paths):\n    return [fnmatch.translate(pat) for pat in find_paths]\n", "def make_find_paths(find_paths):\n    return [fnmatch.translate(pattern) for pattern in find_paths]\n", "def make_find_paths(find_paths):\n    return [fnmatch.translate(pattern) for pattern in find_paths]\n", "def make_find_paths(find_paths):\n    return [make_find_path(p) for p in find_paths]\n", "def make_find_paths(find_paths):\n    return [fnmatch_exp.sub(r'\\1*', path)\n            for path in find_paths]\n", "def make_find_paths(find_paths):\n    return [fnmatch.translate(pattern) for pattern in find_paths]\n", "def make_find_paths(find_paths):\n    return [fnmatch.translate(pat) for pat in find_paths]\n", "def make_find_paths(find_paths):\n    return [fnmatch.translate(pattern) for pattern in find_paths]\n", "def make_find_paths(find_paths):\n    return [make_find_path(p) for p in find_paths]\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def make_find_paths(find_paths):\n    return [make_find_path(p) for p in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [fnmatch.translate(pat) for pat in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [fnmatch.translate(pattern) for pattern in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [fnmatch.translate(pattern) for pattern in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [make_find_path(p) for p in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [fnmatch_exp.sub(r'\\1*', path)\n            for path in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [fnmatch.translate(pattern) for pattern in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [fnmatch.translate(pat) for pat in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [fnmatch.translate(pattern) for pattern in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n    return [make_find_path(p) for p in find_paths]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_19.py", "name": "is_gitbash", "docstring": "returns True if you run in a Windows gitbash\n\n:return: True if gitbash", "ques_id": "62ece4992e6aefcf4aabbd7a", "code": ["def is_gitbash():\n    try:\n        cmd = \"git config --get remote.origin.url\"\n        return subprocess.check_output(cmd.split(), stderr=subprocess.STDOUT).decode().strip() == \"gitbash\"\n    except:\n        return False\n", "def is_gitbash():\n    return os.name == 'nt'\n", "def is_gitbash():\n    if platform.system() == \"Windows\":\n        try:\n            cmd = \"git config --get remote.origin.url\"\n            return subprocess.check_output(cmd.split(), stderr=subprocess.STDOUT).decode().strip() == \"https://github.com/Microsoft/GitBash\"\n        except:\n            return False\n    return False\n", "def is_gitbash():\n\n    # gitbash is the standard install scheme for Git\n    # http://bitbucket.org/pitrou/git-bash/issues/23\n    return os.name == \"nt\" and \"git\" in sys.platform.lower()\n", "def is_gitbash():\n\n    if os.name == 'nt':\n        return False\n\n    try:\n        cmd = 'git config --get remote.origin.url'\n        return subprocess.check_output(cmd.split(), stderr=subprocess.STDOUT).decode().strip() == 'gitbash'\n    except:\n        return False\n", "def is_gitbash():\n\n    # gitbash returns 1 if the file is located on a git repo, and is called \"git config\" which returns 0 for\n    # obvious reasons, so we check that here.\n    return os.path.isfile('/usr/local/git.git') and os.access('/usr/local/git.git', os.X_OK)\n", "def is_gitbash():\n    if platform.system() == \"Windows\":\n        return \"gitbash.exe\" in subprocess.check_output([\"git\", \"config\", \"--get\", \"user.name\"]).decode(\"utf-8\").strip()\n    return False\n", "def is_gitbash():\n    return os.name == 'nt'\n", "def is_gitbash():\n    # gitbash returns the command line if the file is executed in the directory where git.exe is located\n    return os.path.exists('/usr/bin/git')\n", "def is_gitbash():\n    return platform.system() == \"Windows\"\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_gitbash():\n    try:\n        cmd = \"git config --get remote.origin.url\"\n        return subprocess.check_output(cmd.split(), stderr=subprocess.STDOUT).decode().strip() == \"gitbash\"\n    except:\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    return os.name == 'nt'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    if platform.system() == \"Windows\":\n        try:\n            cmd = \"git config --get remote.origin.url\"\n            return subprocess.check_output(cmd.split(), stderr=subprocess.STDOUT).decode().strip() == \"https://github.com/Microsoft/GitBash\"\n        except:\n            return False\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\n    # gitbash is the standard install scheme for Git\n    # http://bitbucket.org/pitrou/git-bash/issues/23\n    return os.name == \"nt\" and \"git\" in sys.platform.lower()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\n    if os.name == 'nt':\n        return False\n\n    try:\n        cmd = 'git config --get remote.origin.url'\n        return subprocess.check_output(cmd.split(), stderr=subprocess.STDOUT).decode().strip() == 'gitbash'\n    except:\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\n    # gitbash returns 1 if the file is located on a git repo, and is called \"git config\" which returns 0 for\n    # obvious reasons, so we check that here.\n    return os.path.isfile('/usr/local/git.git') and os.access('/usr/local/git.git', os.X_OK)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    if platform.system() == \"Windows\":\n        return \"gitbash.exe\" in subprocess.check_output([\"git\", \"config\", \"--get\", \"user.name\"]).decode(\"utf-8\").strip()\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n    return os.name == 'nt'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n    # gitbash returns the command line if the file is executed in the directory where git.exe is located\n    return os.path.exists('/usr/bin/git')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n    return platform.system() == \"Windows\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_9.py", "name": "write_configuration", "docstring": "Given a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.", "ques_id": "62ece4992e6aefcf4aabbd7b", "code": ["def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.isfile(config_filename):\n        print('\\n[ERROR] Configuration file \"{}\" already exists. Aborting.'.format(config_filename))\n        sys.exit(1)\n\n    with open(config_filename, 'w') as outfile:\n        yaml.safe_dump(rendered_config, outfile, default_flow_style=mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\n    dirname = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        error(\"Configuration file already exists at '{}'\".format(config_filename))\n        return\n\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with open(config_filename, 'w') as f:\n        f.write(rendered_config)\n\n    if mode == 0o600:\n        shutil.copymode(config_filename, config_filename + '.new')\n\n    elif mode == 0o700:\n        os.rename(config_filename, config_filename + '.new')\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n    if not overwrite and os.path.isdir(directory):\n        sys.stderr.write(\"ERROR: Configuration directory '%s' already exists. Overwrite is False.\\n\" % directory)\n        sys.exit(1)\n    if not overwrite and not os.path.isdir(directory):\n        os.makedirs(directory)\n    with open(config_filename, \"w\") as configfile:\n        configfile.write(rendered_config)\n    os.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    dirname = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        sys.stderr.write(\"Overwriting existing %s\\n\" % config_filename)\n        return\n\n    # Create directories if needed\n    if dirname and not os.path.isdir(dirname):\n        os.makedirs(dirname)\n\n    # Write YAML file\n    with open(config_filename, mode) as f:\n        f.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        click.secho('Overwrite {}?'.format(config_filename), fg='red')\n        if click.confirm('Overwrite {}?'.format(config_filename)):\n            with open(config_filename, 'w') as file_pointer:\n                file_pointer.write(rendered_config)\n            click.secho('Success!', fg='green')\n    else:\n        if not overwrite:\n            click.secho('{} already exists.  Did you overwrite it?'.format(config_filename), fg='yellow')\n        else:\n            os.makedirs(directory, mode=mode)\n            with open(config_filename, 'w') as file_pointer:\n                file_pointer.write(rendered_config)\n            click.secho('Success!', fg='green')\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    # TODO: validate the input filename\n    # TODO: validate the rendered config\n    # TODO: validate the mode of writing\n    # TODO: validate the overwrite of the file\n\n    # create the directory to write the config file\n    try:\n        os.makedirs(os.path.dirname(config_filename))\n    except OSError:\n        # directory already exists\n        pass\n\n    # write the file to the filesystem\n    with open(config_filename, mode) as f:\n        f.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.exists(config_filename):\n        abort(\"Configuration file {} already exists! Overwrite it?\".format(config_filename))\n\n    dir_path = os.path.dirname(config_filename)\n\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\n    with open(config_filename, 'w') as config_file:\n        yaml.safe_dump(rendered_config, config_file, default_flow_style=mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.exists(config_filename):\n        abort(\"Configuration file {} already exists. Overwrite it?\".format(config_filename))\n\n    dirname = os.path.dirname(config_filename)\n    if dirname and not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with open(config_filename, \"w\") as stream:\n        stream.write(yaml.dump(rendered_config, default_flow_style=mode))\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    # Create directories for the config file\n    dirname = os.path.dirname(config_filename)\n    if not overwrite and os.path.isdir(dirname):\n        return\n\n    # Write the config file\n    with open(config_filename, mode) as config_file:\n        config_file.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not os.path.exists(os.path.dirname(config_filename)):\n        os.makedirs(os.path.dirname(config_filename))\n    with open(config_filename, \"w\") as config_file:\n        if not overwrite:\n            # Don't overwrite existing files if overwrite=False\n            return\n        yaml.safe_dump(rendered_config, config_file, default_flow_style=mode)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.isfile(config_filename):\n        print('\\n[ERROR] Configuration file \"{}\" already exists. Aborting.'.format(config_filename))\n        sys.exit(1)\n\n    with open(config_filename, 'w') as outfile:\n        yaml.safe_dump(rendered_config, outfile, default_flow_style=mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\n    dirname = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        error(\"Configuration file already exists at '{}'\".format(config_filename))\n        return\n\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with open(config_filename, 'w') as f:\n        f.write(rendered_config)\n\n    if mode == 0o600:\n        shutil.copymode(config_filename, config_filename + '.new')\n\n    elif mode == 0o700:\n        os.rename(config_filename, config_filename + '.new')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n    if not overwrite and os.path.isdir(directory):\n        sys.stderr.write(\"ERROR: Configuration directory '%s' already exists. Overwrite is False.\\n\" % directory)\n        sys.exit(1)\n    if not overwrite and not os.path.isdir(directory):\n        os.makedirs(directory)\n    with open(config_filename, \"w\") as configfile:\n        configfile.write(rendered_config)\n    os.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    dirname = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        sys.stderr.write(\"Overwriting existing %s\\n\" % config_filename)\n        return\n\n    # Create directories if needed\n    if dirname and not os.path.isdir(dirname):\n        os.makedirs(dirname)\n\n    # Write YAML file\n    with open(config_filename, mode) as f:\n        f.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    directory = os.path.dirname(config_filename)\n    if not overwrite and os.path.exists(config_filename):\n        click.secho('Overwrite {}?'.format(config_filename), fg='red')\n        if click.confirm('Overwrite {}?'.format(config_filename)):\n            with open(config_filename, 'w') as file_pointer:\n                file_pointer.write(rendered_config)\n            click.secho('Success!', fg='green')\n    else:\n        if not overwrite:\n            click.secho('{} already exists.  Did you overwrite it?'.format(config_filename), fg='yellow')\n        else:\n            os.makedirs(directory, mode=mode)\n            with open(config_filename, 'w') as file_pointer:\n                file_pointer.write(rendered_config)\n            click.secho('Success!', fg='green')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    # TODO: validate the input filename\n    # TODO: validate the rendered config\n    # TODO: validate the mode of writing\n    # TODO: validate the overwrite of the file\n\n    # create the directory to write the config file\n    try:\n        os.makedirs(os.path.dirname(config_filename))\n    except OSError:\n        # directory already exists\n        pass\n\n    # write the file to the filesystem\n    with open(config_filename, mode) as f:\n        f.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.exists(config_filename):\n        abort(\"Configuration file {} already exists! Overwrite it?\".format(config_filename))\n\n    dir_path = os.path.dirname(config_filename)\n\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\n    with open(config_filename, 'w') as config_file:\n        yaml.safe_dump(rendered_config, config_file, default_flow_style=mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not overwrite and os.path.exists(config_filename):\n        abort(\"Configuration file {} already exists. Overwrite it?\".format(config_filename))\n\n    dirname = os.path.dirname(config_filename)\n    if dirname and not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with open(config_filename, \"w\") as stream:\n        stream.write(yaml.dump(rendered_config, default_flow_style=mode))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    # Create directories for the config file\n    dirname = os.path.dirname(config_filename)\n    if not overwrite and os.path.isdir(dirname):\n        return\n\n    # Write the config file\n    with open(config_filename, mode) as config_file:\n        config_file.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    if not os.path.exists(os.path.dirname(config_filename)):\n        os.makedirs(os.path.dirname(config_filename))\n    with open(config_filename, \"w\") as config_file:\n        if not overwrite:\n            # Don't overwrite existing files if overwrite=False\n            return\n        yaml.safe_dump(rendered_config, config_file, default_flow_style=mode)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_22.py", "name": "oneline", "docstring": "converts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:", "ques_id": "62ece4992e6aefcf4aabbd7c", "code": ["def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\", script])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"echo '%s'\" % line for line in script.split(\"\\n\")])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([str(x) for x in script.split(seperator)])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh\", \"-X\", \"-o\", \"StrictHostKeyChecking=no\", \"-N\", \"-i\", script])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"echo '%s'\" % line for line in script.split(\"\\n\")])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator))\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh\", \"-o\", \"StrictHostKeyChecking=no\", \"-i\", script])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([str(c) for c in script.split(\"\\n\")])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([str(x) for x in script.split(\"\\n\")])\n", "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh -X\", script])\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\", script])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"echo '%s'\" % line for line in script.split(\"\\n\")])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([str(x) for x in script.split(seperator)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh\", \"-X\", \"-o\", \"StrictHostKeyChecking=no\", \"-N\", \"-i\", script])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"echo '%s'\" % line for line in script.split(\"\\n\")])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join(script.split(seperator))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh\", \"-o\", \"StrictHostKeyChecking=no\", \"-i\", script])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([str(c) for c in script.split(\"\\n\")])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([str(x) for x in script.split(\"\\n\")])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n    return seperator.join([\"ssh -X\", script])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_16.py", "name": "subprocess_run_helper", "docstring": "Run a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.", "ques_id": "62ece4992e6aefcf4aabbd7d", "code": ["def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    extra_env['TZ'] = 'UTC'\n    extra_env['USER'] = getpass.getuser()\n\n    with Timeout(timeout, False):\n        with PreexecHook(_set_signal_handlers):\n            return func(*args, **extra_env)\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    env.update(extra_env or {})\n    proc = subprocess.Popen(\n        [sys.executable, '-c', 'import time; time.sleep(%s)' % timeout],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env)\n    stdout, stderr = proc.communicate(input=func(*args))\n    if proc.returncode is not None:\n        raise RuntimeError(\n            'Command \"{}\" failed with exit code {}:\\n{}'.format(\n               ''.join(map(str, args)),\n                proc.returncode,\n                stderr.decode('utf-8')))\n    return stdout\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n\n    proc = subprocess.Popen(\n        [sys.executable, '-c', 'import time; time.sleep(60)'] + list(args),\n        stdout=subprocess.PIPE,\n        env=env)\n    out, _ = proc.communicate(timeout)\n    retcode = proc.wait()\n    if retcode:\n        raise subprocess.CalledProcessError(retcode, func.__name__, out)\n    return out\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n    proc = subprocess.Popen(\n        args,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env,\n        timeout=timeout\n    )\n    stdout, stderr = proc.communicate()\n    if proc.wait()!= 0:\n        msg = \"Command failed:\\n{}\\n{}\".format(stdout, stderr)\n        raise subprocess.CalledProcessError(\n            proc.returncode,\n            func.__name__,\n            msg\n        )\n    return stdout\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n    process = subprocess.Popen(\n        args,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env,\n        timeout=timeout,\n    )\n    stdout, stderr = process.communicate()\n    return process.returncode, stdout, stderr\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n\n    def run_with_timeout(timeout):\n        process = subprocess.Popen(\n            [sys.executable, func] + list(args), env=env, stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE)\n        timeout_time = time.time() + timeout\n        while not process.poll():\n            time.sleep(0.1)\n            if time.time() > timeout_time:\n                break\n        stdout, stderr = process.communicate()\n        return process.returncode, stdout, stderr\n\n    return run_with_timeout\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    env.update(extra_env or {})\n    proc = subprocess.Popen(\n        [sys.executable, '-c', 'import time; time.sleep(%s); time.sleep(10)' % timeout],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env)\n    proc.communicate(input=func(*args))\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    env.update(extra_env or {})\n    proc = subprocess.Popen(\n        [sys.executable, '-c', 'import time; time.sleep(%d); print(time.time())' % timeout],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env)\n    stdout, stderr = proc.communicate(input=func(*args))\n    if proc.wait()!= 0:\n        raise Exception(\"Timeout running %s\" % func.__name__)\n    return stdout\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    env = os.environ.copy()\n    env.update(extra_env)\n    try:\n        proc = subprocess.Popen(\n            [sys.executable, func] + list(args),\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE)\n        proc.wait(timeout)\n        stdout, stderr = proc.communicate()\n    except subprocess.TimeoutExpired:\n        proc.kill()\n        proc.wait()\n        stdout, stderr = proc.communicate()\n    if stderr:\n        sys.stderr.write(stderr)\n    return stdout\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n    proc = subprocess.Popen(\n        [sys.executable, '-c', func] + list(args),\n        env=env,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE)\n    stdout, stderr = proc.communicate()\n    if proc.wait()!= 0:\n        raise RuntimeError(\n            'Command {0!r} failed with exit status {1}'.format(\n               ''.join(map(pipes.quote, args)),\n                proc.returncode))\n    return stdout\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    extra_env['TZ'] = 'UTC'\n    extra_env['USER'] = getpass.getuser()\n\n    with Timeout(timeout, False):\n        with PreexecHook(_set_signal_handlers):\n            return func(*args, **extra_env)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    env.update(extra_env or {})\n    proc = subprocess.Popen(\n        [sys.executable, '-c', 'import time; time.sleep(%s)' % timeout],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env)\n    stdout, stderr = proc.communicate(input=func(*args))\n    if proc.returncode is not None:\n        raise RuntimeError(\n            'Command \"{}\" failed with exit code {}:\\n{}'.format(\n               ''.join(map(str, args)),\n                proc.returncode,\n                stderr.decode('utf-8')))\n    return stdout\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n\n    proc = subprocess.Popen(\n        [sys.executable, '-c', 'import time; time.sleep(60)'] + list(args),\n        stdout=subprocess.PIPE,\n        env=env)\n    out, _ = proc.communicate(timeout)\n    retcode = proc.wait()\n    if retcode:\n        raise subprocess.CalledProcessError(retcode, func.__name__, out)\n    return out\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n    proc = subprocess.Popen(\n        args,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env,\n        timeout=timeout\n    )\n    stdout, stderr = proc.communicate()\n    if proc.wait()!= 0:\n        msg = \"Command failed:\\n{}\\n{}\".format(stdout, stderr)\n        raise subprocess.CalledProcessError(\n            proc.returncode,\n            func.__name__,\n            msg\n        )\n    return stdout\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n    process = subprocess.Popen(\n        args,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env,\n        timeout=timeout,\n    )\n    stdout, stderr = process.communicate()\n    return process.returncode, stdout, stderr\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n\n    def run_with_timeout(timeout):\n        process = subprocess.Popen(\n            [sys.executable, func] + list(args), env=env, stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE)\n        timeout_time = time.time() + timeout\n        while not process.poll():\n            time.sleep(0.1)\n            if time.time() > timeout_time:\n                break\n        stdout, stderr = process.communicate()\n        return process.returncode, stdout, stderr\n\n    return run_with_timeout\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    env.update(extra_env or {})\n    proc = subprocess.Popen(\n        [sys.executable, '-c', 'import time; time.sleep(%s); time.sleep(10)' % timeout],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env)\n    proc.communicate(input=func(*args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    env.update(extra_env or {})\n    proc = subprocess.Popen(\n        [sys.executable, '-c', 'import time; time.sleep(%d); print(time.time())' % timeout],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        env=env)\n    stdout, stderr = proc.communicate(input=func(*args))\n    if proc.wait()!= 0:\n        raise Exception(\"Timeout running %s\" % func.__name__)\n    return stdout\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    if extra_env is None:\n        extra_env = {}\n    env = os.environ.copy()\n    env.update(extra_env)\n    try:\n        proc = subprocess.Popen(\n            [sys.executable, func] + list(args),\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE)\n        proc.wait(timeout)\n        stdout, stderr = proc.communicate()\n    except subprocess.TimeoutExpired:\n        proc.kill()\n        proc.wait()\n        stdout, stderr = proc.communicate()\n    if stderr:\n        sys.stderr.write(stderr)\n    return stdout\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n    env = os.environ.copy()\n    if extra_env is not None:\n        env.update(extra_env)\n    proc = subprocess.Popen(\n        [sys.executable, '-c', func] + list(args),\n        env=env,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE)\n    stdout, stderr = proc.communicate()\n    if proc.wait()!= 0:\n        raise RuntimeError(\n            'Command {0!r} failed with exit status {1}'.format(\n               ''.join(map(pipes.quote, args)),\n                proc.returncode))\n    return stdout\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_6.py", "name": "_resolve_string", "docstring": "Get the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.", "ques_id": "62ece4992e6aefcf4aabbd7e", "code": ["def _resolve_string(matcher):\n    name, default = matcher.group(1), matcher.group(2)\n    if default is None:\n        default = os.getenv(name)\n    if default is not None:\n        try:\n            return os.environ[name]\n        except KeyError:\n            raise Error(\"The variable {} is not defined in the environment.\".format(name))\n    return default\n", "def _resolve_string(matcher):\n    name, default = matcher.groups()\n    if default is None:\n        raise EnvironmentError(f\"No default value provided for {name!r}\")\n    value = os.environ.get(name)\n    if value is None:\n        raise EnvironmentError(f\"Variable {name!r} is not defined\")\n    return value if value is not default else default\n", "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) or os.environ.get(name, '')\n    value = os.environ.get(name)\n\n    if value is None:\n        if default is not None:\n            raise Error(\"Missing {0} in the environment\".format(name))\n        raise Error(\"Missing {0} in the environment\".format(name))\n\n    return value.format(default)\n", "def _resolve_string(matcher):\n    name, default = matcher.groups()\n    value = os.environ.get(name, default)\n    if value is None:\n        raise Error('Variable \"{}\" is not defined'.format(name))\n    return value\n", "def _resolve_string(matcher):\n    name, default = matcher.group('name'), matcher.group('default')\n    if default is not None:\n        try:\n            return os.environ[name]\n        except KeyError:\n            error_msg = \"Variable '{}' is not defined in the {} environment\"\n            raise exceptions.VariableNotFound(error_msg.format(name, os.environ.get('NAME', '<unknown name>')))\n    return os.environ.get(name, default)\n", "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = os.getenv(name, '')\n    if default == '':\n        raise Error('Missing environment variable {}'.format(name))\n    return os.getenv(name, default)\n", "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) or os.environ.get(name)\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is None:\n            raise\n        return default\n", "def _resolve_string(matcher):\n    name, default = matcher.groups()\n    if default is None:\n        raise EnvironmentError('Variable {} is not defined'.format(name))\n    return os.environ.get(name, default)\n", "def _resolve_string(matcher):\n    name, default = matcher.groups()\n    if default is not None:\n        default = os.environ.get(name, default)\n    value = os.environ.get(name)\n    if value is None:\n        raise Error('The {0} variable is not set.'.format(name))\n    if default is False:\n        if value is True:\n            value = True\n        else:\n            raise Error('The {0} variable must be True or False.'.format(name))\n    return value, default\n", "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) or os.environ.get(name)\n    if default is None:\n        raise Error('Environment variable {} is not defined'.format(name))\n    return os.environ.get(name, default)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _resolve_string(matcher):\n    name, default = matcher.group(1), matcher.group(2)\n    if default is None:\n        default = os.getenv(name)\n    if default is not None:\n        try:\n            return os.environ[name]\n        except KeyError:\n            raise Error(\"The variable {} is not defined in the environment.\".format(name))\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name, default = matcher.groups()\n    if default is None:\n        raise EnvironmentError(f\"No default value provided for {name!r}\")\n    value = os.environ.get(name)\n    if value is None:\n        raise EnvironmentError(f\"Variable {name!r} is not defined\")\n    return value if value is not default else default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) or os.environ.get(name, '')\n    value = os.environ.get(name)\n\n    if value is None:\n        if default is not None:\n            raise Error(\"Missing {0} in the environment\".format(name))\n        raise Error(\"Missing {0} in the environment\".format(name))\n\n    return value.format(default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name, default = matcher.groups()\n    value = os.environ.get(name, default)\n    if value is None:\n        raise Error('Variable \"{}\" is not defined'.format(name))\n    return value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name, default = matcher.group('name'), matcher.group('default')\n    if default is not None:\n        try:\n            return os.environ[name]\n        except KeyError:\n            error_msg = \"Variable '{}' is not defined in the {} environment\"\n            raise exceptions.VariableNotFound(error_msg.format(name, os.environ.get('NAME', '<unknown name>')))\n    return os.environ.get(name, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = os.getenv(name, '')\n    if default == '':\n        raise Error('Missing environment variable {}'.format(name))\n    return os.getenv(name, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) or os.environ.get(name)\n    try:\n        return os.environ[name]\n    except KeyError:\n        if default is None:\n            raise\n        return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name, default = matcher.groups()\n    if default is None:\n        raise EnvironmentError('Variable {} is not defined'.format(name))\n    return os.environ.get(name, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name, default = matcher.groups()\n    if default is not None:\n        default = os.environ.get(name, default)\n    value = os.environ.get(name)\n    if value is None:\n        raise Error('The {0} variable is not set.'.format(name))\n    if default is False:\n        if value is True:\n            value = True\n        else:\n            raise Error('The {0} variable must be True or False.'.format(name))\n    return value, default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n    name = matcher.group(1)\n    default = matcher.group(2) or os.environ.get(name)\n    if default is None:\n        raise Error('Environment variable {} is not defined'.format(name))\n    return os.environ.get(name, default)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_2.py", "name": "_parse_image_ref", "docstring": "Parse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:", "ques_id": "62ece4992e6aefcf4aabbd7f", "code": ["def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    if not image_href:\n        raise ValueError('Image href cannot be empty')\n\n    image_id = image_href.split('/')[-1]\n\n    netloc = urlparse(image_href).netloc\n    use_ssl = False\n\n    if '@' in netloc:\n        netloc, _, use_ssl = netloc.rpartition('@')\n\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = image_href.split(':')[1].lower() == 'true'\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    url_pieces = urlparse(image_href)\n    path = url_pieces.path.lstrip('/')\n\n    if '//' in path:\n        raise ValueError('Invalid image reference: %s' % path)\n\n    netloc = path.rsplit(':', 1)[0]\n    use_ssl = not path.startswith('https://')\n\n    return path, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n\n    if parsed.netloc is None:\n        raise ValueError('Invalid image href: {}'.format(image_href))\n\n    netloc = parsed.netloc.split(':', 1)[0]\n    use_ssl = bool(parsed.scheme in ('http', 'https'))\n\n    return parsed.path, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = 'on' if 'https://' in image_href else 'off'\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    netloc = image_href.split('/')[-1]\n    image_id = image_href.split('/')[0]\n    use_ssl = False\n    if 'https://' in image_href:\n        image_ssl = image_href.split('https://')[1]\n        use_ssl = True\n    else:\n        image_ssl = None\n\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    netloc, image_id = image_href.split('/', 1)\n    use_ssl = not bool(netloc)\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    if not image_href:\n        raise ValueError('Image href cannot be empty')\n\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = False\n\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    if not image_href:\n        raise ValueError('Image href cannot be empty')\n\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = bool(image_href.split(':')[-1])\n\n    return image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    url_components = urlparse(image_href)\n    if not url_components.netloc:\n        raise ValueError('Invalid image reference: %s' % image_href)\n    netloc = url_components.netloc.split(':')[0]\n    image_id = url_components.path.split('/')[-1]\n    use_ssl = False if url_components.scheme == 'https' else True\n    return image_id, netloc, use_ssl\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    if not image_href:\n        raise ValueError('Image href cannot be empty')\n\n    image_id = image_href.split('/')[-1]\n\n    netloc = urlparse(image_href).netloc\n    use_ssl = False\n\n    if '@' in netloc:\n        netloc, _, use_ssl = netloc.rpartition('@')\n\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = image_href.split(':')[1].lower() == 'true'\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    url_pieces = urlparse(image_href)\n    path = url_pieces.path.lstrip('/')\n\n    if '//' in path:\n        raise ValueError('Invalid image reference: %s' % path)\n\n    netloc = path.rsplit(':', 1)[0]\n    use_ssl = not path.startswith('https://')\n\n    return path, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    parsed = urlparse(image_href)\n\n    if parsed.netloc is None:\n        raise ValueError('Invalid image href: {}'.format(image_href))\n\n    netloc = parsed.netloc.split(':', 1)[0]\n    use_ssl = bool(parsed.scheme in ('http', 'https'))\n\n    return parsed.path, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = 'on' if 'https://' in image_href else 'off'\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    netloc = image_href.split('/')[-1]\n    image_id = image_href.split('/')[0]\n    use_ssl = False\n    if 'https://' in image_href:\n        image_ssl = image_href.split('https://')[1]\n        use_ssl = True\n    else:\n        image_ssl = None\n\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    netloc, image_id = image_href.split('/', 1)\n    use_ssl = not bool(netloc)\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    if not image_href:\n        raise ValueError('Image href cannot be empty')\n\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = False\n\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    if not image_href:\n        raise ValueError('Image href cannot be empty')\n\n    image_id = image_href.split('/')[-1]\n    netloc = image_href.split(':')[0]\n    use_ssl = bool(image_href.split(':')[-1])\n\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    url_components = urlparse(image_href)\n    if not url_components.netloc:\n        raise ValueError('Invalid image reference: %s' % image_href)\n    netloc = url_components.netloc.split(':')[0]\n    image_id = url_components.path.split('/')[-1]\n    use_ssl = False if url_components.scheme == 'https' else True\n    return image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_13.py", "name": "remove_ending_os_sep", "docstring": "Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError", "ques_id": "62ece4992e6aefcf4aabbd80", "code": ["def remove_ending_os_sep(input_list):\n    # Test if list is not empty\n    if not input_list:\n        raise TypeError(\"List is empty.\")\n\n    # Test if list has at least one string\n    if not input_list[0]:\n        raise TypeError(\"List contains no strings.\")\n\n    # Remove trailing seperator character\n    if input_list[-1] == os.sep:\n        input_list = input_list[:-1]\n\n    # Return processed list\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    if isinstance(input_list, str):\n        return [s for s in input_list.rstrip(os.path.sep) if s]\n    else:\n        err_msg = \"input_list must be a string\"\n        raise TypeError(err_msg)\n", "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    if len(input_list) == 0:\n        raise ValueError(\"input_list cannot be empty\")\n    if len(input_list) == 1:\n        return input_list\n\n    last_char = input_list[-1]\n    if last_char == os.sep:\n        return input_list[:-1]\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError(\"Input parameter must be a list\")\n\n    if len(input_list) <= 1:\n        raise TypeError(\"Input parameter must have at least one element\")\n\n    processed_list = []\n    for item in input_list:\n        if item!= os.sep:\n            processed_list.append(item)\n\n    return processed_list\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) < 1:\n        raise TypeError(\"The input list is empty\")\n\n    if input_list[-1]!= os.sep:\n        input_list.pop()\n\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError('input_list must be a list')\n\n    for index, item in enumerate(input_list):\n        if isinstance(item, str):\n            if len(item) > 1:\n                input_list[index] = item.rstrip(os.sep)\n\n            if len(item) == 1:\n                input_list[index] = item\n                input_list[index] = remove_ending_os_sep(input_list[index])\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) > 1:\n        for i, val in enumerate(input_list):\n            if val!= os.path.sep:\n                return input_list[:i]\n    else:\n        raise TypeError('Input list should have only one element.')\n", "def remove_ending_os_sep(input_list):\n    if len(input_list) > 1 and input_list[-1] == os.sep:\n        return input_list[:-1]\n    return input_list\n", "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError('input_list must be a list')\n\n    if len(input_list) == 0:\n        raise ValueError('input_list must not be empty')\n\n    if len(input_list) == 1:\n        return input_list\n\n    if input_list[-1]!= os.sep:\n        return input_list\n\n    return input_list[:-1]\n", "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError('Input must be a list')\n\n    if len(input_list) == 0:\n        raise ValueError('Input list is empty')\n\n    if len(input_list) == 1:\n        return input_list\n\n    last_char = input_list[-1]\n    if last_char == '/':\n        return input_list[:-1]\n    elif last_char!= '\\\\':\n        return input_list\n\n    return input_list[:-1]\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def remove_ending_os_sep(input_list):\n    # Test if list is not empty\n    if not input_list:\n        raise TypeError(\"List is empty.\")\n\n    # Test if list has at least one string\n    if not input_list[0]:\n        raise TypeError(\"List contains no strings.\")\n\n    # Remove trailing seperator character\n    if input_list[-1] == os.sep:\n        input_list = input_list[:-1]\n\n    # Return processed list\n    return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if isinstance(input_list, str):\n        return [s for s in input_list.rstrip(os.path.sep) if s]\n    else:\n        err_msg = \"input_list must be a string\"\n        raise TypeError(err_msg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    if len(input_list) == 0:\n        raise ValueError(\"input_list cannot be empty\")\n    if len(input_list) == 1:\n        return input_list\n\n    last_char = input_list[-1]\n    if last_char == os.sep:\n        return input_list[:-1]\n    return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError(\"Input parameter must be a list\")\n\n    if len(input_list) <= 1:\n        raise TypeError(\"Input parameter must have at least one element\")\n\n    processed_list = []\n    for item in input_list:\n        if item!= os.sep:\n            processed_list.append(item)\n\n    return processed_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) < 1:\n        raise TypeError(\"The input list is empty\")\n\n    if input_list[-1]!= os.sep:\n        input_list.pop()\n\n    return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError('input_list must be a list')\n\n    for index, item in enumerate(input_list):\n        if isinstance(item, str):\n            if len(item) > 1:\n                input_list[index] = item.rstrip(os.sep)\n\n            if len(item) == 1:\n                input_list[index] = item\n                input_list[index] = remove_ending_os_sep(input_list[index])\n    return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) > 1:\n        for i, val in enumerate(input_list):\n            if val!= os.path.sep:\n                return input_list[:i]\n    else:\n        raise TypeError('Input list should have only one element.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if len(input_list) > 1 and input_list[-1] == os.sep:\n        return input_list[:-1]\n    return input_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError('input_list must be a list')\n\n    if len(input_list) == 0:\n        raise ValueError('input_list must not be empty')\n\n    if len(input_list) == 1:\n        return input_list\n\n    if input_list[-1]!= os.sep:\n        return input_list\n\n    return input_list[:-1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError('Input must be a list')\n\n    if len(input_list) == 0:\n        raise ValueError('Input list is empty')\n\n    if len(input_list) == 1:\n        return input_list\n\n    last_char = input_list[-1]\n    if last_char == '/':\n        return input_list[:-1]\n    elif last_char!= '\\\\':\n        return input_list\n\n    return input_list[:-1]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_17.py", "name": "get_pattern", "docstring": "This method converts the given string to regex pattern", "ques_id": "62ece4992e6aefcf4aabbd82", "code": ["def get_pattern(pattern, strip=True):\n    regex = re.compile(pattern)\n    if strip:\n        regex = regex.strip()\n    return regex\n", "def get_pattern(pattern, strip=True):\n    return re.compile('|'.join(\n        (\n            r'(?P<%s>' % p.replace('{', '(?P<'))\n            for p in pattern.split('|')\n        )\n    ) + (\n        r')' * (len(pattern) - len(pattern.lstrip('{}')))\n    ))\n", "def get_pattern(pattern, strip=True):\n    if strip:\n        pattern = re.compile(pattern.strip())\n    return pattern\n", "def get_pattern(pattern, strip=True):\n    if not isinstance(pattern, str):\n        raise TypeError('The given string must be a string')\n    if not isinstance(strip, bool):\n        raise TypeError('The given strip must be a boolean')\n    regex_pattern = re.compile(pattern)\n    if strip:\n        return regex_pattern.strip()\n    return regex_pattern\n", "def get_pattern(pattern, strip=True):\n    if strip:\n        return re.compile(re.sub(r'\\s+', '\\\\\\\\', pattern))\n    else:\n        return re.compile(re.sub(r'\\\\\\\\', '\\\\' + pattern, pattern))\n", "def get_pattern(pattern, strip=True):\n\n    return re.compile(\n        ''.join(\n            (\n                pattern,\n                '|' if strip else '.*',\n            )\n        )\n    )\n", "def get_pattern(pattern, strip=True):\n    regex = re.compile(pattern)\n    if strip:\n        return regex.strip()\n    else:\n        return regex\n", "def get_pattern(pattern, strip=True):\n    return re.compile(\"|\".join(f\"(?:{p})\" for p in pattern.split(\"|\"))) if strip else re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.UNICODE | re.IGNORECASE | re.VERBOSE) if strip else re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n    return re.compile(\n        \"(?:\"\n        \"(?P<regex>\"\n        \"(?P<type>\"\n        + pattern\n        + \"|\"\n        + \"(?:\"\n        + \"|\".join(\"(?:\" + x + \")\" for x in PATTERN_TYPES)\n        + \"))\"\n        \"(?:\"\n        \"(?P<replacement>\"\n        + pattern\n        + \"(?:\"\n        + \"(?P<replacement_strip>\"\n        + \"(?:\"\n        + \"|\".join(\"(?:\" + x + \")\" for x in PATTERN_REPLACEMENT_TYPES)\n        + \")))?)?\"\n        \"(?:\"\n        \"(?P<pattern>\"\n        + pattern\n        + \"(?:\"\n        + \"(?P<pattern_strip>\"\n        + \"(?:\"\n        + \"|\".join(\"(?:\" + x + \")\" for x in PATTERN_PATTERNS)\n        + \")))?)?)?\""], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_pattern(pattern, strip=True):\n    regex = re.compile(pattern)\n    if strip:\n        regex = regex.strip()\n    return regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    return re.compile('|'.join(\n        (\n            r'(?P<%s>' % p.replace('{', '(?P<'))\n            for p in pattern.split('|')\n        )\n    ) + (\n        r')' * (len(pattern) - len(pattern.lstrip('{}')))\n    ))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    if strip:\n        pattern = re.compile(pattern.strip())\n    return pattern\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n    if not isinstance(pattern, str):\n        raise TypeError('The given string must be a string')\n    if not isinstance(strip, bool):\n        raise TypeError('The given strip must be a boolean')\n    regex_pattern = re.compile(pattern)\n    if strip:\n        return regex_pattern.strip()\n    return regex_pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    if strip:\n        return re.compile(re.sub(r'\\s+', '\\\\\\\\', pattern))\n    else:\n        return re.compile(re.sub(r'\\\\\\\\', '\\\\' + pattern, pattern))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\n    return re.compile(\n        ''.join(\n            (\n                pattern,\n                '|' if strip else '.*',\n            )\n        )\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    regex = re.compile(pattern)\n    if strip:\n        return regex.strip()\n    else:\n        return regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    return re.compile(\"|\".join(f\"(?:{p})\" for p in pattern.split(\"|\"))) if strip else re.compile(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    return re.compile(pattern, re.UNICODE | re.IGNORECASE | re.VERBOSE) if strip else re.compile(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n    return re.compile(\n        \"(?:\"\n        \"(?P<regex>\"\n        \"(?P<type>\"\n        + pattern\n        + \"|\"\n        + \"(?:\"\n        + \"|\".join(\"(?:\" + x + \")\" for x in PATTERN_TYPES)\n        + \"))\"\n        \"(?:\"\n        \"(?P<replacement>\"\n        + pattern\n        + \"(?:\"\n        + \"(?P<replacement_strip>\"\n        + \"(?:\"\n        + \"|\".join(\"(?:\" + x + \")\" for x in PATTERN_REPLACEMENT_TYPES)\n        + \")))?)?\"\n        \"(?:\"\n        \"(?P<pattern>\"\n        + pattern\n        + \"(?:\"\n        + \"(?P<pattern_strip>\"\n        + \"(?:\"\n        + \"|\".join(\"(?:\" + x + \")\" for x in PATTERN_PATTERNS)\n        + \")))?)?)?\"", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_7.py", "name": "run_command", "docstring": "Call the given command(s).", "ques_id": "62ece4992e6aefcf4aabbd83", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_4.py", "name": "is_ipv4", "docstring": "Test if IPv4 address or not\n    ", "ques_id": "62ece4992e6aefcf4aabbd84", "code": ["def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n    try:\n        socket.inet_aton(target)\n    except socket.error:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_11.py", "name": "find_roots", "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader", "ques_id": "62ece4992e6aefcf4aabbd85", "code": ["def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    if graph.has_non_empty_attributes(prop):\n        for subclass in graph[prop]:\n            roots.update(find_roots(graph, subclass))\n    return roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form\n    (child, prop, parent), i.e. the direction of RDFS.subClassOf or\n    SKOS.broader\n    \"\"\"\n    if not roots:\n        roots = set()\n    if isinstance(graph, Graph):\n        for (u, p, o) in graph.triples((prop, RDF.subClassOf, None)):\n            roots.add(u)\n            roots.add(p)\n            roots.add(o)\n    elif isinstance(graph, Query):\n        for (u, p, o) in graph.triples((prop, RDF.subClassOf, None)):\n            roots.add(u)\n            roots.add(p)\n            roots.add(o)\n    elif isinstance(graph, RDFS.SubClassOf):\n        for (u, p, o) in graph.triples((prop", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for node in graph.subjects(RDFS.subClassOf, prop):\n        # Make a new root for the given node\n        if node not in roots:\n            roots.add(node)\n\n            # Make a new set of all parents of the given node\n            for parent in graph.prefix_object(node):\n                if parent not in roots:\n                    roots.add(parent)\n\n    return roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of RDFS.subClassOf\n    or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for pred, obj in graph.triples((prop, RDFS.subClassOf, None)):\n        if pred == SKOS.broader:\n            roots.update(find_roots(graph, obj, None))\n        elif pred == RDFS.superclassOf:\n            # Special case for SKOS.superclassOf\n            if isinstance(obj, BNode):\n                roots.add(obj)\n                continue\n            # Special case for SKOS.broader\n            if isinstance(obj, BNode) and obj.label() == prop.label():\n        ", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form\n\n    (child, prop, parent), i.e. the direction of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for u in graph.subjects(RDF.type, SKOS.broader):\n        if prop in graph[u]:\n            if u in roots:\n                continue\n            for p in graph[u][prop]:\n                roots.add(u)\n                roots.add(p)\n    return roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for child, predecessor in graph.predecessors(prop):\n        if predecessor in (SKOS.broader, SKOS.broader_indirect):\n            roots.add(child)\n        else:\n            roots |= find_roots(graph, predecessor, roots)\n\n    return roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of the\n    sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of RDFS.subClassOf\n    or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    if graph.has_triple(prop):\n        for sub_class in graph[prop]:\n            roots.update(find_roots(graph, sub_class, roots))\n    return roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    for u in graph.subjects(RDFS.subClassOf, prop):\n        if is_p(graph, u):\n            for p in graph.previous(u):\n                if p not in roots:\n                    roots.add(p)\n                    find_roots(graph, p, roots)\n    return roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    if isinstance(graph, Graph):\n        for (u, p, o) in graph.triples((prop, RDFS.subClassOf, None)):\n            roots.add(graph.single_source_node(u))\n            roots.add(graph.single_source_node(p))\n            roots.add(graph.single_source_node(o))\n    return roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for c in graph.subjects(RDF.type, SKOS.concept):\n        for p in graph.predecessors(c):\n            # Avoid duplicate roots\n            if p in roots:\n                continue\n\n            if prop in c.props:\n                if c in graph[p][prop]:\n                    roots.add(c)\n    return roots\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    if graph.has_non_empty_attributes(prop):\n        for subclass in graph[prop]:\n            roots.update(find_roots(graph, subclass))\n    return roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form\n    (child, prop, parent), i.e. the direction of RDFS.subClassOf or\n    SKOS.broader\n    \"\"\"\n    if not roots:\n        roots = set()\n    if isinstance(graph, Graph):\n        for (u, p, o) in graph.triples((prop, RDF.subClassOf, None)):\n            roots.add(u)\n            roots.add(p)\n            roots.add(o)\n    elif isinstance(graph, Query):\n        for (u, p, o) in graph.triples((prop, RDF.subClassOf, None)):\n            roots.add(u)\n            roots.add(p)\n            roots.add(o)\n    elif isinstance(graph, RDFS.SubClassOf):\n        for (u, p, o) in graph.triples((prop", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for node in graph.subjects(RDFS.subClassOf, prop):\n        # Make a new root for the given node\n        if node not in roots:\n            roots.add(node)\n\n            # Make a new set of all parents of the given node\n            for parent in graph.prefix_object(node):\n                if parent not in roots:\n                    roots.add(parent)\n\n    return roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of RDFS.subClassOf\n    or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for pred, obj in graph.triples((prop, RDFS.subClassOf, None)):\n        if pred == SKOS.broader:\n            roots.update(find_roots(graph, obj, None))\n        elif pred == RDFS.superclassOf:\n            # Special case for SKOS.superclassOf\n            if isinstance(obj, BNode):\n                roots.add(obj)\n                continue\n            # Special case for SKOS.broader\n            if isinstance(obj, BNode) and obj.label() == prop.label():\n        ", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form\n\n    (child, prop, parent), i.e. the direction of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for u in graph.subjects(RDF.type, SKOS.broader):\n        if prop in graph[u]:\n            if u in roots:\n                continue\n            for p in graph[u][prop]:\n                roots.add(u)\n                roots.add(p)\n    return roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction\n    of RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for child, predecessor in graph.predecessors(prop):\n        if predecessor in (SKOS.broader, SKOS.broader_indirect):\n            roots.add(child)\n        else:\n            roots |= find_roots(graph, predecessor, roots)\n\n    return roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of the\n    sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of RDFS.subClassOf\n    or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    if graph.has_triple(prop):\n        for sub_class in graph[prop]:\n            roots.update(find_roots(graph, sub_class, roots))\n    return roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots of\n    the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    for u in graph.subjects(RDFS.subClassOf, prop):\n        if is_p(graph, u):\n            for p in graph.previous(u):\n                if p not in roots:\n                    roots.add(p)\n                    find_roots(graph, p, roots)\n    return roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    if isinstance(graph, Graph):\n        for (u, p, o) in graph.triples((prop, RDFS.subClassOf, None)):\n            roots.add(graph.single_source_node(u))\n            roots.add(graph.single_source_node(p))\n            roots.add(graph.single_source_node(o))\n    return roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    find_roots(graph, rdflib.RDFS.subClassOf) will return a set of all roots\n    of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n\n    for c in graph.subjects(RDF.type, SKOS.concept):\n        for p in graph.predecessors(c):\n            # Avoid duplicate roots\n            if p in roots:\n                continue\n\n            if prop in c.props:\n                if c in graph[p][prop]:\n                    roots.add(c)\n    return roots\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_1.py", "name": "_dump_string", "docstring": "Dump to a py2-unicode or py3-string", "ques_id": "62ece4992e6aefcf4aabbd86", "code": ["def _dump_string(obj, dumper=None):\n\n    if not dumper:\n        dumper = AttrDumper()\n\n    if isinstance(obj, six.binary_type):\n        obj = obj.decode('utf-8')\n\n    dumper.representer.represent_unicode(obj, dumper.representer)\n    return dumper.represent_scalar(str(obj), 'text/plain')\n", "def _dump_string(obj, dumper=None):\n\n    if dumper is None:\n        dumper = Dumper\n\n    if isinstance(obj, string_types):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, list):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj, style='|')\n    elif isinstance(obj, dict):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj, style='|')\n    elif isinstance(obj, set):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:set', obj, style='|')\n    elif isinstance(obj, tuple):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:tuple', obj, style='|')\n    elif isinstance(obj, dict_):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:dict', obj, style='|')\n    elif isinstance(obj, (datetime, date, time)):\n        return dumpe", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        if isinstance(obj, six.binary_type):\n            dumper = salt.utils.stringutils.to_unicode\n        elif isinstance(obj, six.text_type):\n            dumper = salt.utils.stringutils.to_unicode\n        else:\n            raise TypeError('Can not dump to python string')\n    return dumper(obj)\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    if isinstance(obj, six.text_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, six.binary_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj, style='|')\n    return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj)\n", "def _dump_string(obj, dumper=None):\n    if not isinstance(obj, six.binary_type):\n        obj = obj.encode('utf-8')\n    if dumper is None:\n        dumper = py3ds9.dumpers.Py3Ds9Dumper()\n    return dumper.dump_string(obj)\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n\n    if isinstance(obj, six.binary_type):\n        dumper.represent_scalar(u'tag:yaml.org,2002:str', u'', style='|')\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, six.text_type):\n        dumper.represent_scalar(u'tag:yaml.org,2002:str', u'', style='|')\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    else:\n        raise TypeError(\"Type %s not supported.\" % type(obj))\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = Dumper\n    if isinstance(obj, six.text_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, six.binary_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj, style='|')\n    elif isinstance(obj, collections.Mapping):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj, style='|')\n    elif isinstance(obj, collections.Iterable):\n        return dumper.represent_iterable(u'tag:yaml.org,2002:list', obj, style='|')\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n", "def _dump_string(obj, dumper=None):\n\n    if dumper is None:\n        dumper = yaml.SafeDumper\n\n    if isinstance(obj, six.text_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, six.binary_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj, style='|')\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n", "def _dump_string(obj, dumper=None):\n\n    if isinstance(obj, six.binary_type):\n        obj = obj.decode('utf-8','surrogatepass')\n\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    elif not isinstance(dumper, yaml.SafeDumper):\n        raise TypeError('dumper must be a subclass of yaml.SafeDumper')\n\n    return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj,\n                                  style='|')\n", "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    if isinstance(obj, six.string_types):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, list):\n        return dumper.represent_sequence(u'tag:yaml.org,2002:seq', obj, flow_style=False)\n    elif isinstance(obj, dict):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj, flow_style=False)\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _dump_string(obj, dumper=None):\n\n    if not dumper:\n        dumper = AttrDumper()\n\n    if isinstance(obj, six.binary_type):\n        obj = obj.decode('utf-8')\n\n    dumper.representer.represent_unicode(obj, dumper.representer)\n    return dumper.represent_scalar(str(obj), 'text/plain')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\n    if dumper is None:\n        dumper = Dumper\n\n    if isinstance(obj, string_types):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, list):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj, style='|')\n    elif isinstance(obj, dict):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj, style='|')\n    elif isinstance(obj, set):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:set', obj, style='|')\n    elif isinstance(obj, tuple):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:tuple', obj, style='|')\n    elif isinstance(obj, dict_):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:dict', obj, style='|')\n    elif isinstance(obj, (datetime, date, time)):\n        return dumpe", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        if isinstance(obj, six.binary_type):\n            dumper = salt.utils.stringutils.to_unicode\n        elif isinstance(obj, six.text_type):\n            dumper = salt.utils.stringutils.to_unicode\n        else:\n            raise TypeError('Can not dump to python string')\n    return dumper(obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    if isinstance(obj, six.text_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, six.binary_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj, style='|')\n    return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if not isinstance(obj, six.binary_type):\n        obj = obj.encode('utf-8')\n    if dumper is None:\n        dumper = py3ds9.dumpers.Py3Ds9Dumper()\n    return dumper.dump_string(obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n\n    if isinstance(obj, six.binary_type):\n        dumper.represent_scalar(u'tag:yaml.org,2002:str', u'', style='|')\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, six.text_type):\n        dumper.represent_scalar(u'tag:yaml.org,2002:str', u'', style='|')\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    else:\n        raise TypeError(\"Type %s not supported.\" % type(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = Dumper\n    if isinstance(obj, six.text_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, six.binary_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj, style='|')\n    elif isinstance(obj, collections.Mapping):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj, style='|')\n    elif isinstance(obj, collections.Iterable):\n        return dumper.represent_iterable(u'tag:yaml.org,2002:list', obj, style='|')\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\n    if dumper is None:\n        dumper = yaml.SafeDumper\n\n    if isinstance(obj, six.text_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, six.binary_type):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:bytes', obj, style='|')\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\n    if isinstance(obj, six.binary_type):\n        obj = obj.decode('utf-8','surrogatepass')\n\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    elif not isinstance(dumper, yaml.SafeDumper):\n        raise TypeError('dumper must be a subclass of yaml.SafeDumper')\n\n    return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj,\n                                  style='|')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n    if dumper is None:\n        dumper = yaml.SafeDumper\n    if isinstance(obj, six.string_types):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n    elif isinstance(obj, list):\n        return dumper.represent_sequence(u'tag:yaml.org,2002:seq', obj, flow_style=False)\n    elif isinstance(obj, dict):\n        return dumper.represent_mapping(u'tag:yaml.org,2002:map', obj, flow_style=False)\n    else:\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', obj, style='|')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_8.py", "name": "build_app_logger", "docstring": "General purpose application logger. Useful mainly for debugging", "ques_id": "62ece4992e6aefcf4aabbd87", "code": ["def build_app_logger(name='app', logfile='app.log', debug=True):\n    log_formatter = logging.Formatter(\n        '[%(levelname)s] %(asctime)s %(name)s: %(message)s')\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n    app_logger.addHandler(logging.FileHandler(logfile, 'w'))\n    if debug:\n        app_logger.addHandler(logging.StreamHandler())\n    app_logger.propagate = False\n    return app_logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.INFO)\n\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n\n    app_logger.addHandler(handler)\n    if debug:\n        app_logger.setLevel(logging.DEBUG)\n    return app_logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n    # create the logging format and color\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n\n    # add handler to logger\n    logger.addHandler(handler)\n\n    # add formatter to logger\n    logger.setFormatter(formatter)\n\n    # add logger to logger list\n    app_loggers.append(logger)\n\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))\n    if debug:\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.DEBUG)\n        logger.addHandler(handler)\n    else:\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.INFO)\n        logger.addHandler(handler)\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    # Create logger\n    log = logging.getLogger(name)\n    if debug:\n        handler = logging.StreamHandler()\n        handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))\n        log.addHandler(handler)\n\n    # Setup logging stream\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(logging.Formatter('%(message)s'))\n    log.addHandler(stream_handler)\n\n    # Log to stderr\n    if debug:\n        logger = logging.getLogger('stderr')\n        logger.setLevel(logging.DEBUG)\n        logger.addHandler(stream_handler)\n    else:\n        logger = logging.getLogger('stdout')\n        logger.setLevel(logging.INFO)\n        logger.addHandler(stream_handler)\n\n    # Log to file\n    if logfile:\n        log_file_handler = logging.FileHandler(logfile)\n        log_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n        log.addHandler(log_file", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    log = logging.getLogger(name)\n    log.setLevel(logging.DEBUG if debug else logging.INFO)\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    log.addHandler(handler)\n    return log\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    if debug:\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\n            '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n    else:\n        handler = logging.FileHandler(logfile)\n        handler.setFormatter(logging.Formatter(\n            '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'))\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logformat = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n    logdatefmt = '%m/%d/%Y %I:%M:%S %p'\n    handler = logging.StreamHandler()\n    handler.setLevel(logging.INFO)\n    handler.setFormatter(logging.Formatter(logformat, logdatefmt))\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n    logger.addHandler(handler)\n    if debug:\n        logger.setLevel(logging.DEBUG)\n    return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.INFO)\n\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n\n    if debug:\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n\n    return logger\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    log_formatter = logging.Formatter(\n        '[%(levelname)s] %(asctime)s %(name)s: %(message)s')\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n    app_logger.addHandler(logging.FileHandler(logfile, 'w'))\n    if debug:\n        app_logger.addHandler(logging.StreamHandler())\n    app_logger.propagate = False\n    return app_logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    app_logger = logging.getLogger(name)\n    app_logger.setLevel(logging.INFO)\n\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.INFO)\n\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n\n    app_logger.addHandler(handler)\n    if debug:\n        app_logger.setLevel(logging.DEBUG)\n    return app_logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n    # create the logging format and color\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n\n    # add handler to logger\n    logger.addHandler(handler)\n\n    # add formatter to logger\n    logger.setFormatter(formatter)\n\n    # add logger to logger list\n    app_loggers.append(logger)\n\n    return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))\n    if debug:\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.DEBUG)\n        logger.addHandler(handler)\n    else:\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.INFO)\n        logger.addHandler(handler)\n    return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    # Create logger\n    log = logging.getLogger(name)\n    if debug:\n        handler = logging.StreamHandler()\n        handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))\n        log.addHandler(handler)\n\n    # Setup logging stream\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(logging.Formatter('%(message)s'))\n    log.addHandler(stream_handler)\n\n    # Log to stderr\n    if debug:\n        logger = logging.getLogger('stderr')\n        logger.setLevel(logging.DEBUG)\n        logger.addHandler(stream_handler)\n    else:\n        logger = logging.getLogger('stdout')\n        logger.setLevel(logging.INFO)\n        logger.addHandler(stream_handler)\n\n    # Log to file\n    if logfile:\n        log_file_handler = logging.FileHandler(logfile)\n        log_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n        log.addHandler(log_file", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    log = logging.getLogger(name)\n    log.setLevel(logging.DEBUG if debug else logging.INFO)\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    log.addHandler(handler)\n    return log\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    if debug:\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\n            '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n    else:\n        handler = logging.FileHandler(logfile)\n        handler.setFormatter(logging.Formatter(\n            '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'))\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n    return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logformat = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n    logdatefmt = '%m/%d/%Y %I:%M:%S %p'\n    handler = logging.StreamHandler()\n    handler.setLevel(logging.INFO)\n    handler.setFormatter(logging.Formatter(logformat, logdatefmt))\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n    logger.addHandler(handler)\n    if debug:\n        logger.setLevel(logging.DEBUG)\n    return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    handler = logging.FileHandler(logfile)\n    handler.setLevel(logging.INFO)\n\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n\n    if debug:\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n\n    return logger\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_6.py", "name": "make_array", "docstring": "Function to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create", "ques_id": "62ece4992e6aefcf4aabbd88", "code": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.ones(shape, dtype=dtype) * np.nan\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.full(shape, np.nan, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.ones(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.ones(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.ones(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.full(shape, np.nan, dtype=dtype)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.ones(shape, dtype=dtype) * np.nan\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype=dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.full(shape, np.nan, dtype=dtype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.ones(shape, dtype=dtype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.ones(shape, dtype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.ones(shape, dtype=dtype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n    return np.full(shape, np.nan, dtype=dtype)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_2.py", "name": "gaussian", "docstring": "Gaussian centered around 0.2 with a sigma of 0.1.", "ques_id": "62ece4992e6aefcf4aabbd89", "code": ["def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-np.power(x-0.2, 2)/(2*np.power(0.1, 2)))\n", "def gaussian(x):\n    return np.exp(-(x**2)/(2*np.pi**2)) * 0.2 + 0.1\n", "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x - 0.2)**2/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-np.power(x - 0.2, 2) / (2 * np.power(0.1, 2)))\n", "def gaussian(x):\n    return np.exp(-(x**2) / (2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "def gaussian(x):\n    return np.exp(-(x**2) / (2*np.pi*0.2**2))\n", "def gaussian(x):\n    return np.exp(-np.power(x-0.2,2)/(2*np.power(0.1,2)))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-np.power(x-0.2, 2)/(2*np.power(0.1, 2)))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*np.pi**2)) * 0.2 + 0.1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x - 0.2)**2/(2*0.1**2))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n    return np.exp(-np.power(x - 0.2, 2) / (2 * np.power(0.1, 2)))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2) / (2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2)/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-(x**2) / (2*np.pi*0.2**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n    return np.exp(-np.power(x-0.2,2)/(2*np.power(0.1,2)))\n", "is_pass": true, "return_code": 0}]}
{"file_path": "plib_runnable/test_13.py", "name": "load_configurations", "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.", "ques_id": "62ece4992e6aefcf4aabbd8a", "code": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_config_files(config_filenames, resolve_env)\n    logging.basicConfig(\n        format=\"%(levelname)s:%(name)s:%(message)s\",\n        level=logging.DEBUG,\n        stream=sys.stderr,\n        filename=os.devnull,\n    )\n    return configs, configs\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = _load_configurations_from_filenames(config_filenames,\n                                                  overrides, resolve_env)\n    return configs, _validate_configurations(configs)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_config_files(config_filenames, resolve_env=resolve_env)\n\n    parser = ConfigParser()\n    parser.read_file(configs[0])\n\n    # Load the default logging configuration\n    logging.config.fileConfig(\n        os.path.join(os.path.dirname(__file__), 'logging.conf'))\n\n    # Load any configuration files that were specified with the --config option\n    if len(config_filenames) > 1:\n        default_config = load_config_files(config_filenames[1:], resolve_env=resolve_env)\n        parser.read_file(default_config)\n\n    # Validate the configurations\n    for config_filename in config_filenames[:-1]:\n        configs = validate_configs(configs, config_filename)\n\n    # Validate the configurations\n    for config_filename in config_filenames[-1:]:\n        configs = validate_configs(configs, config_filename)\n\n    # Validate the logging configuration\n    if overrides:\n        configs = validate_configs(configs, overrides)\n    return configs, configs['logging']\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_configs(config_filenames, resolve_env=resolve_env)\n    validate_configurations(configs, overrides=overrides)\n    return configs, config_filenames\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    return load_configs(config_filenames, _load_configuration, overrides,\n                        resolve_env)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n\n    for config_filename in config_filenames:\n        try:\n            config = load_configuration(config_filename, resolve_env)\n        except ParseError as exc:\n            errors.append(exc)\n        else:\n            configs[config_filename] = config\n\n    if overrides:\n        configs.update(overrides)\n\n    return configs, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_config_files(config_filenames, resolve_env=resolve_env)\n\n    errors = []\n    for filename, config in configs.items():\n        try:\n            validate_config(config, overrides=overrides)\n        except ValidationError as e:\n            errors.append(e)\n            continue\n\n        result = config_to_result(config)\n        yield filename, result\n\n    if errors:\n        log.warning(\"Failed to load %d configuration files:\", len(errors))\n        for e in errors:\n            log.warning(\"  %s\", e)\n\n    log.debug(\"Valid configurations: %s\", configs)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_configuration_files(config_filenames, resolve_env)\n    logging.config.fileConfig(configs[0], defaults=configs[1])\n\n    # If a custom override was given, use it instead of loading the default\n    # configuration.\n    if overrides:\n        logging.config.dictConfig(overrides)\n\n    return configs[0], configs[1]\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    # type: (Sequence[Text], Optional[Mapping[Text, Mapping[Text, Text]]], bool) -> Tuple[Mapping[Text, Mapping[Text, Text]], Sequence[logging.LogRecord]]\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file.\n\n    Return the results as a tuple of:\n        dict of configuration filename to corresponding parsed configuration,\n        and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    return _load_configurations(config_filenames, overrides, resolve_env)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = []\n    errors = []\n    for config_filename in config_filenames:\n        try:\n            config = load_config_file(config_filename, resolve_env=resolve_env)\n        except ConfigFileNotFound:\n            errors.append(config_filename)\n            continue\n        except ConfigFileValidationError as e:\n            errors.append(str(e))\n            continue\n        configs.append(config)\n\n    if overrides is not None:\n        configs.extend(overrides)\n\n    return configs, errors\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_config_files(config_filenames, resolve_env)\n    logging.basicConfig(\n        format=\"%(levelname)s:%(name)s:%(message)s\",\n        level=logging.DEBUG,\n        stream=sys.stderr,\n        filename=os.devnull,\n    )\n    return configs, configs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = _load_configurations_from_filenames(config_filenames,\n                                                  overrides, resolve_env)\n    return configs, _validate_configurations(configs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_config_files(config_filenames, resolve_env=resolve_env)\n\n    parser = ConfigParser()\n    parser.read_file(configs[0])\n\n    # Load the default logging configuration\n    logging.config.fileConfig(\n        os.path.join(os.path.dirname(__file__), 'logging.conf'))\n\n    # Load any configuration files that were specified with the --config option\n    if len(config_filenames) > 1:\n        default_config = load_config_files(config_filenames[1:], resolve_env=resolve_env)\n        parser.read_file(default_config)\n\n    # Validate the configurations\n    for config_filename in config_filenames[:-1]:\n        configs = validate_configs(configs, config_filename)\n\n    # Validate the configurations\n    for config_filename in config_filenames[-1:]:\n        configs = validate_configs(configs, config_filename)\n\n    # Validate the logging configuration\n    if overrides:\n        configs = validate_configs(configs, overrides)\n    return configs, configs['logging']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_configs(config_filenames, resolve_env=resolve_env)\n    validate_configurations(configs, overrides=overrides)\n    return configs, config_filenames\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    return load_configs(config_filenames, _load_configuration, overrides,\n                        resolve_env)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = {}\n    errors = []\n\n    for config_filename in config_filenames:\n        try:\n            config = load_configuration(config_filename, resolve_env)\n        except ParseError as exc:\n            errors.append(exc)\n        else:\n            configs[config_filename] = config\n\n    if overrides:\n        configs.update(overrides)\n\n    return configs, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_config_files(config_filenames, resolve_env=resolve_env)\n\n    errors = []\n    for filename, config in configs.items():\n        try:\n            validate_config(config, overrides=overrides)\n        except ValidationError as e:\n            errors.append(e)\n            continue\n\n        result = config_to_result(config)\n        yield filename, result\n\n    if errors:\n        log.warning(\"Failed to load %d configuration files:\", len(errors))\n        for e in errors:\n            log.warning(\"  %s\", e)\n\n    log.debug(\"Valid configurations: %s\", configs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = load_configuration_files(config_filenames, resolve_env)\n    logging.config.fileConfig(configs[0], defaults=configs[1])\n\n    # If a custom override was given, use it instead of loading the default\n    # configuration.\n    if overrides:\n        logging.config.dictConfig(overrides)\n\n    return configs[0], configs[1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    # type: (Sequence[Text], Optional[Mapping[Text, Mapping[Text, Text]]], bool) -> Tuple[Mapping[Text, Mapping[Text, Text]], Sequence[logging.LogRecord]]\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file.\n\n    Return the results as a tuple of:\n        dict of configuration filename to corresponding parsed configuration,\n        and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    return _load_configurations(config_filenames, overrides, resolve_env)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    configs = []\n    errors = []\n    for config_filename in config_filenames:\n        try:\n            config = load_config_file(config_filename, resolve_env=resolve_env)\n        except ConfigFileNotFound:\n            errors.append(config_filename)\n            continue\n        except ConfigFileValidationError as e:\n            errors.append(str(e))\n            continue\n        configs.append(config)\n\n    if overrides is not None:\n        configs.extend(overrides)\n\n    return configs, errors\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_3.py", "name": "force_string", "docstring": "This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. ", "ques_id": "62ece4992e6aefcf4aabbd8b", "code": ["def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode(\"utf-8\")\n    else:\n        return obj\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, str):\n        return obj.encode(\"utf-8\")\n    else:\n        raise TypeError(\"force_string must be called with a string or bytes object as input\")\n", "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"Type %s not handled by force_string\" % type(obj))\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, six.string_types):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"force_string received unexpected type of %r\" % type(obj))\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"force_string received %s of type %s\" %\n                        (obj, type(obj)))\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"Type %s not handled by force_string\" % type(obj))\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"Type %s not supported.\" % type(obj))\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, str):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"Type %s not handled as string for parameter 'obj'.\" % type(obj))\n", "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    elif isinstance(obj, list):\n        return [force_string(i) for i in obj]\n    elif isinstance(obj, dict):\n        return {force_string(key): force_string(value)\n                for key, value in iteritems(obj)}\n    elif isinstance(obj, (int, float, bool, str, type(None))):\n        return str(obj)\n    elif isinstance(obj, (list, tuple)):\n        return list(force_string(i) for i in obj)\n    elif isinstance(obj, dict):\n        return dict(force_string(key) for key in obj)\n    else:\n        return obj\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode(\"utf-8\")\n    else:\n        return obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, str):\n        return obj.encode(\"utf-8\")\n    else:\n        raise TypeError(\"force_string must be called with a string or bytes object as input\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"Type %s not handled by force_string\" % type(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, six.string_types):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"force_string received unexpected type of %r\" % type(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"force_string received %s of type %s\" %\n                        (obj, type(obj)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"Type %s not handled by force_string\" % type(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"Type %s not supported.\" % type(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, str):\n        return obj.encode('utf-8')\n    else:\n        raise TypeError(\"Type %s not handled as string for parameter 'obj'.\" % type(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n    if isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    elif isinstance(obj, list):\n        return [force_string(i) for i in obj]\n    elif isinstance(obj, dict):\n        return {force_string(key): force_string(value)\n                for key, value in iteritems(obj)}\n    elif isinstance(obj, (int, float, bool, str, type(None))):\n        return str(obj)\n    elif isinstance(obj, (list, tuple)):\n        return list(force_string(i) for i in obj)\n    elif isinstance(obj, dict):\n        return dict(force_string(key) for key in obj)\n    else:\n        return obj\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "from_ticks", "docstring": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)", "ques_id": "62e60723d76274f8a4026b76", "code": ["def from_ticks(cls, ticks, tz=None):\n    return cls._from_time(\n        _time.mktime(_time.gmtime(ticks)[:6] + (0,)), tz)\n", "def from_ticks(cls, ticks, tz=None):\n    if ticks < 0:\n        raise ValueError(\"ticks must be >= 0\")\n    if ticks > 86400000000000:\n        raise ValueError(\"ticks must be <= 86400000000000\")\n\n    if tz is None:\n        tz = dateutil_tz.tzlocal()\n\n    return cls(tz.localize(ticks * 1e-9, is_dst=None))\n", "def from_ticks(cls, ticks, tz=None):\n    return cls.from_datetime(datetime.utcfromtimestamp(ticks), tz)\n", "def from_ticks(cls, ticks, tz=None):\n    if not 0 <= ticks < 86400000000:\n        raise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000)\")\n\n    return cls._from_time_struct(_time_from_ticks(ticks, tz))\n", "def from_ticks(cls, ticks, tz=None):\n    if not 0 <= ticks < 86400000000:\n        raise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000000)\")\n\n    return cls._from_time(\n        time.gmtime(ticks * 1000000000), tz=tz)\n", "def from_ticks(cls, ticks, tz=None):\n    if ticks < 0:\n        raise ValueError('ticks must be greater than zero')\n    if ticks > (86400000000 * 1000):\n        raise ValueError('ticks must be less than 86400000000000')\n\n    if tz is None:\n        tz = _get_timezone()\n\n    return cls(tz.localize(datetime.time(ticks // 1000, ticks % 1000)))\n", "def from_ticks(cls, ticks, tz=None):\n    if not 0 <= ticks < 86400000000:\n        raise ValueError('ticks out of bounds (0 <= ticks < 86400000000)')\n\n    if tz is None:\n        tz = _get_timezone('UTC')\n\n    dt = datetime.datetime.utcfromtimestamp(ticks / 1000000000)\n    return cls.from_datetime(dt, tz)\n", "def from_ticks(cls, ticks, tz=None):\n    if not 0 <= ticks < 86400000000:\n        raise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000)\")\n\n    return cls._from_time(\n        Time.from_ns(ticks, tz),\n        tz\n    )\n", "def from_ticks(cls, ticks, tz=None):\n    tz = tz or dateutil_tz.tzlocal()\n    return cls(ticks * 1000000000, tz)\n", "def from_ticks(cls, ticks, tz=None):\n    return cls._from_timestamp(*time.gmtime(ticks * 1e3 / 1000000000, tz))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def from_ticks(cls, ticks, tz=None):\n    return cls._from_time(\n        _time.mktime(_time.gmtime(ticks)[:6] + (0,)), tz)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if ticks < 0:\n        raise ValueError(\"ticks must be >= 0\")\n    if ticks > 86400000000000:\n        raise ValueError(\"ticks must be <= 86400000000000\")\n\n    if tz is None:\n        tz = dateutil_tz.tzlocal()\n\n    return cls(tz.localize(ticks * 1e-9, is_dst=None))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    return cls.from_datetime(datetime.utcfromtimestamp(ticks), tz)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if not 0 <= ticks < 86400000000:\n        raise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000)\")\n\n    return cls._from_time_struct(_time_from_ticks(ticks, tz))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if not 0 <= ticks < 86400000000:\n        raise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000000)\")\n\n    return cls._from_time(\n        time.gmtime(ticks * 1000000000), tz=tz)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if ticks < 0:\n        raise ValueError('ticks must be greater than zero')\n    if ticks > (86400000000 * 1000):\n        raise ValueError('ticks must be less than 86400000000000')\n\n    if tz is None:\n        tz = _get_timezone()\n\n    return cls(tz.localize(datetime.time(ticks // 1000, ticks % 1000)))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if not 0 <= ticks < 86400000000:\n        raise ValueError('ticks out of bounds (0 <= ticks < 86400000000)')\n\n    if tz is None:\n        tz = _get_timezone('UTC')\n\n    dt = datetime.datetime.utcfromtimestamp(ticks / 1000000000)\n    return cls.from_datetime(dt, tz)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    if not 0 <= ticks < 86400000000:\n        raise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000)\")\n\n    return cls._from_time(\n        Time.from_ns(ticks, tz),\n        tz\n    )\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    tz = tz or dateutil_tz.tzlocal()\n    return cls(ticks * 1000000000, tz)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n    return cls._from_timestamp(*time.gmtime(ticks * 1e3 / 1000000000, tz))\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_async/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "ques_id": "62e60873d76274f8a4026bd8", "code": ["def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        versions = cls.VERSIONS\n    else:\n        versions = [protocol_version] + list(protocol_version)\n\n    return {version: cls.handler_for_version(version) for version in versions}\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        versions = cls._protocol_versions\n        return {v: k for k, v in versions.items()}\n    else:\n        v = protocol_version\n        if not isinstance(v, tuple):\n            raise TypeError(\n                'protocol_version must be a tuple or None; got {!r}'.format(v))\n        return {v[0]: k for k, v in versions.items() if v[1] == v[2]}\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    elif protocol_version in cls._protocol_handlers:\n        return cls._protocol_handlers[protocol_version]\n    else:\n        raise TypeError(\n            'protocol_version {} not supported by {}'.format(\n                protocol_version, cls.__name__))\n", "def protocol_handlers(cls, protocol_version=None):\n    version_tuples = _protocol_version_to_handlers.get(protocol_version, None)\n    if version_tuples is None:\n        version_tuples = _protocol_version_to_handlers[protocol_version] = {}\n    return version_tuples\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        versions = cls._VERSIONS\n    else:\n        versions = [protocol_version] + list(cls._VERSIONS)\n\n    handlers = {}\n    for version in versions:\n        handlers[version] = getattr(BoltHandler, 'protocol_' + str(version))\n\n    return handlers\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    else:\n        return cls._protocol_handlers.get(protocol_version, {})\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        versions = cls._protocol_versions\n    else:\n        versions = [protocol_version] + cls._protocol_versions\n\n    return {version: cls._handler_for_version(version) for version in versions}\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return _protocol_handlers\n\n    return {\n        version: protocol_handler\n        for version, protocol_handler in _protocol_handlers.items()\n        if version >= protocol_version\n    }\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return _protocol_handlers\n    elif not isinstance(protocol_version, tuple):\n        raise TypeError(\n            'protocol_version must be a tuple or None, not {}'.format(\n                protocol_version))\n    return {\n        version: getattr(Bolt, 'handle_{}'.format(version))\n        for version in _protocol_handlers\n        if (version in protocol_version)\n    }\n", "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return _PROTOCOL_HANDLER_CLASSES\n    else:\n        try:\n            return {\n                version: getattr(BoltProtocol, 'handle_{}'.format(version))\n                for version in _PROTOCOL_HANDLER_CLASSES[protocol_version]\n            }\n        except KeyError:\n            raise TypeError(\n                'protocol_version {} is not supported'.format(protocol_version))\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        versions = cls.VERSIONS\n    else:\n        versions = [protocol_version] + list(protocol_version)\n\n    return {version: cls.handler_for_version(version) for version in versions}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        versions = cls._protocol_versions\n        return {v: k for k, v in versions.items()}\n    else:\n        v = protocol_version\n        if not isinstance(v, tuple):\n            raise TypeError(\n                'protocol_version must be a tuple or None; got {!r}'.format(v))\n        return {v[0]: k for k, v in versions.items() if v[1] == v[2]}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    elif protocol_version in cls._protocol_handlers:\n        return cls._protocol_handlers[protocol_version]\n    else:\n        raise TypeError(\n            'protocol_version {} not supported by {}'.format(\n                protocol_version, cls.__name__))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    version_tuples = _protocol_version_to_handlers.get(protocol_version, None)\n    if version_tuples is None:\n        version_tuples = _protocol_version_to_handlers[protocol_version] = {}\n    return version_tuples\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        versions = cls._VERSIONS\n    else:\n        versions = [protocol_version] + list(cls._VERSIONS)\n\n    handlers = {}\n    for version in versions:\n        handlers[version] = getattr(BoltHandler, 'protocol_' + str(version))\n\n    return handlers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return cls._protocol_handlers\n    else:\n        return cls._protocol_handlers.get(protocol_version, {})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        versions = cls._protocol_versions\n    else:\n        versions = [protocol_version] + cls._protocol_versions\n\n    return {version: cls._handler_for_version(version) for version in versions}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return _protocol_handlers\n\n    return {\n        version: protocol_handler\n        for version, protocol_handler in _protocol_handlers.items()\n        if version >= protocol_version\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return _protocol_handlers\n    elif not isinstance(protocol_version, tuple):\n        raise TypeError(\n            'protocol_version must be a tuple or None, not {}'.format(\n                protocol_version))\n    return {\n        version: getattr(Bolt, 'handle_{}'.format(version))\n        for version in _protocol_handlers\n        if (version in protocol_version)\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n    if protocol_version is None:\n        return _PROTOCOL_HANDLER_CLASSES\n    else:\n        try:\n            return {\n                version: getattr(BoltProtocol, 'handle_{}'.format(version))\n                for version in _PROTOCOL_HANDLER_CLASSES[protocol_version]\n            }\n        except KeyError:\n            raise TypeError(\n                'protocol_version {} is not supported'.format(protocol_version))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/api.py", "project": "neo4j/neo4j-python-driver", "name": "from_raw_values", "docstring": "Create a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]", "ques_id": "62e60e3bd76274f8a4026d1a", "code": ["def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n", "def from_raw_values(cls, values):\n    return cls(values)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n    return cls(values)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/split.py", "project": "ynikitenko/lena", "name": "_get_seq_with_type", "docstring": "Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).", "ques_id": "62b87b199a0c4fa8b80b354c", "code": ["def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        seq = seq.__class__, bufsize\n    else:\n        seq = seq, bufsize\n    return seq\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        # This is already a sequence, return it directly.\n        return seq, seq._type\n    if isinstance(seq, type):\n        # This is a type, return it as-is.\n        return seq, seq\n    if isinstance(seq, basestring):\n        # This is a string instead, so just return it.\n        return seq, seq\n    if bufsize is None:\n        bufsize = sys.getsizeof(seq)\n    if not isinstance(seq, (list, tuple)):\n        raise TypeError(\"'seq' must be a sequence, not %r\" % seq)\n    if len(seq) < bufsize:\n        raise ValueError(\"'seq' must have at least %d elements, not %d\"\n                         % (bufsize, len(seq)))\n    # This is a bit of a mess: we create a new list of the right size\n    ", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is not None:\n            return seq._get_typed_seq(bufsize)\n        else:\n            return seq\n    else:\n        return (seq, type(seq))\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (_get_seq_as_str(seq), _get_seq_as_type(seq, bufsize))\n    elif isinstance(seq, (str, bytes, bytearray)):\n        return (_get_seq_as_str(seq), _get_seq_as_type(seq, bufsize))\n    else:\n        raise TypeError('seq must be a string or bytes, bytearray, '\n                        'or Sequence, not %r' % seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (seq.__class__, (type(seq),) + _get_seq_with_type(seq.__class__, bufsize))\n    else:\n        return (seq, type(seq))\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        # if we were given an iterator then we need to convert it into a\n        # tuple containing the underlying data.  This avoids having\n        # to make an empty tuple for something else than an iterator.\n        if isinstance(seq, Iterator):\n            seq = tuple(seq)\n        return seq, type(seq)\n    else:\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getrefcount(seq)\n        t = _SequenceType\n    else:\n        t = _GenericType\n\n    if hasattr(seq, '__len__'):\n        if bufsize is None:\n            bufsize = sys.getrefcount(seq)\n        return tuple(_get_seq_with_type(i, bufsize) for i in seq)\n    else:\n        return (t(seq), t)\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, type):\n        return seq, seq.__name__\n    elif isinstance(seq, Sequence):\n        return seq, type(seq).__name__\n    elif isinstance(seq, type) and issubclass(seq, BaseException):\n        return seq, 'exception'\n    else:\n        return seq, 'generic'\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (seq.__class__, type(seq))\n    else:\n        return (type(seq), type(seq))\n", "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence) or isinstance(seq, Mapping):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (tuple(seq), type(seq))\n    else:\n        return (seq, type(seq))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        seq = seq.__class__, bufsize\n    else:\n        seq = seq, bufsize\n    return seq\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        # This is already a sequence, return it directly.\n        return seq, seq._type\n    if isinstance(seq, type):\n        # This is a type, return it as-is.\n        return seq, seq\n    if isinstance(seq, basestring):\n        # This is a string instead, so just return it.\n        return seq, seq\n    if bufsize is None:\n        bufsize = sys.getsizeof(seq)\n    if not isinstance(seq, (list, tuple)):\n        raise TypeError(\"'seq' must be a sequence, not %r\" % seq)\n    if len(seq) < bufsize:\n        raise ValueError(\"'seq' must have at least %d elements, not %d\"\n                         % (bufsize, len(seq)))\n    # This is a bit of a mess: we create a new list of the right size\n    ", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is not None:\n            return seq._get_typed_seq(bufsize)\n        else:\n            return seq\n    else:\n        return (seq, type(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (_get_seq_as_str(seq), _get_seq_as_type(seq, bufsize))\n    elif isinstance(seq, (str, bytes, bytearray)):\n        return (_get_seq_as_str(seq), _get_seq_as_type(seq, bufsize))\n    else:\n        raise TypeError('seq must be a string or bytes, bytearray, '\n                        'or Sequence, not %r' % seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (seq.__class__, (type(seq),) + _get_seq_with_type(seq.__class__, bufsize))\n    else:\n        return (seq, type(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        # if we were given an iterator then we need to convert it into a\n        # tuple containing the underlying data.  This avoids having\n        # to make an empty tuple for something else than an iterator.\n        if isinstance(seq, Iterator):\n            seq = tuple(seq)\n        return seq, type(seq)\n    else:\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getrefcount(seq)\n        t = _SequenceType\n    else:\n        t = _GenericType\n\n    if hasattr(seq, '__len__'):\n        if bufsize is None:\n            bufsize = sys.getrefcount(seq)\n        return tuple(_get_seq_with_type(i, bufsize) for i in seq)\n    else:\n        return (t(seq), t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, type):\n        return seq, seq.__name__\n    elif isinstance(seq, Sequence):\n        return seq, type(seq).__name__\n    elif isinstance(seq, type) and issubclass(seq, BaseException):\n        return seq, 'exception'\n    else:\n        return seq, 'generic'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (seq.__class__, type(seq))\n    else:\n        return (type(seq), type(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n    if isinstance(seq, Sequence) or isinstance(seq, Mapping):\n        if bufsize is None:\n            bufsize = sys.getsizeof(seq)\n        return (tuple(seq), type(seq))\n    else:\n        return (seq, type(seq))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.", "ques_id": "62b87b4f9a0c4fa8b80b3581", "code": ["def scale(self, other=None, recompute=False):\n\n    if self._scale is not None and not recompute:\n        if other is None:\n            other = self._scale\n        else:\n            try:\n                self._scale = other / self._count\n            except ZeroDivisionError:\n                raise LenaValueError(\"scale of %s cannot be recomputed\" % (self,))\n\n    self._count = 0\n    self._sum = 0\n\n    if self._scale is not None:\n        self._update_scale(other)\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        other = self._hist.scale\n    elif not isinstance(other, numbers.Number):\n        raise LenaHistError(\n            \"Attempted to rescale histogram with a non-numerical value\")\n\n    if self._scale is not None and other!= self._scale:\n        if recompute:\n            self.recompute()\n        else:\n            raise LenaHistError(\n                \"Histogram scale was not set properly after changing (filling)\")\n\n    self._scale = other\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        # compute scale of the histogram\n        try:\n            scale = self._hist.scale()\n        except AttributeError:\n            scale = 0.0\n        else:\n            if recompute:\n                self.recompute()\n                scale = self._hist.scale()\n\n        return scale\n    else:\n        # set scale of the histogram\n        self._hist.scale(other)\n\n        # update scale if necessary\n        if recompute:\n            self.recompute()\n", "def scale(self, other=None, recompute=False):\n\n    if self._scale is not None and not recompute and other is not None:\n        raise LenaHistoError(\"Scale of histo already computed!\")\n\n    if self._scale is None and other is not None:\n        self._scale = self._compute_scale(other)\n    elif self._scale is None and recompute and other is None:\n        self.recompute(fill=False)\n\n    return self._scale\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self.scale is not None:\n            return\n        other = self.new()\n\n    # if scale is not computed, compute it\n    if not self.scale:\n        self._compute_scale(other)\n\n    # otherwise, set scale of self to the provided value\n    else:\n        self._scale = other\n        if recompute:\n            self.recompute()\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        # compute from the size of the histogram\n        return self._hist.scale(recompute)\n\n    # otherwise, we can scale the histogram with the given other\n    # (unless explicitly asked to recompute)\n    else:\n        if not recompute:\n            return self._hist.scale(other)\n\n        # otherwise, compute the scale of the histogram with the same\n        # parameters as the other histogram and store the scale in the\n        # scale attribute\n        self._hist.scale(other, recompute)\n        self.setScale(other.getScale())\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if not self.is_scaled:\n            self.scale_to_zero()\n            recompute = True\n    else:\n        if not self.is_scaled:\n            self.set_scale(other)\n            recompute = True\n\n    return recompute\n", "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        scale = self._scale\n    else:\n        if self._scale is not None and other is not None:\n            raise LenaError(\"Histograms with scale equal to zero cannot be rescaled\")\n        else:\n            self._scale = other\n            if recompute:\n                self.recompute()\n", "def scale(self, other=None, recompute=False):\n\n    # check if scale was not computed yet\n    if self._scale is None and other is not None:\n        if recompute:\n            self._scale = self._compute_scale(other)\n        else:\n            self._scale = other\n    else:\n        if recompute:\n            self._scale = 0.0\n\n    return self._scale\n", "def scale(self, other=None, recompute=False):\n\n    if other is not None:\n        if self._scale is not None and self._scale!= other:\n            raise LenaHistogramError(\"Histograms with scale equal to zero cannot be rescaled\")\n        self._scale = other\n        self._update_scale(recompute)\n    else:\n        return self._scale\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None, recompute=False):\n\n    if self._scale is not None and not recompute:\n        if other is None:\n            other = self._scale\n        else:\n            try:\n                self._scale = other / self._count\n            except ZeroDivisionError:\n                raise LenaValueError(\"scale of %s cannot be recomputed\" % (self,))\n\n    self._count = 0\n    self._sum = 0\n\n    if self._scale is not None:\n        self._update_scale(other)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        other = self._hist.scale\n    elif not isinstance(other, numbers.Number):\n        raise LenaHistError(\n            \"Attempted to rescale histogram with a non-numerical value\")\n\n    if self._scale is not None and other!= self._scale:\n        if recompute:\n            self.recompute()\n        else:\n            raise LenaHistError(\n                \"Histogram scale was not set properly after changing (filling)\")\n\n    self._scale = other\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        # compute scale of the histogram\n        try:\n            scale = self._hist.scale()\n        except AttributeError:\n            scale = 0.0\n        else:\n            if recompute:\n                self.recompute()\n                scale = self._hist.scale()\n\n        return scale\n    else:\n        # set scale of the histogram\n        self._hist.scale(other)\n\n        # update scale if necessary\n        if recompute:\n            self.recompute()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if self._scale is not None and not recompute and other is not None:\n        raise LenaHistoError(\"Scale of histo already computed!\")\n\n    if self._scale is None and other is not None:\n        self._scale = self._compute_scale(other)\n    elif self._scale is None and recompute and other is None:\n        self.recompute(fill=False)\n\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if self.scale is not None:\n            return\n        other = self.new()\n\n    # if scale is not computed, compute it\n    if not self.scale:\n        self._compute_scale(other)\n\n    # otherwise, set scale of self to the provided value\n    else:\n        self._scale = other\n        if recompute:\n            self.recompute()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        # compute from the size of the histogram\n        return self._hist.scale(recompute)\n\n    # otherwise, we can scale the histogram with the given other\n    # (unless explicitly asked to recompute)\n    else:\n        if not recompute:\n            return self._hist.scale(other)\n\n        # otherwise, compute the scale of the histogram with the same\n        # parameters as the other histogram and store the scale in the\n        # scale attribute\n        self._hist.scale(other, recompute)\n        self.setScale(other.getScale())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        if not self.is_scaled:\n            self.scale_to_zero()\n            recompute = True\n    else:\n        if not self.is_scaled:\n            self.set_scale(other)\n            recompute = True\n\n    return recompute\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is None:\n        scale = self._scale\n    else:\n        if self._scale is not None and other is not None:\n            raise LenaError(\"Histograms with scale equal to zero cannot be rescaled\")\n        else:\n            self._scale = other\n            if recompute:\n                self.recompute()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    # check if scale was not computed yet\n    if self._scale is None and other is not None:\n        if recompute:\n            self._scale = self._compute_scale(other)\n        else:\n            self._scale = other\n    else:\n        if recompute:\n            self._scale = 0.0\n\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\n    if other is not None:\n        if self._scale is not None and self._scale!= other:\n            raise LenaHistogramError(\"Histograms with scale equal to zero cannot be rescaled\")\n        self._scale = other\n        self._update_scale(recompute)\n    else:\n        return self._scale\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.", "ques_id": "62b87b519a0c4fa8b80b3583", "code": ["def scale(self, other=None):\n    if other is None:\n        return self._scale\n    else:\n        if isinstance(other, (int, float)):\n            self._scale = other\n        else:\n            try:\n                self._scale = float(other)\n            except ValueError:\n                raise LenaError(\n                    \"Unrecognized scale '%s' for graph '%s'.\" % (other, self.name)\n                )\n", "def scale(self, other=None):\n    if other is not None:\n        if isinstance(other, Lena):\n            other = other.to_lena()\n        elif not isinstance(other, Lena):\n            raise TypeError('unsupported type to scale graph with')\n        self._lena.scale(other)\n        self.rescale_graph_fields()\n    return self._lena.scale\n", "def scale(self, other=None):\n    if other is not None:\n        self._check_other_coord(other)\n        self._check_scale(other)\n        self._scale = other\n        self._check_rescaling()\n    return self._scale\n", "def scale(self, other=None):\n    if other is not None:\n        if not isinstance(other, Lena):\n            other = self.__class__(other)\n        self._rescale(other)\n    return self._scale\n", "def scale(self, other=None):\n    if other:\n        self._rescale(other, scale=True)\n    return self._scale\n", "def scale(self, other=None):\n    if other:\n        if not isinstance(other, Lena):\n            other = self.__class__(other)\n        if self.__class__ is other.__class__:\n            if self.__dict__ == other.__dict__:\n                return self.__dict__\n            else:\n                self.__dict__ = other.__dict__\n                self.__init__()\n        else:\n            raise TypeError(\n                \"Graphics can only scale the coordinates of other \"\n                \"Lena objects (not {})\".format(self.__class__))\n    else:\n        self.__init__()\n        self.__scale = self.__last\n    return self.__scale\n", "def scale(self, other=None):\n    if other is not None:\n        other = self._check_other_coord(other)\n        self._update_scale(other)\n    return self._scale\n", "def scale(self, other=None):\n    if other is not None:\n        self._check_other_coord(other)\n        self._rescale(other)\n    return self._scale\n", "def scale(self, other=None):\n    if other is not None:\n        if not isinstance(other, numbers.Number):\n            raise TypeError('Graph.scale requires number arguments')\n        if other < 0:\n            other = self._nodes[-1][1] * -1\n        elif other >= self._nodes[-1][1]:\n            other = self._nodes[-1][1]\n        self._check_scale(other)\n    self._set_scale(self._nodes, other)\n", "def scale(self, other=None):\n    if other:\n        if not isinstance(other, Lena):\n            other = Lena(other)\n        self._check_dimensions(other)\n        self._check_same_lena()\n        self._check_valid()\n        self._scale = other._scale\n        self._last = other._last\n    else:\n        return self._scale\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    else:\n        if isinstance(other, (int, float)):\n            self._scale = other\n        else:\n            try:\n                self._scale = float(other)\n            except ValueError:\n                raise LenaError(\n                    \"Unrecognized scale '%s' for graph '%s'.\" % (other, self.name)\n                )\n", "is_pass": true, "return_code": 0}, {"generate_code": "def scale(self, other=None):\n    if other is not None:\n        if isinstance(other, Lena):\n            other = other.to_lena()\n        elif not isinstance(other, Lena):\n            raise TypeError('unsupported type to scale graph with')\n        self._lena.scale(other)\n        self.rescale_graph_fields()\n    return self._lena.scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other is not None:\n        self._check_other_coord(other)\n        self._check_scale(other)\n        self._scale = other\n        self._check_rescaling()\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other is not None:\n        if not isinstance(other, Lena):\n            other = self.__class__(other)\n        self._rescale(other)\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other:\n        self._rescale(other, scale=True)\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other:\n        if not isinstance(other, Lena):\n            other = self.__class__(other)\n        if self.__class__ is other.__class__:\n            if self.__dict__ == other.__dict__:\n                return self.__dict__\n            else:\n                self.__dict__ = other.__dict__\n                self.__init__()\n        else:\n            raise TypeError(\n                \"Graphics can only scale the coordinates of other \"\n                \"Lena objects (not {})\".format(self.__class__))\n    else:\n        self.__init__()\n        self.__scale = self.__last\n    return self.__scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other is not None:\n        other = self._check_other_coord(other)\n        self._update_scale(other)\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other is not None:\n        self._check_other_coord(other)\n        self._rescale(other)\n    return self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other is not None:\n        if not isinstance(other, numbers.Number):\n            raise TypeError('Graph.scale requires number arguments')\n        if other < 0:\n            other = self._nodes[-1][1] * -1\n        elif other >= self._nodes[-1][1]:\n            other = self._nodes[-1][1]\n        self._check_scale(other)\n    self._set_scale(self._nodes, other)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n    if other:\n        if not isinstance(other, Lena):\n            other = Lena(other)\n        self._check_dimensions(other)\n        self._check_same_lena()\n        self._check_valid()\n        self._scale = other._scale\n        self._last = other._last\n    else:\n        return self._scale\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "hist_to_graph", "docstring": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.", "ques_id": "62b87b869a0c4fa8b80b35e1", "code": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuple with fields *mean*, *mean_error* and a context\n    one would use.\n\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate* scale\n    >>> make_value = lambda bin_: (\"x\", \"y_mean\", \"y_mean_error\")\n    >>> get_coordinate = lambda bin_: \"middle\"\n    >>> field_names = (\"x\", \"y\")\n    >>> scale = lam", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example, to create\n    a graph with errors from a histogram where bins contain a named tuple with\n    fields *mean*, *mean_error` and a context one would be.\n\n    For example, to create a graph with errors from a histogram where bins contain\n    named tuple with fields *mean*, *mean_error* and a context one would be.\n\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate* define what the\n    coordinate of a graph point created from a histogram bin will be.\n    >>> make_value = lambda bin_: (\"x\", \"y_mean\", \"y_mean_error\")\n    >>> field_names = (\"x\", \"y\")\n    >>> scale = None", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate* defines what the coordinate of a graph\n    >>> make_value = lambda bin_: (\"left\", \"right\")\n    >>> field_names = (\"mean\", \"mean_error\")\n    >>> scale = None\n    >>> hist.make_value = make_value\n    >>> hist.get_coordinate = make_value\n    >>> graph = hist_to_graph(hist, make_value=make_value, get_coordinate=get_coordinate, field_names=field_names, scale=scale)\n    >>>", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    be.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuples without context.  This option could be\n    used to create a numeric graph.\n\n    *field_names* set field names of the graph.\n\n    The number must be the same as the dimension of the result.  For a\n    *make_value* above it would be *(\"x\", \"y_mean\", \"y_mean_error\")*.  This\n    can be used to create a numeric graph's scale (unknown by default).\n\n    >>> make_value ", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would be.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuple with fields *mean*, *mean_error* and a context\n    one would be.\n\n    *field_names* set field names of the graph.\n\n    Though it can be \"left\" (default), \"right\" and \"middle\". *field_names*\n    set field names of the graph.\n\n    We can use *scale* to set the scale of the graph when created a numeric\n    graph.\n\n    Return the resulting graph.\n    \"\"\"\n    return hist_to", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuple with fields *mean*, *mean_error* and a context\n    one would use.\n\n    *field_names* set field names of the graph.\n\n    This can be used to create the graph's scale (unknown by default).\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate* defines\n                                                   ", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain a named tuple with fields *mean*, *mean_error`.\n\n    This option could be used to create a numeric graph.\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate* defines what the coordinate of a graph point created from a histogram bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = lambda bin_: (\"x\", \"y_mean\",", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For\n    example, to create a graph with errors from a histogram where bins\n    contain a named tuple with fields *mean*, *mean_error` and a context\n    one would use.\n\n    For example, to create a graph with errors from a histogram where bins\n    contain named tuples without context. This can be used to create a\n    numeric graph.\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate*                                                                                                                         ", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example, to\n    create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuple with fields *mean*, *mean_error` and a context\n    one would use.\n\n\n    *field_names* set field names of the graph. The number must be the same\n    as the dimension of the result. For a *make_value* above they would be\n    *(\"x\", \"y_mean\", \"y_mean_error\")*. *scale* behaves the same as the\n    histogram scale. If it is ``True``, it uses the ", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n    This option could be used to create graph's error bars.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain a named tuple with fields *mean*, *mean_error` and\n    a context one would be.\n\n    *field_names* set field names of the graph.\n\n    The number must be the same as the dimension of the result. For a\n    *make_value* above they would be.    *scale* behaves the same as the\n    graph's scale.    *hist* must contain only numeric bins (without context).\n\n    Return the resulting graph.\n    \"\"\"\n    if scale is None:\n        scale = get_coordinate(hist)\n\n    result = {}\n    for key in field_names:\n        resu"], "level": "project_runnable", "generate_results": [{"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuple with fields *mean*, *mean_error* and a context\n    one would use.\n\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate* scale\n    >>> make_value = lambda bin_: (\"x\", \"y_mean\", \"y_mean_error\")\n    >>> get_coordinate = lambda bin_: \"middle\"\n    >>> field_names = (\"x\", \"y\")\n    >>> scale = lam", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example, to create\n    a graph with errors from a histogram where bins contain a named tuple with\n    fields *mean*, *mean_error` and a context one would be.\n\n    For example, to create a graph with errors from a histogram where bins contain\n    named tuple with fields *mean*, *mean_error* and a context one would be.\n\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate* define what the\n    coordinate of a graph point created from a histogram bin will be.\n    >>> make_value = lambda bin_: (\"x\", \"y_mean\", \"y_mean_error\")\n    >>> field_names = (\"x\", \"y\")\n    >>> scale = None", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate* defines what the coordinate of a graph\n    >>> make_value = lambda bin_: (\"left\", \"right\")\n    >>> field_names = (\"mean\", \"mean_error\")\n    >>> scale = None\n    >>> hist.make_value = make_value\n    >>> hist.get_coordinate = make_value\n    >>> graph = hist_to_graph(hist, make_value=make_value, get_coordinate=get_coordinate, field_names=field_names, scale=scale)\n    >>>", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    be.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuples without context.  This option could be\n    used to create a numeric graph.\n\n    *field_names* set field names of the graph.\n\n    The number must be the same as the dimension of the result.  For a\n    *make_value* above it would be *(\"x\", \"y_mean\", \"y_mean_error\")*.  This\n    can be used to create a numeric graph's scale (unknown by default).\n\n    >>> make_value ", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would be.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuple with fields *mean*, *mean_error* and a context\n    one would be.\n\n    *field_names* set field names of the graph.\n\n    Though it can be \"left\" (default), \"right\" and \"middle\". *field_names*\n    set field names of the graph.\n\n    We can use *scale* to set the scale of the graph when created a numeric\n    graph.\n\n    Return the resulting graph.\n    \"\"\"\n    return hist_to", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuple with fields *mean*, *mean_error* and a context\n    one would use.\n\n    *field_names* set field names of the graph.\n\n    This can be used to create the graph's scale (unknown by default).\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate* defines\n                                                   ", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example,\n    to create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain a named tuple with fields *mean*, *mean_error`.\n\n    This option could be used to create a numeric graph.\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) * get_coordinate* defines what the coordinate of a graph point created from a histogram bin will be. It can be \"left\" (default), \"right\" and \"middle\".\n\n    >>> make_value = lambda bin_: (\"x\", \"y_mean\",", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For\n    example, to create a graph with errors from a histogram where bins\n    contain a named tuple with fields *mean*, *mean_error` and a context\n    one would use.\n\n    For example, to create a graph with errors from a histogram where bins\n    contain named tuples without context. This can be used to create a\n    numeric graph.\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error) *get_coordinate*                                                                                                                         ", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars. For example, to\n    create a graph with errors from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error` and a context one would\n    use.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain named tuple with fields *mean*, *mean_error` and a context\n    one would use.\n\n\n    *field_names* set field names of the graph. The number must be the same\n    as the dimension of the result. For a *make_value* above they would be\n    *(\"x\", \"y_mean\", \"y_mean_error\")*. *scale* behaves the same as the\n    histogram scale. If it is ``True``, it uses the ", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n\n    By default it is bin content.\n\n    *make_value* accepts a single value (bin content) without context.\n    This option could be used to create graph's error bars.\n\n    For example, to create a graph with errors from a histogram where\n    bins contain a named tuple with fields *mean*, *mean_error` and\n    a context one would be.\n\n    *field_names* set field names of the graph.\n\n    The number must be the same as the dimension of the result. For a\n    *make_value* above they would be.    *scale* behaves the same as the\n    graph's scale.    *hist* must contain only numeric bins (without context).\n\n    Return the resulting graph.\n    \"\"\"\n    if scale is None:\n        scale = get_coordinate(hist)\n\n    result = {}\n    for key in field_names:\n        resu", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "_verify", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "ques_id": "62b8b4baeb7e40a82d2d1136", "code": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    if vtype is None:\n        vtype = iface.providedBy(candidate)\n\n    if not tentative and vtype is not IType:\n        raise zope.interface.Invalid(\"Candidate must provide %s\" % iface.__name__)\n\n    if not hasattr(candidate, 'providedBy'):\n        raise zope.interface.Invalid(\"Candidate must provide %s\" % iface.__name__)\n\n    if vtype is IType:\n        if not hasattr(candidate, 'implements'):\n            raise zope.interface.Invalid(\"Candidate doesn't provide %s\" % iface.__name__)\n\n        if not candidate.implements(iface):\n            raise zope.interface.Invalid(\"Candidate doesn't provide %s\" % iface.__name__)\n\n    if not hasattr(candidate, '__call__'):\n        raise zope.interface.Invalid(\"Candidate must provide %s\" % iface.__name__)\n\n    if vtype is IType:\n        if not hasattr(candidate, 'validate'):\n            raise", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if vtype is None:\n        vtype = candidate.type\n    if not issubclass(vtype, InterfaceClass):\n        return False\n    if tentative:\n        return False\n    if iface.providedBy(candidate) is False:\n        return False\n    if iface.providedBy(candidate) is True:\n        return True\n    if not hasattr(candidate, '__call__'):\n        return False\n    if not hasattr(candidate, '__getitem__'):\n        return False\n    if vtype is InterfaceClass:\n        # Only allow claim of a class\n        # We don't want to expose the claim to the implementer, so we can't just test that\n        # it doesn't get passed in as an attribute.\n        return False\n    if iface.providedBy(candidate.__call__) is False:\n        return False\n    if iface.providedBy(candidate.__call__) is True:\n        return True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if not tentative:\n        if not hasattr(candidate, 'providedBy'):\n            if vtype is None:\n                raise zope.interface.Invalid(\n                    \"{} does not provide {}\".format(candidate, iface.__name__))\n            else:\n                raise zope.interface.Invalid(\n                    \"{} does not provide {} with type {}\".format(\n                        candidate, iface.__name__, vtype.__name__))\n\n    return True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if not candidate.providedBy(iface):\n        return False\n\n    # The candidate's class declares that it `implements` the interface, or\n    # itself declares that it `provides` it.  The candidate declares all\n    # the necessary methods, then reported.\n    if not tentative:\n        # Check if the candidate's `provides` method has the correct signature\n        # and reported all the necessary attributes\n        verifyClass(iface, iface.providedBy)\n\n        # Check if the candidate defines all the required attributes\n        for name, type_ in iface.__dict__.items():\n            verifyAttribute(iface, name, type_)\n\n    # The candidate has the correct signature and has no extra methods (already\n    # verified), so we check if the candidate actually provides the interface.\n    if not iface.providedBy(candidate):\n        return False\n\n    return True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if vtype is not None:\n        if vtype not in iface.providedBy:\n            raise zope.interface.Invalid(\n                u'%s not in %s' % (vtype, iface.providedBy))\n        return True\n\n    if tentative or candidate.providedBy(iface):\n        return True\n\n    if not hasattr(candidate, '__call__'):\n        raise zope.interface.Invalid(\n            u'%s should be callable' % iface.__name__)\n\n    # Check for an explicit ``provides`` attribute\n    if hasattr(iface, 'provides'):\n        try:\n            iface.provides(iface.provides)\n        except zope.interface.Invalid as ex:\n            if not ex.args[0].startswith('Unexpected interface '):\n                raise\n\n    # Check for an implementation claim on the class\n    if hasattr(iface, '__call__'):\n        try:\n            iface(ifac", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if not candidate.providedBy(iface):\n        return False\n    if tentative:\n        raise zope.interface.Invalid(\n            'Candidate claims must be provided by %s' % iface)\n    if vtype is None:\n        vtype = iface.__class__\n    if hasattr(iface, 'providedBy'):\n        return vtype.providedBy(candidate)\n    if hasattr(candidate, 'providedBy'):\n        return vtype.providedBy(iface)\n    raise zope.interface.Invalid(\n        'Candidate claims must provide %s or %s' % (iface, candidate))\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if tentative and hasattr(candidate, 'providedBy'):\n        return True\n\n    if hasattr(candidate, 'provides') and not tentative:\n        if not candidate.provides(iface):\n            raise zope.interface.Invalid(\n                \"Candidate %s does not provide %s\" % (\n                    candidate, iface))\n\n    # Make sure the candidate's class declares that it `implements` the\n    # interface, or that it `provides` it (if it `implements`)\n\n    if hasattr(candidate, 'provides'):\n        iface = candidate.provides(iface)\n\n    # Make sure the candidate has the correct signature\n    if not hasattr(candidate, 'provides'):\n        if vtype is None:\n            vtype = type(candidate)\n        if not hasattr(candidate, 'provides')(vtype):\n            raise zope.interface.Invalid(\n                \"Candidate %s does not provide ", "def _verify(iface, candidate, tentative=False, vtype=None):\n    # If the candidate has an implementation, check that it provides the\n    # interface\n    if candidate.__implementation__ is not None:\n        # Make sure the candidate claims that it provides the interface\n        if not iface.providedBy(candidate):\n            raise zope.interface.Invalid(\n                \"%s claims the interface %r, but the candidate doesn't \"\n                \"provide it.\" % (iface, candidate))\n        # Make sure the candidate declares the interface\n        if not iface.providedBy(candidate.__implementation__):\n            raise zope.interface.Invalid(\n                \"%s claims the interface %r, but the candidate doesn't \"\n                \"declare it.\" % (iface, candidate))\n\n    # Make sure the candidate defines all the necessary methods\n    if not hasattr(candidate, '__providedBy__'):\n        raise zope.interface.Invalid(\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if tentative:\n        if hasattr(candidate, 'providedBy'):\n            return False\n        if hasattr(candidate, 'provides'):\n            return False\n        if hasattr(candidate, '__call__') and not callable(candidate):\n            return False\n        return True\n    return _verify_interface(iface, candidate, vtype=vtype)\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n    if tentative:\n        return True\n    if vtype is None:\n        vtype = iface.providedBy(candidate)\n    if not vtype:\n        return True\n    if iface.providedBy(candidate) is False:\n        raise zope.interface.Invalid(\n            \"Candidate claim must provide %s %s\" % (vtype, iface.name))\n    if not iface.providedBy(candidate):\n        raise zope.interface.Invalid(\n            \"Candidate claim must declare %s %s\" % (vtype, iface.name))\n    if not hasattr(candidate, 'providedBy'):\n        raise zope.interface.Invalid(\n            \"Candidate claim must declare %s %s\" % (vtype, iface.name))\n    return True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if vtype is None:\n        vtype = iface.providedBy(candidate)\n\n    if not tentative and vtype is not IType:\n        raise zope.interface.Invalid(\"Candidate must provide %s\" % iface.__name__)\n\n    if not hasattr(candidate, 'providedBy'):\n        raise zope.interface.Invalid(\"Candidate must provide %s\" % iface.__name__)\n\n    if vtype is IType:\n        if not hasattr(candidate, 'implements'):\n            raise zope.interface.Invalid(\"Candidate doesn't provide %s\" % iface.__name__)\n\n        if not candidate.implements(iface):\n            raise zope.interface.Invalid(\"Candidate doesn't provide %s\" % iface.__name__)\n\n    if not hasattr(candidate, '__call__'):\n        raise zope.interface.Invalid(\"Candidate must provide %s\" % iface.__name__)\n\n    if vtype is IType:\n        if not hasattr(candidate, 'validate'):\n            raise", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if vtype is None:\n        vtype = candidate.type\n    if not issubclass(vtype, InterfaceClass):\n        return False\n    if tentative:\n        return False\n    if iface.providedBy(candidate) is False:\n        return False\n    if iface.providedBy(candidate) is True:\n        return True\n    if not hasattr(candidate, '__call__'):\n        return False\n    if not hasattr(candidate, '__getitem__'):\n        return False\n    if vtype is InterfaceClass:\n        # Only allow claim of a class\n        # We don't want to expose the claim to the implementer, so we can't just test that\n        # it doesn't get passed in as an attribute.\n        return False\n    if iface.providedBy(candidate.__call__) is False:\n        return False\n    if iface.providedBy(candidate.__call__) is True:\n        return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if not tentative:\n        if not hasattr(candidate, 'providedBy'):\n            if vtype is None:\n                raise zope.interface.Invalid(\n                    \"{} does not provide {}\".format(candidate, iface.__name__))\n            else:\n                raise zope.interface.Invalid(\n                    \"{} does not provide {} with type {}\".format(\n                        candidate, iface.__name__, vtype.__name__))\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if not candidate.providedBy(iface):\n        return False\n\n    # The candidate's class declares that it `implements` the interface, or\n    # itself declares that it `provides` it.  The candidate declares all\n    # the necessary methods, then reported.\n    if not tentative:\n        # Check if the candidate's `provides` method has the correct signature\n        # and reported all the necessary attributes\n        verifyClass(iface, iface.providedBy)\n\n        # Check if the candidate defines all the required attributes\n        for name, type_ in iface.__dict__.items():\n            verifyAttribute(iface, name, type_)\n\n    # The candidate has the correct signature and has no extra methods (already\n    # verified), so we check if the candidate actually provides the interface.\n    if not iface.providedBy(candidate):\n        return False\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if vtype is not None:\n        if vtype not in iface.providedBy:\n            raise zope.interface.Invalid(\n                u'%s not in %s' % (vtype, iface.providedBy))\n        return True\n\n    if tentative or candidate.providedBy(iface):\n        return True\n\n    if not hasattr(candidate, '__call__'):\n        raise zope.interface.Invalid(\n            u'%s should be callable' % iface.__name__)\n\n    # Check for an explicit ``provides`` attribute\n    if hasattr(iface, 'provides'):\n        try:\n            iface.provides(iface.provides)\n        except zope.interface.Invalid as ex:\n            if not ex.args[0].startswith('Unexpected interface '):\n                raise\n\n    # Check for an implementation claim on the class\n    if hasattr(iface, '__call__'):\n        try:\n            iface(ifac", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if not candidate.providedBy(iface):\n        return False\n    if tentative:\n        raise zope.interface.Invalid(\n            'Candidate claims must be provided by %s' % iface)\n    if vtype is None:\n        vtype = iface.__class__\n    if hasattr(iface, 'providedBy'):\n        return vtype.providedBy(candidate)\n    if hasattr(candidate, 'providedBy'):\n        return vtype.providedBy(iface)\n    raise zope.interface.Invalid(\n        'Candidate claims must provide %s or %s' % (iface, candidate))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if tentative and hasattr(candidate, 'providedBy'):\n        return True\n\n    if hasattr(candidate, 'provides') and not tentative:\n        if not candidate.provides(iface):\n            raise zope.interface.Invalid(\n                \"Candidate %s does not provide %s\" % (\n                    candidate, iface))\n\n    # Make sure the candidate's class declares that it `implements` the\n    # interface, or that it `provides` it (if it `implements`)\n\n    if hasattr(candidate, 'provides'):\n        iface = candidate.provides(iface)\n\n    # Make sure the candidate has the correct signature\n    if not hasattr(candidate, 'provides'):\n        if vtype is None:\n            vtype = type(candidate)\n        if not hasattr(candidate, 'provides')(vtype):\n            raise zope.interface.Invalid(\n                \"Candidate %s does not provide ", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    # If the candidate has an implementation, check that it provides the\n    # interface\n    if candidate.__implementation__ is not None:\n        # Make sure the candidate claims that it provides the interface\n        if not iface.providedBy(candidate):\n            raise zope.interface.Invalid(\n                \"%s claims the interface %r, but the candidate doesn't \"\n                \"provide it.\" % (iface, candidate))\n        # Make sure the candidate declares the interface\n        if not iface.providedBy(candidate.__implementation__):\n            raise zope.interface.Invalid(\n                \"%s claims the interface %r, but the candidate doesn't \"\n                \"declare it.\" % (iface, candidate))\n\n    # Make sure the candidate defines all the necessary methods\n    if not hasattr(candidate, '__providedBy__'):\n        raise zope.interface.Invalid(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if tentative:\n        if hasattr(candidate, 'providedBy'):\n            return False\n        if hasattr(candidate, 'provides'):\n            return False\n        if hasattr(candidate, '__call__') and not callable(candidate):\n            return False\n        return True\n    return _verify_interface(iface, candidate, vtype=vtype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    if tentative:\n        return True\n    if vtype is None:\n        vtype = iface.providedBy(candidate)\n    if not vtype:\n        return True\n    if iface.providedBy(candidate) is False:\n        raise zope.interface.Invalid(\n            \"Candidate claim must provide %s %s\" % (vtype, iface.name))\n    if not iface.providedBy(candidate):\n        raise zope.interface.Invalid(\n            \"Candidate claim must declare %s %s\" % (vtype, iface.name))\n    if not hasattr(candidate, 'providedBy'):\n        raise zope.interface.Invalid(\n            \"Candidate claim must declare %s %s\" % (vtype, iface.name))\n    return True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyObject", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "ques_id": "62b8b4baeb7e40a82d2d1137", "code": ["def verifyObject(iface, candidate, tentative=False):\n    if candidate.__class__ is not iface:\n        return False\n    if tentative:\n        # This is a tentative check, nothing to verify\n        return True\n    if hasattr(candidate, 'providedBy'):\n        verifyObject(iface, candidate.providedBy, tentative=True)\n    elif hasattr(candidate, 'provides'):\n        verifyObject(iface, candidate.provides, tentative=True)\n    elif hasattr(candidate, 'implements'):\n        verifyObject(iface, candidate.implements, tentative=True)\n    else:\n        # This is a check for an attribute that doesn't exist\n        if hasattr(candidate, '__call__'):\n            return False\n        if not tentative:\n            raise TypeError(\n                '%s is not a valid class or attribute of %s' %\n                (candidate, iface))\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    # XXX: We might want to check for invalid claims, not just the interface\n    #       claims, and if the claim fails or the interface is not valid.  That\n    #       way, we can simply log the claim failure and raise the error.\n    claim = verifyClaim(iface, candidate)\n    if claim is False:\n        return False\n    if not tentative:\n        try:\n            iface.providedBy(claim)\n        except ComponentLookupError:\n            return False\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    if tentative:\n        return True\n    if hasattr(candidate, 'providedBy'):\n        return verifyObject(iface, candidate.providedBy(iface), False)\n    if hasattr(candidate, 'provides'):\n        try:\n            verifyObject(iface, candidate.provides(iface), False)\n        except KeyError:\n            if tentative:\n                raise\n            return False\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    if tentative and hasattr(candidate, 'providedBy'):\n        # In this case the previous condition checks for the provided\n        # interface to be true (i.e. it means that the candidate is\n        # explicitly provided by the context of an object).  This is\n        # performed by the candidate declaration step, so the first\n        # error reported is propagated.\n        return True\n    # All other errors are propagated immediately.\n    return zope.interface.verify.verifyObject(iface, candidate, True)\n", "def verifyObject(iface, candidate, tentative=False):\n    if not hasattr(candidate, 'providedBy'):\n        return False\n    if tentative or not hasattr(candidate, 'provides'):\n        return True\n    if iface.providedBy(candidate):\n        return True\n    if not hasattr(candidate, 'provides'):\n        return False\n    for name, method in inspect.getmembers(candidate):\n        if not hasattr(method, 'provides'):\n            return False\n        verifyObject(method, getattr(candidate, name), tentative=True)\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    if not candidate.implementedBy(iface):\n        return False\n    if tentative and hasattr(candidate, 'providedBy'):\n        if not candidate.providedBy(iface):\n            raise zope.interface.Invalid(\n                \"%s does not provide %r\" % (candidate, iface))\n        return False\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    if not isinstance(candidate, Candidate):\n        return False\n    if not tentative:\n        try:\n            verifyObject(iface.providedBy, candidate, False)\n        except zope.interface.Invalid:\n            return False\n    try:\n        verifyObject(iface.providedBy, candidate, True)\n    except zope.interface.Invalid:\n        return False\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    if not candidate.providedBy(iface):\n        return False\n\n    if tentative:\n        # Check that the candidate declares the interface\n        # rather than the candidate itself.\n        providedBy = iface.providedBy\n        if providedBy is None:\n            return False\n        if not providedBy(candidate):\n            raise Invalid(\n                u\"%s claims %s which is not provided by %s\" % (\n                    iface.__name__, candidate.__name__,\n                    providedBy.__name__))\n\n    # Make sure the candidate has the correct signature\n    verifyObjectSignature(iface, candidate, iface.providedBy)\n\n    # Make sure the candidate defines all the required attributes\n    verifyObjectAttributes(iface, candidate)\n\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    if not hasattr(candidate, 'providedBy'):\n        return False\n    if tentative or candidate.providedBy(iface):\n        return True\n    errors = []\n    for name, result in getattr(candidate, '__provides__', {}).items():\n        try:\n            result(iface)\n        except TypeError:\n            errors.append(\n                \"%r instance of %r has an invalid type %r\" %\n                (name, iface, type(iface)))\n    if errors:\n        raise Invalid(\n            \"Multiple %s claims %r of %r\" %\n            (candidate.__class__.__name__, iface, iface.__class__))\n    return True\n", "def verifyObject(iface, candidate, tentative=False):\n    if candidate.__providedBy__ is None:\n        if tentative:\n            raise zope.interface.Invalid(\"%r does not provide %r\" % (candidate, iface))\n        return False\n\n    # Make sure the candidate claims that it provides the interface\n    if not iface.providedBy(candidate):\n        raise zope.interface.Invalid(\n            \"%r does not provide %r\" % (candidate, iface))\n\n    # Make sure the candidate declares all the necessary methods\n    for name, method in inspect.getmembers(candidate):\n        if not hasattr(method, '__call__'):\n            raise zope.interface.Invalid(\n                \"%r does not declare method %r\" % (candidate, name))\n\n    return True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if candidate.__class__ is not iface:\n        return False\n    if tentative:\n        # This is a tentative check, nothing to verify\n        return True\n    if hasattr(candidate, 'providedBy'):\n        verifyObject(iface, candidate.providedBy, tentative=True)\n    elif hasattr(candidate, 'provides'):\n        verifyObject(iface, candidate.provides, tentative=True)\n    elif hasattr(candidate, 'implements'):\n        verifyObject(iface, candidate.implements, tentative=True)\n    else:\n        # This is a check for an attribute that doesn't exist\n        if hasattr(candidate, '__call__'):\n            return False\n        if not tentative:\n            raise TypeError(\n                '%s is not a valid class or attribute of %s' %\n                (candidate, iface))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    # XXX: We might want to check for invalid claims, not just the interface\n    #       claims, and if the claim fails or the interface is not valid.  That\n    #       way, we can simply log the claim failure and raise the error.\n    claim = verifyClaim(iface, candidate)\n    if claim is False:\n        return False\n    if not tentative:\n        try:\n            iface.providedBy(claim)\n        except ComponentLookupError:\n            return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if tentative:\n        return True\n    if hasattr(candidate, 'providedBy'):\n        return verifyObject(iface, candidate.providedBy(iface), False)\n    if hasattr(candidate, 'provides'):\n        try:\n            verifyObject(iface, candidate.provides(iface), False)\n        except KeyError:\n            if tentative:\n                raise\n            return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if tentative and hasattr(candidate, 'providedBy'):\n        # In this case the previous condition checks for the provided\n        # interface to be true (i.e. it means that the candidate is\n        # explicitly provided by the context of an object).  This is\n        # performed by the candidate declaration step, so the first\n        # error reported is propagated.\n        return True\n    # All other errors are propagated immediately.\n    return zope.interface.verify.verifyObject(iface, candidate, True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if not hasattr(candidate, 'providedBy'):\n        return False\n    if tentative or not hasattr(candidate, 'provides'):\n        return True\n    if iface.providedBy(candidate):\n        return True\n    if not hasattr(candidate, 'provides'):\n        return False\n    for name, method in inspect.getmembers(candidate):\n        if not hasattr(method, 'provides'):\n            return False\n        verifyObject(method, getattr(candidate, name), tentative=True)\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if not candidate.implementedBy(iface):\n        return False\n    if tentative and hasattr(candidate, 'providedBy'):\n        if not candidate.providedBy(iface):\n            raise zope.interface.Invalid(\n                \"%s does not provide %r\" % (candidate, iface))\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if not isinstance(candidate, Candidate):\n        return False\n    if not tentative:\n        try:\n            verifyObject(iface.providedBy, candidate, False)\n        except zope.interface.Invalid:\n            return False\n    try:\n        verifyObject(iface.providedBy, candidate, True)\n    except zope.interface.Invalid:\n        return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if not candidate.providedBy(iface):\n        return False\n\n    if tentative:\n        # Check that the candidate declares the interface\n        # rather than the candidate itself.\n        providedBy = iface.providedBy\n        if providedBy is None:\n            return False\n        if not providedBy(candidate):\n            raise Invalid(\n                u\"%s claims %s which is not provided by %s\" % (\n                    iface.__name__, candidate.__name__,\n                    providedBy.__name__))\n\n    # Make sure the candidate has the correct signature\n    verifyObjectSignature(iface, candidate, iface.providedBy)\n\n    # Make sure the candidate defines all the required attributes\n    verifyObjectAttributes(iface, candidate)\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if not hasattr(candidate, 'providedBy'):\n        return False\n    if tentative or candidate.providedBy(iface):\n        return True\n    errors = []\n    for name, result in getattr(candidate, '__provides__', {}).items():\n        try:\n            result(iface)\n        except TypeError:\n            errors.append(\n                \"%r instance of %r has an invalid type %r\" %\n                (name, iface, type(iface)))\n    if errors:\n        raise Invalid(\n            \"Multiple %s claims %r of %r\" %\n            (candidate.__class__.__name__, iface, iface.__class__))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n    if candidate.__providedBy__ is None:\n        if tentative:\n            raise zope.interface.Invalid(\"%r does not provide %r\" % (candidate, iface))\n        return False\n\n    # Make sure the candidate claims that it provides the interface\n    if not iface.providedBy(candidate):\n        raise zope.interface.Invalid(\n            \"%r does not provide %r\" % (candidate, iface))\n\n    # Make sure the candidate declares all the necessary methods\n    for name, method in inspect.getmembers(candidate):\n        if not hasattr(method, '__call__'):\n            raise zope.interface.Invalid(\n                \"%r does not declare method %r\" % (candidate, name))\n\n    return True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyClass", "docstring": "Verify that the *candidate* might correctly provide *iface*.", "ques_id": "62b8b4c1eb7e40a82d2d1139", "code": ["def verifyClass(iface, candidate, tentative=False):\n    if not verifyInterface(iface, candidate):\n        if tentative:\n            return False\n        raise TypeError(\n            \"%r doesn't provide %r\" % (candidate, iface))\n    return True\n", "def verifyClass(iface, candidate, tentative=False):\n    if not verifyInterface(iface, candidate):\n        if tentative:\n            return candidate\n        raise AssertionError(\n            \"%r did not provide %r\" % (iface, candidate))\n    return candidate\n", "def verifyClass(iface, candidate, tentative=False):\n    if not verify(iface, candidate):\n        return False\n\n    if tentative:\n        if issubclass(iface, Configurable) and issubclass(candidate, Configurable):\n            return False\n        else:\n            return True\n\n    # We never got here, but verify that the iface is actually a Configurable\n    # subclass, and that the candidate is actually a Configurable subclass\n    if not isinstance(iface, Configurable) or not isinstance(candidate, Configurable):\n        return False\n\n    if issubclass(iface, Configurable) and not issubclass(candidate, Configurable):\n        return False\n\n    return verify(iface.__implementation__, candidate.__implementation__, tentative)\n", "def verifyClass(iface, candidate, tentative=False):\n    if not verifyInterface(iface, candidate):\n        if tentative:\n            return False\n        raise TypeError(\"%s doesn't provide %s\" % (iface, candidate))\n    return True\n", "def verifyClass(iface, candidate, tentative=False):\n    if not verifyInterface(iface, candidate):\n        if tentative:\n            return False\n        else:\n            raise TypeError(\n                \"%r doesn't provide %r for %r\" % (iface, candidate, iface))\n    return True\n", "def verifyClass(iface, candidate, tentative=False):\n    if issubclass(iface, ICandidate) and not tentative:\n        return\n\n    if not issubclass(iface, ICandidate) and issubclass(candidate, ICandidate):\n        return\n\n    if not issubclass(iface, ICandidate):\n        return  # pragma: no cover\n\n    if not issubclass(candidate, ICandidate):\n        return  # pragma: no cover\n\n    if not candidate.providedBy(iface):\n        return\n\n    if not tentative:\n        if iface.providedBy(candidate):\n            return\n\n    # We don't really need to verify the type of the candidate, just check the interface\n    # and the type of the iface.  It does not matter if the interface is not the\n    # same as the candidate, but it is not as a common mistake.\n    if iface!= candidate:\n        return\n\n    return True\n", "def verifyClass(iface, candidate, tentative=False):\n    if not verifyContent(candidate, iface):\n        if tentative:\n            return False\n        raise TypeError(\n            \"%r doesn't provide %r\" % (candidate, iface))\n    return True\n", "def verifyClass(iface, candidate, tentative=False):\n    if not candidate:\n        return\n    if not issubclass(iface, Candidate):\n        raise TypeError(\"%s does not subclass Candidate\" % iface)\n    if not issubclass(candidate, iface):\n        if tentative:\n            msg = \"%s is not an instance of %s\"\n        else:\n            msg = \"%s is not an instance of %s(%s)\"\n        raise TypeError(msg % (iface, candidate, type(candidate)))\n", "def verifyClass(iface, candidate, tentative=False):\n    verifyClass(IHasTraits, candidate)\n    if not IHasTraits.providedBy(candidate):\n        if tentative:\n            return\n        raise TypeError(\"%s doesn't provide %r\" % (iface, candidate))\n", "def verifyClass(iface, candidate, tentative=False):\n    if not verifyCandidate(iface, candidate):\n        return False\n    if tentative:\n        if not verifyTentative(iface, candidate):\n            return False\n        return True\n    if iface is None:\n        return True\n    if not verifyInterface(iface, candidate):\n        return False\n    return True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    if not verifyInterface(iface, candidate):\n        if tentative:\n            return False\n        raise TypeError(\n            \"%r doesn't provide %r\" % (candidate, iface))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    if not verifyInterface(iface, candidate):\n        if tentative:\n            return candidate\n        raise AssertionError(\n            \"%r did not provide %r\" % (iface, candidate))\n    return candidate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    if not verify(iface, candidate):\n        return False\n\n    if tentative:\n        if issubclass(iface, Configurable) and issubclass(candidate, Configurable):\n            return False\n        else:\n            return True\n\n    # We never got here, but verify that the iface is actually a Configurable\n    # subclass, and that the candidate is actually a Configurable subclass\n    if not isinstance(iface, Configurable) or not isinstance(candidate, Configurable):\n        return False\n\n    if issubclass(iface, Configurable) and not issubclass(candidate, Configurable):\n        return False\n\n    return verify(iface.__implementation__, candidate.__implementation__, tentative)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    if not verifyInterface(iface, candidate):\n        if tentative:\n            return False\n        raise TypeError(\"%s doesn't provide %s\" % (iface, candidate))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    if not verifyInterface(iface, candidate):\n        if tentative:\n            return False\n        else:\n            raise TypeError(\n                \"%r doesn't provide %r for %r\" % (iface, candidate, iface))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    if issubclass(iface, ICandidate) and not tentative:\n        return\n\n    if not issubclass(iface, ICandidate) and issubclass(candidate, ICandidate):\n        return\n\n    if not issubclass(iface, ICandidate):\n        return  # pragma: no cover\n\n    if not issubclass(candidate, ICandidate):\n        return  # pragma: no cover\n\n    if not candidate.providedBy(iface):\n        return\n\n    if not tentative:\n        if iface.providedBy(candidate):\n            return\n\n    # We don't really need to verify the type of the candidate, just check the interface\n    # and the type of the iface.  It does not matter if the interface is not the\n    # same as the candidate, but it is not as a common mistake.\n    if iface!= candidate:\n        return\n\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    if not verifyContent(candidate, iface):\n        if tentative:\n            return False\n        raise TypeError(\n            \"%r doesn't provide %r\" % (candidate, iface))\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    if not candidate:\n        return\n    if not issubclass(iface, Candidate):\n        raise TypeError(\"%s does not subclass Candidate\" % iface)\n    if not issubclass(candidate, iface):\n        if tentative:\n            msg = \"%s is not an instance of %s\"\n        else:\n            msg = \"%s is not an instance of %s(%s)\"\n        raise TypeError(msg % (iface, candidate, type(candidate)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    verifyClass(IHasTraits, candidate)\n    if not IHasTraits.providedBy(candidate):\n        if tentative:\n            return\n        raise TypeError(\"%s doesn't provide %r\" % (iface, candidate))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n    if not verifyCandidate(iface, candidate):\n        return False\n    if tentative:\n        if not verifyTentative(iface, candidate):\n            return False\n        return True\n    if iface is None:\n        return True\n    if not verifyInterface(iface, candidate):\n        return False\n    return True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "determineMetaclass", "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__", "ques_id": "62b8b559eb7e40a82d2d11f6", "code": ["def determineMetaclass(bases, explicit_mc=None):\n    def find_metaclass(bases):\n        for base in bases:\n            if issubclass(base, type) and issubclass(base, type):\n                return base\n            if issubclass(base, type) and issubclass(base, type):\n                return base.__metaclass__\n        return object\n\n    if explicit_mc is None:\n        explicit_mc = []\n    for base in bases:\n        if issubclass(base, type):\n            meta = find_metaclass(base.__bases__)\n            if meta:\n                return meta\n        elif issubclass(base, type) and find_metaclass(base.__bases__) is not meta:\n            return find_metaclass(base.__bases__)\n        explicit_mc.append(base)\n    return object\n", "def determineMetaclass(bases, explicit_mc=None):\n    bases = list(bases)\n    if explicit_mc is None:\n        explicit_mc = []\n    while len(bases) > 1:\n        try:\n            bases.remove(metaclass)\n        except ValueError:\n            break\n        if bases[0] in explicit_mc:\n            raise ValueError(\"Duplicate metaclass %r\" % (bases[0],))\n        explicit_mc.append(bases[0])\n\n    mc = None\n    for base in bases:\n        if base is object:\n            continue\n        if hasattr(base, \"__metaclass__\"):\n            if mc is None:\n                mc = base.__metaclass__\n            else:\n                if base.__metaclass__ is None:\n                    mc = base.__metaclass__\n                else:\n              ", "def determineMetaclass(bases, explicit_mc=None):\n    def is_metaclass(cls):\n        return cls is not type and issubclass(cls, type) and cls is not type\n\n    def get_bases(cls):\n        bases = []\n        for base in cls.__bases__:\n            if is_metaclass(base):\n                bases.append(base)\n            else:\n                bases.extend(get_bases(base))\n        return bases\n\n    bases = get_bases(bases)\n    if explicit_mc is None:\n        explicit_mc = []\n    if not bases:\n        # If no bases are declared, return an empty tuple\n        return tuple()\n    elif len(bases) == 1:\n        # If only one base is declared, return it\n        return bases[0], explicit_mc\n    else:\n        # If more than one base is declared, return only the first two bases\n        return bases[", "def determineMetaclass(bases, explicit_mc=None):\n    for base in bases:\n        if (issubclass(base, type) and\n                issubclass(base.__metaclass__, type) and\n                base.__metaclass__ is not type and\n                base.__metaclass__ is not ExplicitMetaclass):\n            if explicit_mc is None:\n                return base, True\n            elif base.__metaclass__ is explicit_mc:\n                return base, True\n            else:\n                return base, False\n    return None, False\n", "def determineMetaclass(bases, explicit_mc=None):\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            mc = d.get('__metaclass__')\n            if mc is None:\n                mc = type.__new__(cls, name, bases, d)\n            else:\n                bases = [b for b in bases if b!= metaclass]\n                if explicit_mc is not None:\n                    if mc.__name__!= explicit_mc.__name__:\n                        raise TypeError(\"metaclass definition mismatch: %s vs %s\" %\n                                        (mc.__name__, explicit_mc.__name__))\n                elif mc.__name__!= explicit_mc.__name__:\n                    mc.__name__ = explicit_mc", "def determineMetaclass(bases, explicit_mc=None):\n    cls = bases[0]\n    for base in bases[1:]:\n        if issubclass(cls, base):\n            break\n        cls = base\n    if explicit_mc is not None:\n        cls = metaclass(explicit_mc, cls.__name__)\n    return cls\n", "def determineMetaclass(bases, explicit_mc=None):\n    def getMetaclass(cls):\n        cls_meta = getattr(cls, '__metaclass__', None)\n        if cls_meta is None:\n            cls_meta = getattr(cls, '__new__', cls.__new__)\n            if explicit_mc is not None:\n                cls_meta = getattr(cls, '__metaclass__', explicit_mc)\n        return cls_meta\n    result = []\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            result.append(getMetaclass(base))\n        else:\n            result.append(base)\n    return result\n", "def determineMetaclass(bases, explicit_mc=None):\n    def find_metaclass(bases):\n        if explicit_mc is None:\n            for base in bases:\n                if hasattr(base, '__metaclass__'):\n                    return base, base.__metaclass__\n            return None, None\n        else:\n            for base in bases:\n                if hasattr(base, '__metaclass__'):\n                    if base.__metaclass__ == explicit_mc:\n                        return base, base.__metaclass__\n                if base.__metaclass__ == explicit_mc:\n                    return base, None\n            return None, None\n    bases = list(bases)\n    bases.sort()\n    metaclass = find_metaclass(bases)\n    if metaclass is None:\n        metaclass = type('Metaclass', (type('Metaclass', ", "def determineMetaclass(bases, explicit_mc=None):\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            return type.__new__(cls, name, bases, d)\n\n        def __init__(cls, name, this_bases, d):\n            # ignore subclasses of metaclass\n            if not hasattr(d, '__subclasscheck__'):\n                return\n            # find all subclasses of metaclass\n            subclasses = []\n            for c in d.__subclasses__():\n                m = metaclass(c.__name__, c.__bases__, c.__dict__)\n                if m.__dict__.get('__metaclass__') == explicit_mc:\n                    subclasses.append(m)\n            # find first non-abstract subclass of metaclass\n            if len(subclasses) > 0 and subclasses[0].__dict_", "def determineMetaclass(bases, explicit_mc=None):\n    def find_metaclass(bases, current_mc):\n        for base in bases:\n            if base is current_mc:\n                continue\n            if hasattr(base, \"__metaclass__\"):\n                return base\n            if explicit_mc is not None:\n                if hasattr(explicit_mc, \"__metaclass__\"):\n                    return base\n                if explicit_mc is base:\n                    continue\n            raise AssertionError(\"Explicit metaclass not found\")\n        return current_mc\n\n    # find bases with __metaclass__\n    bases = [b for b in bases if b.__metaclass__ is not find_metaclass]\n    if bases:\n        return bases[0], find_metaclass(bases[1:], find_metaclass(bases[0].__bases__, None))\n    else:\n        raise AssertionError"], "level": "file_runnable", "generate_results": [{"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    def find_metaclass(bases):\n        for base in bases:\n            if issubclass(base, type) and issubclass(base, type):\n                return base\n            if issubclass(base, type) and issubclass(base, type):\n                return base.__metaclass__\n        return object\n\n    if explicit_mc is None:\n        explicit_mc = []\n    for base in bases:\n        if issubclass(base, type):\n            meta = find_metaclass(base.__bases__)\n            if meta:\n                return meta\n        elif issubclass(base, type) and find_metaclass(base.__bases__) is not meta:\n            return find_metaclass(base.__bases__)\n        explicit_mc.append(base)\n    return object\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    bases = list(bases)\n    if explicit_mc is None:\n        explicit_mc = []\n    while len(bases) > 1:\n        try:\n            bases.remove(metaclass)\n        except ValueError:\n            break\n        if bases[0] in explicit_mc:\n            raise ValueError(\"Duplicate metaclass %r\" % (bases[0],))\n        explicit_mc.append(bases[0])\n\n    mc = None\n    for base in bases:\n        if base is object:\n            continue\n        if hasattr(base, \"__metaclass__\"):\n            if mc is None:\n                mc = base.__metaclass__\n            else:\n                if base.__metaclass__ is None:\n                    mc = base.__metaclass__\n                else:\n              ", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    def is_metaclass(cls):\n        return cls is not type and issubclass(cls, type) and cls is not type\n\n    def get_bases(cls):\n        bases = []\n        for base in cls.__bases__:\n            if is_metaclass(base):\n                bases.append(base)\n            else:\n                bases.extend(get_bases(base))\n        return bases\n\n    bases = get_bases(bases)\n    if explicit_mc is None:\n        explicit_mc = []\n    if not bases:\n        # If no bases are declared, return an empty tuple\n        return tuple()\n    elif len(bases) == 1:\n        # If only one base is declared, return it\n        return bases[0], explicit_mc\n    else:\n        # If more than one base is declared, return only the first two bases\n        return bases[", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    for base in bases:\n        if (issubclass(base, type) and\n                issubclass(base.__metaclass__, type) and\n                base.__metaclass__ is not type and\n                base.__metaclass__ is not ExplicitMetaclass):\n            if explicit_mc is None:\n                return base, True\n            elif base.__metaclass__ is explicit_mc:\n                return base, True\n            else:\n                return base, False\n    return None, False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            mc = d.get('__metaclass__')\n            if mc is None:\n                mc = type.__new__(cls, name, bases, d)\n            else:\n                bases = [b for b in bases if b!= metaclass]\n                if explicit_mc is not None:\n                    if mc.__name__!= explicit_mc.__name__:\n                        raise TypeError(\"metaclass definition mismatch: %s vs %s\" %\n                                        (mc.__name__, explicit_mc.__name__))\n                elif mc.__name__!= explicit_mc.__name__:\n                    mc.__name__ = explicit_mc", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    cls = bases[0]\n    for base in bases[1:]:\n        if issubclass(cls, base):\n            break\n        cls = base\n    if explicit_mc is not None:\n        cls = metaclass(explicit_mc, cls.__name__)\n    return cls\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    def getMetaclass(cls):\n        cls_meta = getattr(cls, '__metaclass__', None)\n        if cls_meta is None:\n            cls_meta = getattr(cls, '__new__', cls.__new__)\n            if explicit_mc is not None:\n                cls_meta = getattr(cls, '__metaclass__', explicit_mc)\n        return cls_meta\n    result = []\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            result.append(getMetaclass(base))\n        else:\n            result.append(base)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    def find_metaclass(bases):\n        if explicit_mc is None:\n            for base in bases:\n                if hasattr(base, '__metaclass__'):\n                    return base, base.__metaclass__\n            return None, None\n        else:\n            for base in bases:\n                if hasattr(base, '__metaclass__'):\n                    if base.__metaclass__ == explicit_mc:\n                        return base, base.__metaclass__\n                if base.__metaclass__ == explicit_mc:\n                    return base, None\n            return None, None\n    bases = list(bases)\n    bases.sort()\n    metaclass = find_metaclass(bases)\n    if metaclass is None:\n        metaclass = type('Metaclass', (type('Metaclass', ", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            return type.__new__(cls, name, bases, d)\n\n        def __init__(cls, name, this_bases, d):\n            # ignore subclasses of metaclass\n            if not hasattr(d, '__subclasscheck__'):\n                return\n            # find all subclasses of metaclass\n            subclasses = []\n            for c in d.__subclasses__():\n                m = metaclass(c.__name__, c.__bases__, c.__dict__)\n                if m.__dict__.get('__metaclass__') == explicit_mc:\n                    subclasses.append(m)\n            # find first non-abstract subclass of metaclass\n            if len(subclasses) > 0 and subclasses[0].__dict_", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n    def find_metaclass(bases, current_mc):\n        for base in bases:\n            if base is current_mc:\n                continue\n            if hasattr(base, \"__metaclass__\"):\n                return base\n            if explicit_mc is not None:\n                if hasattr(explicit_mc, \"__metaclass__\"):\n                    return base\n                if explicit_mc is base:\n                    continue\n            raise AssertionError(\"Explicit metaclass not found\")\n        return current_mc\n\n    # find bases with __metaclass__\n    bases = [b for b in bases if b.__metaclass__ is not find_metaclass]\n    if bases:\n        return bases[0], find_metaclass(bases[1:], find_metaclass(bases[0].__bases__, None))\n    else:\n        raise AssertionError", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "pop", "docstring": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.", "ques_id": "62b8d22a48ba5a41d1c3f47d", "code": ["def pop(self, key, default=__marker):\n    if key in self._keys:\n        result = self[key]\n        self._keys.remove(key)\n        return result\n    if default is self.__marker:\n        raise KeyError(key)\n    return default\n", "def pop(self, key, default=__marker):\n    if key in self:\n        value = dict.pop(self, key)\n        self._on_pop(key)\n        return value\n    if default is self.__marker:\n        raise KeyError(key)\n    return default\n", "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    return super(Cache, self).pop(key, default)\n", "def pop(self, key, default=__marker):\n    if key not in self:\n        if default is not __marker:\n            return default\n        raise KeyError(key)\n    self._keys.remove(key)\n    return dict.pop(self, key)\n", "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    return super(CaseInsensitiveDict, self).pop(self._keytransform(key), default)\n", "def pop(self, key, default=__marker):\n    if not key:\n        return default\n    key = self._translate_key(key)\n    return super(CaseInsensitiveDict, self).pop(key, default)\n", "def pop(self, key, default=__marker):\n    if key not in self:\n        if default is self.__marker:\n            raise KeyError(key)\n        return default\n    self._keys.remove(key)\n    return dict.pop(self, key)\n", "def pop(self, key, default=__marker):\n    if key in self:\n        result = super(OrderedMultiDict, self).pop(key)\n        self._list_remove(key)\n        return result\n    if default is self.__marker:\n        raise KeyError(key)\n    return default\n", "def pop(self, key, default=__marker):\n    if key in self:\n        value = dict.pop(self, key)\n        self._dirty = True\n        return value\n    if default is self.__marker:\n        raise KeyError(key)\n    return default\n", "def pop(self, key, default=__marker):\n    if key in self:\n        if self._sequence:\n            return self._sequence.pop(key)\n        else:\n            return self._dict.pop(key)\n    elif default is self.__marker:\n        raise KeyError(key)\n    else:\n        return default\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop(self, key, default=__marker):\n    if key in self._keys:\n        result = self[key]\n        self._keys.remove(key)\n        return result\n    if default is self.__marker:\n        raise KeyError(key)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if key in self:\n        value = dict.pop(self, key)\n        self._on_pop(key)\n        return value\n    if default is self.__marker:\n        raise KeyError(key)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    return super(Cache, self).pop(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if key not in self:\n        if default is not __marker:\n            return default\n        raise KeyError(key)\n    self._keys.remove(key)\n    return dict.pop(self, key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if default is self.__marker:\n        default = None\n    return super(CaseInsensitiveDict, self).pop(self._keytransform(key), default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if not key:\n        return default\n    key = self._translate_key(key)\n    return super(CaseInsensitiveDict, self).pop(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if key not in self:\n        if default is self.__marker:\n            raise KeyError(key)\n        return default\n    self._keys.remove(key)\n    return dict.pop(self, key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if key in self:\n        result = super(OrderedMultiDict, self).pop(key)\n        self._list_remove(key)\n        return result\n    if default is self.__marker:\n        raise KeyError(key)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if key in self:\n        value = dict.pop(self, key)\n        self._dirty = True\n        return value\n    if default is self.__marker:\n        raise KeyError(key)\n    return default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n    if key in self:\n        if self._sequence:\n            return self._sequence.pop(key)\n        else:\n            return self._dict.pop(key)\n    elif default is self.__marker:\n        raise KeyError(key)\n    else:\n        return default\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lfu.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least frequently used.", "ques_id": "62b8d23748ba5a41d1c3f497", "code": ["def popitem(self):\n    key = next(iter(self))\n    return key, self.pop(key)\n", "def popitem(self):\n    return self._freqdist.popitem()\n", "def popitem(self):\n    try:\n        return dict.popitem(self)[0]\n    except KeyError as e:\n        raise exceptions.NotFound(e)\n", "def popitem(self):\n    # We use a new dictionary to remove items from the dictionary.\n    d = {}\n    for key in self._freqdist:\n        if key not in self:\n            break\n        d[key] = self[key]\n    # Then we remove the item with the highest frequency.\n    key, val = max(d.items(), key=lambda x: x[1])\n    del d[key]\n    return key, val\n", "def popitem(self):\n    freq = dict.popitem(self)\n    self._remove_frequency(freq)\n    return freq\n", "def popitem(self):\n    return self._freqdist.popitem()\n", "def popitem(self):\n    return dict.popitem(self)[0]\n", "def popitem(self):\n    return self.__popitem__(0)\n", "def popitem(self):\n    result = dict.popitem(self)\n    try:\n        del self._freq[result[0]]\n    except KeyError:\n        raise KeyError(result)\n    return result\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n        return (key, self[key])\n    except KeyError as e:\n        raise exceptions.NotFound(str(e))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    key = next(iter(self))\n    return key, self.pop(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self._freqdist.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return dict.popitem(self)[0]\n    except KeyError as e:\n        raise exceptions.NotFound(e)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    # We use a new dictionary to remove items from the dictionary.\n    d = {}\n    for key in self._freqdist:\n        if key not in self:\n            break\n        d[key] = self[key]\n    # Then we remove the item with the highest frequency.\n    key, val = max(d.items(), key=lambda x: x[1])\n    del d[key]\n    return key, val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    freq = dict.popitem(self)\n    self._remove_frequency(freq)\n    return freq\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self._freqdist.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return dict.popitem(self)[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self.__popitem__(0)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    result = dict.popitem(self)\n    try:\n        del self._freq[result[0]]\n    except KeyError:\n        raise KeyError(result)\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n        return (key, self[key])\n    except KeyError as e:\n        raise exceptions.NotFound(str(e))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least recently used.", "ques_id": "62b8d23a48ba5a41d1c3f499", "code": ["def popitem(self):\n    try:\n        key = self._list_pop()\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n    value = dict.pop(self, key)\n    self._list_add(value)\n    return (key, value)\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        return key, value\n    except KeyError as e:\n        raise exceptions.NotFound(e)\n", "def popitem(self):\n    return self._dict.popitem()\n", "def popitem(self):\n    try:\n        key = self._keys.pop()\n    except IndexError:\n        raise KeyError('popitem(): dictionary is already empty')\n    value = dict.pop(self, key)\n    return (key, value)\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        return key, value\n    except KeyError as e:\n        raise exceptions.NotFound('Key \"{}\" not found'.format(e))\n", "def popitem(self):\n    if not self._keys:\n        raise KeyError('popitem(): dictionary is empty')\n\n    key = self._keys[0]\n    value = dict.pop(self, key)\n    self._keys = self._keys[1:]\n    return (key, value)\n", "def popitem(self):\n    return dict.popitem(self)[0]\n", "def popitem(self):\n    if not self:\n        raise KeyError('popitem(): dictionary is empty')\n\n    key = self._list_pop()\n    value = dict.pop(self, key)\n    self._list_add((key, value))\n    return key, value\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        return key, value\n    except KeyError as e:\n        raise exceptions.NotFound(e)\n", "def popitem(self):\n    if not len(self):\n        raise KeyError('popitem(): dictionary is already empty')\n\n    key = self._list_pop()[0]\n    value = dict.pop(self, key)\n    self._list_add((key, value))\n    return (key, value)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    try:\n        key = self._list_pop()\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n    value = dict.pop(self, key)\n    self._list_add(value)\n    return (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        return key, value\n    except KeyError as e:\n        raise exceptions.NotFound(e)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self._dict.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = self._keys.pop()\n    except IndexError:\n        raise KeyError('popitem(): dictionary is already empty')\n    value = dict.pop(self, key)\n    return (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        return key, value\n    except KeyError as e:\n        raise exceptions.NotFound('Key \"{}\" not found'.format(e))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    if not self._keys:\n        raise KeyError('popitem(): dictionary is empty')\n\n    key = self._keys[0]\n    value = dict.pop(self, key)\n    self._keys = self._keys[1:]\n    return (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return dict.popitem(self)[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    if not self:\n        raise KeyError('popitem(): dictionary is empty')\n\n    key = self._list_pop()\n    value = dict.pop(self, key)\n    self._list_add((key, value))\n    return key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        return key, value\n    except KeyError as e:\n        raise exceptions.NotFound(e)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    if not len(self):\n        raise KeyError('popitem(): dictionary is already empty')\n\n    key = self._list_pop()[0]\n    value = dict.pop(self, key)\n    self._list_add((key, value))\n    return (key, value)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/mru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair most recently used.", "ques_id": "62b8d23c48ba5a41d1c3f49b", "code": ["def popitem(self):\n    return self._list.popitem()\n", "def popitem(self):\n    # TODO: iteratively pop *values* and *keys* using the same mechanism as\n    # dict.popitem(). The same result is used for pop(), but more performant\n    # than for popitem().\n    key = self._list.pop()\n    return (key, self[key])\n", "def popitem(self):\n    try:\n        return dict.popitem(self)[0]\n    except KeyError as e:\n        raise exceptions.DoesNotExist(str(e))\n", "def popitem(self):\n    return dict.popitem(self)[0]\n", "def popitem(self):\n    return self._dict.popitem()\n", "def popitem(self):\n    key = self._list.pop()\n    return (key, self[key])\n", "def popitem(self):\n    return dict.popitem(self)[0]\n", "def popitem(self):\n    try:\n        return dict.popitem(self)[0]\n    except KeyError as e:\n        raise exceptions.NotFound('Key `{}` not found.'.format(e))\n", "def popitem(self):\n    try:\n        key = list(self._semaphore)[-1]\n        value = dict.pop(self, key)\n        self._semaphore.clear()\n        return (key, value)\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n", "def popitem(self):\n    return self._list.popitem()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    return self._list.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    # TODO: iteratively pop *values* and *keys* using the same mechanism as\n    # dict.popitem(). The same result is used for pop(), but more performant\n    # than for popitem().\n    key = self._list.pop()\n    return (key, self[key])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return dict.popitem(self)[0]\n    except KeyError as e:\n        raise exceptions.DoesNotExist(str(e))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return dict.popitem(self)[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self._dict.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    key = self._list.pop()\n    return (key, self[key])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return dict.popitem(self)[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return dict.popitem(self)[0]\n    except KeyError as e:\n        raise exceptions.NotFound('Key `{}` not found.'.format(e))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = list(self._semaphore)[-1]\n        value = dict.pop(self, key)\n        self._semaphore.clear()\n        return (key, value)\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self._list.popitem()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/rr.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return a random `(key, value)` pair.", "ques_id": "62b8d23e48ba5a41d1c3f49e", "code": ["def popitem(self):\n    return self._list.popitem()\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = self[key]\n        del self[key]\n        return key, value\n    except StopIteration:\n        raise KeyError('popitem(): dictionary is empty')\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        return (key, value)\n    except StopIteration:\n        raise KeyError\n", "def popitem(self):\n    try:\n        return dict.popitem(self)[0]\n    except KeyError as e:\n        raise exceptions.NotFound(e)\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n    except StopIteration:\n        raise KeyError('popitem(): dictionary is empty')\n    else:\n        value = self.pop(key)\n        return (key, value)\n", "def popitem(self):\n    try:\n        return dict.popitem(self)\n    except KeyError as e:\n        raise exceptions.NotFound('No item found with key \"%s\"' % e)\n", "def popitem(self):\n    return self._make_key_value(*self._random_item())\n", "def popitem(self):\n    return self._dict.popitem()\n", "def popitem(self):\n    try:\n        key = next(iter(self))\n    except StopIteration:\n        raise KeyError('popitem(): dictionary is empty')\n    else:\n        value = self.pop(key)\n        return (key, value)\n", "def popitem(self):\n    try:\n        key = self._keys[0]\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n\n    value = dict.pop(self, key)\n    self._keys = self._keys[1:]\n    return (key, value)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n    return self._list.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = self[key]\n        del self[key]\n        return key, value\n    except StopIteration:\n        raise KeyError('popitem(): dictionary is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n        value = dict.pop(self, key)\n        self._list_remove(key)\n        return (key, value)\n    except StopIteration:\n        raise KeyError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return dict.popitem(self)[0]\n    except KeyError as e:\n        raise exceptions.NotFound(e)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n    except StopIteration:\n        raise KeyError('popitem(): dictionary is empty')\n    else:\n        value = self.pop(key)\n        return (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        return dict.popitem(self)\n    except KeyError as e:\n        raise exceptions.NotFound('No item found with key \"%s\"' % e)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self._make_key_value(*self._random_item())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    return self._dict.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = next(iter(self))\n    except StopIteration:\n        raise KeyError('popitem(): dictionary is empty')\n    else:\n        value = self.pop(key)\n        return (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n    try:\n        key = self._keys[0]\n    except IndexError:\n        raise KeyError('popitem(): dictionary is empty')\n\n    value = dict.pop(self, key)\n    self._keys = self._keys[1:]\n    return (key, value)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_in_regex", "docstring": "Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).", "ques_id": "62b43425903eeb48555d3ea1", "code": ["def _create_in_regex(self) -> Pattern:\n    in_regex = Pattern(self.IN_STYLE_PATTERN)\n    in_regex.set_group(self.IN_GROUP)\n\n    return in_regex\n", "def _create_in_regex(self) -> Pattern:\n\n    if self._in_regex is not None:\n        return self._in_regex\n\n    pattern = \"(?P<\" + self._param.in_name + \">\" + self._param.regex + \")\"\n    self._in_regex = re.compile(pattern, re.IGNORECASE)\n    return self._in_regex\n", "def _create_in_regex(self) -> Pattern:\n    return re.compile(self._in_regex)\n", "def _create_in_regex(self) -> Pattern:\n    if self._re_in:\n        in_str = self._re_in.format(self._param)\n    else:\n        in_str = self._param\n    return re.compile(in_str, re.IGNORECASE)\n", "def _create_in_regex(self) -> Pattern:\n    in_style = self._get_parameter_value(self._in_param)\n    if not in_style:\n        in_style = self._get_parameter_value(self._out_param)\n    if not in_style:\n        return None\n\n    return re.compile(\n        \"^{}$\".format(in_style),\n        flags=re.IGNORECASE | re.UNICODE | re.UNICODE | re.VERBOSE\n    )\n", "def _create_in_regex(self) -> Pattern:\n    in_regex = re.compile(\n        r'''\n        ^\n        (?P<in_style>\n            (?P<param>[a-zA-Z_]+)\n            (?:\\((?P<param_type>\\w+)\\))?\n            \\[(?P<param_regex_group>\\w+)\\]\n        )\n        $\n    ''',\n        re.VERBOSE\n    )\n\n    return in_regex\n", "def _create_in_regex(self) -> Pattern:\n\n    pattern = (\n        r\"\\b(?P<param_name>\"\n        r\"(?:(?P<in_param_name>\"\n        r\"(?:{param_regex_symbol})|{param_regex_braces})\"\n        r\")?)\"\n        r\"\\b(?P<param_value>\"\n        r\"(?P<in_param_value>\"\n        r\"(?P<in_param_value_symbol>\"\n        r\"(?:{param_regex_braces})|{param_regex_braces})\"\n        r\")?)\"\n        r\"\\b(?P<param_unit>\"\n        r\"(?P<in_param_unit>\"\n        r\"(?P<param_unit_symbol>\"\n        r\"(?P<param_regex_braces}))?)\"\n        r\")?)\"\n        r\"\\b(?P<param_required>\"\n        r\"(?P<required_param_name>\"\n        r\"(?P<param_required_value>\"\n        r\"(?P<param_required_unit>\"\n        r\"(?P<param_required_unit_symbol>\"\n        r\"(", "def _create_in_regex(self) -> Pattern:\n    in_regex = self._config[\"in-style\"]\n    if in_regex.startswith(\"(\") and in_regex.endswith(\")\"):\n        in_regex = in_regex[1:-1]\n    in_regex = in_regex.replace(\"(?P<word>.+?)\", r\"\\1\")\n    return re.compile(in_regex, re.UNICODE)\n", "def _create_in_regex(self) -> Pattern:\n    pattern = re.compile(self.IN_PATTERN)\n    return pattern\n", "def _create_in_regex(self) -> Pattern:\n    pattern = \"(?P<{name}>{param_name})\"\n    return re.compile(pattern.format(name=self.in_name,\n                                     param_name=self.param_name))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_in_regex(self) -> Pattern:\n    in_regex = Pattern(self.IN_STYLE_PATTERN)\n    in_regex.set_group(self.IN_GROUP)\n\n    return in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\n    if self._in_regex is not None:\n        return self._in_regex\n\n    pattern = \"(?P<\" + self._param.in_name + \">\" + self._param.regex + \")\"\n    self._in_regex = re.compile(pattern, re.IGNORECASE)\n    return self._in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    return re.compile(self._in_regex)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    if self._re_in:\n        in_str = self._re_in.format(self._param)\n    else:\n        in_str = self._param\n    return re.compile(in_str, re.IGNORECASE)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    in_style = self._get_parameter_value(self._in_param)\n    if not in_style:\n        in_style = self._get_parameter_value(self._out_param)\n    if not in_style:\n        return None\n\n    return re.compile(\n        \"^{}$\".format(in_style),\n        flags=re.IGNORECASE | re.UNICODE | re.UNICODE | re.VERBOSE\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    in_regex = re.compile(\n        r'''\n        ^\n        (?P<in_style>\n            (?P<param>[a-zA-Z_]+)\n            (?:\\((?P<param_type>\\w+)\\))?\n            \\[(?P<param_regex_group>\\w+)\\]\n        )\n        $\n    ''',\n        re.VERBOSE\n    )\n\n    return in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\n    pattern = (\n        r\"\\b(?P<param_name>\"\n        r\"(?:(?P<in_param_name>\"\n        r\"(?:{param_regex_symbol})|{param_regex_braces})\"\n        r\")?)\"\n        r\"\\b(?P<param_value>\"\n        r\"(?P<in_param_value>\"\n        r\"(?P<in_param_value_symbol>\"\n        r\"(?:{param_regex_braces})|{param_regex_braces})\"\n        r\")?)\"\n        r\"\\b(?P<param_unit>\"\n        r\"(?P<in_param_unit>\"\n        r\"(?P<param_unit_symbol>\"\n        r\"(?P<param_regex_braces}))?)\"\n        r\")?)\"\n        r\"\\b(?P<param_required>\"\n        r\"(?P<required_param_name>\"\n        r\"(?P<param_required_value>\"\n        r\"(?P<param_required_unit>\"\n        r\"(?P<param_required_unit_symbol>\"\n        r\"(", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    in_regex = self._config[\"in-style\"]\n    if in_regex.startswith(\"(\") and in_regex.endswith(\")\"):\n        in_regex = in_regex[1:-1]\n    in_regex = in_regex.replace(\"(?P<word>.+?)\", r\"\\1\")\n    return re.compile(in_regex, re.UNICODE)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    pattern = re.compile(self.IN_PATTERN)\n    return pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n    pattern = \"(?P<{name}>{param_name})\"\n    return re.compile(pattern.format(name=self.in_name,\n                                     param_name=self.param_name))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_converter", "docstring": "Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).", "ques_id": "62b43426903eeb48555d3ea2", "code": ["def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._param_spec, self._input_types, self._output_types)\n", "def _create_converter(self) -> _converting._Converter:\n\n    converter = _converting._Converter()\n    converter.add_parameter(_parameters.Parameter.from_str(\n        'x', 'Input value', min=0, max=100))\n\n    return converter\n", "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self)\n", "def _create_converter(self) -> _converting._Converter:\n    style_converter = _converting._Converter()\n    style_converter.add_type(_constants.TYPE_BOOLEAN)\n    style_converter.add_type(_constants.TYPE_NUMBER)\n    style_converter.add_parameter(\"value\", None)\n\n    return style_converter\n", "def _create_converter(self) -> _converting._Converter:\n    converter = _converting._Converter()\n    converter.types = self.types\n    converter.parameters = self.parameters\n    return converter\n", "def _create_converter(self) -> _converting._Converter:\n    converter = _converting._Converter()\n    converter.for_type(_to_enum(self._type), _to_enum(self._to_type))\n\n    return converter\n", "def _create_converter(self) -> _converting._Converter:\n    param_style = self._get_param_style()\n    converter = _converters.get(param_style)\n    if converter is None:\n        raise _errors.UnknownStyle(param_style)\n    return converter\n", "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._parameters)\n", "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._parameters, self._param_defs)\n", "def _create_converter(self) -> _converting._Converter:\n    converter = _converting._Converter()\n    converter.add_type(_constants.Type.STRING, None, None)\n    converter.add_parameter(_constants.Parameter.TIME, None)\n    converter.add_parameter(_constants.Parameter.FRACTION, None)\n    converter.add_parameter(_constants.Parameter.POTENTIAL, None)\n    converter.add_parameter(_constants.Parameter.QUANTITY, None)\n    converter.add_parameter(_constants.Parameter.VOLUME, None)\n    converter.add_parameter(_constants.Parameter.PERCENTAGE, None)\n    converter.add_parameter(_constants.Parameter.PERCENTAGE_UNIT, None)\n    converter.add_parameter(_constants.Parameter.PERCENTAGE_EXPONENTIAL, None)\n    return converter\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._param_spec, self._input_types, self._output_types)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\n    converter = _converting._Converter()\n    converter.add_parameter(_parameters.Parameter.from_str(\n        'x', 'Input value', min=0, max=100))\n\n    return converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    style_converter = _converting._Converter()\n    style_converter.add_type(_constants.TYPE_BOOLEAN)\n    style_converter.add_type(_constants.TYPE_NUMBER)\n    style_converter.add_parameter(\"value\", None)\n\n    return style_converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    converter = _converting._Converter()\n    converter.types = self.types\n    converter.parameters = self.parameters\n    return converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    converter = _converting._Converter()\n    converter.for_type(_to_enum(self._type), _to_enum(self._to_type))\n\n    return converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    param_style = self._get_param_style()\n    converter = _converters.get(param_style)\n    if converter is None:\n        raise _errors.UnknownStyle(param_style)\n    return converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._parameters)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    return _Converter(self._parameters, self._param_defs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n    converter = _converting._Converter()\n    converter.add_type(_constants.Type.STRING, None, None)\n    converter.add_parameter(_constants.Parameter.TIME, None)\n    converter.add_parameter(_constants.Parameter.FRACTION, None)\n    converter.add_parameter(_constants.Parameter.POTENTIAL, None)\n    converter.add_parameter(_constants.Parameter.QUANTITY, None)\n    converter.add_parameter(_constants.Parameter.VOLUME, None)\n    converter.add_parameter(_constants.Parameter.PERCENTAGE, None)\n    converter.add_parameter(_constants.Parameter.PERCENTAGE_UNIT, None)\n    converter.add_parameter(_constants.Parameter.PERCENTAGE_EXPONENTIAL, None)\n    return converter\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/isoparser.py", "project": "pexip/os-python-dateutil", "name": "isoparse", "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0", "ques_id": "62b8966c755ee91dce50a154", "code": ["def isoparse(self, dt_str):\n    if not isinstance(dt_str, six.string_types):\n        raise ValueError('isoparse() requires a string parameter')\n    dt_str = dt_str.replace('T','')\n    dt_str = dt_str.strip()\n\n    # isoparse does not accept fractional seconds, so parse it manually:\n    if dt_str.endswith('.') and dt_str[:-1].endswith('.'):\n        dt_str = dt_str[:-1]\n\n    if self._isoparse_invalid_formats_allowed(dt_str):\n        return self._parse_isoparse(dt_str)\n    elif self._isoparse_invalid_timezone(dt_str):\n        return self._parse_isoparse(dt_str, self._get_timezone(dt_str))\n    else:\n        return self._parse_isoparse(dt_str, self._get_local_tz(dt_str))\n", "def isoparse(self, dt_str):\n    parsed = self.parse_datetime(dt_str)\n    if parsed is None:\n        parsed = self.parse_time(dt_str)\n    if parsed is None:\n        return parsed\n    return parsed\n", "def isoparse(self, dt_str):\n    if self._isoparse_re.match(dt_str):\n        dt_str = self._isoparse_re.sub(r'-', dt_str)\n        return datetime.datetime(*(int(x) for x in dt_str.split('-')))\n\n    raise ValueError('Not a valid ISO-8601 datetime string: %r' % dt_str)\n", "def isoparse(self, dt_str):\n    if isinstance(dt_str, datetime.datetime):\n        return dt_str\n    elif isinstance(dt_str, six.string_types):\n        return self.parse_iso_datetime(dt_str)\n    else:\n        raise ValueError('Unable to parse datetime string:'+ dt_str)\n", "def isoparse(self, dt_str):\n    if not isinstance(dt_str, str):\n        raise ValueError('isoparse() only accepts strings')\n\n    parsed = self._parse(dt_str)\n\n    if parsed is None:\n        return None\n\n    return datetime.datetime(*parsed)\n", "def isoparse(self, dt_str):\n    # type: (str) -> datetime.datetime\n    \"\"\"Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed optionally\n    by a time portion, followed optionally by a time portion - the date\n    and time portions are separated by a single character separator, which\n    is ``T`` in the official standard. Incomplete date formats may *not*\n    be combined with a time portion. Supported date formats are:\n\n    Common: - ``YYYY`` - `YYY-MM` or ``YYY-MM-DD`` or ``YYYMMDD``\n    - `YYYYMMDD` - or ``YYYYYMMDD`` - or ``YYYYYMMDD`` - or ``YYYMMDD``\n                    (uncommon, non-standard standard but is not implemented in\n                    this parser. Support fractional components other than\n                    seconds are part of the standard)\n\n    \"\"\"\n    return datetime.datetime(*(\n  ", "def isoparse(self, dt_str):\n    parsed = parse(dt_str, self._datetime_formats)\n    if parsed['year'] is None:\n        parsed['year'] = parsed['month']\n        parsed['month'] = parsed['day']\n        parsed['day'] = parsed['hour']\n        parsed['hour'] = parsed['minute']\n        parsed['minute'] = parsed['second']\n    return parsed\n", "def isoparse(self, dt_str):\n    return isoparse(dt_str, self.tzinfos)\n", "def isoparse(self, dt_str):\n    return datetime.datetime.strptime(dt_str, ISO8601_DATE_FORMAT).replace(tzinfo=pytz.utc)\n", "def isoparse(self, dt_str):\n    return datetime.datetime.strptime(dt_str, '%Y-%m-%dT%H:%M:%S')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def isoparse(self, dt_str):\n    if not isinstance(dt_str, six.string_types):\n        raise ValueError('isoparse() requires a string parameter')\n    dt_str = dt_str.replace('T','')\n    dt_str = dt_str.strip()\n\n    # isoparse does not accept fractional seconds, so parse it manually:\n    if dt_str.endswith('.') and dt_str[:-1].endswith('.'):\n        dt_str = dt_str[:-1]\n\n    if self._isoparse_invalid_formats_allowed(dt_str):\n        return self._parse_isoparse(dt_str)\n    elif self._isoparse_invalid_timezone(dt_str):\n        return self._parse_isoparse(dt_str, self._get_timezone(dt_str))\n    else:\n        return self._parse_isoparse(dt_str, self._get_local_tz(dt_str))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    parsed = self.parse_datetime(dt_str)\n    if parsed is None:\n        parsed = self.parse_time(dt_str)\n    if parsed is None:\n        return parsed\n    return parsed\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    if self._isoparse_re.match(dt_str):\n        dt_str = self._isoparse_re.sub(r'-', dt_str)\n        return datetime.datetime(*(int(x) for x in dt_str.split('-')))\n\n    raise ValueError('Not a valid ISO-8601 datetime string: %r' % dt_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    if isinstance(dt_str, datetime.datetime):\n        return dt_str\n    elif isinstance(dt_str, six.string_types):\n        return self.parse_iso_datetime(dt_str)\n    else:\n        raise ValueError('Unable to parse datetime string:'+ dt_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    if not isinstance(dt_str, str):\n        raise ValueError('isoparse() only accepts strings')\n\n    parsed = self._parse(dt_str)\n\n    if parsed is None:\n        return None\n\n    return datetime.datetime(*parsed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    # type: (str) -> datetime.datetime\n    \"\"\"Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed optionally\n    by a time portion, followed optionally by a time portion - the date\n    and time portions are separated by a single character separator, which\n    is ``T`` in the official standard. Incomplete date formats may *not*\n    be combined with a time portion. Supported date formats are:\n\n    Common: - ``YYYY`` - `YYY-MM` or ``YYY-MM-DD`` or ``YYYMMDD``\n    - `YYYYMMDD` - or ``YYYYYMMDD`` - or ``YYYYYMMDD`` - or ``YYYMMDD``\n                    (uncommon, non-standard standard but is not implemented in\n                    this parser. Support fractional components other than\n                    seconds are part of the standard)\n\n    \"\"\"\n    return datetime.datetime(*(\n  ", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    parsed = parse(dt_str, self._datetime_formats)\n    if parsed['year'] is None:\n        parsed['year'] = parsed['month']\n        parsed['month'] = parsed['day']\n        parsed['day'] = parsed['hour']\n        parsed['hour'] = parsed['minute']\n        parsed['minute'] = parsed['second']\n    return parsed\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    return isoparse(dt_str, self.tzinfos)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    return datetime.datetime.strptime(dt_str, ISO8601_DATE_FORMAT).replace(tzinfo=pytz.utc)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n    return datetime.datetime.strptime(dt_str, '%Y-%m-%dT%H:%M:%S')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/_parser.py", "project": "pexip/os-python-dateutil", "name": "parse", "docstring": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.", "ques_id": "62b896de755ee91dce50a183", "code": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime\n        object and not ``None``, elements specified in ``timestr``\n        replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n        are ignored and a naive :class:`datetime.datetime` object is\n        returned.\n    :param tzinfos: Additional time zone names / aliases which may\n        be present in the string. This parameter maps time zone\n        names (and optionally offsets from those time zones) to\n        time zones.\n\n   ..doctest::options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                        and not ``None``, elements specified in ``timestr``\n                        replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings are\n                        ignored and a naive :class:`datetime.datetime`\n                        object is returned.\n    :param tzinfos: Additional time zones / aliases which may be\n                     presented in the string. This parameter maps\n                     time zone names (and optionally offsets from\n                         tzinfos) to time zone.\n                    ..doctest::options: +NO", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n\n    :param default: The default datetime object, if this is a datetime\n                    object and not ``None``, elements specified in ``timestr``\n                    replace elements in the default object.\n\n    :param ignoretz: If set ``True``, time zones in parsed strings are\n                    ignored and a naive :class:`datetime.datetime` object\n                    is returned.\n\n    :param tzinfos: Additional time zone names / aliases which may\n                    be present in the string. This parameter maps\n                    time zone names (and optionally offsets from the\n                    time zone) to time zones.\n\n                    doctest: :options: +NORMALIZE_WHITESPACE\n\n   ", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime\n                    object and not ``None``, elements specified in ``timestr``\n                    replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                     are ignored and a naive :class:`datetime.datetime`\n                     object is returned.\n    :param tzinfos: Additional time zone names / aliases which may\n                    be present in the string. This parameter maps\n                    time zone names (and optionally offsets from the\n                    time zone) to time zones.\n\n                   ..doctest::options: +NORMALIZE_WHITESPACE\n    ", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                     and not ``None``, elements specified in ``timestr``\n                     replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                      replace elements in the default object.\n    :param tzinfos: Additional time zone names / aliases which\n                      may be present in the string.\n                      This parameter maps time zone names (and\n                      optionally offsets from those time zones) to\n                      time zones.\n                     ..doctest::options: +NORMALIZE_WHITESPACE\n\n    >>>", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                      and not ``None``, elements specified in ``timestr``\n                      replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                      replace elements specified in the time zone names\n                      and return a naive :class:`datetime.datetime` object\n                      object.\n    :param tzinfos: Additional time zone names / aliases which may\n                     be present in the string. This parameter maps\n                      time zone names (and optionally offsets from\n                      these time zones) to time zone", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n\n    :param default: The default datetime object, if this is a datetime object\n                        and not ``None``, elements specified in ``timestr``\n                        replace elements in the default object.\n\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                        replace elements specified in the string.\n\n    :param tzinfos: Additional time zones / aliases which may be\n                    presented in the string.\n\n   ..doctest::options: +NORMALIZE_WHITESPACE\n\n                    >>> from dateutil.parser import parse\n                    >>> from dateutil.tz import gettz\n                    >>> tzinfos = {\"BRST\": -7200, \"CST", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                     and not ``None``, elements specified in ``timestr``\n                     replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings are\n                     ignored and a naive :class:`datetime.datetime` object\n                     is returned.\n    :param tzinfos: Additional time zone names / aliases which may be\n                     presented in the string. This parameter maps time\n                     zone names (and optionally offsets from these\n                     time zones) to time zones.\n                     This parameter can be a dictionary with\n ", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                                        and not ``None``, elements specified in\n                                                                                                                                                                                                                                                                                                                                         ", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime\n                    object and not ``None``, elements specified in ``timestr``\n                    replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                     are ignored and a naive :class:`datetime.datetime`\n                     object is returned.\n    :param tzinfos: Additional time zone names / aliases which may\n                    be present in the string. This parameter maps\n                    time zone names (and optionally offsets from\n                     those time zones) to time zone.\n                   ..doctest::\n                    >>"], "level": "project_runnable", "generate_results": [{"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime\n        object and not ``None``, elements specified in ``timestr``\n        replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n        are ignored and a naive :class:`datetime.datetime` object is\n        returned.\n    :param tzinfos: Additional time zone names / aliases which may\n        be present in the string. This parameter maps time zone\n        names (and optionally offsets from those time zones) to\n        time zones.\n\n   ..doctest::options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                        and not ``None``, elements specified in ``timestr``\n                        replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings are\n                        ignored and a naive :class:`datetime.datetime`\n                        object is returned.\n    :param tzinfos: Additional time zones / aliases which may be\n                     presented in the string. This parameter maps\n                     time zone names (and optionally offsets from\n                         tzinfos) to time zone.\n                    ..doctest::options: +NO", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n\n    :param default: The default datetime object, if this is a datetime\n                    object and not ``None``, elements specified in ``timestr``\n                    replace elements in the default object.\n\n    :param ignoretz: If set ``True``, time zones in parsed strings are\n                    ignored and a naive :class:`datetime.datetime` object\n                    is returned.\n\n    :param tzinfos: Additional time zone names / aliases which may\n                    be present in the string. This parameter maps\n                    time zone names (and optionally offsets from the\n                    time zone) to time zones.\n\n                    doctest: :options: +NORMALIZE_WHITESPACE\n\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime\n                    object and not ``None``, elements specified in ``timestr``\n                    replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                     are ignored and a naive :class:`datetime.datetime`\n                     object is returned.\n    :param tzinfos: Additional time zone names / aliases which may\n                    be present in the string. This parameter maps\n                    time zone names (and optionally offsets from the\n                    time zone) to time zones.\n\n                   ..doctest::options: +NORMALIZE_WHITESPACE\n    ", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                     and not ``None``, elements specified in ``timestr``\n                     replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                      replace elements in the default object.\n    :param tzinfos: Additional time zone names / aliases which\n                      may be present in the string.\n                      This parameter maps time zone names (and\n                      optionally offsets from those time zones) to\n                      time zones.\n                     ..doctest::options: +NORMALIZE_WHITESPACE\n\n    >>>", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                      and not ``None``, elements specified in ``timestr``\n                      replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                      replace elements specified in the time zone names\n                      and return a naive :class:`datetime.datetime` object\n                      object.\n    :param tzinfos: Additional time zone names / aliases which may\n                     be present in the string. This parameter maps\n                      time zone names (and optionally offsets from\n                      these time zones) to time zone", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n\n    :param default: The default datetime object, if this is a datetime object\n                        and not ``None``, elements specified in ``timestr``\n                        replace elements in the default object.\n\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                        replace elements specified in the string.\n\n    :param tzinfos: Additional time zones / aliases which may be\n                    presented in the string.\n\n   ..doctest::options: +NORMALIZE_WHITESPACE\n\n                    >>> from dateutil.parser import parse\n                    >>> from dateutil.tz import gettz\n                    >>> tzinfos = {\"BRST\": -7200, \"CST", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                     and not ``None``, elements specified in ``timestr``\n                     replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings are\n                     ignored and a naive :class:`datetime.datetime` object\n                     is returned.\n    :param tzinfos: Additional time zone names / aliases which may be\n                     presented in the string. This parameter maps time\n                     zone names (and optionally offsets from these\n                     time zones) to time zones.\n                     This parameter can be a dictionary with\n ", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime object\n                                        and not ``None``, elements specified in\n                                                                                                                                                                                                                                                                                                                                         ", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"Parse the date/time string into a :class:`datetime.datetime` object.\n\n    :param timestr: Any date/time string using the supported formats.\n    :param default: The default datetime object, if this is a datetime\n                    object and not ``None``, elements specified in ``timestr``\n                    replace elements in the default object.\n    :param ignoretz: If set ``True``, time zones in parsed strings\n                     are ignored and a naive :class:`datetime.datetime`\n                     object is returned.\n    :param tzinfos: Additional time zone names / aliases which may\n                    be present in the string. This parameter maps\n                    time zone names (and optionally offsets from\n                     those time zones) to time zone.\n                   ..doctest::\n                    >>", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "ques_id": "62b8a4a4755ee91dce50a3d3", "code": ["def fromutc(self, dt):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=self.UTC)\n    return dt.astimezone(self.UTC)\n", "def fromutc(self, dt):\n    if dt.tzinfo is None:\n        raise ValueError(\"Cannot freeze naive time.\")\n    return dt.astimezone(self)\n", "def fromutc(self, dt):\n\n    if dt.tzinfo is None:\n        raise ValueError('Cannot take timezone-aware datetimes')\n\n    return dt.astimezone(self)\n", "def fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt\n\n    return dt.astimezone(self._tz)\n", "def fromutc(self, dt):\n    if self._tzinfo is not None and dt.tzinfo is not None:\n        dt = dt.astimezone(self._tzinfo)\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=self._tzinfo)\n    return dt.replace(microsecond=0)\n", "def fromutc(self, dt):\n    if self._is_frozen:\n        raise FrozenError(\"Can't change time zone after freezing.\")\n\n    if dt.tzinfo is None:\n        raise ValueError(\"Cannot freeze naive timestamp without timezone.\")\n\n    # Early versions of dateutil send a warning when the time zone\n    # is explicitly set to None to indicate that the time zone is\n    # required to be set.  For backwards compatibility with earlier\n    # versions, we may need to manually set the time zone to the default\n    # time zone if set to the default time zone.\n    if self._default_tz is None:\n        dt = dt.replace(tzinfo=None)\n\n    return dt.astimezone(self)\n", "def fromutc(self, dt):\n    return dt.replace(tzinfo=self)\n", "def fromutc(self, dt):\n    if self._is_ambiguous(dt):\n        # TODO: This is a bit different than fromutc() because it's a\n        #       complicated logic and we don't want to modify the original\n        #       object.\n        dt = dt.astimezone(self._tz)\n    return dt.replace(tzinfo=None)\n", "def fromutc(self, dt):\n    return dt.replace(tzinfo=self)\n", "def fromutc(self, dt):\n    if dt.tzinfo is None:\n        raise ValueError(\"fromutc() requires a timezone-aware datetime object\")\n    return dt.astimezone(self)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fromutc(self, dt):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=self.UTC)\n    return dt.astimezone(self.UTC)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if dt.tzinfo is None:\n        raise ValueError(\"Cannot freeze naive time.\")\n    return dt.astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\n    if dt.tzinfo is None:\n        raise ValueError('Cannot take timezone-aware datetimes')\n\n    return dt.astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if dt.tzinfo is None:\n        return dt\n\n    return dt.astimezone(self._tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self._tzinfo is not None and dt.tzinfo is not None:\n        dt = dt.astimezone(self._tzinfo)\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=self._tzinfo)\n    return dt.replace(microsecond=0)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self._is_frozen:\n        raise FrozenError(\"Can't change time zone after freezing.\")\n\n    if dt.tzinfo is None:\n        raise ValueError(\"Cannot freeze naive timestamp without timezone.\")\n\n    # Early versions of dateutil send a warning when the time zone\n    # is explicitly set to None to indicate that the time zone is\n    # required to be set.  For backwards compatibility with earlier\n    # versions, we may need to manually set the time zone to the default\n    # time zone if set to the default time zone.\n    if self._default_tz is None:\n        dt = dt.replace(tzinfo=None)\n\n    return dt.astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    return dt.replace(tzinfo=self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if self._is_ambiguous(dt):\n        # TODO: This is a bit different than fromutc() because it's a\n        #       complicated logic and we don't want to modify the original\n        #       object.\n        dt = dt.astimezone(self._tz)\n    return dt.replace(tzinfo=None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    return dt.replace(tzinfo=self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n    if dt.tzinfo is None:\n        raise ValueError(\"fromutc() requires a timezone-aware datetime object\")\n    return dt.astimezone(self)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/utils.py", "project": "pexip/os-python-dateutil", "name": "default_tzinfo", "docstring": "Sets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.", "ques_id": "62b8a7b2755ee91dce50a4a7", "code": ["def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if is_naive(dt):\n        return dt.replace(tzinfo=tzinfo)\n    else:\n        return dt\n", "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n    if is_naive(dt):\n        dt = dt.replace(tzinfo=None)\n    return dt.astimezone(tzinfo)\n", "def default_tzinfo(dt, tzinfo):\n    if isinstance(dt, datetime) and dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    elif isinstance(dt, date) and dt.tzinfo is not None:\n        dt = dt.astimezone(tzinfo)\n\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if not is_naive(dt):\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n"], "level": "self_contained", "generate_results": [{"generate_code": "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if is_naive(dt):\n        return dt.replace(tzinfo=tzinfo)\n    else:\n        return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if tzinfo is None:\n        return dt\n    if is_naive(dt):\n        dt = dt.replace(tzinfo=None)\n    return dt.astimezone(tzinfo)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if isinstance(dt, datetime) and dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    elif isinstance(dt, date) and dt.tzinfo is not None:\n        dt = dt.astimezone(tzinfo)\n\n    return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if not is_naive(dt):\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tzinfo)\n    return dt\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/lithium/testcases.py", "project": "MozillaSecurity/lithium", "name": "set_cut_chars", "docstring": "Set the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.", "ques_id": "6305f9991d275c6667163c50", "code": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._pre_delimiter = before\n    self._post_delimiter = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._cut_chars = before, after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._cut_chars = (before, after)\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._delimiters = [before, after]\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    if before is None:\n        self._cut_chars = None\n    else:\n        self._cut_chars = _b_to_u(before)\n    if after is None:\n        self._cut_chars = None\n    else:\n        self._cut_chars = _b_to_u(after)\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    if not isinstance(before, bytes):\n        raise TypeError('before must be a bytes object.')\n    if not isinstance(after, bytes):\n        raise TypeError('after must be a bytes object.')\n    self._cut_chars = before, after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._delimiters = [before, after]\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._delimiter = before\n    self._delimiter_len = len(before)\n    self._split_after = after\n    self._split_delimiter = None\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._cut_chars = before.decode('utf-8') + ',' + after.decode('utf-8')\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._set_chars(_SPLIT_FILE_DELIMITER, before, after)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._pre_delimiter = before\n    self._post_delimiter = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._cut_chars = before, after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._cut_chars = (before, after)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._delimiters = [before, after]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    if before is None:\n        self._cut_chars = None\n    else:\n        self._cut_chars = _b_to_u(before)\n    if after is None:\n        self._cut_chars = None\n    else:\n        self._cut_chars = _b_to_u(after)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    if not isinstance(before, bytes):\n        raise TypeError('before must be a bytes object.')\n    if not isinstance(after, bytes):\n        raise TypeError('after must be a bytes object.')\n    self._cut_chars = before, after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._delimiters = [before, after]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._delimiter = before\n    self._delimiter_len = len(before)\n    self._split_after = after\n    self._split_delimiter = None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._cut_chars = before.decode('utf-8') + ',' + after.decode('utf-8')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    self._set_chars(_SPLIT_FILE_DELIMITER, before, after)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.", "ques_id": "6306292052e177c0ba469f09", "code": ["def identify_request(request: RequestType):\n    if request == \"private.legacy.payload\":\n        return True\n\n    for payload_type in payload_types:\n        if request.startswith(payload_type):\n            return True\n\n    return False\n", "def identify_request(request: RequestType):\n    if request == 'private':\n        if requests.get(request).status_code == 200:\n            return True\n        else:\n            return False\n\n    if request == 'legacy':\n        if requests.get(request).json()['payloadType'] == 'legacy':\n            return True\n        else:\n            return False\n", "def identify_request(request: RequestType):\n    return request.is_legacy_request\n", "def identify_request(request: RequestType):\n    payload = request.get('payload', {})\n    if payload.get('is_legacy_request') and payload.get('is_sandbox'):\n        return True\n    if payload.get('is_legacy_request'):\n        return False\n    if payload.get('is_private_request'):\n        return True\n    return False\n", "def identify_request(request: RequestType):\n    payload = request.get(\"payload\")\n\n    if payload is None:\n        return True\n\n    return (\n        payload.get(\"mimeType\") == \"text/plain\" and payload.get(\"subType\") == \"0\"\n    )\n", "def identify_request(request: RequestType):\n    if request.method == \"POST\":\n        return True\n    if request.headers.get(\"X-Legacy-Application\", \"\") == \"Diaspora\":\n        return True\n    if request.headers.get(\"X-Legacy-Private-Message\", \"\") == \"Legacy Payload\":\n        return True\n    return False\n", "def identify_request(request: RequestType):\n    if request == \"private\":\n        msg_type = \"private\"\n    else:\n        msg_type = request.split(\";\", 1)[0].split(\";\", 1)[0]\n    return msg_type in [\"legacy\", \"payload\"]\n", "def identify_request(request: RequestType):\n    return request['type'] == 'private' and request['subtype'] =='message'\n", "def identify_request(request: RequestType):\n    if request in (\"m.text\", \"m.file\"):\n        return True\n    elif request in (\"m.private\", \"m.legacy\", \"m.custom\"):\n        return False\n    elif request.startswith(\"m.\"):\n        return True\n    elif request in (\"m.text.html\", \"m.file.html\"):\n        return False\n    elif request in (\"m.private.html\", \"m.legacy.html\", \"m.custom.html\"):\n        return True\n    elif request.startswith(\"m.\"):\n        return False\n    else:\n        return None\n", "def identify_request(request: RequestType):\n    if request.method == 'POST':\n        payload = request.POST.get('payload')\n        if isinstance(payload, dict) and'message' in payload:\n            return True\n        if isinstance(payload, list) and len(payload) == 1 and'message' in payload[0]:\n            return True\n    return False\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType):\n    if request == \"private.legacy.payload\":\n        return True\n\n    for payload_type in payload_types:\n        if request.startswith(payload_type):\n            return True\n\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n    if request == 'private':\n        if requests.get(request).status_code == 200:\n            return True\n        else:\n            return False\n\n    if request == 'legacy':\n        if requests.get(request).json()['payloadType'] == 'legacy':\n            return True\n        else:\n            return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n    return request.is_legacy_request\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n    payload = request.get('payload', {})\n    if payload.get('is_legacy_request') and payload.get('is_sandbox'):\n        return True\n    if payload.get('is_legacy_request'):\n        return False\n    if payload.get('is_private_request'):\n        return True\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n    payload = request.get(\"payload\")\n\n    if payload is None:\n        return True\n\n    return (\n        payload.get(\"mimeType\") == \"text/plain\" and payload.get(\"subType\") == \"0\"\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n    if request.method == \"POST\":\n        return True\n    if request.headers.get(\"X-Legacy-Application\", \"\") == \"Diaspora\":\n        return True\n    if request.headers.get(\"X-Legacy-Private-Message\", \"\") == \"Legacy Payload\":\n        return True\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n    if request == \"private\":\n        msg_type = \"private\"\n    else:\n        msg_type = request.split(\";\", 1)[0].split(\";\", 1)[0]\n    return msg_type in [\"legacy\", \"payload\"]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n    return request['type'] == 'private' and request['subtype'] =='message'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n    if request in (\"m.text\", \"m.file\"):\n        return True\n    elif request in (\"m.private\", \"m.legacy\", \"m.custom\"):\n        return False\n    elif request.startswith(\"m.\"):\n        return True\n    elif request in (\"m.text.html\", \"m.file.html\"):\n        return False\n    elif request in (\"m.private.html\", \"m.legacy.html\", \"m.custom.html\"):\n        return True\n    elif request.startswith(\"m.\"):\n        return False\n    else:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n    if request.method == 'POST':\n        payload = request.POST.get('payload')\n        if isinstance(payload, dict) and'message' in payload:\n            return True\n        if isinstance(payload, list) and len(payload) == 1 and'message' in payload[0]:\n            return True\n    return False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/matrix/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Matrix request", "ques_id": "6306292152e177c0ba469f0d", "code": ["def identify_request(request: RequestType) -> bool:\n    return request in (\n        b'GET',\n        b'HEAD',\n        b'POST',\n        b'PUT',\n        b'DELETE',\n        b'TRACE',\n        b'OPTIONS',\n        b'TRACE_PAGE',\n        b'PATCH',\n        b'TRACE_FORM_DATA',\n        b'POST_FORM_DATA',\n        b'PUT_FORM_DATA',\n        b'PATCH_JSON',\n        b'GET_JSON',\n        b'TRACE_PAGE_INFO',\n        b'TRACE_FORM_DATA_INFO',\n        b'TRACE_FORM_DATA_INFO_WITHOUT_CONTENT_TYPE',\n        b'TRACE_FORM_DATA_INFO_WITHOUT_CONTENT_TYPE_WITHOUT_BODY',\n        b'TRACE_FORM_DATA_INFO_WITHOUT_CONTENT_TYPE_AND_BODY',\n    )\n", "def identify_request(request: RequestType) -> bool:\n    return (\n        request in (RequestType.MATRIX, RequestType.MATRIX_ROUTER)\n        and request.value == \"\"\n    )\n", "def identify_request(request: RequestType) -> bool:\n    return request in (\n        RequestType.Matrix,\n        RequestType.Gateway,\n        RequestType.Session,\n        RequestType.Sessions,\n        RequestType.Call,\n        RequestType.Invoke,\n        RequestType.Cancel,\n        RequestType.CancelRequest,\n        RequestType.Reject,\n    )\n", "def identify_request(request: RequestType) -> bool:\n    if isinstance(request, MatrixRequest):\n        return True\n\n    if isinstance(request, RequestType):\n        return True\n\n    raise TypeError(f\"Unknown request type: {request}\")\n", "def identify_request(request: RequestType) -> bool:\n    return request in (\n        b\"GET\",\n        b\"HEAD\",\n        b\"TRACE\",\n        b\"OPTIONS\",\n        b\"TRACE HTTP/1.1\",\n        b\"HEAD /\",\n        b\"HEAD //\",\n        b\"HEAD // HTTP/1.1\",\n        b\"HEAD // HTTPS/1.1\",\n        b\"HEAD // HTTPS\",\n        b\"HEAD // HTTPS HTTP/1.1\",\n        b\"HEAD // HTTPS HTTPS HTTP/1.1\",\n        b\"HEAD // HTTPS HTTPS HTTPS HTTP/1.1\",\n    )\n", "def identify_request(request: RequestType) -> bool:\n    return (\n        request in (RequestType.MATRICE_RECEIVE_RESPONSE, RequestType.MATRICE_SEND_RESPONSE)\n        or (request == RequestType.MATRICE_SEND_RESPONSE and request.payload)\n    )\n", "def identify_request(request: RequestType) -> bool:\n    if isinstance(request, MatrixRequest):\n        return True\n\n    if isinstance(request, StreamRequest):\n        return True\n\n    if isinstance(request, BroadcastRequest):\n        return True\n\n    return False\n", "def identify_request(request: RequestType) -> bool:\n    return request in (\n        RequestType.GET,\n        RequestType.POST,\n        RequestType.PUT,\n        RequestType.DELETE,\n        RequestType.OPTIONS,\n        RequestType.HEAD,\n    )\n", "def identify_request(request: RequestType) -> bool:\n    return request in ['Matrix', 'Matrix/Vector']\n", "def identify_request(request: RequestType) -> bool:\n    return request in _requests\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType) -> bool:\n    return request in (\n        b'GET',\n        b'HEAD',\n        b'POST',\n        b'PUT',\n        b'DELETE',\n        b'TRACE',\n        b'OPTIONS',\n        b'TRACE_PAGE',\n        b'PATCH',\n        b'TRACE_FORM_DATA',\n        b'POST_FORM_DATA',\n        b'PUT_FORM_DATA',\n        b'PATCH_JSON',\n        b'GET_JSON',\n        b'TRACE_PAGE_INFO',\n        b'TRACE_FORM_DATA_INFO',\n        b'TRACE_FORM_DATA_INFO_WITHOUT_CONTENT_TYPE',\n        b'TRACE_FORM_DATA_INFO_WITHOUT_CONTENT_TYPE_WITHOUT_BODY',\n        b'TRACE_FORM_DATA_INFO_WITHOUT_CONTENT_TYPE_AND_BODY',\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n    return (\n        request in (RequestType.MATRIX, RequestType.MATRIX_ROUTER)\n        and request.value == \"\"\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n    return request in (\n        RequestType.Matrix,\n        RequestType.Gateway,\n        RequestType.Session,\n        RequestType.Sessions,\n        RequestType.Call,\n        RequestType.Invoke,\n        RequestType.Cancel,\n        RequestType.CancelRequest,\n        RequestType.Reject,\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n    if isinstance(request, MatrixRequest):\n        return True\n\n    if isinstance(request, RequestType):\n        return True\n\n    raise TypeError(f\"Unknown request type: {request}\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n    return request in (\n        b\"GET\",\n        b\"HEAD\",\n        b\"TRACE\",\n        b\"OPTIONS\",\n        b\"TRACE HTTP/1.1\",\n        b\"HEAD /\",\n        b\"HEAD //\",\n        b\"HEAD // HTTP/1.1\",\n        b\"HEAD // HTTPS/1.1\",\n        b\"HEAD // HTTPS\",\n        b\"HEAD // HTTPS HTTP/1.1\",\n        b\"HEAD // HTTPS HTTPS HTTP/1.1\",\n        b\"HEAD // HTTPS HTTPS HTTPS HTTP/1.1\",\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n    return (\n        request in (RequestType.MATRICE_RECEIVE_RESPONSE, RequestType.MATRICE_SEND_RESPONSE)\n        or (request == RequestType.MATRICE_SEND_RESPONSE and request.payload)\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n    if isinstance(request, MatrixRequest):\n        return True\n\n    if isinstance(request, StreamRequest):\n        return True\n\n    if isinstance(request, BroadcastRequest):\n        return True\n\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n    return request in (\n        RequestType.GET,\n        RequestType.POST,\n        RequestType.PUT,\n        RequestType.DELETE,\n        RequestType.OPTIONS,\n        RequestType.HEAD,\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n    return request in ['Matrix', 'Matrix/Vector']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n    return request in _requests\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/utils.py", "project": "jaywink/federation", "name": "format_dt", "docstring": "Format a datetime in the way that D* nodes expect.", "ques_id": "6306292252e177c0ba469f11", "code": ["def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "def format_dt(dt):\n    if dt is None:\n        return \"null\"\n    elif isinstance(dt, datetime.datetime):\n        return dt.isoformat()\n    elif isinstance(dt, datetime.date):\n        return dt.isoformat() + \"Z\"\n    else:\n        return dt\n", "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "def format_dt(dt):\n    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n", "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n", "def format_dt(dt):\n    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n", "def format_dt(dt):\n    if dt.tzinfo is not None:\n        raise ValueError(\"dt.tzinfo is not None\")\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n", "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "def format_dt(dt):\n    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n", "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    if dt is None:\n        return \"null\"\n    elif isinstance(dt, datetime.datetime):\n        return dt.isoformat()\n    elif isinstance(dt, datetime.date):\n        return dt.isoformat() + \"Z\"\n    else:\n        return dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    if dt.tzinfo is not None:\n        raise ValueError(\"dt.tzinfo is not None\")\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "find_tags", "docstring": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.", "ques_id": "6306292352e177c0ba469f1d", "code": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    if replacer is None:\n        def replacer(match):\n            return match.group(1)\n\n    pattern = r\"\"\"\n    (?P<tags>(?:(?P<replacement>\\S+)\\s+\\:\\s+(?P<original>.*?)))\n    \"\"\"\n\n    return re.findall(pattern, text, flags=re.UNICODE | re.MULTILINE | re.VERBOSE)\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    if replacer is not None:\n        text = replacer(text)\n    return _CODE_PATTERN.findall(text)\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    regex = re.compile(r'\\[(.*?)\\]')\n    replaced = regex.sub(replacer, '', text)\n    return re.findall(regex, replaced)\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    # Strip HTML tags with non-breaking spaces\n    text = re.sub(r'<.*?>', '', text)\n    # Replace HTML entities\n    text = re.sub(r'&(#?)([0-9]+)([^;]*);', r'\\1&nbsp;\\2', text)\n    # Replace code blocks\n    if replacer is not None:\n        text = replacer(text)\n    # Remove html tags and return only the original text\n    return set(re.findall(r'<.*?>', text)), text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    text = re.sub(r\"<!--.*?-->\", \"\", text)  # Remove tags\n    text = re.sub(r\"<(?![^>]*>).*?</\", \"\", text)  # Replace tags with replacer\n\n    if replacer is None:\n        return find_tags_in(text)\n    else:\n        return find_tags_in(replacer(text))\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    tags = set()\n\n    if replacer:\n        text = replacer(text)\n\n    for match in TAGS_REGEX.finditer(text):\n        tag = match.group(1)\n        text = text[:match.start(1)] + replacer(text[match.start(1):match.end(1)]) + text[match.end(1):]\n        tags.add(tag)\n\n    return tags, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    tags = set()\n    for match in _CODE_TAGS_PATTERN.finditer(text):\n        if replacer:\n            tags.add(replacer(match))\n        else:\n            tags.add(match.group())\n    return tags, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    text = CODE_BLOCK_RE.sub(\"\", text)\n    return _find_tags_in_text(text, replacer)\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    tags = set()\n    replacer = replacer or (lambda m: m.group(0))\n    text = text.replace('```', '') if '```' in text else text\n    for match in _CODE_RE.finditer(text):\n        tag = match.group(0)\n        if tag:\n            if replacer(match):\n                text = replacer(match)\n            else:\n                text = text.replace(tag, '')\n        else:\n            text = text.replace(match.group(0), '')\n    return tags, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    tags = set()\n    if replacer is None:\n        def replacer(match):\n            return match.group(1)\n\n    pattern = r'(?:<!\\\\)(.+?)(?:</\\1>|#.+?$)'\n    replace = replacer\n    for match in re.finditer(pattern, text):\n        original = match.group(0)\n        replacement = replacer(match)\n        text = re.sub(re.escape(original), replacement, text, flags=re.I)\n        tags.add(original)\n\n    return tags, text\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    if replacer is None:\n        def replacer(match):\n            return match.group(1)\n\n    pattern = r\"\"\"\n    (?P<tags>(?:(?P<replacement>\\S+)\\s+\\:\\s+(?P<original>.*?)))\n    \"\"\"\n\n    return re.findall(pattern, text, flags=re.UNICODE | re.MULTILINE | re.VERBOSE)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    if replacer is not None:\n        text = replacer(text)\n    return _CODE_PATTERN.findall(text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    regex = re.compile(r'\\[(.*?)\\]')\n    replaced = regex.sub(replacer, '', text)\n    return re.findall(regex, replaced)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    # Strip HTML tags with non-breaking spaces\n    text = re.sub(r'<.*?>', '', text)\n    # Replace HTML entities\n    text = re.sub(r'&(#?)([0-9]+)([^;]*);', r'\\1&nbsp;\\2', text)\n    # Replace code blocks\n    if replacer is not None:\n        text = replacer(text)\n    # Remove html tags and return only the original text\n    return set(re.findall(r'<.*?>', text)), text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    text = re.sub(r\"<!--.*?-->\", \"\", text)  # Remove tags\n    text = re.sub(r\"<(?![^>]*>).*?</\", \"\", text)  # Replace tags with replacer\n\n    if replacer is None:\n        return find_tags_in(text)\n    else:\n        return find_tags_in(replacer(text))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    tags = set()\n\n    if replacer:\n        text = replacer(text)\n\n    for match in TAGS_REGEX.finditer(text):\n        tag = match.group(1)\n        text = text[:match.start(1)] + replacer(text[match.start(1):match.end(1)]) + text[match.end(1):]\n        tags.add(tag)\n\n    return tags, text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    tags = set()\n    for match in _CODE_TAGS_PATTERN.finditer(text):\n        if replacer:\n            tags.add(replacer(match))\n        else:\n            tags.add(match.group())\n    return tags, text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    text = CODE_BLOCK_RE.sub(\"\", text)\n    return _find_tags_in_text(text, replacer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    tags = set()\n    replacer = replacer or (lambda m: m.group(0))\n    text = text.replace('```', '') if '```' in text else text\n    for match in _CODE_RE.finditer(text):\n        tag = match.group(0)\n        if tag:\n            if replacer(match):\n                text = replacer(match)\n            else:\n                text = text.replace(tag, '')\n        else:\n            text = text.replace(match.group(0), '')\n    return tags, text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    tags = set()\n    if replacer is None:\n        def replacer(match):\n            return match.group(1)\n\n    pattern = r'(?:<!\\\\)(.+?)(?:</\\1>|#.+?$)'\n    replace = replacer\n    for match in re.finditer(pattern, text):\n        original = match.group(0)\n        replacement = replacer(match)\n        text = re.sub(re.escape(original), replacement, text, flags=re.I)\n        tags.add(original)\n\n    return tags, text\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "process_text_links", "docstring": "Process links in text, adding some attributes and linkifying textual links.", "ques_id": "6306292352e177c0ba469f1e", "code": ["def process_text_links(text):\n    return linkify_text(text, LinkifyParser())\n", "def process_text_links(text):\n    return linkify_text(text, get_link_attrs())\n", "def process_text_links(text):\n    return linkify(text, get_link_attributes())\n", "def process_text_links(text):\n    linkified = linkify(text)\n    linkified = replace_urls(linkified)\n    return linkified\n", "def process_text_links(text):\n    text = linkify_text(text)\n    return text\n", "def process_text_links(text):\n    text = linkify(text)\n    text = text.replace('[[Category::Link]]', '[[Category::Text]]')\n    text = text.replace('[[Category::Link]] [[Link::Link]]', '[[Category::Link]] [[Link::Text]]')\n    text = text.replace('[[Category::Link]] Category::Link', '[[Category::Link]] Category::Text')\n    text = text.replace('[[Category::Link]] Category::Link Category::Link', '[[Category::Link]] Category::Text Category::Link')\n    text = text.replace('[[Category::Link]] Category::Link Category::Link Category::Link', '[[Category::Link]] Category::Text Category::Link Category::Link')\n    text = text.replace('[[Category::Link]] Category::Link Category::Link Category::Link Category::Link', '[[Category::Link]] Category::Link Category::Link Category::Link')\n    return text\n", "def process_text_links(text):\n    text = linkify_text(text)\n    return text\n", "def process_text_links(text):\n    linkify_regex = re.compile(r'\\[\\[(.*?)\\]\\]', re.DOTALL)\n    linkify_text = re.sub(linkify_regex, linkify_attributes, text)\n    return linkify_text\n", "def process_text_links(text):\n    text = replace_links(text)\n    return linkify(text)\n", "def process_text_links(text):\n    return linkify_text(text, add_attributes=True)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def process_text_links(text):\n    return linkify_text(text, LinkifyParser())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    return linkify_text(text, get_link_attrs())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    return linkify(text, get_link_attributes())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    linkified = linkify(text)\n    linkified = replace_urls(linkified)\n    return linkified\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    text = linkify_text(text)\n    return text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    text = linkify(text)\n    text = text.replace('[[Category::Link]]', '[[Category::Text]]')\n    text = text.replace('[[Category::Link]] [[Link::Link]]', '[[Category::Link]] [[Link::Text]]')\n    text = text.replace('[[Category::Link]] Category::Link', '[[Category::Link]] Category::Text')\n    text = text.replace('[[Category::Link]] Category::Link Category::Link', '[[Category::Link]] Category::Text Category::Link')\n    text = text.replace('[[Category::Link]] Category::Link Category::Link Category::Link', '[[Category::Link]] Category::Text Category::Link Category::Link')\n    text = text.replace('[[Category::Link]] Category::Link Category::Link Category::Link Category::Link', '[[Category::Link]] Category::Link Category::Link Category::Link')\n    return text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    text = linkify_text(text)\n    return text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    linkify_regex = re.compile(r'\\[\\[(.*?)\\]\\]', re.DOTALL)\n    linkify_text = re.sub(linkify_regex, linkify_attributes, text)\n    return linkify_text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    text = replace_links(text)\n    return linkify(text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n    return linkify_text(text, add_attributes=True)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "fetch_content_type", "docstring": "Fetch the HEAD of the remote url to determine the content type.", "ques_id": "6306292652e177c0ba469f34", "code": ["def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url, allow_redirects=True)\n    if response.status_code!= 200:\n        return None\n    return response.headers.get(\"content-type\")\n", "def fetch_content_type(url: str) -> Optional[str]:\n    res = requests.head(url, allow_redirects=True)\n    return res.headers.get(\"content-type\", None)\n", "def fetch_content_type(url: str) -> Optional[str]:\n    res = requests.head(url, allow_redirects=True)\n    return res.headers.get(\"content-type\", None)\n", "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url)\n\n    if response.status_code == 200:\n        return response.headers.get(\"content-type\")\n    else:\n        return None\n", "def fetch_content_type(url: str) -> Optional[str]:\n    res = requests.head(url, allow_redirects=False)\n    if res.status_code == 200 and \"text\" in res.headers.get(\"content-type\", \"\"):\n        return res.headers.get(\"content-type\")\n    return None\n", "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url, allow_redirects=True)\n    return response.headers.get('content-type')\n", "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url)\n    if response.status_code!= 200:\n        return None\n    return response.headers.get(\"content-type\", None)\n", "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url)\n    return cast(Optional[str], response.headers.get(\"content-type\", None))\n", "def fetch_content_type(url: str) -> Optional[str]:\n    try:\n        response = urlopen(url)\n        return response.headers.get(\"Content-Type\", None)\n    except HTTPError:\n        return None\n", "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url, allow_redirects=True)\n    if response.status_code == 200:\n        return response.headers.get(\"content-type\", None)\n    else:\n        return None\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url, allow_redirects=True)\n    if response.status_code!= 200:\n        return None\n    return response.headers.get(\"content-type\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    res = requests.head(url, allow_redirects=True)\n    return res.headers.get(\"content-type\", None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    res = requests.head(url, allow_redirects=True)\n    return res.headers.get(\"content-type\", None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url)\n\n    if response.status_code == 200:\n        return response.headers.get(\"content-type\")\n    else:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    res = requests.head(url, allow_redirects=False)\n    if res.status_code == 200 and \"text\" in res.headers.get(\"content-type\", \"\"):\n        return res.headers.get(\"content-type\")\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url, allow_redirects=True)\n    return response.headers.get('content-type')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url)\n    if response.status_code!= 200:\n        return None\n    return response.headers.get(\"content-type\", None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url)\n    return cast(Optional[str], response.headers.get(\"content-type\", None))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    try:\n        response = urlopen(url)\n        return response.headers.get(\"Content-Type\", None)\n    except HTTPError:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n    response = requests.head(url, allow_redirects=True)\n    if response.status_code == 200:\n        return response.headers.get(\"content-type\", None)\n    else:\n        return None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "test_tag", "docstring": "Test a word whether it could be accepted as a tag.", "ques_id": "6306292a52e177c0ba469f41", "code": ["def test_tag(tag: str) -> bool:\n    return len(tag) > 0 and tag.isalnum()\n", "def test_tag(tag: str) -> bool:\n    return len(tag) > 1 and tag.isalpha()\n", "def test_tag(tag: str) -> bool:\n    return (\n        len(tag) > 1\n        and tag.startswith(\"-\")\n        and not tag.endswith(\"-\")\n    )\n", "def test_tag(tag: str) -> bool:\n    return tag.startswith(\"<\") and tag.endswith(\">\")\n", "def test_tag(tag: str) -> bool:\n    return tag in TAGS\n", "def test_tag(tag: str) -> bool:\n    if tag in TAGS:\n        return True\n    return False\n", "def test_tag(tag: str) -> bool:\n    return len(tag) > 1 and tag.isalpha()\n", "def test_tag(tag: str) -> bool:\n    if not tag or tag.lower() in ['none', '']:\n        return False\n    return True\n", "def test_tag(tag: str) -> bool:\n    return bool(TAG_RE.match(tag))\n", "def test_tag(tag: str) -> bool:\n    return (\n        len(tag) > 1\n        and tag[0].isalpha()\n        and tag[1:].isalpha()\n        and not (tag[0].isdigit() or tag[1:].isdigit())\n    )\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def test_tag(tag: str) -> bool:\n    return len(tag) > 0 and tag.isalnum()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return len(tag) > 1 and tag.isalpha()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return (\n        len(tag) > 1\n        and tag.startswith(\"-\")\n        and not tag.endswith(\"-\")\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return tag.startswith(\"<\") and tag.endswith(\">\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return tag in TAGS\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    if tag in TAGS:\n        return True\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return len(tag) > 1 and tag.isalpha()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n    if not tag or tag.lower() in ['none', '']:\n        return False\n    return True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return bool(TAG_RE.match(tag))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n    return (\n        len(tag) > 1\n        and tag[0].isalpha()\n        and tag[1:].isalpha()\n        and not (tag[0].isdigit() or tag[1:].isdigit())\n    )\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "xml_children_as_dict", "docstring": "Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.", "ques_id": "6306298b52e177c0ba469fdc", "code": ["def xml_children_as_dict(node):\n    children = []\n    for child in node.getchildren():\n        children.append(child.tag)\n    return dict(zip(children, children))\n", "def xml_children_as_dict(node):\n    d = {}\n    for child in node.getchildren():\n        d[child.tag] = child.text\n    return d\n", "def xml_children_as_dict(node):\n    return {child.tag: child.text or '' for child in node}\n", "def xml_children_as_dict(node):\n    result = {}\n    for child in node:\n        tag = child.tag\n        result[tag] = xml_children_as_dict(child)\n    return result\n", "def xml_children_as_dict(node):\n    return dict([(child.tag, child.text) for child in node])\n", "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "def xml_children_as_dict(node):\n    return dict([(child.tag, child.text) for child in node])\n", "def xml_children_as_dict(node):\n    d = {}\n    for c in node.getchildren():\n        d[c.tag] = c.text\n    return d\n", "def xml_children_as_dict(node):\n    return {child.tag: _xml_children_as_dict(child)\n            for child in node}\n", "def xml_children_as_dict(node):\n    d = {}\n    for c in list(node):\n        d[c.tag] = c.text\n    return d\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def xml_children_as_dict(node):\n    children = []\n    for child in node.getchildren():\n        children.append(child.tag)\n    return dict(zip(children, children))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n    d = {}\n    for child in node.getchildren():\n        d[child.tag] = child.text\n    return d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n    return {child.tag: child.text or '' for child in node}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    result = {}\n    for child in node:\n        tag = child.tag\n        result[tag] = xml_children_as_dict(child)\n    return result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    return dict([(child.tag, child.text) for child in node])\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    return {child.tag: child.text for child in node}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    return dict([(child.tag, child.text) for child in node])\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    d = {}\n    for c in node.getchildren():\n        d[c.tag] = c.text\n    return d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n    return {child.tag: _xml_children_as_dict(child)\n            for child in node}\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n    d = {}\n    for c in list(node):\n        d[c.tag] = c.text\n    return d\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "check_sender_and_entity_handle_match", "docstring": "Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.", "ques_id": "6306299052e177c0ba469fe8", "code": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle == entity_handle:\n        return\n\n    if sender_handle == '*' and entity_handle == '*':\n        return\n\n    if sender_handle == entity_handle.split('/')[0]:\n        return\n\n    raise ValueError(\n        'Sender handle mismatch %s!= %s' % (sender_handle, entity_handle))\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise ValueError(\"Sender handle doesn't match entity handle\")\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise BadRequest(\"Sender handle mismatch.\")\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise SenderHandleMismatchError()\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\n    if sender_handle!= entity_handle:\n        raise ValueError(\n            'Sender handle does not match entity handle. Expected: {}, '\n            'got: {}'.format(sender_handle, entity_handle)\n        )\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise SenderHandleMismatch(\n            'Sender and entity handle do not match.')\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle == entity_handle:\n        return\n    if sender_handle is None or entity_handle is None:\n        return\n    raise ValueError(\"Sender and Entity handles must match\")\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle == entity_handle:\n        return True\n    else:\n        return False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise ValueError(\n            'Sender handle mismatch, expected {}, received {}.'.format(\n                sender_handle, entity_handle))\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\n    if sender_handle!= entity_handle:\n        raise ValueError(\"Sender and Entity Handle do not match.\")\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle == entity_handle:\n        return\n\n    if sender_handle == '*' and entity_handle == '*':\n        return\n\n    if sender_handle == entity_handle.split('/')[0]:\n        return\n\n    raise ValueError(\n        'Sender handle mismatch %s!= %s' % (sender_handle, entity_handle))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise ValueError(\"Sender handle doesn't match entity handle\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise BadRequest(\"Sender handle mismatch.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise SenderHandleMismatchError()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\n    if sender_handle!= entity_handle:\n        raise ValueError(\n            'Sender handle does not match entity handle. Expected: {}, '\n            'got: {}'.format(sender_handle, entity_handle)\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise SenderHandleMismatch(\n            'Sender and entity handle do not match.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle == entity_handle:\n        return\n    if sender_handle is None or entity_handle is None:\n        return\n    raise ValueError(\"Sender and Entity handles must match\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle == entity_handle:\n        return True\n    else:\n        return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    if sender_handle!= entity_handle:\n        raise ValueError(\n            'Sender handle mismatch, expected {}, received {}.'.format(\n                sender_handle, entity_handle))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\n    if sender_handle!= entity_handle:\n        raise ValueError(\"Sender and Entity Handle do not match.\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/hostmeta/generators.py", "project": "jaywink/federation", "name": "get_nodeinfo_well_known_document", "docstring": "Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict", "ques_id": "630629b952e177c0ba46a043", "code": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    return {\n        'document_url': url,\n        'document_path': document_path,\n    }\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = \"/\" + uuid.uuid4().hex\n    return {\"document_path\": document_path, \"url\": url}\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    # type: (str, Optional[str]) -> Dict[str, Any]\n    \"\"\"\n    Generate a NodeInfo.well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n\n    :returns: dict\n    \"\"\"\n    # Generate a dictionary of all the node info fields\n    node_info = {}\n    for name, field in six.iteritems(node_info_fields):\n        # Get the field value\n        field_value = getattr(node_info, name)\n        # If a document path is specified, add it to the dictionary\n        if document_path is not None:\n            document_path = '/'.join((document_path, name))\n            if document_path not in field_value:\n                field_value[document_path] = {}\n            field_value[document_path][name] = getattr(field, 'type_name')\n        else:\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    document_path = document_path or ''\n    if document_path:\n        base_url = '%s/%s' % (url, document_path)\n    else:\n        base_url = url\n    return {\n        'base': base_url,\n        'fullName': '%s/#' % base_url,\n        'documentPath': document_path\n    }\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = '/'.join([url.rstrip('/'), 'document.json'])\n\n    with open(document_path, 'r') as f:\n        return json.load(f)\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path:\n        return {\"docs\": {document_path: {\"uri\": url}}}\n    else:\n        return {\"docs\": {\"uri\": url}}\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = '/'.join([url.rstrip('/').split('/')[:-1], 'document.json'])\n\n    return {\n        'document_path': document_path,\n        'document': None,\n       'version': '1.0',\n       'services': {\n           'msn.microsoft.com': {\n                '@id': 'http://msn.microsoft.com/',\n                '@type': '{%s}' % nodeinfo.TYPE_MS_NETWORK_SERVICE,\n               'services': {\n                    'nodeinfo': {\n                        '@id': 'http://msn.microsoft.com/',\n                        '@type': '{%s}' % nodeinfo.TYPE_NODEINFO_SERVICE\n                    },\n                    'nodeinfo': {\n                        '@id': 'http:/", "def get_nodeinfo_well_known_document(url, document_path=None):\n\n    if document_path is None:\n        document_path = '/' + uuid.uuid4().hex\n\n    document_content = {\n        'path': document_path,\n        'url': url,\n        'content': 'This is a document.'\n    }\n\n    return document_content\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    return {\n        '@type': '@id',\n        '@id': url,\n        '@context': 'http://nodeinfo.diaspora.software/1.0',\n        'nodeinfo': {\n            '@type': '@id',\n            '@id': url,\n            '@context': 'http://nodeinfo.diaspora.software/1.0',\n            'document_path': document_path,\n        }\n    }\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = '/'.join([url.rstrip('/'), 'document.well-known.in'])\n\n    document_data = {\n        '@context': DOCUMENT_CONTEXT,\n        '@type': 'NodeInfo',\n        '@id': document_path,\n    }\n\n    return document_data\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    return {\n        'document_url': url,\n        'document_path': document_path,\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = \"/\" + uuid.uuid4().hex\n    return {\"document_path\": document_path, \"url\": url}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    # type: (str, Optional[str]) -> Dict[str, Any]\n    \"\"\"\n    Generate a NodeInfo.well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n\n    :returns: dict\n    \"\"\"\n    # Generate a dictionary of all the node info fields\n    node_info = {}\n    for name, field in six.iteritems(node_info_fields):\n        # Get the field value\n        field_value = getattr(node_info, name)\n        # If a document path is specified, add it to the dictionary\n        if document_path is not None:\n            document_path = '/'.join((document_path, name))\n            if document_path not in field_value:\n                field_value[document_path] = {}\n            field_value[document_path][name] = getattr(field, 'type_name')\n        else:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    document_path = document_path or ''\n    if document_path:\n        base_url = '%s/%s' % (url, document_path)\n    else:\n        base_url = url\n    return {\n        'base': base_url,\n        'fullName': '%s/#' % base_url,\n        'documentPath': document_path\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = '/'.join([url.rstrip('/'), 'document.json'])\n\n    with open(document_path, 'r') as f:\n        return json.load(f)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path:\n        return {\"docs\": {document_path: {\"uri\": url}}}\n    else:\n        return {\"docs\": {\"uri\": url}}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = '/'.join([url.rstrip('/').split('/')[:-1], 'document.json'])\n\n    return {\n        'document_path': document_path,\n        'document': None,\n       'version': '1.0',\n       'services': {\n           'msn.microsoft.com': {\n                '@id': 'http://msn.microsoft.com/',\n                '@type': '{%s}' % nodeinfo.TYPE_MS_NETWORK_SERVICE,\n               'services': {\n                    'nodeinfo': {\n                        '@id': 'http://msn.microsoft.com/',\n                        '@type': '{%s}' % nodeinfo.TYPE_NODEINFO_SERVICE\n                    },\n                    'nodeinfo': {\n                        '@id': 'http:/", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\n    if document_path is None:\n        document_path = '/' + uuid.uuid4().hex\n\n    document_content = {\n        'path': document_path,\n        'url': url,\n        'content': 'This is a document.'\n    }\n\n    return document_content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    return {\n        '@type': '@id',\n        '@id': url,\n        '@context': 'http://nodeinfo.diaspora.software/1.0',\n        'nodeinfo': {\n            '@type': '@id',\n            '@id': url,\n            '@context': 'http://nodeinfo.diaspora.software/1.0',\n            'document_path': document_path,\n        }\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n    if document_path is None:\n        document_path = '/'.join([url.rstrip('/'), 'document.well-known.in'])\n\n    document_data = {\n        '@context': DOCUMENT_CONTEXT,\n        '@type': 'NodeInfo',\n        '@id': document_path,\n    }\n\n    return document_data\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/signatures.py", "project": "jaywink/federation", "name": "verify_relayable_signature", "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.", "ques_id": "630629d052e177c0ba46a0a1", "code": ["def verify_relayable_signature(public_key, doc, signature):\n\n    if not isinstance(doc, etree._Element):\n        raise ValueError(\"The claimed author didn't generate the document\")\n\n    if not isinstance(signature, etree._Element):\n        raise ValueError(\"The claimed author didn't generate the signature\")\n\n    signature_elem = etree.XML(signature.decode('utf-8'))\n\n    if signature_elem.find(\".//RelayState\") is None:\n        raise ValueError(\"The claimed author didn't generate the signature\")\n\n    if not _verify_signature(public_key, doc, signature_elem):\n        raise ValueError(\"The claimed author didn't claim the expected relay state\")\n", "def verify_relayable_signature(public_key, doc, signature):\n    return verify_signature(public_key, doc, signature,'relayable')\n", "def verify_relayable_signature(public_key, doc, signature):\n    return _verify_signature(public_key, doc, signature, relayable_sig_version)\n", "def verify_relayable_signature(public_key, doc, signature):\n    if not signature:\n        return\n\n    doc = etree.fromstring(doc)\n    relay_signature = doc.find('relaySignature')\n    if not relay_signature:\n        return\n\n    if relay_signature.text!= signature:\n        raise VerificationError(\"Message does not verify signature\")\n", "def verify_relayable_signature(public_key, doc, signature):\n    return verify_signature(public_key, doc, signature)\n", "def verify_relayable_signature(public_key, doc, signature):\n    if not verify_signature(public_key, doc, signature):\n        raise SignatureMismatch()\n", "def verify_relayable_signature(public_key, doc, signature):\n\n    if not isinstance(signature, str):\n        raise TypeError(\"signature must be a string\")\n\n    if not isinstance(public_key, str):\n        raise TypeError(\"public_key must be a string\")\n\n    if len(signature)!= 64:\n        raise ValueError(\"signature must be 64 bytes\")\n\n    pk = etree.XML(public_key)\n    sk = etree.XML(doc)\n\n    cs = _etree.XML(_pk.xml_text())\n    cs_sig = _etree.XML(_pk.xml_signature())\n\n    if cs!= cs_sig:\n        raise SignatureMismatch(_pk.xml_text(), _pk.xml_signature())\n", "def verify_relayable_signature(public_key, doc, signature):\n    signature_data = serialize_xml_element(doc)\n    return verify_signature(public_key, signature_data, signature)\n", "def verify_relayable_signature(public_key, doc, signature):\n    if not isinstance(doc, etree._Element):\n        raise ValueError('doc must be an etree._Element')\n    if not isinstance(signature, etree._Element):\n        raise ValueError('signature must be an etree._Element')\n    if not isinstance(public_key, etree._Element):\n        raise ValueError('public_key must be an etree._Element')\n    if not len(signature) == 64:\n        raise ValueError('signature must be 64 chars')\n    if not len(public_key) == 64:\n        raise ValueError('public_key must be 64 chars')\n\n    xml_root = doc.getroot()\n    relay_node = xml_root.find('{%s}Relay' % ns_map['taxii'])\n    if relay_node is None:\n        raise ValueError('XML root missing relay')\n    if relay_node.attrib['relay_id']!= str(uuid.uuid4()):\n        raise ValueError('invalid message relay_id: %s'", "def verify_relayable_signature(public_key, doc, signature):\n    try:\n        return _verify_signature(public_key, doc, signature)\n    except SignatureMismatch:\n        return False\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\n    if not isinstance(doc, etree._Element):\n        raise ValueError(\"The claimed author didn't generate the document\")\n\n    if not isinstance(signature, etree._Element):\n        raise ValueError(\"The claimed author didn't generate the signature\")\n\n    signature_elem = etree.XML(signature.decode('utf-8'))\n\n    if signature_elem.find(\".//RelayState\") is None:\n        raise ValueError(\"The claimed author didn't generate the signature\")\n\n    if not _verify_signature(public_key, doc, signature_elem):\n        raise ValueError(\"The claimed author didn't claim the expected relay state\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    return verify_signature(public_key, doc, signature,'relayable')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    return _verify_signature(public_key, doc, signature, relayable_sig_version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    if not signature:\n        return\n\n    doc = etree.fromstring(doc)\n    relay_signature = doc.find('relaySignature')\n    if not relay_signature:\n        return\n\n    if relay_signature.text!= signature:\n        raise VerificationError(\"Message does not verify signature\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    return verify_signature(public_key, doc, signature)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    if not verify_signature(public_key, doc, signature):\n        raise SignatureMismatch()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\n    if not isinstance(signature, str):\n        raise TypeError(\"signature must be a string\")\n\n    if not isinstance(public_key, str):\n        raise TypeError(\"public_key must be a string\")\n\n    if len(signature)!= 64:\n        raise ValueError(\"signature must be 64 bytes\")\n\n    pk = etree.XML(public_key)\n    sk = etree.XML(doc)\n\n    cs = _etree.XML(_pk.xml_text())\n    cs_sig = _etree.XML(_pk.xml_signature())\n\n    if cs!= cs_sig:\n        raise SignatureMismatch(_pk.xml_text(), _pk.xml_signature())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    signature_data = serialize_xml_element(doc)\n    return verify_signature(public_key, signature_data, signature)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    if not isinstance(doc, etree._Element):\n        raise ValueError('doc must be an etree._Element')\n    if not isinstance(signature, etree._Element):\n        raise ValueError('signature must be an etree._Element')\n    if not isinstance(public_key, etree._Element):\n        raise ValueError('public_key must be an etree._Element')\n    if not len(signature) == 64:\n        raise ValueError('signature must be 64 chars')\n    if not len(public_key) == 64:\n        raise ValueError('public_key must be 64 chars')\n\n    xml_root = doc.getroot()\n    relay_node = xml_root.find('{%s}Relay' % ns_map['taxii'])\n    if relay_node is None:\n        raise ValueError('XML root missing relay')\n    if relay_node.attrib['relay_id']!= str(uuid.uuid4()):\n        raise ValueError('invalid message relay_id: %s'", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n    try:\n        return _verify_signature(public_key, doc, signature)\n    except SignatureMismatch:\n        return False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "parse_diaspora_webfinger", "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html", "ques_id": "630629e052e177c0ba46a0c4", "code": ["def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except ValueError:\n        return get_webfinger_json(document)\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except ValueError as e:\n        return json.loads(webfinger_xrd(document))\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except ValueError:\n        return xrd.load(document)\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n    if not document:\n        return {}\n\n    return json.loads(re.sub(\n        r\"[\\r\\n]+\", \" \",\n        re.sub(r\"^\\s+\", \"\", document, flags=re.MULTILINE)\n    ))\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n    if not document:\n        return {}\n    try:\n        return json.loads(document)\n    except ValueError:\n        return xrd.parse(document)\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n    # Remove the \"{\" from the front of the string\n    document = document[1:-1]\n\n    # JSON does not support quoted strings (yet)\n    if document.startswith(\"'\") and document.endswith(\"'\"):\n        return json.loads(document)\n\n    try:\n        return json.loads(document)\n    except json.decoder.JSONDecodeError as e:\n        raise ValueError(\n            \"Failed to parse webfinger JSON document: {}\".format(e)\n        ) from e\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except json.JSONDecodeError as exc:\n        if \"Invalid or corrupt webfinger document\" in str(exc):\n            raise ValueError(\"webfinger file was not found, or corrupt \"\n                             \"data was detected\")\n        raise\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n    if not re.match(r'^\\w+\\.\\w+\\.\\w+$', document):\n        raise ValueError('File does not match format \"w3d\"')\n\n    with open(document) as f:\n        data = json.load(f)\n\n    if'resources' not in data:\n        raise ValueError('Missing \"resources\" element in webfinger document')\n\n    if 'diaspora_version' not in data['resources']:\n        raise ValueError('Missing \"diaspora_version\" element in webfinger document')\n\n    if 'webfinger_version' not in data['resources']['diaspora_version']:\n        raise ValueError('Missing \"webfinger_version\" element in webfinger document')\n\n    if 'webfinger_version' not in data['resources']['diaspora_version']['webfinger_version']:\n        raise ValueError('Missing \"webfinger_version\" element in webfinger document')\n\n    return data\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n    if not isinstance(document, str):\n        raise TypeError(\"document must be a string\")\n\n    if not document.startswith('{'):\n        raise ValueError(\"document must start with '{'\")\n\n    try:\n        data = json.loads(document)\n    except JSONDecodeError:\n        raise ValueError(\"JSON document is not valid\")\n\n    if 'type' not in data or data['type']!= 'webfinger':\n        raise ValueError(\"JSON document does not start with 'webfinger'\")\n\n    if 'parameters' not in data:\n        raise ValueError(\"JSON document does not contain parameters\")\n\n    params = data['parameters']\n    if len(params) < 1:\n        raise ValueError(\"JSON document does not contain any parameters\")\n\n    return params\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except json.JSONDecodeError:\n        return xrd_to_webfinger(document)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except ValueError:\n        return get_webfinger_json(document)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except ValueError as e:\n        return json.loads(webfinger_xrd(document))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except ValueError:\n        return xrd.load(document)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    if not document:\n        return {}\n\n    return json.loads(re.sub(\n        r\"[\\r\\n]+\", \" \",\n        re.sub(r\"^\\s+\", \"\", document, flags=re.MULTILINE)\n    ))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    if not document:\n        return {}\n    try:\n        return json.loads(document)\n    except ValueError:\n        return xrd.parse(document)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    # Remove the \"{\" from the front of the string\n    document = document[1:-1]\n\n    # JSON does not support quoted strings (yet)\n    if document.startswith(\"'\") and document.endswith(\"'\"):\n        return json.loads(document)\n\n    try:\n        return json.loads(document)\n    except json.decoder.JSONDecodeError as e:\n        raise ValueError(\n            \"Failed to parse webfinger JSON document: {}\".format(e)\n        ) from e\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except json.JSONDecodeError as exc:\n        if \"Invalid or corrupt webfinger document\" in str(exc):\n            raise ValueError(\"webfinger file was not found, or corrupt \"\n                             \"data was detected\")\n        raise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    if not re.match(r'^\\w+\\.\\w+\\.\\w+$', document):\n        raise ValueError('File does not match format \"w3d\"')\n\n    with open(document) as f:\n        data = json.load(f)\n\n    if'resources' not in data:\n        raise ValueError('Missing \"resources\" element in webfinger document')\n\n    if 'diaspora_version' not in data['resources']:\n        raise ValueError('Missing \"diaspora_version\" element in webfinger document')\n\n    if 'webfinger_version' not in data['resources']['diaspora_version']:\n        raise ValueError('Missing \"webfinger_version\" element in webfinger document')\n\n    if 'webfinger_version' not in data['resources']['diaspora_version']['webfinger_version']:\n        raise ValueError('Missing \"webfinger_version\" element in webfinger document')\n\n    return data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    if not isinstance(document, str):\n        raise TypeError(\"document must be a string\")\n\n    if not document.startswith('{'):\n        raise ValueError(\"document must start with '{'\")\n\n    try:\n        data = json.loads(document)\n    except JSONDecodeError:\n        raise ValueError(\"JSON document is not valid\")\n\n    if 'type' not in data or data['type']!= 'webfinger':\n        raise ValueError(\"JSON document does not start with 'webfinger'\")\n\n    if 'parameters' not in data:\n        raise ValueError(\"JSON document does not contain parameters\")\n\n    params = data['parameters']\n    if len(params) < 1:\n        raise ValueError(\"JSON document does not contain any parameters\")\n\n    return params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    try:\n        return json.loads(document)\n    except json.JSONDecodeError:\n        return xrd_to_webfinger(document)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "try_retrieve_webfinger_document", "docstring": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.", "ques_id": "630629e152e177c0ba46a0d1", "code": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\n    try:\n        return urlopen(URL_WEBFINGER_DOCUMENT.format(handle)).read().decode(\"utf-8\")\n    except HTTPError:\n        return None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        handle.open()\n        handle.seek(0)\n        return handle.read()\n    except Exception:\n        return None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        return urlopen(\"https://rfc7033.net/rfc/\" + handle).read()\n    except URLError:\n        return None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        with urlopen(handle) as response:\n            return response.read().decode(\"utf-8\")\n    except HTTPError as e:\n        if e.code == 404:\n            return None\n        else:\n            raise\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        with open(handle, \"rb\") as f:\n            return f.read().decode(\"utf-8\")\n    except (OSError, IOError):\n        return None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        handle.open()\n        handle.seek(0)\n        return handle.read()\n    except (urllib.error.URLError, urllib.error.HTTPError):\n        return None\n    finally:\n        handle.close()\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        return urlopen(DOCUMENT_URL + handle).read().decode(\"utf-8\")\n    except HTTPError:\n        return None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\n    try:\n        return urlopen(URL_WEBFINGER_DOCUMENT.format(handle)).read().decode('utf-8')\n    except HTTPError:\n        return None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        with open(handle, \"rb\") as handle:\n            return handle.read()\n    except (OSError, FileNotFoundError):\n        return None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        return urlopen(DOCUMENT_URL + handle).read().decode(\"utf-8\")\n    except HTTPError:\n        return None\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\n    try:\n        return urlopen(URL_WEBFINGER_DOCUMENT.format(handle)).read().decode(\"utf-8\")\n    except HTTPError:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        handle.open()\n        handle.seek(0)\n        return handle.read()\n    except Exception:\n        return None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        return urlopen(\"https://rfc7033.net/rfc/\" + handle).read()\n    except URLError:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        with urlopen(handle) as response:\n            return response.read().decode(\"utf-8\")\n    except HTTPError as e:\n        if e.code == 404:\n            return None\n        else:\n            raise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        with open(handle, \"rb\") as f:\n            return f.read().decode(\"utf-8\")\n    except (OSError, IOError):\n        return None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        handle.open()\n        handle.seek(0)\n        return handle.read()\n    except (urllib.error.URLError, urllib.error.HTTPError):\n        return None\n    finally:\n        handle.close()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        return urlopen(DOCUMENT_URL + handle).read().decode(\"utf-8\")\n    except HTTPError:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\n    try:\n        return urlopen(URL_WEBFINGER_DOCUMENT.format(handle)).read().decode('utf-8')\n    except HTTPError:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        with open(handle, \"rb\") as handle:\n            return handle.read()\n    except (OSError, FileNotFoundError):\n        return None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    try:\n        return urlopen(DOCUMENT_URL + handle).read().decode(\"utf-8\")\n    except HTTPError:\n        return None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_and_parse_diaspora_webfinger", "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict", "ques_id": "630629e152e177c0ba46a0d2", "code": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    response.raise_for_status()\n    return response.json()\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    response.raise_for_status()\n    return response.json()\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    return parse_diaspora_webfinger(response.text)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    response.raise_for_status()\n    return response.json()\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    doc = retrieve_diaspora_document(handle)\n    return parse_webfinger_document(doc)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    if response.status_code!= 200:\n        raise RemoteServerError(\"Unable to retrieve remote Diaspora webfinger document: %s\" % response.text)\n    return parse_diaspora_webfinger_response(response)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    response.raise_for_status()\n\n    return parse_diaspora_webfinger_response(response)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    return _parse_diaspora_webfinger_response(response)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    if response.status_code!= 200:\n        raise ValueError('Received status code {}.'.format(response.status_code))\n    return parse_diaspora_webfinger_response(response)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n    r = requests.get(handle)\n    if r.status_code == 200:\n        return r.json()\n    else:\n        return {}\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    response.raise_for_status()\n    return response.json()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    response.raise_for_status()\n    return response.json()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    return parse_diaspora_webfinger(response.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    response.raise_for_status()\n    return response.json()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    doc = retrieve_diaspora_document(handle)\n    return parse_webfinger_document(doc)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    if response.status_code!= 200:\n        raise RemoteServerError(\"Unable to retrieve remote Diaspora webfinger document: %s\" % response.text)\n    return parse_diaspora_webfinger_response(response)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    response.raise_for_status()\n\n    return parse_diaspora_webfinger_response(response)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    return _parse_diaspora_webfinger_response(response)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    response = requests.get(handle)\n    if response.status_code!= 200:\n        raise ValueError('Received status code {}.'.format(response.status_code))\n    return parse_diaspora_webfinger_response(response)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    r = requests.get(handle)\n    if r.status_code == 200:\n        return r.json()\n    else:\n        return {}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_diaspora_host_meta", "docstring": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance", "ques_id": "630629e252e177c0ba46a0d6", "code": ["def retrieve_diaspora_host_meta(host):\n    url = DIAsporaConfig.host_meta_url.format(host=host)\n    response = requests.get(url)\n    response.raise_for_status()\n    return XRD(response.text)\n", "def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        DIAsporaHostMetaDatabaseUrl.format(host=host),\n        auth=(settings.USER, settings.PASSWORD),\n        timeout=60\n    )\n\n    response.raise_for_status()\n\n    return XRD(response.text)\n", "def retrieve_diaspora_host_meta(host):\n    r = requests.get(DIAspora.HOST_META_URL.format(host=host))\n    return XRD.from_xml(r.text)\n", "def retrieve_diaspora_host_meta(host):\n    if host == 'localhost':\n        host = '127.0.0.1'\n    url = 'http://%s/xrd' % host\n    response = requests.get(url)\n    response.raise_for_status()\n    return XRD(response.text)\n", "def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        DIAspora.host_meta_url.format(host)\n    )\n    response.raise_for_status()\n    return XRD(response.content)\n", "def retrieve_diaspora_host_meta(host):\n    return get_xrd_from_url(\n        DIAsporaClient.get_host_meta_url(host)\n    )\n", "def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        DIAspora.host_meta_url.format(host=host),\n        auth=(settings.USER, settings.PASSWORD),\n    )\n    response.raise_for_status()\n    return XRD(response.content)\n", "def retrieve_diaspora_host_meta(host):\n    res = requests.get(\n        DIAspora.host_meta_url.format(host=host),\n        auth=(settings.USER, settings.PASSWORD))\n    res.raise_for_status()\n    return XRD(res.text)\n", "def retrieve_diaspora_host_meta(host):\n    url = DIAspora.host_meta_url.format(host)\n    response = requests.get(url)\n    response.raise_for_status()\n    return XRD(response.text)\n", "def retrieve_diaspora_host_meta(host):\n    url = (\n        \"https://diaspora.mitre.org/xrds/{host}\"\n    ).format(host=host)\n    response = requests.get(url)\n    response.raise_for_status()\n    return XRD(response.text)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def retrieve_diaspora_host_meta(host):\n    url = DIAsporaConfig.host_meta_url.format(host=host)\n    response = requests.get(url)\n    response.raise_for_status()\n    return XRD(response.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        DIAsporaHostMetaDatabaseUrl.format(host=host),\n        auth=(settings.USER, settings.PASSWORD),\n        timeout=60\n    )\n\n    response.raise_for_status()\n\n    return XRD(response.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    r = requests.get(DIAspora.HOST_META_URL.format(host=host))\n    return XRD.from_xml(r.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    if host == 'localhost':\n        host = '127.0.0.1'\n    url = 'http://%s/xrd' % host\n    response = requests.get(url)\n    response.raise_for_status()\n    return XRD(response.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        DIAspora.host_meta_url.format(host)\n    )\n    response.raise_for_status()\n    return XRD(response.content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    return get_xrd_from_url(\n        DIAsporaClient.get_host_meta_url(host)\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    response = requests.get(\n        DIAspora.host_meta_url.format(host=host),\n        auth=(settings.USER, settings.PASSWORD),\n    )\n    response.raise_for_status()\n    return XRD(response.content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    res = requests.get(\n        DIAspora.host_meta_url.format(host=host),\n        auth=(settings.USER, settings.PASSWORD))\n    res.raise_for_status()\n    return XRD(res.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    url = DIAspora.host_meta_url.format(host)\n    response = requests.get(url)\n    response.raise_for_status()\n    return XRD(response.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n    url = (\n        \"https://diaspora.mitre.org/xrds/{host}\"\n    ).format(host=host)\n    response = requests.get(url)\n    response.raise_for_status()\n    return XRD(response.text)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "send_document", "docstring": "Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)", "ques_id": "630629e752e177c0ba46a0fb", "code": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    resp = requests.post(\n        url, data=data, timeout=timeout, headers=make_auth_header(url), *args, **kwargs\n    )\n    resp.raise_for_status()\n    return (resp.status_code, resp)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(\n        url,\n        data=data,\n        headers={\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        },\n        timeout=timeout,\n        *args,\n        **kwargs,\n    )\n    if response.status_code not in range(200, 300):\n        response.raise_for_status()\n    return response.status_code, response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    return _send_document(url, data, timeout, method, *args, **kwargs)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = None\n\n    try:\n        response = requests.post(\n            url,\n            data=data,\n            timeout=timeout,\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"application/json\",\n            },\n            *args,\n            **kwargs,\n        )\n    except (requests.exceptions.RequestException, ValueError) as e:\n        response = e\n\n    if response and response.status_code in (200, 201):\n        return (response.status_code, response)\n\n    return (None, response)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    if \"data\" in kwargs:\n        data = kwargs[\"data\"]\n\n    response = requests.post(\n        url,\n        data=data,\n        headers={\"content-type\": \"application/json\"},\n        timeout=timeout,\n        *args,\n        **kwargs\n    )\n\n    if response.status_code == 200:\n        return response.status_code, response\n\n    return None, response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = None\n\n    try:\n        response = requests.post(\n            url, data=data, timeout=timeout, headers={\"Content-Type\": \"application/x-www-form-urlencoded\"}, *args, **kwargs\n        )\n    except RequestException as e:\n        return (None, e)\n\n    if response.status_code!= 200:\n        return (None, response)\n\n    try:\n        return (response.status_code, json.loads(response.text))\n    except ValueError:\n        return (None, response.text)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(\n        url,\n        data=data,\n        timeout=timeout,\n        headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        *args,\n        **kwargs,\n    )\n\n    if response.status_code not in (200, 201, 202, 204):\n        try:\n            response.raise_for_status()\n        except HTTPError as e:\n            if e.response.status_code in (404, 400):\n                return None, None\n            raise\n\n    return response.status_code, response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(\n        url, data=data, timeout=timeout, *args, **kwargs\n    )\n    if response.status_code!= requests.codes.ok:\n        response.raise_for_status()\n    return (\n        response.status_code,\n        _extract_error_from_response(response),\n    )\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n    return _handle_response(response, method=method)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(url, data=data, timeout=timeout,\n                             headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n                             *args, **kwargs)\n    response.raise_for_status()\n    return response.status_code, response.json()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    resp = requests.post(\n        url, data=data, timeout=timeout, headers=make_auth_header(url), *args, **kwargs\n    )\n    resp.raise_for_status()\n    return (resp.status_code, resp)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(\n        url,\n        data=data,\n        headers={\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        },\n        timeout=timeout,\n        *args,\n        **kwargs,\n    )\n    if response.status_code not in range(200, 300):\n        response.raise_for_status()\n    return response.status_code, response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    return _send_document(url, data, timeout, method, *args, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = None\n\n    try:\n        response = requests.post(\n            url,\n            data=data,\n            timeout=timeout,\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"application/json\",\n            },\n            *args,\n            **kwargs,\n        )\n    except (requests.exceptions.RequestException, ValueError) as e:\n        response = e\n\n    if response and response.status_code in (200, 201):\n        return (response.status_code, response)\n\n    return (None, response)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    if \"data\" in kwargs:\n        data = kwargs[\"data\"]\n\n    response = requests.post(\n        url,\n        data=data,\n        headers={\"content-type\": \"application/json\"},\n        timeout=timeout,\n        *args,\n        **kwargs\n    )\n\n    if response.status_code == 200:\n        return response.status_code, response\n\n    return None, response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = None\n\n    try:\n        response = requests.post(\n            url, data=data, timeout=timeout, headers={\"Content-Type\": \"application/x-www-form-urlencoded\"}, *args, **kwargs\n        )\n    except RequestException as e:\n        return (None, e)\n\n    if response.status_code!= 200:\n        return (None, response)\n\n    try:\n        return (response.status_code, json.loads(response.text))\n    except ValueError:\n        return (None, response.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(\n        url,\n        data=data,\n        timeout=timeout,\n        headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        *args,\n        **kwargs,\n    )\n\n    if response.status_code not in (200, 201, 202, 204):\n        try:\n            response.raise_for_status()\n        except HTTPError as e:\n            if e.response.status_code in (404, 400):\n                return None, None\n            raise\n\n    return response.status_code, response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(\n        url, data=data, timeout=timeout, *args, **kwargs\n    )\n    if response.status_code!= requests.codes.ok:\n        response.raise_for_status()\n    return (\n        response.status_code,\n        _extract_error_from_response(response),\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n    return _handle_response(response, method=method)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    response = requests.post(url, data=data, timeout=timeout,\n                             headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n                             *args, **kwargs)\n    response.raise_for_status()\n    return response.status_code, response.json()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/dict_utils.py", "project": "redhat-openstack/infrared", "name": "dict_insert", "docstring": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain", "ques_id": "6306091073426c38ae68acac", "code": ["def dict_insert(dic, val, key, *keys):\n    if not isinstance(key, string_types):\n        raise TypeError('keys must be strings')\n\n    if not isinstance(keys, list):\n        raise TypeError('keys must be a list')\n\n    if len(keys) == 0:\n        raise ValueError('keys must contain at least one key')\n\n    for key in keys:\n        if key not in dic:\n            dic[key] = {}\n\n        dic = dic[key]\n\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    for key in keys:\n        if key not in dic:\n            dic[key] = {}\n        dic = dic[key]\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    if len(keys) == 0:\n        dic[key] = val\n    else:\n        dkey = keys[0]\n        if dkey in dic:\n            dic[dkey] = dict_insert(dic[dkey], val, dkey, *keys[1:])\n        else:\n            dic[dkey] = {}\n            # dic[dkey] = {}\n            # dic[dkey][key] = val\n", "def dict_insert(dic, val, key, *keys):\n    if key in dic:\n        if len(keys) == 0:\n            dic[key].append(val)\n        else:\n            dic[key].insert(0, val)\n    else:\n        dic[key] = [val]\n", "def dict_insert(dic, val, key, *keys):\n    for key in keys:\n        if key in dic:\n            dic[key].append(val)\n        else:\n            dic[key] = [val]\n", "def dict_insert(dic, val, key, *keys):\n    if not keys:\n        dic[key] = val\n    else:\n        parent_key = keys[0]\n        if len(keys) == 1:\n            dic[parent_key] = val\n        else:\n            dic[parent_key] = dict_insert(dic.get(parent_key, {}), val, *keys[1:])\n", "def dict_insert(dic, val, key, *keys):\n    if keys:\n        dkey = '.'.join(keys)\n        if dkey not in dic:\n            dic[dkey] = {}\n        dic = dic[dkey]\n\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    if keys:\n        if key in dic:\n            raise ValueError('Key {} already exists'.format(key))\n        dic[key] = val\n        for k in keys:\n            dict_insert(dic, val, k)\n    else:\n        dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    if keys:\n        key = '.'.join(keys)\n    dic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n    if len(keys) == 0:\n        dic[key] = val\n    else:\n        parent_key = keys[0]\n        if isinstance(dic, dict):\n            if parent_key not in dic:\n                dic[parent_key] = {}\n            dic_val = dic[parent_key]\n            dict_insert(dic_val, val, *keys[1:])\n        else:\n            raise Exception('insert value requires a dictionary object as first argument')\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def dict_insert(dic, val, key, *keys):\n    if not isinstance(key, string_types):\n        raise TypeError('keys must be strings')\n\n    if not isinstance(keys, list):\n        raise TypeError('keys must be a list')\n\n    if len(keys) == 0:\n        raise ValueError('keys must contain at least one key')\n\n    for key in keys:\n        if key not in dic:\n            dic[key] = {}\n\n        dic = dic[key]\n\n    dic[key] = val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    for key in keys:\n        if key not in dic:\n            dic[key] = {}\n        dic = dic[key]\n    dic[key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if len(keys) == 0:\n        dic[key] = val\n    else:\n        dkey = keys[0]\n        if dkey in dic:\n            dic[dkey] = dict_insert(dic[dkey], val, dkey, *keys[1:])\n        else:\n            dic[dkey] = {}\n            # dic[dkey] = {}\n            # dic[dkey][key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if key in dic:\n        if len(keys) == 0:\n            dic[key].append(val)\n        else:\n            dic[key].insert(0, val)\n    else:\n        dic[key] = [val]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    for key in keys:\n        if key in dic:\n            dic[key].append(val)\n        else:\n            dic[key] = [val]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if not keys:\n        dic[key] = val\n    else:\n        parent_key = keys[0]\n        if len(keys) == 1:\n            dic[parent_key] = val\n        else:\n            dic[parent_key] = dict_insert(dic.get(parent_key, {}), val, *keys[1:])\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if keys:\n        dkey = '.'.join(keys)\n        if dkey not in dic:\n            dic[dkey] = {}\n        dic = dic[dkey]\n\n    dic[key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if keys:\n        if key in dic:\n            raise ValueError('Key {} already exists'.format(key))\n        dic[key] = val\n        for k in keys:\n            dict_insert(dic, val, k)\n    else:\n        dic[key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if keys:\n        key = '.'.join(keys)\n    dic[key] = val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n    if len(keys) == 0:\n        dic[key] = val\n    else:\n        parent_key = keys[0]\n        if isinstance(dic, dict):\n            if parent_key not in dic:\n                dic[parent_key] = {}\n            dic_val = dic[parent_key]\n            dict_insert(dic_val, val, *keys[1:])\n        else:\n            raise Exception('insert value requires a dictionary object as first argument')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "tests/test_complex_types.py", "project": "redhat-openstack/infrared", "name": "list_of_file_names", "docstring": "Create a new IniType complex type", "ques_id": "6306091a73426c38ae68acc8", "code": ["def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType('', [\n            IniFile(path, [], spec_option)\n            for path in settings_dirs\n        ])\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return _create_complex_type('listOfFiles', settings_dirs, spec_option)\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [IniType(s, [], spec_option) for s in get_settings_dirs(settings_dirs)]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [IniType('', [\n        ('foo', '[file]', 'file', None),\n        ('bar', '[file]', 'file', None),\n    ])]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return _load_type_class(\n        'list_of_file_names', settings_dirs, spec_option\n    )\n", "def list_of_file_names(settings_dirs, spec_option):\n    return _list_of_file_names_from_keys(settings_dirs, spec_option)\n", "def list_of_file_names(settings_dirs, spec_option):\n    return _create_complex_type(settings_dirs, spec_option, ListOfFileNames)\n", "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType(\"file\", [\n            IniFiles(settings_dirs, spec_option)\n        ])\n    ]\n", "def list_of_file_names(settings_dirs, spec_option):\n    return _generate_ini_type(\n        'complex', None, settings_dirs, spec_option)\n", "def list_of_file_names(settings_dirs, spec_option):\n    return _load_type_class('list_of_file_names', settings_dirs, spec_option)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType('', [\n            IniFile(path, [], spec_option)\n            for path in settings_dirs\n        ])\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return _create_complex_type('listOfFiles', settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [IniType(s, [], spec_option) for s in get_settings_dirs(settings_dirs)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [IniType('', [\n        ('foo', '[file]', 'file', None),\n        ('bar', '[file]', 'file', None),\n    ])]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return _load_type_class(\n        'list_of_file_names', settings_dirs, spec_option\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return _list_of_file_names_from_keys(settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return _create_complex_type(settings_dirs, spec_option, ListOfFileNames)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return [\n        IniType(\"file\", [\n            IniFiles(settings_dirs, spec_option)\n        ])\n    ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return _generate_ini_type(\n        'complex', None, settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n    return _load_type_class('list_of_file_names', settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "ansible_config_manager", "docstring": "Gets the ansible config manager.", "ques_id": "6306091b73426c38ae68acd7", "code": ["def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls)\n", "def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls._get_instance())\n", "def ansible_config_manager(cls):\n    return ConfigManager(cls._get_ansible_config_dir())\n", "def ansible_config_manager(cls):\n    return cls._ansible_config_manager\n", "def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls.get_connection())\n", "def ansible_config_manager(cls):\n    return cls.config_manager_class(cls)\n", "def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls)\n", "def ansible_config_manager(cls):\n    return cls._ansible_config_manager\n", "def ansible_config_manager(cls):\n    return cls._ansible_config_manager\n", "def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls._get_instance())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return ConfigManager(cls._get_ansible_config_dir())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._ansible_config_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls.get_connection())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls.config_manager_class(cls)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._ansible_config_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return cls._ansible_config_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n    return AnsibleConfigManager(cls)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "workspace_manager", "docstring": "Gets the workspace manager.", "ques_id": "6306091b73426c38ae68acd9", "code": ["def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(connection=cls._connection)\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    from mscxyz import __version__\n    return _WorkspaceManager(__version__)\n", "def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(connection=cls._connection)\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(\n            skip_checks=cls._skip_checks)\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    from tests.integration import shared_workspace_manager\n    return shared_workspace_manager()\n", "def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(cls._get_client())\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(\n            service_url=cls._client.get_url(),\n            username=cls._client.username,\n            password=cls._client.password,\n        )\n    return cls._workspace_manager\n", "def workspace_manager(cls):\n    from tests.fixtures.flows import workspace_manager\n    return workspace_manager.WorkspaceManager(cls())\n", "def workspace_manager(cls):\n    from mscxyz import __version__\n    return cls('workspace-manager', __version__)\n", "def workspace_manager(cls):\n    from mscxyz import api_workspace\n    return api_workspace.WorkspaceManager(cls())\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(connection=cls._connection)\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    from mscxyz import __version__\n    return _WorkspaceManager(__version__)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(connection=cls._connection)\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(\n            skip_checks=cls._skip_checks)\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    from tests.integration import shared_workspace_manager\n    return shared_workspace_manager()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(cls._get_client())\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    if cls._workspace_manager is None:\n        cls._workspace_manager = WorkspaceManager(\n            service_url=cls._client.get_url(),\n            username=cls._client.username,\n            password=cls._client.password,\n        )\n    return cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    from tests.fixtures.flows import workspace_manager\n    return workspace_manager.WorkspaceManager(cls())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    from mscxyz import __version__\n    return cls('workspace-manager', __version__)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n    from mscxyz import api_workspace\n    return api_workspace.WorkspaceManager(cls())\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "plugins_manager", "docstring": "Gets the plugin manager.", "ques_id": "6306091b73426c38ae68acda", "code": ["def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return _plugins_manager\n", "def plugins_manager(cls):\n    if cls._plugins_manager is None:\n        cls._plugins_manager = PluginManager(cls)\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    if cls._plugins_manager is None:\n        cls._plugins_manager = PluginManager(cls.get_plugins())\n    return cls._plugins_manager\n", "def plugins_manager(cls):\n    return cls._plugins_manager\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return _plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    if cls._plugins_manager is None:\n        cls._plugins_manager = PluginManager(cls)\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    if cls._plugins_manager is None:\n        cls._plugins_manager = PluginManager(cls.get_plugins())\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n    return cls._plugins_manager\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_content", "docstring": "validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file", "ques_id": "6306091c73426c38ae68acdc", "code": ["def validate_from_content(cls, spec_content=None):\n    if not spec_content:\n        raise IRValidatorException('No spec content provided')\n    spec_content = yaml.safe_load(spec_content)\n    required_fields = cls.SPEC_FIELDS\n    missing_fields = [key for key in required_fields if not spec_content[key]]\n    if missing_fields:\n        raise IRValidatorException('Missing fields: {}'.format(', '.join(missing_fields)))\n    return spec_content\n", "def validate_from_content(cls, spec_content=None):\n    if spec_content is None:\n        raise IRValidatorException(\"No content provided to load spec\")\n\n    # split into data in a list and validate each element\n    data = yaml.load(spec_content)\n    if not all([field in data for field in cls.required_fields()]):\n        raise IRValidatorException(\"Missing mandatory fields in spec file\")\n\n    return data\n", "def validate_from_content(cls, spec_content=None):\n    required_fields = cls.required_fields()\n    if required_fields is None:\n        raise IRValidatorException(\n            '{} requires at least one of {}'.format(cls.__name__, required_fields))\n    data = {}\n    for field in required_fields:\n        if field in spec_content:\n            data[field] = spec_content[field]\n        else:\n            raise IRValidatorException(\n                '{} is required in {}'.format(field, cls.__name__))\n    return data\n", "def validate_from_content(cls, spec_content=None):\n    if not spec_content:\n        raise IRValidatorException('Missing spec content')\n\n    spec_data = yaml.safe_load(spec_content)\n    required_fields = cls._REQUIRED_FIELDS\n    missing_fields = [f for f in required_fields if f not in spec_data]\n    if missing_fields:\n        raise IRValidatorException('Required fields are missing: {}'.format(missing_fields))\n\n    return spec_data\n", "def validate_from_content(cls, spec_content=None):\n    if not spec_content:\n        raise IRValidatorException(\n            'No spec content provided in the input file. '\n            'Please add it to the input file to validate it.'\n        )\n\n    # get required fields\n    required_fields = cls._get_required_fields(spec_content)\n\n    # remove fields that are missing\n    missing_fields = {}\n    for f in required_fields:\n        if f not in spec_content:\n            missing_fields[f] = required_fields[f]\n\n    # validate required fields\n    missing_fields = cls._validate_required_fields(missing_fields)\n\n    if missing_fields:\n        raise IRValidatorException(\n            'Missing fields: {}'.format(missing_fields)\n        )\n\n    return cls._parse_yaml_content(spec_content)\n", "def validate_from_content(cls, spec_content=None):\n    required_fields = cls.REQUIRED_FIELDS\n    if spec_content is None:\n        raise IRValidatorException(\"no spec file content\")\n    if required_fields:\n        missing_fields = [field for field in required_fields if field not in spec_content]\n        if missing_fields:\n            raise IRValidatorException(\"missing fields: {}\".format(missing_fields))\n    return cls.load_from_content(spec_content)\n", "def validate_from_content(cls, spec_content=None):\n    validator_data = cls._load_yaml(spec_content)\n    return validator_data\n", "def validate_from_content(cls, spec_content=None):\n    data = {}\n    if spec_content is not None:\n        data = yaml.safe_load(spec_content)\n    if data is None or not isinstance(data, dict):\n        raise IRValidatorException('Missing required fields in spec file')\n    return data\n", "def validate_from_content(cls, spec_content=None):\n    if spec_content is None:\n        raise IRValidatorException('Missing spec content')\n\n    required_fields = cls.required_fields()\n    missing = [key for key in required_fields if key not in spec_content]\n    if missing:\n        raise IRValidatorException('Missing required fields: {}'.format(', '.join(missing)))\n\n    return cls.load_from_content(spec_content)\n", "def validate_from_content(cls, spec_content=None):\n    if not spec_content:\n        raise IRValidatorException(\"Missing spec content\")\n\n    loaded_spec_dict = cls.load(spec_content)\n    required_fields = cls.REQUIRED_FIELDS\n    missing_fields = []\n\n    for key in required_fields:\n        if key not in loaded_spec_dict:\n            missing_fields.append(key)\n\n    if len(missing_fields) > 0:\n        raise IRValidatorException(\"Missing required fields: {}\".format(', '.join(missing_fields)))\n\n    return loaded_spec_dict\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_content(cls, spec_content=None):\n    if not spec_content:\n        raise IRValidatorException('No spec content provided')\n    spec_content = yaml.safe_load(spec_content)\n    required_fields = cls.SPEC_FIELDS\n    missing_fields = [key for key in required_fields if not spec_content[key]]\n    if missing_fields:\n        raise IRValidatorException('Missing fields: {}'.format(', '.join(missing_fields)))\n    return spec_content\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    if spec_content is None:\n        raise IRValidatorException(\"No content provided to load spec\")\n\n    # split into data in a list and validate each element\n    data = yaml.load(spec_content)\n    if not all([field in data for field in cls.required_fields()]):\n        raise IRValidatorException(\"Missing mandatory fields in spec file\")\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    required_fields = cls.required_fields()\n    if required_fields is None:\n        raise IRValidatorException(\n            '{} requires at least one of {}'.format(cls.__name__, required_fields))\n    data = {}\n    for field in required_fields:\n        if field in spec_content:\n            data[field] = spec_content[field]\n        else:\n            raise IRValidatorException(\n                '{} is required in {}'.format(field, cls.__name__))\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    if not spec_content:\n        raise IRValidatorException('Missing spec content')\n\n    spec_data = yaml.safe_load(spec_content)\n    required_fields = cls._REQUIRED_FIELDS\n    missing_fields = [f for f in required_fields if f not in spec_data]\n    if missing_fields:\n        raise IRValidatorException('Required fields are missing: {}'.format(missing_fields))\n\n    return spec_data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    if not spec_content:\n        raise IRValidatorException(\n            'No spec content provided in the input file. '\n            'Please add it to the input file to validate it.'\n        )\n\n    # get required fields\n    required_fields = cls._get_required_fields(spec_content)\n\n    # remove fields that are missing\n    missing_fields = {}\n    for f in required_fields:\n        if f not in spec_content:\n            missing_fields[f] = required_fields[f]\n\n    # validate required fields\n    missing_fields = cls._validate_required_fields(missing_fields)\n\n    if missing_fields:\n        raise IRValidatorException(\n            'Missing fields: {}'.format(missing_fields)\n        )\n\n    return cls._parse_yaml_content(spec_content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    required_fields = cls.REQUIRED_FIELDS\n    if spec_content is None:\n        raise IRValidatorException(\"no spec file content\")\n    if required_fields:\n        missing_fields = [field for field in required_fields if field not in spec_content]\n        if missing_fields:\n            raise IRValidatorException(\"missing fields: {}\".format(missing_fields))\n    return cls.load_from_content(spec_content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    validator_data = cls._load_yaml(spec_content)\n    return validator_data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    data = {}\n    if spec_content is not None:\n        data = yaml.safe_load(spec_content)\n    if data is None or not isinstance(data, dict):\n        raise IRValidatorException('Missing required fields in spec file')\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    if spec_content is None:\n        raise IRValidatorException('Missing spec content')\n\n    required_fields = cls.required_fields()\n    missing = [key for key in required_fields if key not in spec_content]\n    if missing:\n        raise IRValidatorException('Missing required fields: {}'.format(', '.join(missing)))\n\n    return cls.load_from_content(spec_content)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n    if not spec_content:\n        raise IRValidatorException(\"Missing spec content\")\n\n    loaded_spec_dict = cls.load(spec_content)\n    required_fields = cls.REQUIRED_FIELDS\n    missing_fields = []\n\n    for key in required_fields:\n        if key not in loaded_spec_dict:\n            missing_fields.append(key)\n\n    if len(missing_fields) > 0:\n        raise IRValidatorException(\"Missing required fields: {}\".format(', '.join(missing_fields)))\n\n    return loaded_spec_dict\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_file", "docstring": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file", "ques_id": "6306091c73426c38ae68acdd", "code": ["def validate_from_file(cls, yaml_file=None):\n    if yaml_file is None:\n        raise IRValidatorException('Missing required parameter `yaml_file`')\n\n    with open(yaml_file) as f:\n        try:\n            return cls.from_yaml(yaml.safe_load(f))\n        except yaml.YAMLError as e:\n            raise IRValidatorException('YAML file is not valid: {0}'.format(e))\n", "def validate_from_file(cls, yaml_file=None):\n    yaml_file = yaml_file or cls.DEFAULT_YAML_FILE\n    data = cls.load_from_file(yaml_file)\n    cls.validate(data)\n    return data\n", "def validate_from_file(cls, yaml_file=None):\n    if not yaml_file:\n        raise IRValidatorException('Missing a YAML file')\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.safe_load(f)\n\n    cls.validate(data)\n\n    return data\n", "def validate_from_file(cls, yaml_file=None):\n\n    if yaml_file is None:\n        raise IRValidatorException(\"Missing required file parameter\")\n\n    with open(yaml_file) as yml_file:\n        try:\n            data = yaml.safe_load(yml_file)\n            return cls.validate(data)\n        except yaml.YAMLError as yaml_error:\n            raise IRValidatorException(\"Invalid YAML file\") from yaml_error\n", "def validate_from_file(cls, yaml_file=None):\n    data = load_yaml(yaml_file)\n    cls.validate(data)\n    return data\n", "def validate_from_file(cls, yaml_file=None):\n    try:\n        with open(yaml_file, 'r') as yaml_file:\n            data = yaml.safe_load(yaml_file)\n        return cls.validate(data)\n    except IOError:\n        raise IRValidatorException('Could not load YAML file')\n", "def validate_from_file(cls, yaml_file=None):\n    if not yaml_file:\n        raise IRValidatorException('No YAML file provided')\n\n    with open(yaml_file, 'r') as f:\n        try:\n            data = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            raise IRValidatorException('Unable to parse YAML file {0}:\\n{1}'.format(yaml_file, exc))\n\n    try:\n        cls.validate(data)\n    except IRValidatorException as exc:\n        raise IRValidatorException('Missing required fields: {0}'.format(exc.args[0]))\n\n    return data\n", "def validate_from_file(cls, yaml_file=None):\n    if yaml_file is None:\n        raise IRValidatorException('Missing file path')\n\n    with open(yaml_file, 'r') as file_handle:\n        try:\n            return cls._load_from_file(yaml_file, file_handle)\n        except (yaml.YAMLError, IOError) as exc:\n            raise IRValidatorException(exc)\n", "def validate_from_file(cls, yaml_file=None):\n    yaml_file = cls._load_file(yaml_file)\n\n    missing_keys = []\n    for key in cls.required_keys:\n        if key not in yaml_file:\n            missing_keys.append(key)\n\n    if missing_keys:\n        raise IRValidatorException(\"Missing required keys: {}\".format(missing_keys))\n\n    return yaml_file\n", "def validate_from_file(cls, yaml_file=None):\n    if not yaml_file:\n        raise IRValidatorException(\n            \"Missing required parameter 'yaml_file' in '{}'\".format(cls.__name__)\n        )\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.safe_load(f)\n\n    return cls.validate(data)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_file(cls, yaml_file=None):\n    if yaml_file is None:\n        raise IRValidatorException('Missing required parameter `yaml_file`')\n\n    with open(yaml_file) as f:\n        try:\n            return cls.from_yaml(yaml.safe_load(f))\n        except yaml.YAMLError as e:\n            raise IRValidatorException('YAML file is not valid: {0}'.format(e))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    yaml_file = yaml_file or cls.DEFAULT_YAML_FILE\n    data = cls.load_from_file(yaml_file)\n    cls.validate(data)\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    if not yaml_file:\n        raise IRValidatorException('Missing a YAML file')\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.safe_load(f)\n\n    cls.validate(data)\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\n    if yaml_file is None:\n        raise IRValidatorException(\"Missing required file parameter\")\n\n    with open(yaml_file) as yml_file:\n        try:\n            data = yaml.safe_load(yml_file)\n            return cls.validate(data)\n        except yaml.YAMLError as yaml_error:\n            raise IRValidatorException(\"Invalid YAML file\") from yaml_error\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    data = load_yaml(yaml_file)\n    cls.validate(data)\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    try:\n        with open(yaml_file, 'r') as yaml_file:\n            data = yaml.safe_load(yaml_file)\n        return cls.validate(data)\n    except IOError:\n        raise IRValidatorException('Could not load YAML file')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    if not yaml_file:\n        raise IRValidatorException('No YAML file provided')\n\n    with open(yaml_file, 'r') as f:\n        try:\n            data = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            raise IRValidatorException('Unable to parse YAML file {0}:\\n{1}'.format(yaml_file, exc))\n\n    try:\n        cls.validate(data)\n    except IRValidatorException as exc:\n        raise IRValidatorException('Missing required fields: {0}'.format(exc.args[0]))\n\n    return data\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    if yaml_file is None:\n        raise IRValidatorException('Missing file path')\n\n    with open(yaml_file, 'r') as file_handle:\n        try:\n            return cls._load_from_file(yaml_file, file_handle)\n        except (yaml.YAMLError, IOError) as exc:\n            raise IRValidatorException(exc)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    yaml_file = cls._load_file(yaml_file)\n\n    missing_keys = []\n    for key in cls.required_keys:\n        if key not in yaml_file:\n            missing_keys.append(key)\n\n    if missing_keys:\n        raise IRValidatorException(\"Missing required keys: {}\".format(missing_keys))\n\n    return yaml_file\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n    if not yaml_file:\n        raise IRValidatorException(\n            \"Missing required parameter 'yaml_file' in '{}'\".format(cls.__name__)\n        )\n\n    with open(yaml_file, 'r') as f:\n        data = yaml.safe_load(f)\n\n    return cls.validate(data)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "_include_groups", "docstring": "Resolves the include dict directive in the spec files.", "ques_id": "6306091d73426c38ae68ace5", "code": ["def _include_groups(self, parser_dict):\n    for include_group in parser_dict['include_groups']:\n        self._include_directive_resolver(include_group)\n", "def _include_groups(self, parser_dict):\n    includes = parser_dict.get('includes', [])\n    for include_dict in includes:\n        for include_directive in include_dict.get('include', []):\n            # We need to look for a group named after the include directive.\n            group_name = include_directive.get('group')\n            if group_name:\n                group = self._get_group(group_name)\n                if group:\n                    include_dict['group'] = group\n    return parser_dict\n", "def _include_groups(self, parser_dict):\n    for key in parser_dict:\n        if key in self.include_groups:\n            include_directive = self.include_groups[key]\n            include_directive.directive = parser_dict[key]\n            include_directive.parse()\n            self._include_directives[key] = include_directive\n", "def _include_groups(self, parser_dict):\n    include_groups = []\n    for include_dict in parser_dict.get('included', []):\n        include_groups.append(self._resolve_include(include_dict))\n\n    return include_groups\n", "def _include_groups(self, parser_dict):\n    for key, value in parser_dict.items():\n        if key == 'include':\n            for include in value:\n                include_path = include['path']\n                self._include_file(include_path)\n        else:\n            self._include_file(value)\n", "def _include_groups(self, parser_dict):\n    for include_file, contents in parser_dict.items():\n        with open(include_file) as f:\n            spec_contents = f.read()\n            match = self.INCLUDE_GROUP_REGEX.search(spec_contents)\n            if not match:\n                raise IncludeError(\n                    'Unable to find include group in {0}'.format(include_file))\n            include_group = match.group('include_group')\n            include_group_list = include_group.split(',')\n            for include_group in include_group_list:\n                include_group = include_group.strip()\n                include_group_name, include_group_contents = include_group.split('=')\n                parser_dict[include_group_name] = include_group_contents\n", "def _include_groups(self, parser_dict):\n    for group in parser_dict.get('include_groups', []):\n        if group == '__main__':\n            self._main_file = parser_dict['__main__']\n            break\n\n        self._include_group(group, parser_dict)\n", "def _include_groups(self, parser_dict):\n    include_groups = []\n    for include_file in parser_dict.get('include_file', []):\n        if include_file.startswith('#'):\n            include_groups.append(include_file[1:])\n        elif include_file == '__init__.py':\n            include_groups.append('__init__.py')\n        elif os.path.isabs(include_file):\n            include_groups.append(include_file)\n        else:\n            spec_path = os.path.join(\n                os.path.dirname(__file__),\n                include_file\n            )\n            with open(spec_path) as spec_file:\n                include_groups.append(\n                    spec_file.read().strip()\n                )\n    return include_groups\n", "def _include_groups(self, parser_dict):\n    if not parser_dict:\n        return\n\n    for include_dict in parser_dict.get('includes', []):\n        self._include_dict(include_dict)\n", "def _include_groups(self, parser_dict):\n    if parser_dict:\n        return self._resolve_directive_group(parser_dict, \"include\")\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _include_groups(self, parser_dict):\n    for include_group in parser_dict['include_groups']:\n        self._include_directive_resolver(include_group)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    includes = parser_dict.get('includes', [])\n    for include_dict in includes:\n        for include_directive in include_dict.get('include', []):\n            # We need to look for a group named after the include directive.\n            group_name = include_directive.get('group')\n            if group_name:\n                group = self._get_group(group_name)\n                if group:\n                    include_dict['group'] = group\n    return parser_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    for key in parser_dict:\n        if key in self.include_groups:\n            include_directive = self.include_groups[key]\n            include_directive.directive = parser_dict[key]\n            include_directive.parse()\n            self._include_directives[key] = include_directive\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    include_groups = []\n    for include_dict in parser_dict.get('included', []):\n        include_groups.append(self._resolve_include(include_dict))\n\n    return include_groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    for key, value in parser_dict.items():\n        if key == 'include':\n            for include in value:\n                include_path = include['path']\n                self._include_file(include_path)\n        else:\n            self._include_file(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    for include_file, contents in parser_dict.items():\n        with open(include_file) as f:\n            spec_contents = f.read()\n            match = self.INCLUDE_GROUP_REGEX.search(spec_contents)\n            if not match:\n                raise IncludeError(\n                    'Unable to find include group in {0}'.format(include_file))\n            include_group = match.group('include_group')\n            include_group_list = include_group.split(',')\n            for include_group in include_group_list:\n                include_group = include_group.strip()\n                include_group_name, include_group_contents = include_group.split('=')\n                parser_dict[include_group_name] = include_group_contents\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    for group in parser_dict.get('include_groups', []):\n        if group == '__main__':\n            self._main_file = parser_dict['__main__']\n            break\n\n        self._include_group(group, parser_dict)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    include_groups = []\n    for include_file in parser_dict.get('include_file', []):\n        if include_file.startswith('#'):\n            include_groups.append(include_file[1:])\n        elif include_file == '__init__.py':\n            include_groups.append('__init__.py')\n        elif os.path.isabs(include_file):\n            include_groups.append(include_file)\n        else:\n            spec_path = os.path.join(\n                os.path.dirname(__file__),\n                include_file\n            )\n            with open(spec_path) as spec_file:\n                include_groups.append(\n                    spec_file.read().strip()\n                )\n    return include_groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    if not parser_dict:\n        return\n\n    for include_dict in parser_dict.get('includes', []):\n        self._include_dict(include_dict)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n    if parser_dict:\n        return self._resolve_directive_group(parser_dict, \"include\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_spec_defaults", "docstring": "Resolve arguments' values from spec and other sources.", "ques_id": "6306092373426c38ae68acfa", "code": ["def get_spec_defaults(self):\n    return {}\n", "def get_spec_defaults(self):\n    return {}\n", "def get_spec_defaults(self):\n    return {}\n", "def get_spec_defaults(self):\n    return {}\n", "def get_spec_defaults(self):\n    return {\n        key: getattr(self, key)\n        for key in ('args', 'varargs', 'keywords', 'defaults')\n    }\n", "def get_spec_defaults(self):\n    defaults = {}\n    for key, value in iteritems(self.defaults):\n        if key in self.spec:\n            if isinstance(value, str):\n                value = self.resolve_string(value)\n            setattr(defaults, key, value)\n    return defaults\n", "def get_spec_defaults(self):\n    return {\n        k: self._resolve_spec(v)\n        for k, v in self.spec.items()\n    }\n", "def get_spec_defaults(self):\n    return {}\n", "def get_spec_defaults(self):\n    defaults = super(Blob, self).get_spec_defaults()\n    defaults.update({\n        'data': self.data,\n       'mime_type': self.mime_type,\n        'blob_hash': self.blob_hash,\n    })\n    return defaults\n", "def get_spec_defaults(self):\n    defaults = super(PackageRequirements, self).get_spec_defaults()\n    if defaults is None:\n        return defaults\n    return dict(\n        (key, value)\n        for key, value in iteritems(defaults)\n        if (\n            not isinstance(value, tuple)\n            and not isinstance(value, list)\n        )\n    )\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_spec_defaults(self):\n    return {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return {\n        key: getattr(self, key)\n        for key in ('args', 'varargs', 'keywords', 'defaults')\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    defaults = {}\n    for key, value in iteritems(self.defaults):\n        if key in self.spec:\n            if isinstance(value, str):\n                value = self.resolve_string(value)\n            setattr(defaults, key, value)\n    return defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return {\n        k: self._resolve_spec(v)\n        for k, v in self.spec.items()\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    return {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    defaults = super(Blob, self).get_spec_defaults()\n    defaults.update({\n        'data': self.data,\n       'mime_type': self.mime_type,\n        'blob_hash': self.blob_hash,\n    })\n    return defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n    defaults = super(PackageRequirements, self).get_spec_defaults()\n    if defaults is None:\n        return defaults\n    return dict(\n        (key, value)\n        for key, value in iteritems(defaults)\n        if (\n            not isinstance(value, tuple)\n            and not isinstance(value, list)\n        )\n    )\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_deprecated_args", "docstring": "Returning dict with options which deprecate others.", "ques_id": "6306092973426c38ae68ad01", "code": ["def get_deprecated_args(self):\n    return {\n        \"warn\": {\"key\": \"warning_level\", \"type\": \"int\"},\n        \"crit\": {\"key\": \"critical_level\", \"type\": \"int\"},\n    }\n", "def get_deprecated_args(self):\n    return self._deprecated_args\n", "def get_deprecated_args(self):\n    return self.deprecated_args\n", "def get_deprecated_args(self):\n    return {'protocol': self.protocol, 'host': self.host,\n            'port': self.port, 'db': self.db}\n", "def get_deprecated_args(self):\n    return {}\n", "def get_deprecated_args(self):\n    return {}\n", "def get_deprecated_args(self):\n    return self._deprecated_args\n", "def get_deprecated_args(self):\n    return self._deprecated_args\n", "def get_deprecated_args(self):\n    return {\n        'force': self.force,\n       'skip_check_update': self.skip_check_update,\n        'install_requires': self.install_requires,\n    }\n", "def get_deprecated_args(self):\n    return self._deprecated_args\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_deprecated_args(self):\n    return {\n        \"warn\": {\"key\": \"warning_level\", \"type\": \"int\"},\n        \"crit\": {\"key\": \"critical_level\", \"type\": \"int\"},\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._deprecated_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self.deprecated_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return {'protocol': self.protocol, 'host': self.host,\n            'port': self.port, 'db': self.db}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._deprecated_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._deprecated_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return {\n        'force': self.force,\n       'skip_check_update': self.skip_check_update,\n        'install_requires': self.install_requires,\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n    return self._deprecated_args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_arg_deprecation", "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files", "ques_id": "6306092c73426c38ae68ad02", "code": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecations = []\n\n    for key, value in cli_args.items():\n        if isinstance(value, dict) and 'deprecated' in value:\n            deprecations.append('{}: {}'.format(key, value['deprecated']))\n\n    for key, value in answer_file_args.items():\n        if isinstance(value, dict) and 'deprecated' in value:\n            deprecations.append('{}: {}'.format(key, value['deprecated']))\n\n    deprecations_text = '\\n'.join(deprecations)\n\n    if deprecations_text:\n        print('\\n'.join((\n            'The following arguments are deprecated and will be removed. '\n            'Please refer to the documentation at http://pyapi-gitlab.readthedocs.io/en/latest/#gitlab.Gitlab.{}'\n        ).format(deprecations_text)))\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecated_args = []\n    for key, value in cli_args.items():\n        if key in answer_file_args:\n            deprecated_args.append('--{}={}'.format(key, value))\n        else:\n            deprecated_args.append(key)\n    if deprecated_args:\n        deprecation_message = 'Deprecated arguments: {}'.format(', '.join(deprecated_args))\n        self._print_message(deprecation_message,'red')\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecated_args = {}\n    deprecated_args_file = {}\n    deprecated_args_file[\"stdout\"] = cli_args.stdout\n    deprecated_args_file[\"stderr\"] = cli_args.stderr\n\n    if cli_args.dump_deprecated:\n        deprecated_args['stdout'] = answer_file_args['stdout']\n        deprecated_args['stderr'] = answer_file_args['stderr']\n\n    if cli_args.dump_deprecated_with_version:\n        deprecated_args['version'] = answer_file_args['version']\n\n    if cli_args.dump_deprecated_with_sys_version:\n        deprecated_args['sys_version'] = answer_file_args['sys_version']\n\n    if cli_args.dump_deprecated_with_minor:\n        deprecated_args['minor'] = answer_file_args['minor']\n\n    deprecated_args = json.dumps(deprecated_args, sort_keys=True,\n                                indent=4)\n    deprecated_args_file[\"", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    self._deprecation_warning(cli_args, answer_file_args)\n    self._deprecation_warning(cli_args, answer_file_args)\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecation_warnings = []\n    for arg in cli_args:\n        arg_key = arg['name']\n        if arg_key in answer_file_args:\n            deprecation_warnings.append(\n                '{} argument is deprecated and should not be used in future versions. Use {} instead.'.format(\n                    arg['name'], arg['name']))\n    if deprecation_warnings:\n        self.logger.warning(\n            'The following arguments were deprecated: {}'.format(deprecation_warnings))\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecate_args = dict()\n    for argument in cli_args:\n        if argument in answer_file_args:\n            deprecate_args[argument] = cli_args[argument]\n            del cli_args[argument]\n    if deprecate_args:\n        print(\n            \"[ERROR] {0} arguments are deprecated and will be removed in a future version of the CLI.\".format(\n                len(deprecate_args)))\n        for argument in deprecate_args:\n            print(\n                \"    {0} = {1}\".format(argument, deprecate_args[argument]))\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    self._print_deprecation(\n        'arguments',\n        'files',\n        cli_args,\n        answer_file_args,\n        'files deprecated',\n        'once upon request'\n    )\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecated_arguments = []\n    for key in cli_args:\n        if key in answer_file_args:\n            deprecated_arguments.append(key)\n        else:\n            self.log_warning('{} argument is not in the {} file'.format(key, self.file_name))\n\n    if deprecated_arguments:\n        self.log_warning('The following arguments are deprecated:')\n        for key in deprecated_arguments:\n            self.log_warning('    {}'.format(key))\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecated_args = self.get_deprecated_arguments(cli_args)\n    deprecated_args.update(answer_file_args)\n\n    deprecated_args = self.remove_empty_args(deprecated_args)\n    deprecated_args = self.validate_args(deprecated_args)\n\n    def print_args_list(args_list):\n        if not args_list:\n            print(\"No arguments found in files\")\n            return\n        for arg in args_list:\n            print(arg)\n\n    print_args_list(deprecated_args.keys())\n    print_args_list(deprecated_args.values())\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if cli_args.deprecation is not None:\n        if cli_args.deprecation not in DEPRECATED_ARGS:\n            raise ValueError(\"Invalid deprecation setting: {}\".format(cli_args.deprecation))\n\n        deprecated_args = DEPRECATED_ARGS[cli_args.deprecation]\n        deprecated_args.update(answer_file_args)\n        deprecated_args[\"answer_file\"] = deprecated_args.pop(\"answer_file\")\n\n        self.print_deprecated_args(cli_args, deprecated_args)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecations = []\n\n    for key, value in cli_args.items():\n        if isinstance(value, dict) and 'deprecated' in value:\n            deprecations.append('{}: {}'.format(key, value['deprecated']))\n\n    for key, value in answer_file_args.items():\n        if isinstance(value, dict) and 'deprecated' in value:\n            deprecations.append('{}: {}'.format(key, value['deprecated']))\n\n    deprecations_text = '\\n'.join(deprecations)\n\n    if deprecations_text:\n        print('\\n'.join((\n            'The following arguments are deprecated and will be removed. '\n            'Please refer to the documentation at http://pyapi-gitlab.readthedocs.io/en/latest/#gitlab.Gitlab.{}'\n        ).format(deprecations_text)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecated_args = []\n    for key, value in cli_args.items():\n        if key in answer_file_args:\n            deprecated_args.append('--{}={}'.format(key, value))\n        else:\n            deprecated_args.append(key)\n    if deprecated_args:\n        deprecation_message = 'Deprecated arguments: {}'.format(', '.join(deprecated_args))\n        self._print_message(deprecation_message,'red')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecated_args = {}\n    deprecated_args_file = {}\n    deprecated_args_file[\"stdout\"] = cli_args.stdout\n    deprecated_args_file[\"stderr\"] = cli_args.stderr\n\n    if cli_args.dump_deprecated:\n        deprecated_args['stdout'] = answer_file_args['stdout']\n        deprecated_args['stderr'] = answer_file_args['stderr']\n\n    if cli_args.dump_deprecated_with_version:\n        deprecated_args['version'] = answer_file_args['version']\n\n    if cli_args.dump_deprecated_with_sys_version:\n        deprecated_args['sys_version'] = answer_file_args['sys_version']\n\n    if cli_args.dump_deprecated_with_minor:\n        deprecated_args['minor'] = answer_file_args['minor']\n\n    deprecated_args = json.dumps(deprecated_args, sort_keys=True,\n                                indent=4)\n    deprecated_args_file[\"", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    self._deprecation_warning(cli_args, answer_file_args)\n    self._deprecation_warning(cli_args, answer_file_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecation_warnings = []\n    for arg in cli_args:\n        arg_key = arg['name']\n        if arg_key in answer_file_args:\n            deprecation_warnings.append(\n                '{} argument is deprecated and should not be used in future versions. Use {} instead.'.format(\n                    arg['name'], arg['name']))\n    if deprecation_warnings:\n        self.logger.warning(\n            'The following arguments were deprecated: {}'.format(deprecation_warnings))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecate_args = dict()\n    for argument in cli_args:\n        if argument in answer_file_args:\n            deprecate_args[argument] = cli_args[argument]\n            del cli_args[argument]\n    if deprecate_args:\n        print(\n            \"[ERROR] {0} arguments are deprecated and will be removed in a future version of the CLI.\".format(\n                len(deprecate_args)))\n        for argument in deprecate_args:\n            print(\n                \"    {0} = {1}\".format(argument, deprecate_args[argument]))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    self._print_deprecation(\n        'arguments',\n        'files',\n        cli_args,\n        answer_file_args,\n        'files deprecated',\n        'once upon request'\n    )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecated_arguments = []\n    for key in cli_args:\n        if key in answer_file_args:\n            deprecated_arguments.append(key)\n        else:\n            self.log_warning('{} argument is not in the {} file'.format(key, self.file_name))\n\n    if deprecated_arguments:\n        self.log_warning('The following arguments are deprecated:')\n        for key in deprecated_arguments:\n            self.log_warning('    {}'.format(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    deprecated_args = self.get_deprecated_arguments(cli_args)\n    deprecated_args.update(answer_file_args)\n\n    deprecated_args = self.remove_empty_args(deprecated_args)\n    deprecated_args = self.validate_args(deprecated_args)\n\n    def print_args_list(args_list):\n        if not args_list:\n            print(\"No arguments found in files\")\n            return\n        for arg in args_list:\n            print(arg)\n\n    print_args_list(deprecated_args.keys())\n    print_args_list(deprecated_args.values())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    if cli_args.deprecation is not None:\n        if cli_args.deprecation not in DEPRECATED_ARGS:\n            raise ValueError(\"Invalid deprecation setting: {}\".format(cli_args.deprecation))\n\n        deprecated_args = DEPRECATED_ARGS[cli_args.deprecation]\n        deprecated_args.update(answer_file_args)\n        deprecated_args[\"answer_file\"] = deprecated_args.pop(\"answer_file\")\n\n        self.print_deprecated_args(cli_args, deprecated_args)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_parser_option_specs", "docstring": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options", "ques_id": "6306092d73426c38ae68ad04", "code": ["def get_parser_option_specs(self, command_name):\n    parser_option_specs = []\n    for option in self.command_options[command_name]:\n        option_spec = self.get_parser_option_spec(option)\n        parser_option_specs.append(option_spec)\n    return parser_option_specs\n", "def get_parser_option_specs(self, command_name):\n    parser = self.get_parser(command_name)\n    return parser.option_specs\n", "def get_parser_option_specs(self, command_name):\n    command_options = []\n    for option in self.commands[command_name].options:\n        option_spec = self.get_parser_option_spec(option)\n        if not option_spec:\n            continue\n        command_options.append(option_spec)\n    return command_options\n", "def get_parser_option_specs(self, command_name):\n    option_spec = self.get_command_option_specs(command_name)\n    if 'options' in option_spec:\n        return option_spec['options']\n    else:\n        return []\n", "def get_parser_option_specs(self, command_name):\n    command_parser = self.get_command_parser(command_name)\n    return command_parser.option_specs\n", "def get_parser_option_specs(self, command_name):\n    parser_option_specs = []\n    for command_spec in self.get_command_specs(command_name):\n        parser_option_specs.append(command_spec.get_option_spec())\n\n    return parser_option_specs\n", "def get_parser_option_specs(self, command_name):\n    command = self.get_command(command_name)\n    return command.get_option_specs()\n", "def get_parser_option_specs(self, command_name):\n    if command_name in self.command_options:\n        return self.command_options[command_name]\n    else:\n        return []\n", "def get_parser_option_specs(self, command_name):\n    option_spec = []\n\n    for opt in self.get_parser_options(command_name):\n        option_spec.append(opt)\n\n    return option_spec\n", "def get_parser_option_specs(self, command_name):\n    return self.parser.option_specs(command_name)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_parser_option_specs(self, command_name):\n    parser_option_specs = []\n    for option in self.command_options[command_name]:\n        option_spec = self.get_parser_option_spec(option)\n        parser_option_specs.append(option_spec)\n    return parser_option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    parser = self.get_parser(command_name)\n    return parser.option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    command_options = []\n    for option in self.commands[command_name].options:\n        option_spec = self.get_parser_option_spec(option)\n        if not option_spec:\n            continue\n        command_options.append(option_spec)\n    return command_options\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    option_spec = self.get_command_option_specs(command_name)\n    if 'options' in option_spec:\n        return option_spec['options']\n    else:\n        return []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    command_parser = self.get_command_parser(command_name)\n    return command_parser.option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    parser_option_specs = []\n    for command_spec in self.get_command_specs(command_name):\n        parser_option_specs.append(command_spec.get_option_spec())\n\n    return parser_option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    command = self.get_command(command_name)\n    return command.get_option_specs()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    if command_name in self.command_options:\n        return self.command_options[command_name]\n    else:\n        return []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    option_spec = []\n\n    for opt in self.get_parser_options(command_name):\n        option_spec.append(opt)\n\n    return option_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n    return self.parser.option_specs(command_name)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_option_spec", "docstring": "Gets the specification for the specified option name.", "ques_id": "6306092d73426c38ae68ad05", "code": ["def get_option_spec(self, command_name, argument_name):\n    return self._get_option_spec(command_name, argument_name)[1]\n", "def get_option_spec(self, command_name, argument_name):\n    argument_index = self._get_argument_index(command_name, argument_name)\n\n    return self._get_option_spec(command_name, argument_index)\n", "def get_option_spec(self, command_name, argument_name):\n    return self._get_option_spec(command_name, argument_name,\n                                 self._get_command_options(command_name))\n", "def get_option_spec(self, command_name, argument_name):\n    return self._option_specs[command_name].get(argument_name, None)\n", "def get_option_spec(self, command_name, argument_name):\n    command = self._get_command(command_name)\n    return self._get_option_spec(command, argument_name)\n", "def get_option_spec(self, command_name, argument_name):\n    command = self._get_command(command_name)\n    return command.option_specs.get(argument_name)\n", "def get_option_spec(self, command_name, argument_name):\n    return self._get_option_spec(command_name, argument_name,\n                                  self._get_command_options(command_name))\n", "def get_option_spec(self, command_name, argument_name):\n    argument_index = self._get_argument_index(command_name, argument_name)\n\n    return self._get_option_spec(command_name, argument_index)\n", "def get_option_spec(self, command_name, argument_name):\n    option_name = self._get_option_name(command_name, argument_name)\n    return self._get_option_definition(option_name)\n", "def get_option_spec(self, command_name, argument_name):\n    command = self._get_command(command_name)\n    return self._get_option_spec(command, argument_name)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_option_spec(self, command_name, argument_name):\n    return self._get_option_spec(command_name, argument_name)[1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    argument_index = self._get_argument_index(command_name, argument_name)\n\n    return self._get_option_spec(command_name, argument_index)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    return self._get_option_spec(command_name, argument_name,\n                                 self._get_command_options(command_name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    return self._option_specs[command_name].get(argument_name, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    command = self._get_command(command_name)\n    return self._get_option_spec(command, argument_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    command = self._get_command(command_name)\n    return command.option_specs.get(argument_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    return self._get_option_spec(command_name, argument_name,\n                                  self._get_command_options(command_name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    argument_index = self._get_argument_index(command_name, argument_name)\n\n    return self._get_option_spec(command_name, argument_index)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    option_name = self._get_option_name(command_name, argument_name)\n    return self._get_option_definition(option_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n    command = self._get_command(command_name)\n    return self._get_option_spec(command, argument_name)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_silent_args", "docstring": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names", "ques_id": "6306092d73426c38ae68ad06", "code": ["def get_silent_args(self, args):\n    return self._get_args(args, self.silent_args)\n", "def get_silent_args(self, args):\n    silent = []\n    for arg in args:\n        if self.is_silent(arg):\n            silent.append(arg)\n    return silent\n", "def get_silent_args(self, args):\n    return self._silent_args.get(args, [])\n", "def get_silent_args(self, args):\n    return [arg for arg in args if not self.is_silent(arg)]\n", "def get_silent_args(self, args):\n    return [arg for arg in args if not self._is_silent(arg)]\n", "def get_silent_args(self, args):\n    return [name for name in self._args if not self._args[name]['enabled']]\n", "def get_silent_args(self, args):\n    return [arg for arg in args if self.silent_args.get(arg, False)]\n", "def get_silent_args(self, args):\n    return [a for a in args if not self.is_arg_silent(a)]\n", "def get_silent_args(self, args):\n    return [arg for arg in args if arg!= self.SECRET_KEY]\n", "def get_silent_args(self, args):\n    return [arg for arg in args if not self._is_silent(arg)]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_silent_args(self, args):\n    return self._get_args(args, self.silent_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    silent = []\n    for arg in args:\n        if self.is_silent(arg):\n            silent.append(arg)\n    return silent\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return self._silent_args.get(args, [])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return [arg for arg in args if not self.is_silent(arg)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return [arg for arg in args if not self._is_silent(arg)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return [name for name in self._args if not self._args[name]['enabled']]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return [arg for arg in args if self.silent_args.get(arg, False)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return [a for a in args if not self.is_arg_silent(a)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return [arg for arg in args if arg!= self.SECRET_KEY]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n    return [arg for arg in args if not self._is_silent(arg)]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_requires_args", "docstring": "Check if all the required arguments have been provided.", "ques_id": "6306092d73426c38ae68ad07", "code": ["def validate_requires_args(self, args):\n    missing_args = []\n    for arg in self.required_args:\n        if arg not in args:\n            missing_args.append(arg)\n    if missing_args:\n        raise ValueError(\"Missing required arguments: {}\".format(missing_args))\n", "def validate_requires_args(self, args):\n    missing_args = []\n    for arg in self.required_args:\n        if not args.get(arg):\n            missing_args.append(arg)\n    if missing_args:\n        raise MissingRequiredArguments(missing_args)\n", "def validate_requires_args(self, args):\n    missing = []\n    for key in self.required_args:\n        if key not in args:\n            missing.append(key)\n\n    if missing:\n        raise click.BadParameter(\"Missing required arguments: %s\" %\n                                 missing)\n", "def validate_requires_args(self, args):\n    if not args:\n        raise exceptions.RequirementsError(\n            'Missing required arguments!')\n", "def validate_requires_args(self, args):\n    required_args = ['name', 'address','mail', 'phone']\n    return all(args.get(arg) for arg in required_args)\n", "def validate_requires_args(self, args):\n    return all([args['hostname'], args['token'], args['username'],\n                args['password']])\n", "def validate_requires_args(self, args):\n    return all(args.get(arg_name, '') for arg_name in self.required_args)\n", "def validate_requires_args(self, args):\n    return all([args.get('api_key'), args.get('region'), args.get('version')])\n", "def validate_requires_args(self, args):\n\n    required_args = [\"tls_verify\", \"ca_certs\", \"client_cert\", \"client_key\"]\n    return all(arg in args for arg in required_args)\n", "def validate_requires_args(self, args):\n    required_args = ['name', 'token', 'path']\n\n    if not all(arg in args for arg in required_args):\n        raise exceptions.ArgumentRequired('Missing required arguments.')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_requires_args(self, args):\n    missing_args = []\n    for arg in self.required_args:\n        if arg not in args:\n            missing_args.append(arg)\n    if missing_args:\n        raise ValueError(\"Missing required arguments: {}\".format(missing_args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    missing_args = []\n    for arg in self.required_args:\n        if not args.get(arg):\n            missing_args.append(arg)\n    if missing_args:\n        raise MissingRequiredArguments(missing_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    missing = []\n    for key in self.required_args:\n        if key not in args:\n            missing.append(key)\n\n    if missing:\n        raise click.BadParameter(\"Missing required arguments: %s\" %\n                                 missing)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    if not args:\n        raise exceptions.RequirementsError(\n            'Missing required arguments!')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    required_args = ['name', 'address','mail', 'phone']\n    return all(args.get(arg) for arg in required_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    return all([args['hostname'], args['token'], args['username'],\n                args['password']])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    return all(args.get(arg_name, '') for arg_name in self.required_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    return all([args.get('api_key'), args.get('region'), args.get('version')])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\n    required_args = [\"tls_verify\", \"ca_certs\", \"client_cert\", \"client_key\"]\n    return all(arg in args for arg in required_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n    required_args = ['name', 'token', 'path']\n\n    if not all(arg in args for arg in required_args):\n        raise exceptions.ArgumentRequired('Missing required arguments.')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_get_conditionally_required_args", "docstring": "List arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition", "ques_id": "6306092d73426c38ae68ad08", "code": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    condition = self._get_command_condition(command_name, options_spec)\n    if condition:\n        args = [arg for arg in args if condition(arg)]\n    return args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    matched_args = []\n    for arg_name, arg in zip(args, options_spec):\n        if arg['required_when'](arg_name, args):\n            matched_args.append(arg_name)\n    return matched_args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition (if any)\n    \"\"\"\n    arg_names = []\n    for arg_name, arg in zip(args, options_spec):\n        if self._match_condition(arg, arg_name, command_name):\n            arg_names.append(arg_name)\n    return arg_names\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    args_list = []\n    for item in args:\n        if self._condition(item, required=True):\n            args_list.append(item)\n    return args_list\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    arg_names = []\n    for condition, arg in zip(self.conditions, args):\n        if condition(arg):\n            arg_names.append(arg)\n    return arg_names, [name for name in options_spec if name in arg_names]\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    args = self._get_conditionally_filtered_args(command_name, options_spec,\n                                                                                                    args)\n\n    required_args = []\n    for arg_name, arg_value in args.items():\n        if arg_value is None:\n            continue\n        if self.required_when(arg_name, arg_value):\n            required_args.append(arg_name)\n\n    return required_args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    condition_spec = self._get_condition_spec(command_name, options_spec)\n    condition_args = condition_spec['args']\n\n    # If no condition matches required arguments then return arguments with\n    # ``required_when`` condition set to ``True``\n    if not condition_args:\n        return args\n\n    # If there are condition matches required arguments, then return condition\n    # arguments with ``required_when`` condition set to ``True``\n    for arg_name, arg_value in zip(condition_args, args):\n        if condition_spec['required_when'](arg_name, arg_value):\n            return [arg_name]\n\n    # If no condition matches required arguments t", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    condition = self._get_condition(command_name, options_spec, args)\n\n    return condition.required_when(args)\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    conditions = []\n    for condition in self._conditions:\n        if condition.satisfied(command_name, options_spec, args):\n            conditions.append(condition.name)\n\n    return conditions\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    args_names = []\n    for arg_name, param in self._command_params[command_name].items():\n        if param['required_when'](args, param['condition']):\n            args_names.append(arg_name)\n    return args_names\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    condition = self._get_command_condition(command_name, options_spec)\n    if condition:\n        args = [arg for arg in args if condition(arg)]\n    return args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    matched_args = []\n    for arg_name, arg in zip(args, options_spec):\n        if arg['required_when'](arg_name, args):\n            matched_args.append(arg_name)\n    return matched_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition (if any)\n    \"\"\"\n    arg_names = []\n    for arg_name, arg in zip(args, options_spec):\n        if self._match_condition(arg, arg_name, command_name):\n            arg_names.append(arg_name)\n    return arg_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    args_list = []\n    for item in args:\n        if self._condition(item, required=True):\n            args_list.append(item)\n    return args_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    arg_names = []\n    for condition, arg in zip(self.conditions, args):\n        if condition(arg):\n            arg_names.append(arg)\n    return arg_names, [name for name in options_spec if name in arg_names]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    args = self._get_conditionally_filtered_args(command_name, options_spec,\n                                                                                                    args)\n\n    required_args = []\n    for arg_name, arg_value in args.items():\n        if arg_value is None:\n            continue\n        if self.required_when(arg_name, arg_value):\n            required_args.append(arg_name)\n\n    return required_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    condition_spec = self._get_condition_spec(command_name, options_spec)\n    condition_args = condition_spec['args']\n\n    # If no condition matches required arguments then return arguments with\n    # ``required_when`` condition set to ``True``\n    if not condition_args:\n        return args\n\n    # If there are condition matches required arguments, then return condition\n    # arguments with ``required_when`` condition set to ``True``\n    for arg_name, arg_value in zip(condition_args, args):\n        if condition_spec['required_when'](arg_name, arg_value):\n            return [arg_name]\n\n    # If no condition matches required arguments t", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    condition = self._get_condition(command_name, options_spec, args)\n\n    return condition.required_when(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    conditions = []\n    for condition in self._conditions:\n        if condition.satisfied(command_name, options_spec, args):\n            conditions.append(condition.name)\n\n    return conditions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n             condition\n    \"\"\"\n    args_names = []\n    for arg_name, param in self._command_params[command_name].items():\n        if param['required_when'](args, param['condition']):\n            args_names.append(arg_name)\n    return args_names\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_length_args", "docstring": "Check if value of arguments is not longer than length specified.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad09", "code": ["def validate_length_args(self, args):\n    if len(args) > self.config['length']:\n        raise InvalidArguments('Too long for {0} parameter.'.format(\n            self.config['length']\n        ))\n", "def validate_length_args(self, args):\n    length = args.get('length', 0)\n    if length < 0:\n        raise ValidationError('Length must be greater than zero.')\n", "def validate_length_args(self, args):\n    if len(args) < self.length:\n        raise BadRequest(\"Not enough arguments to specify %s of type %s\" %\n                         (self.name, self.type))\n", "def validate_length_args(self, args):\n    length = args[0]\n    if length is None or length <= 0:\n        raise ValueError(\"Length of arguments must be greater than 0\")\n", "def validate_length_args(self, args):\n    if len(args) > self.length:\n        raise InvalidLengthError('The length argument must be less than {}.'.format(self.length))\n", "def validate_length_args(self, args):\n    arg_length = len(args)\n    if arg_length > self.length:\n        raise BadRequest('Not enough arguments for {}'.format(self.__class__.__name__))\n", "def validate_length_args(self, args):\n    if len(args) > self.length:\n        self._error(\"Not enough arguments.\")\n", "def validate_length_args(self, args):\n    if len(args) > self.__length:\n        raise ValidationError('{} argument(s) exceeds length of {}.'.format(len(args), self.__length))\n", "def validate_length_args(self, args):\n    length = args['length']\n    if length is not None and length > self.max_length:\n        raise BadRequest(\"Length is greater than maximum allowed.\")\n", "def validate_length_args(self, args):\n    if len(args) > self.__length:\n        raise ValueError('Not enough arguments.')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_length_args(self, args):\n    if len(args) > self.config['length']:\n        raise InvalidArguments('Too long for {0} parameter.'.format(\n            self.config['length']\n        ))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    length = args.get('length', 0)\n    if length < 0:\n        raise ValidationError('Length must be greater than zero.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args) < self.length:\n        raise BadRequest(\"Not enough arguments to specify %s of type %s\" %\n                         (self.name, self.type))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    length = args[0]\n    if length is None or length <= 0:\n        raise ValueError(\"Length of arguments must be greater than 0\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args) > self.length:\n        raise InvalidLengthError('The length argument must be less than {}.'.format(self.length))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    arg_length = len(args)\n    if arg_length > self.length:\n        raise BadRequest('Not enough arguments for {}'.format(self.__class__.__name__))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args) > self.length:\n        self._error(\"Not enough arguments.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args) > self.__length:\n        raise ValidationError('{} argument(s) exceeds length of {}.'.format(len(args), self.__length))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    length = args['length']\n    if length is not None and length > self.max_length:\n        raise BadRequest(\"Length is greater than maximum allowed.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n    if len(args) > self.__length:\n        raise ValueError('Not enough arguments.')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_choices_args", "docstring": "Check if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad0a", "code": ["def validate_choices_args(self, args):\n    choices = self.get_choices()\n    if args not in choices:\n        raise ArgumentError('{} is not a valid choice'.format(args))\n", "def validate_choices_args(self, args):\n    choices = args['choices']\n\n    if len(choices) == 0:\n        return\n\n    for choice in choices:\n        if choice not in self._choices:\n            raise InvalidChoiceException(\n                '{} is not a valid choice.'.format(choice))\n", "def validate_choices_args(self, args):\n    choices = args.get('choices')\n\n    if not choices:\n        return\n\n    for choice in choices:\n        if choice not in self.CHOICES:\n            self._error(\n                'Must specify \"{}\" (one of {})'.format(\n                    choice, ', '.join(self.CHOICES)))\n", "def validate_choices_args(self, args):\n    if args is None:\n        return\n\n    for arg in args:\n        if arg not in self.choices:\n            raise ArgumentError(\n                '{0} is not a valid choice. Must be one of {1}'\n               .format(arg, self.choices)\n            )\n", "def validate_choices_args(self, args):\n    if args['--choices'] and args['--choices'] not in self.CHOICES:\n        self.fail('choices argument must be one of {}'.format(\n            self.CHOICES))\n", "def validate_choices_args(self, args):\n    choices = self.get_choices()\n\n    if args[0] not in choices:\n        raise ValidationError(\n            \"'{}' is not a valid choice for {}\".format(\n                args[0], self.__class__.__name__))\n", "def validate_choices_args(self, args):\n    for choice in self.choices:\n        if choice.value == args:\n            return choice\n", "def validate_choices_args(self, args):\n    for choice_name, choice_args in self.choices.items():\n        if choice_args is not None:\n            if args[choice_name] not in choice_args:\n                self._error(\n                    field=self._get_field_name(),\n                    message='{} must be one of {}'.format(\n                        choice_name, ', '.join(choice_args)\n                    )\n                )\n", "def validate_choices_args(self, args):\n    choices = self.get_choices()\n    if args not in choices:\n        raise ArgumentTypeError(\"{} is not a valid choice\".format(args))\n", "def validate_choices_args(self, args):\n    choices = self.get_choices()\n    if args not in choices:\n        self._error(\"{} is not a valid choice\".format(args))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_choices_args(self, args):\n    choices = self.get_choices()\n    if args not in choices:\n        raise ArgumentError('{} is not a valid choice'.format(args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    choices = args['choices']\n\n    if len(choices) == 0:\n        return\n\n    for choice in choices:\n        if choice not in self._choices:\n            raise InvalidChoiceException(\n                '{} is not a valid choice.'.format(choice))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    choices = args.get('choices')\n\n    if not choices:\n        return\n\n    for choice in choices:\n        if choice not in self.CHOICES:\n            self._error(\n                'Must specify \"{}\" (one of {})'.format(\n                    choice, ', '.join(self.CHOICES)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    if args is None:\n        return\n\n    for arg in args:\n        if arg not in self.choices:\n            raise ArgumentError(\n                '{0} is not a valid choice. Must be one of {1}'\n               .format(arg, self.choices)\n            )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    if args['--choices'] and args['--choices'] not in self.CHOICES:\n        self.fail('choices argument must be one of {}'.format(\n            self.CHOICES))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    choices = self.get_choices()\n\n    if args[0] not in choices:\n        raise ValidationError(\n            \"'{}' is not a valid choice for {}\".format(\n                args[0], self.__class__.__name__))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    for choice in self.choices:\n        if choice.value == args:\n            return choice\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    for choice_name, choice_args in self.choices.items():\n        if choice_args is not None:\n            if args[choice_name] not in choice_args:\n                self._error(\n                    field=self._get_field_name(),\n                    message='{} must be one of {}'.format(\n                        choice_name, ', '.join(choice_args)\n                    )\n                )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    choices = self.get_choices()\n    if args not in choices:\n        raise ArgumentTypeError(\"{} is not a valid choice\".format(args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n    choices = self.get_choices()\n    if args not in choices:\n        self._error(\"{} is not a valid choice\".format(args))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_min_max_args", "docstring": "Check if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad0b", "code": ["def validate_min_max_args(self, args):\n    if args is None or args == '':\n        return\n\n    min_value = args.get('min', None)\n    max_value = args.get('max', None)\n\n    if min_value is None:\n        return\n\n    if max_value is None:\n        return\n\n    if min_value > max_value:\n        raise InvalidArgumentException('Minimum value cannot be greater than maximum value.')\n", "def validate_min_max_args(self, args):\n    args = list(args)\n    if len(args) < 2:\n        return\n    try:\n        min_val, max_val = args[0], args[1]\n    except IndexError:\n        raise ValueError(\"Minimum value is not specified.\")\n    if min_val < self.minimum_value and max_val > self.maximum_value:\n        raise ValueError(\"Minimum value must be less than maximum value.\")\n", "def validate_min_max_args(self, args):\n    min_value = args[self._arg_name]\n    max_value = args[self._arg_name]\n    if not (min_value is None or min_value >= self._min_value):\n        raise InvalidArgument('Minimum value is not greater than {}'.format(self._min_value))\n    if not (max_value is None or max_value <= self._max_value):\n        raise InvalidArgument('Maximum value is not greater than {}'.format(self._max_value))\n", "def validate_min_max_args(self, args):\n    try:\n        if args[0] < self._args[0] or args[0] > self._args[1]:\n            raise ValueError()\n        if args[1] < self._args[1] or args[1] > self._args[2]:\n            raise ValueError()\n    except IndexError:\n        raise ValueError()\n", "def validate_min_max_args(self, args):\n    try:\n        if args[0] < args[1]:\n            raise ValueError\n    except IndexError:\n        raise ValueError(\"Minimum value of arguments must be greater \"\n                         \"than Maximum value.\")\n", "def validate_min_max_args(self, args):\n    if args is None:\n        return\n    for arg in args:\n        if not self._validate_range_arg(arg):\n            return False\n    return True\n", "def validate_min_max_args(self, args):\n    if args is not None and len(args) > 0:\n        if args[0] < self._min or args[0] > self._max:\n            self.error('Minimum is {} and maximum is {}'.format(self._min, self._max))\n", "def validate_min_max_args(self, args):\n    if self._validate_args_type(args, int) and not (self._validate_args_type(args, float) and self._validate_args_type(args, str)):\n        if self._validate_args_type(args, float) and self._validate_args_type(args, str) and self._validate_args_type(args, int):\n            if self._validate_args_min(args) and self._validate_args_max(args):\n                if self._validate_args_min(args) > self._validate_args_max(args):\n                    raise ValueError(\"Minimum value of parameter is greater than maximum value.\")\n            elif self._validate_args_min(args) and not self._validate_args_min(args):\n                raise ValueError(\"Minimum value of parameter is not greater than maximum value.\")\n            elif self._validate_args_max(args) and not self._validate_args_max(args):\n                raise ValueError(\"Maximum value of parameter is greater than mini", "def validate_min_max_args(self, args):\n    if args is None:\n        return\n\n    if (args[0] < self.min_value or args[0] > self.max_value):\n        raise InvalidArgument('Minimum value must be within range {0} and {1}.'.format(self.min_value, self.max_value))\n", "def validate_min_max_args(self, args):\n    try:\n        min_val = float(args[0])\n        max_val = float(args[1])\n        if min_val < self.__parameter.minimum and max_val > self.__parameter.maximum:\n            raise ValueError()\n    except ValueError:\n        self.__logger.error(\"Minimum value for parameter %s is not within the range of %f and %f.\",\n                            self.__parameter.name, self.__parameter.minimum, self.__parameter.maximum)\n        self.__logger.error(\"Valid range is (%f, %f)\", self.__parameter.minimum, self.__parameter.maximum)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_min_max_args(self, args):\n    if args is None or args == '':\n        return\n\n    min_value = args.get('min', None)\n    max_value = args.get('max', None)\n\n    if min_value is None:\n        return\n\n    if max_value is None:\n        return\n\n    if min_value > max_value:\n        raise InvalidArgumentException('Minimum value cannot be greater than maximum value.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    args = list(args)\n    if len(args) < 2:\n        return\n    try:\n        min_val, max_val = args[0], args[1]\n    except IndexError:\n        raise ValueError(\"Minimum value is not specified.\")\n    if min_val < self.minimum_value and max_val > self.maximum_value:\n        raise ValueError(\"Minimum value must be less than maximum value.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    min_value = args[self._arg_name]\n    max_value = args[self._arg_name]\n    if not (min_value is None or min_value >= self._min_value):\n        raise InvalidArgument('Minimum value is not greater than {}'.format(self._min_value))\n    if not (max_value is None or max_value <= self._max_value):\n        raise InvalidArgument('Maximum value is not greater than {}'.format(self._max_value))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    try:\n        if args[0] < self._args[0] or args[0] > self._args[1]:\n            raise ValueError()\n        if args[1] < self._args[1] or args[1] > self._args[2]:\n            raise ValueError()\n    except IndexError:\n        raise ValueError()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    try:\n        if args[0] < args[1]:\n            raise ValueError\n    except IndexError:\n        raise ValueError(\"Minimum value of arguments must be greater \"\n                         \"than Maximum value.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    if args is None:\n        return\n    for arg in args:\n        if not self._validate_range_arg(arg):\n            return False\n    return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    if args is not None and len(args) > 0:\n        if args[0] < self._min or args[0] > self._max:\n            self.error('Minimum is {} and maximum is {}'.format(self._min, self._max))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    if self._validate_args_type(args, int) and not (self._validate_args_type(args, float) and self._validate_args_type(args, str)):\n        if self._validate_args_type(args, float) and self._validate_args_type(args, str) and self._validate_args_type(args, int):\n            if self._validate_args_min(args) and self._validate_args_max(args):\n                if self._validate_args_min(args) > self._validate_args_max(args):\n                    raise ValueError(\"Minimum value of parameter is greater than maximum value.\")\n            elif self._validate_args_min(args) and not self._validate_args_min(args):\n                raise ValueError(\"Minimum value of parameter is not greater than maximum value.\")\n            elif self._validate_args_max(args) and not self._validate_args_max(args):\n                raise ValueError(\"Maximum value of parameter is greater than mini", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    if args is None:\n        return\n\n    if (args[0] < self.min_value or args[0] > self.max_value):\n        raise InvalidArgument('Minimum value must be within range {0} and {1}.'.format(self.min_value, self.max_value))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n    try:\n        min_val = float(args[0])\n        max_val = float(args[1])\n        if min_val < self.__parameter.minimum and max_val > self.__parameter.maximum:\n            raise ValueError()\n    except ValueError:\n        self.__logger.error(\"Minimum value for parameter %s is not within the range of %f and %f.\",\n                            self.__parameter.name, self.__parameter.minimum, self.__parameter.maximum)\n        self.__logger.error(\"Valid range is (%f, %f)\", self.__parameter.minimum, self.__parameter.maximum)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "create_complex_argumet_type", "docstring": "Build the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance", "ques_id": "6306092e73426c38ae68ad0d", "code": ["def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    if spec_option is None:\n        spec_option = {}\n    return self._create_argument_type(subcommand, type_name, option_name,\n                                      spec_option, self._complex_spec,\n                                      self._is_required)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # create the argument definition\n    argument_definition = {\n        'name': type_name,\n        'description': 'A complex type for'+ type_name,\n       'required': False,\n        'type':'string',\n        'optional': False,\n       'multiple': False,\n        'choices': spec_option['choices'],\n        'default': None\n    }\n    # create the sub-command argument\n    sub_argument_definition = {\n        'name': subcommand,\n        'description': 'A complex sub-command for'+ subcommand,\n       'required': False,\n        'type':'string',\n        'choices': spec_option['commands'][subcommand],\n    ", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    if subcommand not in self.subcommands:\n        raise ApplicationError('subcommand %s not found' % subcommand)\n    if type_name not in self.subcommands[subcommand]:\n        raise ApplicationError('type_name %s not found' % type_name)\n    if option_name not in self.subcommands[subcommand][type_name]:\n        raise ApplicationError('option_name %s not found' % option_name)\n    if spec_option not in self.subcommands[subcommand][type_name][option_name]:\n        raise ApplicationError('spec_option %s not found' % spec_option)\n    return self.subcommands[subcommand][type_name][option_name][spec_option]\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    subcommand_name = self._get_subcommand_name(subcommand)\n    type_name = self._get_type_name(type_name)\n    option_name = self._get_option_name(option_name)\n    spec_option = self._get_spec_option_name(spec_option)\n\n    complex_type = self.get_class_instance(complex, subcommand_name, type_name)\n    if spec_option is not None:\n        complex_type = self._build_spec_option(complex_type, spec_option)\n\n    return complex_type\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    if self.__is_valid_complex_type(subcommand, type_name, option_name, spec_option):\n        return self.__build_complex_type(subcommand, type_name, option_name, spec_option)\n    else:\n        return None\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # noinspection PyTypeChecker\n    return self._build_complex_type(subcommand, type_name, option_name,\n                                     spec_option, False)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    complex_type = self._create_complex_type(subcommand, type_name, option_name,\n                                            spec_option)\n    self._add_argument(subcommand, complex_type, option_name, spec_option)\n    return complex_type\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    type_class = self.get_complex_type_class(type_name)\n    return type_class(subcommand, option_name, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    complex_type = self.__create_complex_type(subcommand, type_name, option_name)\n    if spec_option:\n        self.__add_option(complex_type, spec_option)\n    return complex_type\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # build the specification\n    spec = self._get_specification(subcommand, type_name, option_name)\n    # create the type\n    t = self._get_complex_type(spec['cls'], spec_option)\n    # return the type\n    return t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    if spec_option is None:\n        spec_option = {}\n    return self._create_argument_type(subcommand, type_name, option_name,\n                                      spec_option, self._complex_spec,\n                                      self._is_required)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # create the argument definition\n    argument_definition = {\n        'name': type_name,\n        'description': 'A complex type for'+ type_name,\n       'required': False,\n        'type':'string',\n        'optional': False,\n       'multiple': False,\n        'choices': spec_option['choices'],\n        'default': None\n    }\n    # create the sub-command argument\n    sub_argument_definition = {\n        'name': subcommand,\n        'description': 'A complex sub-command for'+ subcommand,\n       'required': False,\n        'type':'string',\n        'choices': spec_option['commands'][subcommand],\n    ", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    if subcommand not in self.subcommands:\n        raise ApplicationError('subcommand %s not found' % subcommand)\n    if type_name not in self.subcommands[subcommand]:\n        raise ApplicationError('type_name %s not found' % type_name)\n    if option_name not in self.subcommands[subcommand][type_name]:\n        raise ApplicationError('option_name %s not found' % option_name)\n    if spec_option not in self.subcommands[subcommand][type_name][option_name]:\n        raise ApplicationError('spec_option %s not found' % spec_option)\n    return self.subcommands[subcommand][type_name][option_name][spec_option]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    subcommand_name = self._get_subcommand_name(subcommand)\n    type_name = self._get_type_name(type_name)\n    option_name = self._get_option_name(option_name)\n    spec_option = self._get_spec_option_name(spec_option)\n\n    complex_type = self.get_class_instance(complex, subcommand_name, type_name)\n    if spec_option is not None:\n        complex_type = self._build_spec_option(complex_type, spec_option)\n\n    return complex_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    if self.__is_valid_complex_type(subcommand, type_name, option_name, spec_option):\n        return self.__build_complex_type(subcommand, type_name, option_name, spec_option)\n    else:\n        return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # noinspection PyTypeChecker\n    return self._build_complex_type(subcommand, type_name, option_name,\n                                     spec_option, False)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    complex_type = self._create_complex_type(subcommand, type_name, option_name,\n                                            spec_option)\n    self._add_argument(subcommand, complex_type, option_name, spec_option)\n    return complex_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    type_class = self.get_complex_type_class(type_name)\n    return type_class(subcommand, option_name, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    complex_type = self.__create_complex_type(subcommand, type_name, option_name)\n    if spec_option:\n        self.__add_option(complex_type, spec_option)\n    return complex_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n    \"\"\"\n    Build the complex argument type\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # build the specification\n    spec = self._get_specification(subcommand, type_name, option_name)\n    # create the type\n    t = self._get_complex_type(spec['cls'], spec_option)\n    # return the type\n    return t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_nested_custom_and_control_args", "docstring": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)", "ques_id": "6306092e73426c38ae68ad0f", "code": ["def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for arg in args:\n        if arg.startswith('--'):\n            nested_args[arg[2:]] = args.pop(arg)\n        elif arg.startswith('-'):\n            control_args[arg[1:]] = args.pop(arg)\n    return control_args, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    custom = {}\n    nested_args = {}\n    for arg in args:\n        if arg.startswith('--'):\n            if arg.startswith('--custom='):\n                nested_args.update(yaml.load(arg[7:]))\n            elif arg.startswith('--nested-custom='):\n                control_args = yaml.load(arg[8:])\n                nested_args = control_args.pop('vars', {})\n        else:\n            custom.update(yaml.load(arg))\n    return custom, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    if 'control' in args:\n        return args['control'], args\n    elif 'are' in args:\n        return args['are'], args\n    else:\n        return {}, {}\n", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n\n    if args.get('--variable-list'):\n        for variable in args.get('--variable-list'):\n            if variable.get('name'):\n                nested_args[variable.get('name')] = {}\n            else:\n                control_args[variable.get('name')] = {}\n\n    if args.get('--variable'):\n        for variable in args.get('--variable'):\n            if variable.get('name'):\n                control_args[variable.get('name')] = {}\n            else:\n                nested_args[variable.get('name')] = {}\n\n    return control_args, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for k, v in args.items():\n        if k in ('control', 'are'):\n            control_args.update(v)\n        elif k in ('custom', 'vars'):\n            nested_args.update(v)\n    return control_args, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for k, v in args.items():\n        if isinstance(v, dict):\n            nested_args[k] = v\n        else:\n            control_args[k] = v\n    return control_args, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for k, v in vars(args).items():\n        if isinstance(v, dict):\n            nested_args[k] = v\n        elif isinstance(v, list):\n            control_args[k] = v\n        else:\n            nested_args[k] = v\n    return control_args, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for arg in args:\n        if 'args' in arg:\n            control_args.update(arg['args'])\n        elif 'playbook_arguments' in arg:\n            nested_args.update(arg['playbook_arguments'])\n\n    return control_args, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for arg in args:\n        if arg[0] == '--':\n            if arg[1] in ['--variable', '--variable_from_cli', '--variable_from_ansible']:\n                nested_args.update(self.get_variable_args(args))\n            else:\n                control_args.update(self.get_custom_args(args))\n        else:\n            nested_args[arg[0]] = arg[1]\n    return control_args, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n\n    if args is not None:\n        for arg in args:\n            if arg[0] == '--':\n                if arg[1] == 'control':\n                    control_args = args\n                else:\n                    nested_args = args\n            else:\n                nested_args[arg[0]] = arg[1:]\n\n    return control_args, nested_args\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for arg in args:\n        if arg.startswith('--'):\n            nested_args[arg[2:]] = args.pop(arg)\n        elif arg.startswith('-'):\n            control_args[arg[1:]] = args.pop(arg)\n    return control_args, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    custom = {}\n    nested_args = {}\n    for arg in args:\n        if arg.startswith('--'):\n            if arg.startswith('--custom='):\n                nested_args.update(yaml.load(arg[7:]))\n            elif arg.startswith('--nested-custom='):\n                control_args = yaml.load(arg[8:])\n                nested_args = control_args.pop('vars', {})\n        else:\n            custom.update(yaml.load(arg))\n    return custom, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    if 'control' in args:\n        return args['control'], args\n    elif 'are' in args:\n        return args['are'], args\n    else:\n        return {}, {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n\n    if args.get('--variable-list'):\n        for variable in args.get('--variable-list'):\n            if variable.get('name'):\n                nested_args[variable.get('name')] = {}\n            else:\n                control_args[variable.get('name')] = {}\n\n    if args.get('--variable'):\n        for variable in args.get('--variable'):\n            if variable.get('name'):\n                control_args[variable.get('name')] = {}\n            else:\n                nested_args[variable.get('name')] = {}\n\n    return control_args, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for k, v in args.items():\n        if k in ('control', 'are'):\n            control_args.update(v)\n        elif k in ('custom', 'vars'):\n            nested_args.update(v)\n    return control_args, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for k, v in args.items():\n        if isinstance(v, dict):\n            nested_args[k] = v\n        else:\n            control_args[k] = v\n    return control_args, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for k, v in vars(args).items():\n        if isinstance(v, dict):\n            nested_args[k] = v\n        elif isinstance(v, list):\n            control_args[k] = v\n        else:\n            nested_args[k] = v\n    return control_args, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for arg in args:\n        if 'args' in arg:\n            control_args.update(arg['args'])\n        elif 'playbook_arguments' in arg:\n            nested_args.update(arg['playbook_arguments'])\n\n    return control_args, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    for arg in args:\n        if arg[0] == '--':\n            if arg[1] in ['--variable', '--variable_from_cli', '--variable_from_ansible']:\n                nested_args.update(self.get_variable_args(args))\n            else:\n                control_args.update(self.get_custom_args(args))\n        else:\n            nested_args[arg[0]] = arg[1]\n    return control_args, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n\n    if args is not None:\n        for arg in args:\n            if arg[0] == '--':\n                if arg[1] == 'control':\n                    control_args = args\n                else:\n                    nested_args = args\n            else:\n                nested_args[arg[0]] = arg[1:]\n\n    return control_args, nested_args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/settings.py", "project": "redhat-openstack/infrared", "name": "merge_extra_vars", "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars", "ques_id": "6306092e73426c38ae68ad11", "code": ["def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(vars_dict, dict):\n        for key, value in vars_dict.items():\n            if key not in extra_vars:\n                extra_vars.append(key)\n            else:\n                extra_vars[extra_vars.index(key)] = value\n\n    return vars_dict.update(extra_vars)\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if vars_dict is None:\n        return vars_dict\n\n    extra_vars = extra_vars or []\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict.update(extra_var)\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = os.environ.get(extra_var, '')\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, dict):\n        for key, value in extra_vars.items():\n            if key not in vars_dict:\n                vars_dict[key] = value\n            else:\n                vars_dict[key] = merge_extra_vars(vars_dict[key], value)\n    elif extra_vars and isinstance(extra_vars[0], dict):\n        vars_dict.update(extra_vars[0])\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if vars_dict is None:\n        return vars_dict\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = vars_dict['default']\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if vars_dict is None:\n        return extra_vars\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = os.environ.get(extra_var)\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, dict):\n        vars_dict.update(extra_vars)\n    else:\n        vars_dict.update(vars_dict)\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, str):\n        extra_vars = [extra_vars]\n\n    vars_dict.update(extra_vars)\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = {}\n\n        vars_dict[extra_var] = vars_dict[extra_var] or {}\n\n    return vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, str):\n        extra_vars = [extra_vars]\n\n    vars_dict.update(dict(\n        (key, value)\n        for key, value in vars_dict.items()\n        if key not in extra_vars\n    ))\n\n    return vars_dict\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(vars_dict, dict):\n        for key, value in vars_dict.items():\n            if key not in extra_vars:\n                extra_vars.append(key)\n            else:\n                extra_vars[extra_vars.index(key)] = value\n\n    return vars_dict.update(extra_vars)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if vars_dict is None:\n        return vars_dict\n\n    extra_vars = extra_vars or []\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict.update(extra_var)\n\n    return vars_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = os.environ.get(extra_var, '')\n\n    return vars_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, dict):\n        for key, value in extra_vars.items():\n            if key not in vars_dict:\n                vars_dict[key] = value\n            else:\n                vars_dict[key] = merge_extra_vars(vars_dict[key], value)\n    elif extra_vars and isinstance(extra_vars[0], dict):\n        vars_dict.update(extra_vars[0])\n    return vars_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if vars_dict is None:\n        return vars_dict\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = vars_dict['default']\n\n    return vars_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if vars_dict is None:\n        return extra_vars\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = os.environ.get(extra_var)\n\n    return vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, dict):\n        vars_dict.update(extra_vars)\n    else:\n        vars_dict.update(vars_dict)\n\n    return vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, str):\n        extra_vars = [extra_vars]\n\n    vars_dict.update(extra_vars)\n\n    return vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    for extra_var in extra_vars:\n        if extra_var not in vars_dict:\n            vars_dict[extra_var] = {}\n\n        vars_dict[extra_var] = vars_dict[extra_var] or {}\n\n    return vars_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n    if extra_vars is None:\n        extra_vars = []\n\n    if isinstance(extra_vars, str):\n        extra_vars = [extra_vars]\n\n    vars_dict.update(dict(\n        (key, value)\n        for key, value in vars_dict.items()\n        if key not in extra_vars\n    ))\n\n    return vars_dict\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "ansible_playbook", "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.", "ques_id": "6306092f73426c38ae68ad13", "code": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Get the current playbook name\n    playbook_name = os.path.basename(playbook_path)\n\n    # Inject the variables to the arguments\n    if extra_vars is not None:\n        ansible_args['extra_vars'] = extra_vars\n\n    # Run the playbook\n    playbook = 'ansible-playbook -vvvvvv --vvvvvv {}'.format(playbook_name)\n    if verbose is not None:\n        ansible_args['verbosity'] = verbose\n    if ansible_args is not None:\n   ", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    if ansible_args is None:\n        ansible_args = dict()\n\n    # If the 'ansible-playbook' command is not present in the current\n    # plugin settings, we assume it's being run directly\n    if ir_workspace.settings.get('ansible-playbook', None):\n        ir_workspace.settings['ansible-playbook'] = True\n        return ansible_playbook_run(\n            ir_workspace,\n        ", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Initialize Ansible options for the playbook\n    ansible_args = ansible_args or {}\n    ansible_args['verbosity'] = verbose\n    ansible_args['extra_vars'] = extra_vars\n\n    # Execute Ansible playbook\n    return call_ansible(ir_workspace, ir_plugin, playbook_path, **ansible_args)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Import the plugin module\n    importlib.import_module('ansible.plugins.ansible_playbook')\n\n    # Initialize the arguments that will be passed to the ansible playbook\n    args = {\n        'ir_workspace': ir_workspace,\n        'ir_plugin': ir_plugin,\n        'playbook_path': playbook_path,\n       'verbose': verbose,\n        'extra_vars': extra_vars,\n     ", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    ir_hosts = ir_workspace.get_hosts()\n    ir_playbook = ir_workspace.get_playbook(playbook_path)\n\n    # Assert that the playbook is valid\n    if ir_playbook is None:\n        raise InfraredPluginError(\n            \"'{}' playbook is not present in the '{}' workspace.\".format(\n                playbook_path, ir_workspace.name))\n\n    # Assert that the plugin is valid\n    if ir_plugin is None:\n        raise InfraredPlugi", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Get the current playbook directory\n    playbook_dir = os.path.dirname(playbook_path)\n\n    # Run the 'ansible-playbook' CLI\n    playbook_args = [\n        '-i', ir_workspace.id,\n        '-vvvv',\n        '--limit-to=%s' % ir_workspace.workflow_limit,\n        '--limit-reserved=%s' % ir_workspace.workflow_limit,\n        '--sudo-skip=tru", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    if extra_vars is None:\n        extra_vars = {}\n    if ansible_args is None:\n        ansible_args = {}\n    if verbose is None:\n        verbose = ir_plugin.verbose\n\n    # Invoke the 'ansible-playbook' CLI\n    playbook_cmd = ['ansible-playbook', playbook_path, '-vvv']\n\n    # Extra variables to pass to the Ansible CLI\n    for key, value", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    workspace_plugin = ir_workspace.get_plugin('infrared')\n    workspace_plugin.start_playbook(playbook_path, ir_plugin, verbose, extra_vars,\n                                   ansible_args)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly\n                         to Ansible.\n    \"\"\"\n    # TODO: Improve the call to the calling function in the fact that the playbook\n    #       needs to be a single file, not a list of files\n    # TODO: Improve the call to the calling function in the fact that the playbook\n    #       needs to be a list of files\n    # TODO: Improve the call to the calling function in the fact that the playbook\n    #       needs", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Run the playbook\n    return run_playbook(ir_workspace, ir_plugin, playbook_path,\n                        verbose=verbose, extra_vars=extra_vars,\n                        ansible_args=ansible_args)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Get the current playbook name\n    playbook_name = os.path.basename(playbook_path)\n\n    # Inject the variables to the arguments\n    if extra_vars is not None:\n        ansible_args['extra_vars'] = extra_vars\n\n    # Run the playbook\n    playbook = 'ansible-playbook -vvvvvv --vvvvvv {}'.format(playbook_name)\n    if verbose is not None:\n        ansible_args['verbosity'] = verbose\n    if ansible_args is not None:\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    if ansible_args is None:\n        ansible_args = dict()\n\n    # If the 'ansible-playbook' command is not present in the current\n    # plugin settings, we assume it's being run directly\n    if ir_workspace.settings.get('ansible-playbook', None):\n        ir_workspace.settings['ansible-playbook'] = True\n        return ansible_playbook_run(\n            ir_workspace,\n        ", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Initialize Ansible options for the playbook\n    ansible_args = ansible_args or {}\n    ansible_args['verbosity'] = verbose\n    ansible_args['extra_vars'] = extra_vars\n\n    # Execute Ansible playbook\n    return call_ansible(ir_workspace, ir_plugin, playbook_path, **ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Import the plugin module\n    importlib.import_module('ansible.plugins.ansible_playbook')\n\n    # Initialize the arguments that will be passed to the ansible playbook\n    args = {\n        'ir_workspace': ir_workspace,\n        'ir_plugin': ir_plugin,\n        'playbook_path': playbook_path,\n       'verbose': verbose,\n        'extra_vars': extra_vars,\n     ", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    ir_hosts = ir_workspace.get_hosts()\n    ir_playbook = ir_workspace.get_playbook(playbook_path)\n\n    # Assert that the playbook is valid\n    if ir_playbook is None:\n        raise InfraredPluginError(\n            \"'{}' playbook is not present in the '{}' workspace.\".format(\n                playbook_path, ir_workspace.name))\n\n    # Assert that the plugin is valid\n    if ir_plugin is None:\n        raise InfraredPlugi", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Get the current playbook directory\n    playbook_dir = os.path.dirname(playbook_path)\n\n    # Run the 'ansible-playbook' CLI\n    playbook_args = [\n        '-i', ir_workspace.id,\n        '-vvvv',\n        '--limit-to=%s' % ir_workspace.workflow_limit,\n        '--limit-reserved=%s' % ir_workspace.workflow_limit,\n        '--sudo-skip=tru", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    if extra_vars is None:\n        extra_vars = {}\n    if ansible_args is None:\n        ansible_args = {}\n    if verbose is None:\n        verbose = ir_plugin.verbose\n\n    # Invoke the 'ansible-playbook' CLI\n    playbook_cmd = ['ansible-playbook', playbook_path, '-vvv']\n\n    # Extra variables to pass to the Ansible CLI\n    for key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    workspace_plugin = ir_workspace.get_plugin('infrared')\n    workspace_plugin.start_playbook(playbook_path, ir_plugin, verbose, extra_vars,\n                                   ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly\n                         to Ansible.\n    \"\"\"\n    # TODO: Improve the call to the calling function in the fact that the playbook\n    #       needs to be a single file, not a list of files\n    # TODO: Improve the call to the calling function in the fact that the playbook\n    #       needs to be a list of files\n    # TODO: Improve the call to the calling function in the fact that the playbook\n    #       needs", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An InfraredWorkspace object represents the active\n        workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Run the playbook\n    return run_playbook(ir_workspace, ir_plugin, playbook_path,\n                        verbose=verbose, extra_vars=extra_vars,\n                        ansible_args=ansible_args)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "_run_playbook", "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results", "ques_id": "6306093273426c38ae68ad15", "code": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_results = AnsibleResults()\n\n    logger.debug(\"Running ansible with vars {}\".format(vars_dict))\n\n    results = cli_args.inventory_hosts\n    if vars_dict.get(\"ansible_group\"):\n        results = cli_args.inventory_hosts.split(\",\")\n\n    results = [x for x in results if x]\n\n    # TODO: remove duplicated hosts and run them with the same group instead of using the same hosts dict\n    results = list(set(results + [ir_workspace.host]))\n\n    # TODO: remove unnecessary hosts from the inventory and run them with the same group instead of using the same hosts dict\n    results = list(set(results + ir_plugin.inventory.get_inventory_hosts()))\n\n    results = [_run_ansible_vars(vars_dict, x, ir_workspace, ir_plugin) for x in results]\n\n    ansible_results.results = results\n    ansible_results.stdout = cli_args.ansible_output\n    ansible_r", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    if cli_args.check:\n        return _run_check(cli_args, vars_dict, ir_workspace, ir_plugin)\n    return _run_playbook_async(cli_args, vars_dict, ir_workspace, ir_plugin)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    vars_dict.update(ir_plugin.get_extra_vars())\n\n    # Inject the vars into the ansible vars dict\n    for k, v in vars_dict.items():\n        vars_dict[k] = ir_workspace.get_variable(k)\n\n    # Pass the vars dict to Ansible to get the ansible result\n    return cli_args, vars_dict\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # Initialize the ir plugin\n    ir_plugin_instance = ir_plugin(cli_args, ir_workspace, ir_plugin.get_client_plugin())\n    # Initialize the Ansible variables\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_plugin_instance.get_vars())\n    # Initialize the Ansible playbook\n    ansible = Ansible(cli_args, ir_workspace, ir_plugin.get_client_plugin(), ir_plugin_instance.get_token(), ir_workspace.get_token(), ir_plugin_instance.get_vars(), ir_vars=ansible_vars)\n    # Run the playbook\n    return ansible.run()\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    ansible_args = [\n        '-vvv',\n        '-e',\n        'ANSIBLE_SETUP_PY=%s' % ir_plugin.get_ansible_setup_py(),\n    ]\n\n    ansible_args.extend(cli_args)\n\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(vars_dict)\n    ansible_args.extend(['--vars', json.dumps(ansible_vars)])\n\n    return cli_ir_workspace.run_playbook('infrared_cli.yml', ansible_args)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_cli = AnsibleCLI(cli_args, ir_workspace, ir_plugin)\n    return ansible_cli.run_playbook(vars_dict)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, Workspace) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    ansible_vars = vars_dict.copy()\n    ansible_vars[\"ir_workspace\"] = ir_workspace\n    ansible_vars[\"ir_plugin\"] = ir_plugin\n\n    ansible_result = cli_args[0](ansible_vars, ir_workspace, ir_plugin)\n    return cli_args[1:] if cli_args else ansible_result\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_args = []\n    ansible_args.extend(cli_args)\n    ansible_args.extend(['-vvv', '--check-only'])\n    ansible_args.extend(vars_dict)\n\n    return run_ansible_cli(ansible_args, ir_workspace, ir_plugin)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    if cli_args.fail_ok or cli_args.exit_ok:\n        return {'results': False,'message': ''}\n    else:\n        return ansible.run_playbook(cli_args.playbook, vars_dict, ir_workspace, ir_plugin)\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # TODO: get the ansible vars from the cli args\n    # TODO: initialize variables for the ir_workspace\n    # TODO: run the playbook with the variables dict\n    ir_plugin.logger.info('Running playbook with vars %s', vars_dict)\n    ir_workspace.start_session()\n    try:\n        results = ir_workspace.run_playbook(cli_args)\n    except Exception as e:\n        ir_workspace.logger.error('It was not possible to run the playbook, Error: %s', e)\n        raise AnsibleExit(exit_code=1)\n    ir_workspace.stop_session()\n    return results\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_results = AnsibleResults()\n\n    logger.debug(\"Running ansible with vars {}\".format(vars_dict))\n\n    results = cli_args.inventory_hosts\n    if vars_dict.get(\"ansible_group\"):\n        results = cli_args.inventory_hosts.split(\",\")\n\n    results = [x for x in results if x]\n\n    # TODO: remove duplicated hosts and run them with the same group instead of using the same hosts dict\n    results = list(set(results + [ir_workspace.host]))\n\n    # TODO: remove unnecessary hosts from the inventory and run them with the same group instead of using the same hosts dict\n    results = list(set(results + ir_plugin.inventory.get_inventory_hosts()))\n\n    results = [_run_ansible_vars(vars_dict, x, ir_workspace, ir_plugin) for x in results]\n\n    ansible_results.results = results\n    ansible_results.stdout = cli_args.ansible_output\n    ansible_r", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    if cli_args.check:\n        return _run_check(cli_args, vars_dict, ir_workspace, ir_plugin)\n    return _run_playbook_async(cli_args, vars_dict, ir_workspace, ir_plugin)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    vars_dict.update(ir_plugin.get_extra_vars())\n\n    # Inject the vars into the ansible vars dict\n    for k, v in vars_dict.items():\n        vars_dict[k] = ir_workspace.get_variable(k)\n\n    # Pass the vars dict to Ansible to get the ansible result\n    return cli_args, vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # Initialize the ir plugin\n    ir_plugin_instance = ir_plugin(cli_args, ir_workspace, ir_plugin.get_client_plugin())\n    # Initialize the Ansible variables\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(ir_plugin_instance.get_vars())\n    # Initialize the Ansible playbook\n    ansible = Ansible(cli_args, ir_workspace, ir_plugin.get_client_plugin(), ir_plugin_instance.get_token(), ir_workspace.get_token(), ir_plugin_instance.get_vars(), ir_vars=ansible_vars)\n    # Run the playbook\n    return ansible.run()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, InfraredWorkspace, InfraredPlugin) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    ansible_args = [\n        '-vvv',\n        '-e',\n        'ANSIBLE_SETUP_PY=%s' % ir_plugin.get_ansible_setup_py(),\n    ]\n\n    ansible_args.extend(cli_args)\n\n    ansible_vars = vars_dict.copy()\n    ansible_vars.update(vars_dict)\n    ansible_args.extend(['--vars', json.dumps(ansible_vars)])\n\n    return cli_ir_workspace.run_playbook('infrared_cli.yml', ansible_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_cli = AnsibleCLI(cli_args, ir_workspace, ir_plugin)\n    return ansible_cli.run_playbook(vars_dict)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # type: (dict, dict, Workspace) -> dict\n    \"\"\"\n    Runs an ansible cli with vars dict\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An InfraredWorkspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    ansible_vars = vars_dict.copy()\n    ansible_vars[\"ir_workspace\"] = ir_workspace\n    ansible_vars[\"ir_plugin\"] = ir_plugin\n\n    ansible_result = cli_args[0](ansible_vars, ir_workspace, ir_plugin)\n    return cli_args[1:] if cli_args else ansible_result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    ansible_args = []\n    ansible_args.extend(cli_args)\n    ansible_args.extend(['-vvv', '--check-only'])\n    ansible_args.extend(vars_dict)\n\n    return run_ansible_cli(ansible_args, ir_workspace, ir_plugin)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    if cli_args.fail_ok or cli_args.exit_ok:\n        return {'results': False,'message': ''}\n    else:\n        return ansible.run_playbook(cli_args.playbook, vars_dict, ir_workspace, ir_plugin)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    # TODO: get the ansible vars from the cli args\n    # TODO: initialize variables for the ir_workspace\n    # TODO: run the playbook with the variables dict\n    ir_plugin.logger.info('Running playbook with vars %s', vars_dict)\n    ir_workspace.start_session()\n    try:\n        results = ir_workspace.run_playbook(cli_args)\n    except Exception as e:\n        ir_workspace.logger.error('It was not possible to run the playbook, Error: %s', e)\n        raise AnsibleExit(exit_code=1)\n    ir_workspace.stop_session()\n    return results\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_convert_non_cli_args", "docstring": "Casts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments", "ques_id": "63060ada73426c38ae68ad31", "code": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    for key in values_dict:\n        if key in self._non_cli_argument_types:\n            values_dict[key] = type(values_dict[key])(values_dict[key])\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\n    for key, value in values_dict.items():\n        if key in self.non_cli_arguments:\n            values_dict[self.non_cli_arguments[key]] = self._string_value(\n                value)\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in iteritems(values_dict):\n        if isinstance(value, bool):\n            values_dict[key] = str(value).lower()\n        elif isinstance(value, str):\n            values_dict[key] = self._convert_string(parser_name, value)\n        elif isinstance(value, int):\n            values_dict[key] = self._convert_int(parser_name, value)\n        elif isinstance(value, float):\n            values_dict[key] = self._convert_float(parser_name, value)\n        elif isinstance(value, list):\n            values_dict[key] = self._convert_list(parser_name, value)\n        elif isinstance(value, dict):\n            values_dict[key] = self._convert_dict(parser_name, value)\n    return values_dict\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in iteritems(values_dict):\n        if key in self._cli_arg_types:\n            values_dict[key] = self._cli_arg_types[key](value)\n        else:\n            values_dict[key] = str(value)\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if key in self._cli_arg_types:\n            values_dict[key] = self._cli_arg_types[key](value)\n        elif key in self._string_types:\n            values_dict[key] = str(value)\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if isinstance(value, str):\n            values_dict[key] = self._string_to_cli_arg(value)\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if key in self._CLI_ARGS_TYPES:\n            values_dict[key] = self._convert_args(\n                parser_name, value, key)\n    return values_dict\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in iteritems(values_dict):\n        if key == 'values_dict':\n            values_dict[key] = self._convert_to_cli_string(value)\n        else:\n            values_dict[key] = self._convert_to_cli_value(value)\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if type(value) is not str:\n            values_dict[key] = str(value)\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key in values_dict.keys():\n        if key in self.ARGS_TYPES:\n            values_dict[key] = self._convert_args(\n                parser_name, values_dict[key])\n        else:\n            values_dict[key] = str(values_dict[key])\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key in values_dict:\n        if key in self._non_cli_argument_types:\n            values_dict[key] = type(values_dict[key])(values_dict[key])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\n    for key, value in values_dict.items():\n        if key in self.non_cli_arguments:\n            values_dict[self.non_cli_arguments[key]] = self._string_value(\n                value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in iteritems(values_dict):\n        if isinstance(value, bool):\n            values_dict[key] = str(value).lower()\n        elif isinstance(value, str):\n            values_dict[key] = self._convert_string(parser_name, value)\n        elif isinstance(value, int):\n            values_dict[key] = self._convert_int(parser_name, value)\n        elif isinstance(value, float):\n            values_dict[key] = self._convert_float(parser_name, value)\n        elif isinstance(value, list):\n            values_dict[key] = self._convert_list(parser_name, value)\n        elif isinstance(value, dict):\n            values_dict[key] = self._convert_dict(parser_name, value)\n    return values_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in iteritems(values_dict):\n        if key in self._cli_arg_types:\n            values_dict[key] = self._cli_arg_types[key](value)\n        else:\n            values_dict[key] = str(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if key in self._cli_arg_types:\n            values_dict[key] = self._cli_arg_types[key](value)\n        elif key in self._string_types:\n            values_dict[key] = str(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if isinstance(value, str):\n            values_dict[key] = self._string_to_cli_arg(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if key in self._CLI_ARGS_TYPES:\n            values_dict[key] = self._convert_args(\n                parser_name, value, key)\n    return values_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in iteritems(values_dict):\n        if key == 'values_dict':\n            values_dict[key] = self._convert_to_cli_string(value)\n        else:\n            values_dict[key] = self._convert_to_cli_value(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key, value in values_dict.items():\n        if type(value) is not str:\n            values_dict[key] = str(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n    for key in values_dict.keys():\n        if key in self.ARGS_TYPES:\n            values_dict[key] = self._convert_args(\n                parser_name, values_dict[key])\n        else:\n            values_dict[key] = str(values_dict[key])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_plugins.py", "project": "redhat-openstack/infrared", "name": "get_plugin_spec_flatten_dict", "docstring": "Creates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties", "ques_id": "63060b1a73426c38ae68ad3e", "code": ["def get_plugin_spec_flatten_dict(plugin_dir):\n    spec_dict = {}\n    for file_path in glob.glob(os.path.join(plugin_dir, \"*.py\")):\n        spec_dict.update(_flatten_dict(file_path, plugin_dir))\n\n    return spec_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        \"plugin_dir\": plugin_dir,\n        \"api_version\": None,\n        \"api_version_path\": None,\n        \"init_api_version_path\": None,\n        \"api_version_methods\": None,\n        \"api_version_parameters\": None,\n        \"api_version_response_type\": None,\n        \"api_version_response_type_plugin\": None,\n        \"api_version_methods_plugin\": None,\n        \"api_version_path_plugin\": None,\n        \"api_version_methods_plugin\": None,\n        \"api_version_response_type_plugin\": None,\n        \"api_version_response_type_plugin\": None,\n        \"api_version_response_type_plugin_options\": None,\n        \"api_version_response_type_plugin_args\": None,\n        \"api_version_response_type_plugin_kwargs\": None,\n        \"api_version_response_type_plugin_kwargs_plugin\": None,\n        \"api_version_response_type_plugin_requires\":", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        \"plugin_spec\": plugin_dir,\n        \"plugin_name\": os.path.basename(plugin_dir),\n        \"plugin_description\": \"\",\n        \"plugin_version\": \"\",\n        \"plugin_author\": \"\",\n        \"plugin_author_email\": \"\",\n        \"plugin_details\": {},\n        \"plugin_dependencies\": [],\n        \"plugin_tests\": []\n    }\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    spec_dict = {}\n    for plugin_file in os.listdir(plugin_dir):\n        if not plugin_file.endswith(\".py\"):\n            continue\n        spec_dict.update(get_plugin_spec_flatten_dict_from_file(os.path.join(plugin_dir, plugin_file)))\n    return spec_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        \"plugin_name\": os.path.basename(plugin_dir),\n        \"plugin_dir\": plugin_dir,\n        \"plugin_info\": get_plugin_info(plugin_dir),\n        \"plugin_status\": get_plugin_status(plugin_dir),\n        \"plugin_version\": get_plugin_version(plugin_dir),\n        \"plugin_author\": get_plugin_author(plugin_dir),\n        \"plugin_author_email\": get_plugin_author_email(plugin_dir),\n        \"plugin_description\": get_plugin_description(plugin_dir),\n        \"plugin_dependencies\": get_plugin_dependencies(plugin_dir),\n        \"plugin_requirements\": get_plugin_requirements(plugin_dir),\n        \"plugin_install_command\": get_plugin_install_command(plugin_dir),\n        \"plugin_sdk_version\": get_sdk_version(plugin_dir),\n    }\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    plugin_spec_dict = {}\n\n    for plugin_path in find_plugin_files(plugin_dir):\n        plugin_spec_dict.update(get_plugin_spec_from_file(plugin_path))\n\n    return plugin_spec_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        'name': plugin_dir.name,\n        'description': plugin_dir.description,\n       'versions': get_plugin_spec_flatten_dict_list(plugin_dir.versions, []),\n       'versions_dict': {},\n    }\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        'name': os.path.basename(plugin_dir),\n        'description': 'A plugin for the Galaxy API v{}'.format(__version__),\n       'versions': {\n            'python': '3.3.3',\n            'test': '2.3.4',\n           'sandbox': '2.3.4',\n            'container': '2.3.4',\n        },\n        'groups': {\n            'plugin': {\n                'type': 'plugins',\n                'path': plugin_dir,\n            },\n        },\n        'dependencies': {},\n    }\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    result = {}\n    for p_file in os.listdir(plugin_dir):\n        if p_file.endswith(\".py\"):\n            path = os.path.join(plugin_dir, p_file)\n            result.update(get_plugin_spec_flatten_dict(path))\n    return result\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {'name': plugin_dir, 'description': '','version': '', 'dependencies': {}, 'inputs': []}\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    spec_dict = {}\n    for file_path in glob.glob(os.path.join(plugin_dir, \"*.py\")):\n        spec_dict.update(_flatten_dict(file_path, plugin_dir))\n\n    return spec_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        \"plugin_dir\": plugin_dir,\n        \"api_version\": None,\n        \"api_version_path\": None,\n        \"init_api_version_path\": None,\n        \"api_version_methods\": None,\n        \"api_version_parameters\": None,\n        \"api_version_response_type\": None,\n        \"api_version_response_type_plugin\": None,\n        \"api_version_methods_plugin\": None,\n        \"api_version_path_plugin\": None,\n        \"api_version_methods_plugin\": None,\n        \"api_version_response_type_plugin\": None,\n        \"api_version_response_type_plugin\": None,\n        \"api_version_response_type_plugin_options\": None,\n        \"api_version_response_type_plugin_args\": None,\n        \"api_version_response_type_plugin_kwargs\": None,\n        \"api_version_response_type_plugin_kwargs_plugin\": None,\n        \"api_version_response_type_plugin_requires\":", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        \"plugin_spec\": plugin_dir,\n        \"plugin_name\": os.path.basename(plugin_dir),\n        \"plugin_description\": \"\",\n        \"plugin_version\": \"\",\n        \"plugin_author\": \"\",\n        \"plugin_author_email\": \"\",\n        \"plugin_details\": {},\n        \"plugin_dependencies\": [],\n        \"plugin_tests\": []\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    spec_dict = {}\n    for plugin_file in os.listdir(plugin_dir):\n        if not plugin_file.endswith(\".py\"):\n            continue\n        spec_dict.update(get_plugin_spec_flatten_dict_from_file(os.path.join(plugin_dir, plugin_file)))\n    return spec_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        \"plugin_name\": os.path.basename(plugin_dir),\n        \"plugin_dir\": plugin_dir,\n        \"plugin_info\": get_plugin_info(plugin_dir),\n        \"plugin_status\": get_plugin_status(plugin_dir),\n        \"plugin_version\": get_plugin_version(plugin_dir),\n        \"plugin_author\": get_plugin_author(plugin_dir),\n        \"plugin_author_email\": get_plugin_author_email(plugin_dir),\n        \"plugin_description\": get_plugin_description(plugin_dir),\n        \"plugin_dependencies\": get_plugin_dependencies(plugin_dir),\n        \"plugin_requirements\": get_plugin_requirements(plugin_dir),\n        \"plugin_install_command\": get_plugin_install_command(plugin_dir),\n        \"plugin_sdk_version\": get_sdk_version(plugin_dir),\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    plugin_spec_dict = {}\n\n    for plugin_path in find_plugin_files(plugin_dir):\n        plugin_spec_dict.update(get_plugin_spec_from_file(plugin_path))\n\n    return plugin_spec_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        'name': plugin_dir.name,\n        'description': plugin_dir.description,\n       'versions': get_plugin_spec_flatten_dict_list(plugin_dir.versions, []),\n       'versions_dict': {},\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {\n        'name': os.path.basename(plugin_dir),\n        'description': 'A plugin for the Galaxy API v{}'.format(__version__),\n       'versions': {\n            'python': '3.3.3',\n            'test': '2.3.4',\n           'sandbox': '2.3.4',\n            'container': '2.3.4',\n        },\n        'groups': {\n            'plugin': {\n                'type': 'plugins',\n                'path': plugin_dir,\n            },\n        },\n        'dependencies': {},\n    }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    result = {}\n    for p_file in os.listdir(plugin_dir):\n        if p_file.endswith(\".py\"):\n            path = os.path.join(plugin_dir, p_file)\n            result.update(get_plugin_spec_flatten_dict(path))\n    return result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n    return {'name': plugin_dir, 'description': '','version': '', 'dependencies': {}, 'inputs': []}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/ansible_config.py", "project": "redhat-openstack/infrared", "name": "inject_config", "docstring": "Set the environment variable for config path, if it is undefined.", "ques_id": "63060b1b73426c38ae68ad42", "code": ["def inject_config(self):\n    if self.path is not None:\n        return super().inject_config()\n\n    if self.name in os.environ:\n        return os.environ[self.name]\n\n    return None\n", "def inject_config(self):\n    env = os.environ.get\n    config_path = env('CONFIG_PATH')\n\n    if not config_path:\n        return\n\n    if not os.path.exists(config_path):\n        os.makedirs(config_path)\n\n    self.add('CONFIG_PATH', config_path)\n", "def inject_config(self):\n    if self.path is not None:\n        os.environ['PYTEST_CONFIG_PATH'] = self.path\n", "def inject_config(self):\n    env_path = os.environ.get('CONFIG_PATH')\n\n    if not env_path:\n        return\n\n    self.add(env_path, scope='config')\n", "def inject_config(self):\n    if not self.config_path:\n        return\n\n    os.environ['PATH'] = self.config_path\n", "def inject_config(self):\n    if not self.cfg_path:\n        return\n\n    env_vars = {\n        'HOME': os.path.expanduser('~'),\n        'CONFIG_PATH': os.path.expanduser(self.cfg_path),\n    }\n\n    os.environ.update(env_vars)\n", "def inject_config(self):\n    env_name = 'CONFIG_' + self.name.upper()\n\n    if env_name in os.environ:\n        path = os.pathsep.join([self.path, os.environ[env_name]])\n        os.environ[env_name] = path\n", "def inject_config(self):\n    if not self.env_path:\n        return\n\n    try:\n        os.environ[self.env_path] = self.load_config()\n    except Exception as e:\n        log.warn(\"Error while setting %s: %s\", self.env_path, e)\n", "def inject_config(self):\n    env = os.environ.get\n    config_path = env('CONFIG_PATH')\n\n    if not config_path:\n        return\n\n    if config_path in self.environ:\n        return\n\n    self.environ[config_path] = env(config_path)\n", "def inject_config(self):\n    path = os.environ.get('PATH')\n    if path is not None:\n        return path\n\n    return self.inject_env('config')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def inject_config(self):\n    if self.path is not None:\n        return super().inject_config()\n\n    if self.name in os.environ:\n        return os.environ[self.name]\n\n    return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    env = os.environ.get\n    config_path = env('CONFIG_PATH')\n\n    if not config_path:\n        return\n\n    if not os.path.exists(config_path):\n        os.makedirs(config_path)\n\n    self.add('CONFIG_PATH', config_path)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n    if self.path is not None:\n        os.environ['PYTEST_CONFIG_PATH'] = self.path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    env_path = os.environ.get('CONFIG_PATH')\n\n    if not env_path:\n        return\n\n    self.add(env_path, scope='config')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n    if not self.config_path:\n        return\n\n    os.environ['PATH'] = self.config_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    if not self.cfg_path:\n        return\n\n    env_vars = {\n        'HOME': os.path.expanduser('~'),\n        'CONFIG_PATH': os.path.expanduser(self.cfg_path),\n    }\n\n    os.environ.update(env_vars)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    env_name = 'CONFIG_' + self.name.upper()\n\n    if env_name in os.environ:\n        path = os.pathsep.join([self.path, os.environ[env_name]])\n        os.environ[env_name] = path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    if not self.env_path:\n        return\n\n    try:\n        os.environ[self.env_path] = self.load_config()\n    except Exception as e:\n        log.warn(\"Error while setting %s: %s\", self.env_path, e)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n    env = os.environ.get\n    config_path = env('CONFIG_PATH')\n\n    if not config_path:\n        return\n\n    if config_path in self.environ:\n        return\n\n    self.environ[config_path] = env(config_path)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n    path = os.environ.get('PATH')\n    if path is not None:\n        return path\n\n    return self.inject_env('config')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/main.py", "project": "redhat-openstack/infrared", "name": "extend_cli", "docstring": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.", "ques_id": "63060b1b73426c38ae68ad43", "code": ["def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('spec', help='display spec information')\n    parser.add_argument('--version', action='version', version='%(prog)s'+ __version__)\n    parser.add_argument('--all', action='store_true', help='display all specs')\n    parser.add_argument('--json', action='store_true', help='output specs as JSON')\n    parser.add_argument('--output', metavar='FILE', help='output file name')\n    parser.add_argument('--debug', action='store_true', help='enable debug mode')\n    parser.add_argument('--interactive', action='store_true', help='interactive mode')\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(\n        \"install\", help=\"Install a package or group of packages to \"\n        \"the current virtualenv.\",\n    )\n    parser.add_argument(\n        \"--requirements\",\n        nargs=\"+\",\n        help=\"The name of the requirements file to be installed.\",\n    )\n    parser.add_argument(\n        \"--upgrade\",\n        action=\"store_true\",\n        help=\"Install the package or group of packages to the \"\n        \"upgraded environment.\",\n    )\n    parser.add_argument(\n        \"--no-deps\",\n        action=\"store_true\",\n        help=\"Don't install any dependencies for the current \"\n        \"virtualenv.\",\n    )\n    parser.add_argument(\n        \"--skip-dependencies\",\n        action=\"store_true\",\n        help=\"Skip the dependencies check for the current \"\n        \"virtualenv.\",\n    )\n    parser.set_defaults(func=self.install", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('install', help='Install a new package.')\n    parser.add_argument('--upgrade', action='store_true', help='Upgrade the package to the latest available version.')\n    parser.add_argument('--reinstall', action='store_true', help='Reinstall the package to the latest available version.')\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('add', help='add an episode to the database')\n    parser.add_argument('name', help='the name of the episode to add')\n    parser.add_argument('--description', default='', help='the description of the episode')\n    parser.add_argument('--release_date', default='', help='the release date of the episode')\n    parser.add_argument('--number', default='', help='the number of the episode to add')\n    parser.add_argument('--season', default='', help='the season of the episode to add')\n    parser.add_argument('--episode', default='', help='the episode number of the episode to add')\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('test', help='test a single test case')\n    parser.add_argument('--dry-run', action='store_true',\n                        help='run the test case without actually making any changes')\n    parser.add_argument('test_name', nargs='?', help='test name', default=None)\n    parser.add_argument('--failure-tolerance', type=float, default=None,\n                        help='tolerance for failure test')\n    parser.add_argument('--success-tolerance', type=float, default=None,\n                        help='tolerance for success test')\n    parser.add_argument('--test-id', type=str, default=None,\n                        help='test id for the test case')\n    parser.add_argument('--failure-threshold', type=float, default=None,\n                        help='threshold for failure test')\n    parse", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('generate', help='Generate a random password.')\n    parser.add_argument('--length', type=int, default=12, help='The length of the password')\n    parser.add_argument('--iterations', type=int, default=8, help='Number of iterations to generate the password')\n    parser.add_argument('--salt', type=str, default='password', help='The salt to use for the password')\n    parser.add_argument('--output', type=str, default='password', help='The name of the output file')\n", "def extend_cli(self, root_subparsers):\n    self._register_cli_opts(root_subparsers)\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(\n        self.cli_argument.name,\n        help=self.cli_argument.description,\n        parents=[root_subparsers])\n\n    parser.set_defaults(func=self.handle)\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('spec', help='show spec information')\n    parser.add_argument('--name', action='store', dest='name', help='the name of the spec')\n    parser.add_argument('--version', action='store', dest='version', help='the version of the spec')\n    parser.add_argument('--env', action='store', dest='env', help='environment variables to set for the spec')\n    parser.add_argument('--config', action='store', dest='config', help='configuration file for the spec')\n    parser.add_argument('--output', action='store', dest='output', help='path to write output files to')\n", "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('install', help='install a new package')\n    parser.add_argument('package_name', help='the package to be installed', nargs='?')\n    parser.add_argument('--upgrade', action='store_true', help='upgrade the package to an upgraded version')\n    parser.add_argument('--check-only', action='store_true', help='check only the package for updates')\n    parser.add_argument('--remove-old', action='store_true', help='remove the package from the index and reinstall it')\n    parser.add_argument('--require-all', action='store_true', help='require all packages')\n    parser.add_argument('--save-dir', default='.', help='the directory where the package is saved')\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('spec', help='display spec information')\n    parser.add_argument('--version', action='version', version='%(prog)s'+ __version__)\n    parser.add_argument('--all', action='store_true', help='display all specs')\n    parser.add_argument('--json', action='store_true', help='output specs as JSON')\n    parser.add_argument('--output', metavar='FILE', help='output file name')\n    parser.add_argument('--debug', action='store_true', help='enable debug mode')\n    parser.add_argument('--interactive', action='store_true', help='interactive mode')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(\n        \"install\", help=\"Install a package or group of packages to \"\n        \"the current virtualenv.\",\n    )\n    parser.add_argument(\n        \"--requirements\",\n        nargs=\"+\",\n        help=\"The name of the requirements file to be installed.\",\n    )\n    parser.add_argument(\n        \"--upgrade\",\n        action=\"store_true\",\n        help=\"Install the package or group of packages to the \"\n        \"upgraded environment.\",\n    )\n    parser.add_argument(\n        \"--no-deps\",\n        action=\"store_true\",\n        help=\"Don't install any dependencies for the current \"\n        \"virtualenv.\",\n    )\n    parser.add_argument(\n        \"--skip-dependencies\",\n        action=\"store_true\",\n        help=\"Skip the dependencies check for the current \"\n        \"virtualenv.\",\n    )\n    parser.set_defaults(func=self.install", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('install', help='Install a new package.')\n    parser.add_argument('--upgrade', action='store_true', help='Upgrade the package to the latest available version.')\n    parser.add_argument('--reinstall', action='store_true', help='Reinstall the package to the latest available version.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('add', help='add an episode to the database')\n    parser.add_argument('name', help='the name of the episode to add')\n    parser.add_argument('--description', default='', help='the description of the episode')\n    parser.add_argument('--release_date', default='', help='the release date of the episode')\n    parser.add_argument('--number', default='', help='the number of the episode to add')\n    parser.add_argument('--season', default='', help='the season of the episode to add')\n    parser.add_argument('--episode', default='', help='the episode number of the episode to add')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('test', help='test a single test case')\n    parser.add_argument('--dry-run', action='store_true',\n                        help='run the test case without actually making any changes')\n    parser.add_argument('test_name', nargs='?', help='test name', default=None)\n    parser.add_argument('--failure-tolerance', type=float, default=None,\n                        help='tolerance for failure test')\n    parser.add_argument('--success-tolerance', type=float, default=None,\n                        help='tolerance for success test')\n    parser.add_argument('--test-id', type=str, default=None,\n                        help='test id for the test case')\n    parser.add_argument('--failure-threshold', type=float, default=None,\n                        help='threshold for failure test')\n    parse", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('generate', help='Generate a random password.')\n    parser.add_argument('--length', type=int, default=12, help='The length of the password')\n    parser.add_argument('--iterations', type=int, default=8, help='Number of iterations to generate the password')\n    parser.add_argument('--salt', type=str, default='password', help='The salt to use for the password')\n    parser.add_argument('--output', type=str, default='password', help='The name of the output file')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    self._register_cli_opts(root_subparsers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser(\n        self.cli_argument.name,\n        help=self.cli_argument.description,\n        parents=[root_subparsers])\n\n    parser.set_defaults(func=self.handle)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('spec', help='show spec information')\n    parser.add_argument('--name', action='store', dest='name', help='the name of the spec')\n    parser.add_argument('--version', action='store', dest='version', help='the version of the spec')\n    parser.add_argument('--env', action='store', dest='env', help='environment variables to set for the spec')\n    parser.add_argument('--config', action='store', dest='config', help='configuration file for the spec')\n    parser.add_argument('--output', action='store', dest='output', help='path to write output files to')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n    parser = root_subparsers.add_parser('install', help='install a new package')\n    parser.add_argument('package_name', help='the package to be installed', nargs='?')\n    parser.add_argument('--upgrade', action='store_true', help='upgrade the package to an upgraded version')\n    parser.add_argument('--check-only', action='store_true', help='check only the package for updates')\n    parser.add_argument('--remove-old', action='store_true', help='remove the package from the index and reinstall it')\n    parser.add_argument('--require-all', action='store_true', help='require all packages')\n    parser.add_argument('--save-dir', default='.', help='the directory where the package is saved')\n", "is_pass": false, "return_code": 1}]}
